[
{
"vote": 1,
"title": "An Emirati girl publishes books to help kids with autism",
"text": null,
"date": "2023-10-23"
},
{
"vote": 8,
"title": "Do you have a specific hypothesis you would like to test, but don't have the time?",
"text": "[deleted]",
"date": "2023-07-05"
},
{
"vote": 1,
"title": "Empowering Remote Work with Azure Virtual Desktop",
"text": null,
"date": "2023-06-11"
},
{
"vote": 1,
"title": "Why Not Ternary Computers? - Laptotech",
"text": null,
"date": "2023-06-11"
},
{
"vote": 3,
"title": "The \"Memory-efficient NLLB-200: Language-specific Expert Pruning\" paper: Is gate statistics available anywhere?",
"text": "The paper\n promises to release it:\n> We will release the ids of the pruned experts, along with other expertsâ€™ gathered statistics so that anyone with a single 32GB GPU can use the NLLB-200 model at inference.\n\n\n...\n\n\n> All gate statistics will be openly released to foster future research. \n\n\nTo the best of my knowledge, no such data (or code) is available anywhere. Should I wait for the paper to be accepted for publication? What's the best way to contact authors?\n\n\nI really want to take a more detailed look at how the experts handle various Slavic languages.",
"date": "2023-06-10"
},
{
"vote": 2,
"title": "Seeking Suggestions: Enhancing Chat Interaction with PDFs and Large Videos",
"text": "There are numerous plugins available for ChatGPT that allow for source changes. However, I've noticed that many of these have a limited context window and/or lack the ability to \nfully interact\n with the actual data source. They often operate solely as text extractors. In particular, I'm looking for a solution that possesses advanced \nreasoning skills\n and, most importantly, can answer \"When?\" questions. This capability is crucial as it allows for \nverifiable information\n, which is a key requirement for my needs.\n\n\nDo you know of any \nfree\n services that can overcome these limitations, specifically for videos and PDF documents?\n\n\nHere are a few examples of what I've found so far:\n\n\n\n\nChatWithVideo\n is a \nfree\n bot on Telegram that can answer questions like \"When in the video?\" with a good level of reasoning. It can also process up to 40,000 tokens, which is a plus. However, it's slower than I'd like, and I need the same functionality for PDFs.\n\n\n\n\nAskYourPdf\n and \nChatPDF\n are both able to quickly answer \"where?\" in a PDF pretty fast and offer a \nfree\n tier. On the downside, they can't handle PDFs with 40,000 tokens and their reasoning capability seems a bit lacking. This is a tradeoff between speed and reasoning compared to the previous bot.\n\n\n\n\nThe ChatWithPDF and ChatWithVideo plugins for GPT-4 are \nfree\n (if you have PLUS access), but their interaction with the data source is limited. They essentially function as text extractors.\n\n\n\n\nChatYouTube\n is a \nfree\n web application that operates fairly quickly. However, it truncates the video and can't answer \"Where?\" questions.\n\n\n\n\n\n\n&#x200B;\n\n\nHas anyone else encountered this issue or found any alternative solutions? I'd love to hear about your experiences or any suggestions you might have.",
"date": "2023-06-10"
},
{
"vote": 44,
"title": "/r/LanguageTechnology will be joining the protests and going dark on the 12th",
"text": "On June 12th /r/LanguageTechnology will join the thousands of other subreddits protesting the recent policy changes that are forcing the closure of many popular 3rd part mobile apps, like /r/apolloapp, /r/redditisfun/, /r/redditsync/, and many others.\n\n\nUsers who are not already aware of these changes can visit /r/Save3rdPartyApps for more information about what's changing, the impact these changes are going to have, and the less than ideal way Reddit Inc has gone about handling all this.\n\n\nUsers of 3rd party apps should double check the subreddits focused on the apps they use to find out if they will be able to keep using those apps after the policy changes go into affect at the end of the month.",
"date": "2023-06-10"
},
{
"vote": 49,
"title": "Introducing SlimPajama-627B: the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.",
"text": "SlimPajama cleans and deduplicates RedPajama-1T, reducing the total token count and file size by 50%. It's half the size and trains twice as fast!\n\n\nItâ€™s the highest quality dataset when training to 600B tokens and, when upsampled, performs equal or better than RedPajama.  It was no mean feat to deduplicate data on this scale â€“ existing tools do not scale to a trillion tokens. We built a custom parallel data pre-processing pipeline and are sharing the code open source with the community.\n\n\nWeâ€™d like to thank our partner Opentensor for supporting this project. And credit goes to Together Compute and the entire team that created the RedPajama dataset!\n\n\n&#x200B;\n\n\n\n\nSlimPajama dataset - \nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B\n\n\nLibraries for data pre-processing - \nhttps://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama\n\n\nRead our blog - \nhttps://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama",
"date": "2023-06-09"
},
{
"vote": 1,
"title": "Preprocessing Tweet",
"text": "[deleted]",
"date": "2023-06-09"
},
{
"vote": 0,
"title": "Philosophy blender: Using ChatGPT to create novel and authentic philosophy.",
"text": "Check out my newest program:  Philosophy Blender.  My program takes in several philosophy books as input and uses NLP plus ChatGPT to create novel philosophical insights using the books as a template.  Hence, ChatGPT is now a professional philosopher, complete with tendency to sip wine on the Left Bank and beret.\n\n\nThe program is here on my Github:\n\n\nhttps://github.com/danielmachinelearning/Philosophy_blender\n\n\nAnd you can check out my Medium article on the topic:\nhttps://medium.com/@danielmachinelearning/blending-philosophy-books-with-t5-transformers-top2vec-and-chatgpt-to-gain-novel-philosophical-2f4b0f09c90b",
"date": "2023-06-09"
},
{
"vote": 2,
"title": "Looking to get into NLP. Suggestions?",
"text": "Hi,\n\n\nThis is more of a career question, but nevertheless. I apologize if this post doesn't fit here.\n\n\nI'm from Europe and recently moved to the US. I'm remotely finishing my MA in Translation in my home country and during my studies, I've discovered that I really like the computer-based aspect of translation, such as machine translation and machine learning in general. My education is linguistic-based and as part of my graduate programs, there were only a few computer science-based courses on the curriculum. I have a vast knowledge of corpus linguistics and statistical and neural machine translation, although more from a theoretical stand point. I am also familiar with Python and the concepts of machine learning. Most of this knowledge was acquired through my own interest in the subjects or as part of additional or optional coursework. I understand that NLP is a field that requires a deep understanding of mathematical concepts, which I unfortunately lack because of the structure of the courses I have completed. I am, however, more than prepared to broaden my knowledge on these subjects and have already started learning and revising on my own. Right now, I also have a lot of time I can spend on this (6 months+). I've come across a few graduate courses, such as eCornell's Natural Language Processing With Python Program, but I am not familiar with these kinds of certifications as there are none in my home country.\n\n\nSo I guess my questions are:\nDo you think this transition is possible for me?\nAre these kinds of graduate courses worth it?\nWould I have to start my education all over, meaning doing a BSc in Data Science/Data Analytics/CompSci?\nDo you know of any resources to help my transition?\n\n\nI know this is a stretch, but it doesn't hurt to ask I guess. Thanks for your help.",
"date": "2023-06-08"
},
{
"vote": 1,
"title": "I created a flashcards app for learning new vocabulary (Anki but pretty and Web based)",
"text": "[deleted]",
"date": "2023-06-08"
},
{
"vote": 6,
"title": "Multimodal vector search",
"text": "Hi All,\n\n\nHere\n  is some recent work on multimodal vector search. There are lots of  interesting features that come from CLIP based models when used for  retrieval and paired with things like query expansion and relevance  feedback. This allows for multi-term queries, using negative terms,  multi modal queries, andÂ modifying results with context. I have been  also pretty interested in modifying the CLIP training (like here \nhttps://arxiv.org/abs/2303.15343\n), using task vectors (\nhttps://arxiv.org/abs/2212.04089\n, \nhttps://arxiv.org/abs/2109.01903\n), prefix tuning (\nhttps://arxiv.org/abs/2101.00190\n) and activation vectors (\nhttps://www.alignmentforum.org/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector\n) to steer search results in particular (learned) directions. I would love to hear if anyone else has been using these methods.\n\n\nArticle: \nhttps://github.com/jn2clark/articles/blob/main/MultimodalSearch/article.md\nCode: \nhttps://github.com/jn2clark/articles/blob/main/MultimodalSearch/index_and_search.py",
"date": "2023-06-08"
},
{
"vote": 17,
"title": "Open-source data collection for LLM fine-tuning and RLHF: Introducing Argilla Feedback",
"text": null,
"date": "2023-06-05"
},
{
"vote": 2,
"title": "Open-source LLM data curation and fine-tuning for everyone",
"text": "[removed]",
"date": "2023-06-05"
},
{
"vote": 3,
"title": "Searching multiple documents via Extractive or Abstractive Q&amp;A",
"text": "Hi,\nI'm interested in applying a more sophisticated search to my local Zotero library of academic papers (somewhere between 1-2k pdfs), than the built-in keyword based search. I was trying to find what the state of the art Extractive or Abstractive QA tool is that can be run locally (average laptop, no GPU) and how they compare in performance.\n\n\nIs there any benchmark for this? Is anyone using this locally to search through work-related documents?",
"date": "2023-06-05"
},
{
"vote": 0,
"title": "Paraphrasing tool that changes sentiment",
"text": "I need a tool that will paraphrase text (one-line heading) and change its sentiment, for example from negative to positive (or at least make a very negative statement less negative). ChatGPT4 can do this quite well and so does hugchat (free). The problem is I need to give a very detailed prompt and the output sometimes contains additional comments, plus these tools are very slow. I'd like to eiter find a tutorial that would help me build a model that can paraphrase and change sentiment or find a ready model that takes text and desired sentiment as input and returns parraphrased text.",
"date": "2023-06-05"
},
{
"vote": 3,
"title": "How do dependency parser models differ from POS taggers (in terms of training, techniques, etc.)?",
"text": "I don't understand how a dependency parser assigns correct tags. What would the training data look like for a primitive model? What additional techniques are necessary for it to function?\n\n\nIs the whole process similar to POS tagging? For example, for a basic POS tagger, you could build a dataset with manually tagged words, then make the model train on it. To increase the accuracy, you could apply the Markov chain rule to eliminate the impossible tag sequences or to correctly tag homographs.\n\n\nIn this case, 'apple' would always be a noun (if we always tagged it as a noun in the training set) no matter the sentence. But 'apple' can have many dependency tags in the training set.\n\n\nTL;DR / example\n\n\nSo, if I have a sentence: \"I will have a tasty apple pie today.\", what is the step by step process for the dependency parser to assign the dependency tags?",
"date": "2023-06-04"
},
{
"vote": 12,
"title": "TokenMonster Ungreedy ~ 35% faster inference and 35% increased context-length for large language models (compared to tiktoken). Benchmarks included",
"text": "From the\n \nGitHub\n:\n\n\nTokenMonster is an ungreedy tokenizer and vocabulary builder, outperforming tiktoken by 35%. In fact, TokenMonster's smallest 24000 vocabulary consistently uses less tokens than tiktoken's largest 100256 vocabulary to tokenize the same text. Save the tokens! \nSee benchmark\n.\n\n\nGiven a text dataset, a vocabulary-size and a maximum-token-length, TokenMonster selects the tokens that optimally represent your dataset at that vocabulary size. It can do this at reasonable speed (within 24 hours) on server hardware, at a cost of around $8. \nPrebuilt vocabularies\n are provided, as well as tools to train your own vocabularies & native implementations in Go, Python & Javascript for tokenization and detokenization using the prebuilt or your own vocabularies.\n\n\nYou can \ntest TokenMonster in your browser here\n, tokenizing live in native Javascript.\n\n\nTokenMonster is a novel approach to tokenization with broad-ranging use potential, but its primary motivation is to increase the inference speed and context-length of large language models. By selecting better tokens, text can be represented with 35% less tokens compared to other modern tokenizing methods, increasing the speed of inference, training and the length of text by 35%. The code-optimized tokenizers do even better, \nsee for yourself\n.\n\n\nI also believe that TokenMonster vocabularies will improve the comprehension of Large Language Models. For more details see \nThe Philosophy of Tokenization\n.\n\n\nFeatures\n\n\n\n\nOutperforms other tokenization algorithms (\nbenchmark\n)\n\n\nLonger text generation at faster speed\n\n\nSelects the optimal vocabulary\n\n\nUngreedy\n\n\nSupports UTF-8, UTF-16 and binary\n\n\nSuccessfully identifies words, subwords, common phrases and figures of speech by itself\n\n\nWorks with HTML tags, sequential spaces, tabs, etc. without wasting context\n\n\nAverages 5.5 characters per token\n\n\nNo GPU needed",
"date": "2023-06-04"
},
{
"vote": 5,
"title": "Sampling from embedding space of S-BERT",
"text": "Hello\n\n\nI'm using S-BERT to get embeddings from text:\n\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(&#039;sentence-transformers/all-MiniLM-L6-v2&#039;)\nmodel.max_seq_length = 256 \nembedding = model.encode(text)\n\n\n\nIs it possible to sample new text from the embeddings?",
"date": "2023-06-04"
},
{
"vote": 7,
"title": "[Project] surv_ai: An Open Source Framework for Modeling and Sentiment Analysis using AI Agents, Inspired by Classical Ensemble Classifiers",
"text": null,
"date": "2023-06-02"
},
{
"vote": 10,
"title": "How do I evaluate a n-gram language model with NLTK?",
"text": "I have a corpus with a list of sentences and I want to build a n-gram language model with it. I use the padded_everygram_pipeline function from NLTK to build my ngrams and then fit a model. This works fine. I want to calculate the perplexity with lm.perplexity(test_data). The NLTK doc says that the function expects a list of ngrams. If I transform my test data with the padded_everygram_pipeline to ngrams, the lm.perplexity functions gives me a DivisionByZero error. What is the go to way to evaluate this model?",
"date": "2023-06-01"
},
{
"vote": 1,
"title": "Free ChatGPT course for software devs",
"text": "[removed]",
"date": "2023-06-01"
},
{
"vote": 7,
"title": "Chicago Newspapers Join Fight Against AIâ€™s Theft of Local News",
"text": null,
"date": "2023-06-01"
},
{
"vote": 11,
"title": "Calculating document similarity in a special domain",
"text": "I am working on a nlp-project and need to know if i am on the right track.\nA little background first. I work for a software-company that provides software for a very specific domain (energy-sector)\nThe customer-support receives support tickets from end-users for a variety of different software-modules and problems.\n\n\nThe goal is to find:\n\n\nA ) Similar support-tickets from other users, that have been solved to show the support-agent a possible solution to the current problem.\n\n\nB ) From a set of published FAQs and help-articles, find the most relevant article to provide the end-user with a possible solution.\n\n\nWhat i've done so far:\n\n\n\n\nBuild a pipeline for data-preprocessing, tokenisation, etc.\n\n\nTrained word-embeddings using Word2Vec (Fasttext gave worst results, haven't tried glove)\n\n\nTrained a NER-Model (More as a prototype, since i only trained a specific domain with limited training-data)\n\n\nBuild a dataset for validating my model (Those are support-tickets that can be grouped by the same problem. The set consists of 600 support-tickets with a varying group-size of 2-20 tickets)\n\n\n\n\nFor comparing vectors and finding most similar tickets i use annoy (approximate nearest neighbor).\n\n\nI've tried different ways to generate document-vectors:\n\n\n\n\nAverage-Vector from the complete text (worst results)\n\n\nbag-of-words approach using only Nouns, Proper-Nouns and Verbs (best result)\n\n\nbag-of-words approach using only entites recognized from my NER-Model\n\n\nmixed approaches using all tokens, and weighting entities\n\n\nextracting entities and the surrounding subtree of the sentence (context of the entity).\n\n\n\n\nIn task A my model will find 26% of all duplicates, while marking 74% wrong. (depending on the parameters i use for annoy i can get an accuracy of 96%, but this will only find 5% of all duplicates)\nI guess what i need to know is, if my approach to this problem is generally correct, or if i am missing something fundamentally.\n\n\nAm i right the that the quality of the word-embeddings and the way i represent the support-tickets (bag-of-words, entities, etc) are the most important parts of this pipeline?\n\n\nI think, the biggest problem i have, is that there is too much noise in the data. An example for a typical support ticket would be:\n\n\n*Regarding the mentioned meter[Enitity] , a contract-change[Enitity] took place on February 10, 2023. Unfortunately, the associated configuration-order[Enitity] has failed. I have just initiated it again, but it has encountered an error once more:\n\n\nError Type: The called system encountered an error, but no error detail was returned or it was incomplete. The 'ErrorType' attribute is empty, although it is mandatory!\nError: Did not find import of XXX-Profile[Entity]\n\n\nAccording to the Grid-Access-Provider[Enitity], the mentioned profile[Enitity] is also not active.\nWhat should be done in this situation?*\n\n\nThe ticket could be summarized into \"The configuration order has failed with the error \"Did not find import of XXX-Profile\"\nWhat would be a good approach to \"extracting the actual problem\" And is that that right direction for my problem?",
"date": "2023-06-01"
},
{
"vote": 1,
"title": "New to r/speechtechnology",
"text": "[removed]",
"date": "2023-05-31"
},
{
"vote": 0,
"title": "Seeking NLP Internship Opportunities",
"text": "Hello, Redditors!\n\n\nI hope this post finds you all in good spirits. Today, I'm reaching out to the community because I'm actively searching for internship opportunities in the exciting field of Natural Language Processing (NLP). If you or someone you know has any leads, guidance, or suggestions, I would greatly appreciate your help!\n\n\nAbout Me:\n\n\nI am an enthusiastic and dedicated individual with a strong passion for NLP. I have been actively studying and working on NLP projects, and I am eager to gain real-world experience through an internship. I have a solid foundation in ML algorithms, deep learning, and NLP techniques. I am proficient in Python and have experience working with popular NLP libraries such as NLTK, spaCy, and Transformers. Additionally, I have developed projects involving text classification, sentiment analysis, and named entity recognition.\n\n\nWhat I'm Looking For:\n\n\nI am seeking an NLP internship opportunity where I can apply and further develop my skills and knowledge in this field. Ideally, the internship would offer:\n\n\nHands-on Experience: I am eager to work on real-world NLP projects and contribute to meaningful solutions. An internship that provides practical experience with data preprocessing, model development, and evaluation would be highly beneficial.\n\n\nMentorship and Guidance: A supportive environment with experienced professionals who can mentor and guide me through the internship would be invaluable. I am eager to learn from individuals who can provide insights into best practices, industry trends, and advanced techniques.\n\n\nCollaborative Culture: An internship that fosters collaboration and teamwork, allowing for knowledge exchange and skill-building within a diverse and dynamic team, would be ideal.\n\n\nWhat I Offer:\n\n\nStrong foundation in NLP concepts and techniques\nProficiency in Python programming language\nExperience with ML algorithms and deep learning frameworks\nProject experience in text classification, sentiment analysis, and named entity recognition\nStrong problem-solving and analytical skills\nDedication and commitment to learning and growth\nHow to Get in Touch:\n\n\nIf you are aware of any NLP internship opportunities, have advice on where to look, or can provide guidance on how to enhance my chances of securing an internship, please feel free to leave a comment or send me a private message. I am open to any leads, suggestions, or networking opportunities that may be available.\n\n\nThank you for your time and support! I'm excited to embark on an enriching NLP internship experience and contribute to the field.",
"date": "2023-05-31"
},
{
"vote": 6,
"title": "How to annotate compound words to build NER models?",
"text": "Hey,\n\n\nI'm trying to build an NER model using a Bert-like model\n\n\nMy question is about the entities I'm trying to identify, All of them are composed of many words, they're usually seperatd by either a white space, comma, a dash or even a dot sometimes.\n\n\nHere's an example :\n\n\n\n\nQLQ-BR23\n  which is composed of 2 abreviations and seperated by a dash\n\n\nFunctional Assessment of Cancer Therapy - Lung\n\n\n\n\nI'm working on annotating my texts using Doccano but I'm not quite sure how manually anotate these words using the IOB scheme :\n\n\nshould I seperate the comma, dot, dash and so on from the words and treat it as part of the entity's name or should I keep the words attached by a comma and consider them as a whole entity? I guess it depends on how my bert like model will do the tokenizeation?\n\n\n&#x200B;\n\n\nHELP!!",
"date": "2023-05-31"
},
{
"vote": 1,
"title": "AI generated content should be scanned for additional information patterns",
"text": null,
"date": "2023-05-31"
},
{
"vote": 1,
"title": "I made an AI friend",
"text": "[removed]",
"date": "2023-05-30"
},
{
"vote": 3,
"title": "What is the word for \"the objective of a statement/conversation\"",
"text": "I'm building a chat bot and I'd like it to start conversations with me. To plan this, I'm looking for words that describe what kind of statement each statement is, and what the \"point\" of a conversation is. Like why you bothered to open your mouth. \n\n\nA great outcome would be my describing \"these are the types of conversations people have\" and \"these are the types of statements people make\". The robots in the movie Ad Astra are full of quips and encouragements for instance, not just statements of altitude or commands.\n\n\nIdeally these words would come with an exhaustive list of instances of the type. For instance, a statement might be labelled \"banter\" and banter would by an instance of the type \"intention\". (See? I don't have a word for it) An example of a great exhaustive list is \nthis wheel of emotions\n. Is this wheel truly exhaustive? Might as well be.\n\n\nIdeally as well, each instance of the type would feel like the same conceptual level of analysis, so \"musical critique\" and \"banter\" are not on the same level of analysis.\n\n\nThanks for considering my question, I'm a dilettante in this field so I don't know how best to ask this. I believe this is the field of pragmatics, so I'm willing to pick up a book if it would help me.",
"date": "2023-05-29"
},
{
"vote": 3,
"title": "Resources for Document-Writing Models?",
"text": "Are models to co-pilot documents/fill out forms conceptually different than question-answering models?\n\n\nIf so, any resources (blog posts, tutorials) on training that kind of model?",
"date": "2023-05-29"
},
{
"vote": 5,
"title": "Debugging embedding clusters",
"text": "Im working with Text embeddings and clusters right now. \nIs there any tools where I can quickly Experiment mit different k-means clusters , similarity algorithms and so on ? What is the process there?",
"date": "2023-05-28"
},
{
"vote": 5,
"title": "String distance based network for fuzzy matching?",
"text": null,
"date": "2023-05-28"
},
{
"vote": 6,
"title": "POS taggers question",
"text": "I have run the following sentence \n\n\n\"My sister is \na dependent\n and lives with our parents.\"\n\n\nthrough various POS taggers, including nltk, spacy, stanford and free claws tagger. However, all of them identify the word \"dependent\" as an adjective. I am wondering if there is any POS tagger available that would recognize \"dependent\" as a noun instead.",
"date": "2023-05-28"
},
{
"vote": 2,
"title": "How Do We Evaluate Task/Domain-Specific Models?",
"text": "My team is working on a military service-specific smaller (7b) model, and an eventual concern is how to evaluate performance.  \n\n\n\n\nMost Benchmarks seem a better choice for general purpose models than for measuring how much better a model now performs on service-relevant tasks\n\n\nHaving SME's judge output might work but I know in my field (and the service) there will be concerns about human subjectivity.\n\n\n\n\nOne thing I've considered is \ncultural consensus modeling\n: if you ask members of a group to answers questions, culturally knowledgable members tend to load on the same answers, while less knowledgable ones have lower agreement.  Imagine losing the answer key to a multiple choice test: the smart kids will tend to share answers, but the dumb kids tend to guess and thus have less agreement--the answers from the group with high loading are correct.  So maybe rather than SME ratings of model answers, modeling SME answers to prompts and then measuring how well the pre-trained vs fine-tuned models align with the consensus?\n\n\nWould love to hear some suggestions or if anyone knows of emerging best practices in evaluating these kind of fine-tuned models.",
"date": "2023-05-28"
},
{
"vote": 16,
"title": "Research directions not related to include optimization of Transformers (compute, memory, etc.)?",
"text": "Honestly, I'm a bit fed up with the strong focus on squeezing the last bit of performance out of transformers. To lighten my mood I wanted to ask the community, if they've come across something interesting/different in their area.\nFor example, I've found \n\"Thinking Like Transformers\"\n by to be an enlightening fresh take.",
"date": "2023-05-27"
},
{
"vote": 0,
"title": "Want to learn about different preprocessinf techniques?",
"text": "If you want to, you can check out \npreprocessing techniques in Python\n to learn techniques you can use to improve your IR system!.",
"date": "2023-05-26"
},
{
"vote": 1,
"title": "Paraphraser using NLP",
"text": "Hello guys,\n\n\nI am working on with a paper where we need to make a paraphraser for bengali language. Can anyone shed some light on how we should proceed? \n\n\nWe are very lost at this moment and donâ€™t know how to execute this task.",
"date": "2023-05-26"
},
{
"vote": 2,
"title": "Chatbot to match FAQ knowledge base. Is there anything better I can do?",
"text": "Hi all, \n\n\nI'm currently working on a project where I need to generate answers for given queries. I have a knowledge base consisting of FAQs. My current approach involves encoding both the FAQ questions and the user query, and then using a similarity metric (specifically cosine similarity) to determine the most relevant FAQs. I would like to know if there are any additional techniques I can use to improve the matching between the given query and the FAQs.\n\n\nAlso, if any of you have experience with query matching in production bots, I would greatly appreciate hearing about any novel techniques you have utilized. Thank you in advance!",
"date": "2023-05-26"
},
{
"vote": 3,
"title": "[T] Introducing Model Lab - A new tool to make sense of training LLMs",
"text": null,
"date": "2023-05-25"
},
{
"vote": 5,
"title": "We mapped out the most popular git hosting services in India with text-to-location models",
"text": null,
"date": "2023-05-25"
},
{
"vote": 8,
"title": "[R] Parallel Attention and Feed-Forward Net Design for Pre-training and Inference on Transformers",
"text": "Hello Everyone, we just proposed a \nnew variant of transformers\n architecture that allows parallel execution of both attention sub-layers and feed-forward sub-layers in a single layer of transformers models ! We tested this parallel design on RoBERTa-large and bert-large-uncased, achieving similar performance to the traditional design on the GLUE benchmark. I believe it can be a valuable contribution to the NLP field to reduce both the training and inference times for the Transformer models. I would be glad to know what y'all think, thank you !",
"date": "2023-05-25"
},
{
"vote": 0,
"title": "JOB HIRING",
"text": null,
"date": "2023-05-24"
},
{
"vote": 1,
"title": "I made a free online text-to-speech tool as an implementation of Meta's Massively Multilingual Speech (MMS) â€“ Supports 1144 Languages and Dialects!",
"text": "[deleted]",
"date": "2023-05-24"
},
{
"vote": 9,
"title": "Confusion about Erasmus LCT Program for ML Enthusiast: Seeking Advice on Career Prospects!",
"text": "I earned my bachelor's degree in computer science back in August 2022, and I've been working at a local software company in Pakistan ever since.\n\n\nMy passion for machine learning (ML) has led me to apply to various programs, and I even made it onto the reserve list for the prestigious Erasmus Language and Communication Technologies (LCT) program. In addition, I applied to a few other places and was fortunate enough to receive acceptance from some of them.\n\n\nJust last week, I received the news that I've been officially accepted into the LCT program, with the following university allocations:\n\n\n\n\nYear 1: University of the Basque Country \n\n\nYear 2: University of Lorraine\n\n\n\n\nInitially, I was overjoyed about this opportunity, but as I delved deeper into my research, I found myself facing a wave of confusion. My primary goal in studying ML and natural language processing (NLP) is to enter the job market and embark on a professional career. I don't have any plans for pursuing a PhD or further academic studies. The confusion arises from the fact that the LCT program is more geared towards students who wish to pursue advanced studies, with many alumni choosing to go down the path of PhDs and research positions at universities. Finding individuals who have ventured into the tech market directly has been a bit of a challenge.\n\n\nAnother concern that has come to my attention is that there have been whispers about the courses offered at the University of the Basque Country being perceived as 'superficial.' Naturally, this raises doubts about whether this program will truly equip me to kickstart my career in the NLP field. I have limited knowledge about the reputation of the University of Lorraine, which adds to my uncertainty.\n\n\nIf I decide not to pursue this program, I have a couple of alternative options on the table. One is the Artificial Intelligence master's program at Manchester Metropolitan University, and the other is trying my luck in another admission cycle within my home country.\n\n\nOne notable advantage of the Erasmus LCT program is its stellar reputation, which surpasses that of Manchester Metropolitan University. Moreover, it offers a scholarship, which is a significant factor for me considering my financial circumstances.\n\n\nUltimately, my personal reasons drive me to aim for a job in either the UK Tech Market or the US tech market. So, my question is: Will the Erasmus LCT degree be recognized by employers outside the EU? Additionally, if I were to pursue a job in France's Tech Market, how receptive would employers be towards this degree?\n\n\nApologies for the lengthy post, but I wanted to provide you all with the complete picture before seeking your valuable advice. Thanks in advance!",
"date": "2023-05-22"
},
{
"vote": 5,
"title": "Tuning Large Language Models for Recommendation Tasks",
"text": null,
"date": "2023-05-21"
},
{
"vote": 0,
"title": "jobs involving work in south korea?? or foreign countries in general",
"text": "i know this is thinking way ahead, but im just curious. \n\n\nim an undergrad student planning to double major in CS and ling. i also want to minor in korean, out of my own interest\n\n\nmy goal is to go into the field of nlp and such. would i be able to combine my interests and do any work in korea, visiting korea, or regarding the korean language?\n\n\nim from the US and hoping to be fluent in korean by the time i graduate.\n\n\ni would really like to travel just in general. i want to see singapore, edinburgh, japan for example. any hopes of combining this with work, or is it better to just keep these as personal vacations?",
"date": "2023-05-21"
},
{
"vote": 11,
"title": "Interesting research topics in NLP",
"text": "I am writing my master thesis in AI next year, and I am looking for potential topics I can write about. What do you think are some interesting research topics in NLP right now?",
"date": "2023-05-20"
},
{
"vote": 7,
"title": "Seeking Resources on Open-source Large Language Models Specializing in Causality Tasks",
"text": "Happy Friday, \n/r/languageTechnology\n community, \n\n\nI'm currently on a quest to discover resources - academic papers, GitHub repositories containing relevant experiments or testing setups, etc. - that focus on open-source large language models (LLMs) adept at handling tasks related to causality.\n\n\nWhen I refer to LLMs, I'm particularly interested in the following established models, for which I'll also provide their Hugging Face Model Hub names where applicable:\n\n\n\n\nBERT (\nbert-base-cased\n, \nbert-base-uncased\n)\n\n\nRoBERTa (\nroberta-base\n)\n\n\nT5 (\nt5-base\n, \nt5-v1_1-base\n)\n\n\nDeBERTa (\ndeberta-v3-base\n)\n\n\n\n\nI'd also love to explore some of the more recent entrants to the LLM scene, which I classify as 'proper LLMs' given their considerable size:\n\n\n\n\nFLAN\n\n\nChinchilla\n\n\nPaLM and PaLM2\n\n\n\n\nMoreover, there's a collection of smaller, newly-developed LLMs that have caught my attention:\n\n\n\n\nGPT-NeoX-20B\n\n\nLLaMa\n\n\nAlpaca\n\n\nKoala\n\n\nCodeGen\n (I included this model as there's some evidence in recent months that LLMs trained on code-related tasks better capture long-range dependencies compared to LLMs trained on 'just words')\n\n\n\n\nI'm in search of benchmarks or metrics that evaluate these models' capabilities in terms of causality and logical reasoning related to causality. As I plan to develop a tool that requires a deep understanding of causality, it's essential that the model I select is open-source and can be re-purposed in a non-commercial, research-oriented context. (I'm going to take a model and fine-tune it on a dataset as part of some research work that I'm hoping to engage in, so am not making a penny off of this.)\n\n\nAny pointers, suggestions or resources are greatly appreciated!",
"date": "2023-05-19"
},
{
"vote": 1,
"title": "Hi guys, my team has developed the first generative coding engine AI and it will change the world of coding forever.",
"text": "[removed]",
"date": "2023-05-18"
},
{
"vote": 6,
"title": "Understanding Adaptive Machine Translation",
"text": null,
"date": "2023-05-18"
},
{
"vote": 7,
"title": "Question for employers.",
"text": "Hi, \n\n\nIâ€™m building a resume (CV) for NLP/AI jobs and Iâ€™m approaching this new career path from an academic background. I have multiple publications that apply various NLP methods such as sentiment analysis, topic modeling, word2vec, data processing, and latent semantic analysis (corpus linguistics). I have a background and an undergrad in philosophy and Iâ€™ve used these techniques to aid in philosophical research (along with Bible studies). \n\n\nMy question: would this suffice for employment? My reasoning is that these publications demonstrate my ability to think outside of the usual NLP projects and show that I can use these methods in novel and complex ways. My worry is that none of these are explicitly industry related. \n\n\nAny advice would be greatly appreciated. Iâ€™m already enrolled in coursea classes for machine learning mathematics and tensorflow to demonstrate ability with transformers.\n\n\nThank you!",
"date": "2023-05-18"
},
{
"vote": 1,
"title": "Language Proficiency Study",
"text": "[removed]",
"date": "2023-05-17"
},
{
"vote": 0,
"title": "Language model as a service",
"text": "What is the most advanced research",
"date": "2023-05-17"
},
{
"vote": 15,
"title": "Resources for learning new trends in NLP",
"text": "Hello I have pretty much completed the intermediate part of NLP and created a good amount of projects\nNow I wanna learn what the rest of the world is doing in this field.\nAny place where I can get the latest news on NLP or blogs of people discussing them?\nThank you",
"date": "2023-05-17"
},
{
"vote": 4,
"title": "Entity extraction techniques &amp; use cases",
"text": "Entity extraction involves identifying and categorizing key information elements within unstructured text, such as people's names, locations, organizations, dates, and more. This categorization brings incredible benefits to businesses, including enhanced information retrieval, improved customer service, competitive intelligence, streamlined processes, and personalized marketing. ðŸ“ŠðŸ’¼\n\n\nThe article below dives deep into the world of entity extraction, also known as named entity recognition (NER), and how it can revolutionize businesses across various industries. ðŸš€\n\n\nThe article also explores different entity extraction techniques like rule-based approaches, machine learning-based approaches, and hybrid approaches. \nIt also covers popular use cases for entity extraction, such as sentiment analysis, content recommendations, knowledge graph creation, and even managing customer relationships! ðŸ’¼ðŸ’¡\n\n\nSo, if you're curious about leveraging entity extraction, read the full article here : \nhttps://ubiai.tools/blog/article/mastering-entity-extraction-for-Business-success\n \n\n\nEnjoy reading and leave your comments below! ðŸ“–ðŸ’¬\n\n\nP.S. Share this with your fellow  data enthusiasts! Spread the knowledge! ðŸŒðŸš€",
"date": "2023-05-16"
},
{
"vote": 0,
"title": "Textraction.ai released! AI Text Parsing API",
"text": "It allows extracting custom user-defined entities from free text. Very exciting! \n\n\nIt can extract exact values (e.g. names, prices, dates), as well as provide ChatGPT-like semantic answers (e.g. text summary).\n\n\nI like the interactive demo on their website (\nhttps://www.textraction.ai/\n) - it allowed me to try my own texts and entities within minutes. It works great :)\n\n\nThe service is accessible also as an API for any purpose  via the RapidAPI platform: \nhttps://rapidapi.com/textractionai/api/ai-textraction\n (sign up to RapidAPI and get your own token)",
"date": "2023-05-16"
},
{
"vote": 8,
"title": "Building Multi task AI agent with LangChain and using Aim to trace and visualize the executions",
"text": "Let's play..\n\n\nWe prompt agent to list 5 interesting facts about supernovae, find the last exploded supernova, and solve a math problem.\n\n\naim-langchain demo\n\n\nAim repo:Â \nhttps://github.com/aimhubio/aim\n\n\nLangChain repo:Â \nhttps://github.com/hwchase17/langchain",
"date": "2023-05-16"
},
{
"vote": 3,
"title": "Comparing language use by different categories of people",
"text": "Hi NLP Experts,\n\n\nI'm a beginner in NLP and I would like to ask some advice to approach this toy problem of mine.\n\n\nI have a corpora of tweets for different people. The people in this dataset are already divided into categories, and I want to train a model to see if I can identify the category these people belong to just solely on the language used in their tweets.\n\n\nI did a simple w-shingling approach (n=1 to 5) and ranked the importance of each token to each category using tf-idf, but I encountered some problems with 1) misspellings / 2) hashtags / 3) use of slang / 4) repeated mentions of names particular to an account, that increases the tf-idf of irrelevant tokens.\n\n\nI'm thinking of ideas to overcome this. I'm also not sure if I'm approaching this problem the correct way.\n\n\nI appreciate the help!",
"date": "2023-05-16"
},
{
"vote": 7,
"title": "Rhetorical Structure Parsers that run on Python?",
"text": "I am interested in using Rhetorical Structure Parsers (RSPs) to do sentiment analysis. For my thesis, I am going to be using lexicon-based models (VADER and SO-CAL) and test whether RSPs improve accuracy consistently or whether it depends on rhetorical complexity (I am testing three different domains for this).\n\n\nI did find a few RSP options which run on Java and Scala, but I did not manage to run them even using a wrapper. Some examples:\nhttps://github.com/allenai/processors-corenlp\n\n\nhttps://github.com/jiyfeng/DPLP\n\n\nAny ideas of a RSP that I could use? The DPLP model is supposed to run on Python actually, but its documentation is very poor and I could not make it work after a whole week trying.",
"date": "2023-05-14"
},
{
"vote": 10,
"title": "Domain specific chatbot. Semantic search isn't enough.",
"text": "Hi guys, I'm struggling to find a reliable solution to this specific problem.\n\n\nI have a huge dataset with chat conversations, about several topics. I want to ask questions and retrieve information about these conversations in a chatbot way.\n\n\nI have tried semantic search with chat gpt to answer questions about these conversations. The problem is that semantic search only returns top similar sentences, and doesn't â€˜readâ€™ all conversations, thatâ€™s not enough to answer generic questions, just very specific ones. For example, if I ask â€œWhat are these people talking about person X?â€ it will return only the top sentences (through semantic similarity) and that will not tell the whole story. The LLMâ€™s models have a limit of tokens, so I canâ€™t send the whole dataset as context.\n\n\nIs there any approach to giving a reliable answer based on reading all the messages?\n\n\nAny ideas on how to approach this problem?",
"date": "2023-05-13"
},
{
"vote": 1,
"title": "What model to finetune on my dataset",
"text": "I have a big dataset of csv files with 15 parameters with each rows with avg 20000 character. I need to fine-tune a model so I can ask questions about the dataset, what model should I use for this question answer task and how to feed the model this dataset.",
"date": "2023-05-12"
},
{
"vote": 14,
"title": "Need an honest opinion on this study plan for NLP",
"text": "Would this be enough to land me a position in NLP?: \nhttps://www.unibz.it/en/faculties/education/master-applied-linguistics/study-plan/\n\n\nOr are there not enough CS modules?",
"date": "2023-05-12"
},
{
"vote": 0,
"title": "Is Bert Multiclassification enough for MS thesis",
"text": "[deleted]",
"date": "2023-05-10"
},
{
"vote": 0,
"title": "Funk, Blues, and The Great Leap",
"text": null,
"date": "2023-05-10"
},
{
"vote": 5,
"title": "[Research] \"Language models can explain neurons in language models\" from OpenAI",
"text": "This new piece of research was released today (2023-05-09) by OpenAI: \nhttps://openai.com/research/language-models-can-explain-neurons-in-language-models\n\n\nUsing the \nneuron-explainer\n tool, you can drill down into individual neurons, e.g., here's one for \"transition words at the beginning of sentences\" like \nhowever\n, \nadditionally\n, \nsince\n, etc. \nhttps://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html#/layers/15/neurons/4538\n\n\nThere's also an option to submit a better explanation for what the neuron is doing. (OpenAI is crowdsourcing the reinforcement learning human feedback - RLHF - portion of model building, it seems.) Perhaps you think you have a better label for the neuron that is acting as \"transition words at the beginning of sentences\". You can click on the \nSubmit Better Explanation\n button and fill out the Google Form to submit a revised explanation. (Note that you need to be logged into your Google account to submit this form.)\n\n\n\n\nPaper: \nhttps://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html\n\n\n\"View Neurons\": \nhttps://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html\n\n\nCode and dataset: \nhttps://github.com/openai/automated-interpretability",
"date": "2023-05-10"
},
{
"vote": 11,
"title": "Hosted Embedding Marketplace â€“ Stop scraping every new data source, load it as embeddings on the fly.",
"text": "We are building a hosted embedding marketplace for builders to augment their leaner open-source LLMs with relevant context. This lets you avoid all the infra for finding, cleaning, and indexing public and third-party datasets, while maintaining the accuracy that comes with larger LLMs.\n\n\nWill be opening up early access soon, if you have any questions be sure to reach out and ask!\n\n\nLearn more here",
"date": "2023-05-09"
},
{
"vote": 0,
"title": "What Are Autonomous Agents? And why are they the next AI wave after ChatGPT?",
"text": null,
"date": "2023-05-09"
},
{
"vote": 1,
"title": "Can you give me some advice on choosing a school for NLP Master's degree in France?",
"text": "I have a Bachelor's degree in French language from China, and for my Master's, I want to study NLP. \n\n\n&#x200B;\n\n\nI have currently been admitted to the University of BesanÃ§on, Paris 13 University, University of OrlÃ©ans, and University of Lorraine. The universities of BesanÃ§on, Paris 13, and OrlÃ©ans have offered a reduction in tuition fees. Although I prefer to go to Paris, I have heard that the area around Paris 13 University is not very safe. The University of Lorraine does not offer a reduction in tuition fees. \n\n\nCan you give me advice on which one to choose?\n\n\n&#x200B;\n\n\nThank you in advance!",
"date": "2023-05-09"
},
{
"vote": 1,
"title": "Learning and Working on NLP/LLM APIs",
"text": null,
"date": "2023-05-09"
},
{
"vote": 7,
"title": "SBERT models gone from huggingface?",
"text": "What happened to all the sbert models on huggingface?\nhttps://huggingface.co/sentence-transformers\n is 404 now",
"date": "2023-05-09"
},
{
"vote": 1,
"title": "Ablation studies on datasets for LLMs?",
"text": "Hi experts,\n\n\n&#x200B;\n\n\nIs there any ablation study on the effects of each dataset on the (qualitative) performance of LLMs?\n\n\n&#x200B;\n\n\nI couldn't find any serious research except some qualitative assumptions here and there in several papers. Has anyone seen good researches/papers?\n\n\n&#x200B;\n\n\nThanks!",
"date": "2023-05-08"
},
{
"vote": 1,
"title": "Optimized voice recognition models?",
"text": "If I wanted to group human voices by similarities to create optimized voice recognition models through fine tuning, how many and what categories do you think would make sense? The use case would be a system that loads models depending on who it talks to, but would also have a more general and probably slower model that needs more RAM. It's meant as something between a model optimized for one specific voice and a general model. Ideally I would have one that sorts voices, then then the other ones can be trained on voices it has sorted into buckets. \n\n\nHelloPi gave me these characteristics to make distinctions after I pushed her to give me a full list: \n> I can try! Physical characteristics (male/female, age), vocal qualities (timbre, pitch, tone, volume, clarity, intonation, accent, resonance), emotional content (happy, sad, angry, excited, etc), dialect (regional or social), and style (formal or informal). Thatâ€™s a lot of categories, but thereâ€™s still room for even more nuance within each of them.\n\n\nCan you think of more? Any ideas about the ones mentioned? Since it's about different people I would probably ignore emotion as a distinction.",
"date": "2023-05-08"
},
{
"vote": 26,
"title": "RasaGPT: First headless LLM chatbot built on top of Rasa, Langchain and FastAPI",
"text": null,
"date": "2023-05-08"
},
{
"vote": 0,
"title": "It's not much work, but an honest work - Looking for (ChatGPT) App Feedbacks!",
"text": "[removed]",
"date": "2023-05-07"
},
{
"vote": 1,
"title": "Can we watch YouTube without ads?",
"text": "[removed]",
"date": "2023-05-07"
},
{
"vote": 1,
"title": "fast.ai - Mojo may be the biggest programming language advance in decades",
"text": null,
"date": "2023-05-06"
},
{
"vote": 3,
"title": "Further pretraining and fine-tune",
"text": "Hello all!\n\n\nI got my hands on a relevant, domain specific custom dataset that might help improve my text classification task.\n\n\nInitially I was fine tuning BERT large, with my small separate dataset (3200 short documents) for a text classification task, but it performed badly and I think it's because BERT large wasn't trained with this domain. \n\n\nI now have a large 7000+ relevant dataset and was hoping to get some juicy tips from the frisky language tech community.\n\n\nThis is my plan, further pretrain BERT large with the 7000+ dataset and then fine tune with the smaller one on a text classification task.\n\n\nIs this plan trash? If so what would you recommend?\n\n\nIf my plan might work, I have 0 experience on pretraining and hoping to get the best tutorials you all have come across.\n\n\nThanks everyone and happy Friday!",
"date": "2023-05-06"
},
{
"vote": 2,
"title": "Fine-Tuning LLM models",
"text": "Hi guys! I have a question: does fine-tuning a model on your own data give knowledge to the model?",
"date": "2023-05-05"
},
{
"vote": 2,
"title": "How to find unique words/phrases in a group of documents?",
"text": "I was wondering if anybody can advise on how to best approach the following problem. I have a group of 20 or so resumes belonging to similar candidates all of whom have been labeled as having very strong \"leadership\" soft skills. The resumes do not contain any obvious words that would directly attribute the skill to them (i.e. \"I'm a strong leader\")\n\n\nHow would I go about identifying what if any attributes exist more frequently in the 20 resumes compared to the thousands of others that I have?\n\n\nThank you!",
"date": "2023-05-05"
},
{
"vote": 1,
"title": "How do benchmarking datasets work for knowledge probing?",
"text": "I have a dataset with a split between training data and testing data, and a model that is pre-trained on data in the same domain as the dataset data. If I fine-tune the model on this dataset using the training split of the dataset it will learn the knowledge in that split but also the structure of the data, right? \n\n\nBut the testing split and the training split should be different. So then the fine-tuning is only helpful for the format of the output? And then can't I just as well just use a few-shot demonstration technique to get the format I want and skip the fine-tuning? \n\n\nBut if I want to use this dataset to determine how good the model is at information extraction/knowledge probing how does it help to fine-tune it on the dataset? shouldn't it have the same performance if it is not fine tuned on the dataset as if it was? since it is equally likely to have seen the knowledge in the testing split even if it is not fine tuned?\n\n\nAnd how does a benchmark really work if it contains only one correct answer per question when the model might have five different correct answers? Like the dataset might contain \"Q: the sky is what colour? A: blue\" and the model might have information about the sky being able to be pink at sunrise, blue at noon, red at sunset, and black at night. And if the model outputs anything other than blue if will be judged as bad. Is the dataset bad or is the model bad?",
"date": "2023-05-05"
},
{
"vote": 30,
"title": "MosaicML MPT-7B: A Commercially-Usable LLaMa-Quality Model trained for $200k",
"text": null,
"date": "2023-05-05"
},
{
"vote": 20,
"title": "HyperDB: A hyper-fast local vector database for use with LLM Agents.",
"text": null,
"date": "2023-05-03"
},
{
"vote": 4,
"title": "2D Tokenization for Large Language Models",
"text": null,
"date": "2023-05-02"
},
{
"vote": 1,
"title": "Some of these BOTS are on something ...",
"text": null,
"date": "2023-05-02"
},
{
"vote": 11,
"title": "Why Language Models Hallucinate",
"text": "Hi there,\n\n\nI have made a video \nhere\n where I explain the possible reasons behind language models' hallucinations.\n\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
"date": "2023-05-01"
},
{
"vote": 4,
"title": "Side Project: Wordbots, an online tactical card game in which players write their own cards, which get converted into JavaScript using a semantic CCG parser",
"text": "Hi all, my long-running side project has just hit beta and I thought I'd share with r/LT. It's an tactical card game (think Hearthstone, Slay the Spire, etc) where players write their own cards in a restricted subset of English, which then gets parsed into JavaScript via a semantic CCG parser.\n\n\nCheck it out!\n\nhttps://app.wordbots.io/\n\n\nMore details on the parsing machinery here:\n\nhttps://app.wordbots.io/how-it-works",
"date": "2023-05-01"
},
{
"vote": 1,
"title": "Resources for optimised searching",
"text": "I've collected quite a large dataset of Reddit data from a subreddit that is about an institution - there are a variety of questions, comments, and irrelevant posts (complaints, rants, etc.)\n\n\nMy end goal is a system that, provided a question regarding the institution, returns a collection of posts/comments/etc extracts that contain information used to answer this question.\n\n\nThere's a lot of complexity to this. For example, a post could ask a question and have answers in the comments. Only one comment may answer the question, and there may be many other useless comments. In comment chains, there may be answers to questions that are unrelated to the post completely but could still be useful in another context.\n\n\nThen, I somehow need to match the meaning of the question to all of these potential post/comment, comment/comment etc. question and answer pairs and have some way of ranking them as we want to limit the returned results in terms of characters.\n\n\nI essentially need to develop this process, and I have quite a bit of spare time to do so. I was thinking about working through \nhttps://web.stanford.edu/~jurafsky/slp3/\n over the course of a few weeks, but this might be overkill for what I'm doing. Does anyone have any resources or suggestions for fields I should double down on for this type of thing?",
"date": "2023-04-30"
},
{
"vote": 0,
"title": "Sam Altman Predicts AI Will Either Make Tons Of Money Or End The World As We Know It",
"text": null,
"date": "2023-04-30"
},
{
"vote": 3,
"title": "ChatGPT and Privacy for Computational Activities",
"text": "Although UtopiaP2P has the advantage of a decentralized and secure communication network for computational and digital citizenship activities, the general availability of ChatGPT on UtopiaP2P Messenger aims to simplify the lives of its users.\n\n\nThe ChatGPT UtopiaP2P app client works as a 24/7 free and free wall assistant. Using OpenAI's revolutionary language processing technology, ChatGPT Assistant can understand user requests very quickly and accurately, showing UtopiaP2P users that they are committed to providing customers with the best potential and user experience. The integrated ChatGPT helper is exactly what users use in Utopia's Get applications. one of the tools you need to improve. \n\n\nUtopiaP2P Messenger is fully decentralized with end-to-end encryption for all chats/emails, phone and video calls, and file transfers. It also includes an ecosystem of products such as secure email clients, personal cloud storage services, and cryptocurrency wallets designed to provide end-to-end communication and productivity solutions for users. \n\n\nUtopiaP2P has developed a messaging application that is both secure and easy to use, combining modern technology with an intuitive user interface and an inclusive ChatGPT assistant for users to have a more satisfying experience in the application client.\n\n\nChatGPT Assistant is now available for Utopia Messenger users. To use this feature, simply download the app and start talking to the assistant immediately.\n\n\n&#x200B;\n\n\nhttps://u.is/en/\n\n\nhttps://twitter.com/UtopiaP2P",
"date": "2023-04-30"
},
{
"vote": 2,
"title": "Semantic matching of query to context",
"text": "I am looking to do some matching between \n\n\nString A: some context\nString B: a question regarding about something within that context\n\n\nThe issue is that not necessarily the entire context section will be regarding that specific question. \n\n\nImagine taking a paragraph from a Wikipedia article as context, then the question might be about something that is mentioned in the article. For example a paragraph that talks about some building A, when it was built who the owners were, who the architect was etc. and you want to match it to the question \"who was the architect of building A\" \n\n\nCurrently I'm going at it using sentence transformers and a vector database, but have been getting subpar results. I'm in the process of trying some different models to see if I can improve my results, but i was wondering if this is the best way of going about this? Or should I be doing something completely different?",
"date": "2023-04-30"
},
{
"vote": 5,
"title": "Video presentation on the evolution of language",
"text": null,
"date": "2023-04-29"
},
{
"vote": 5,
"title": "Idiom Handling",
"text": "I'm doing an assignment on how NLP systems handle idiomatic expressions. Does anyone have any information or sources that would be helpful.\n\n\nThanks in advance.",
"date": "2023-04-29"
},
{
"vote": 1,
"title": "Translator app/website that translates videos I uploads to it to whatever language I choose [help]",
"text": "[removed]",
"date": "2023-04-29"
},
{
"vote": 4,
"title": "I'm not understanding few-shotting",
"text": "If I try to few-shot my model like this:\n\n\n\"demonstration 1.\n\n\n###\n\n\ndemonstration 2. \n\n\n###\n\n\ndemonstration 3.\n\n\n###\n\n\nGenerate text... \"\n\n\nMy generated text is heavily influenced by the demonstration facts. I want the output to extract knowledge from the model embeddings but I mostly get output that is related to the demonstrations. What should I think about when trying to manually prompt a model with demonstrations?",
"date": "2023-04-28"
},
{
"vote": 5,
"title": "Huggingface not saving model checkpoint",
"text": "I am trying to train T5 model. This is how my training arguments look like:\n\n\nargs = Seq2SeqTrainingArguments(\n    model_dir,\n    evaluation_strategy=&quot;steps&quot;,\n    eval_steps=100,\n    logging_strategy=&quot;steps&quot;,\n    logging_steps=100,\n    save_strategy=&quot;steps&quot;,\n    save_steps=200,\n    learning_rate=4e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=&quot;rouge1&quot;,\n    report_to=&quot;tensorboard&quot;\n)\n\n\n\nMy model trained for 7600 steps. But the last model saved was for checkpoint 1800:\n\n\ntrainer screenshot\n\n\nWhy is this so?",
"date": "2023-04-28"
},
{
"vote": 14,
"title": "Who uses the INCEpTION platform to manually annotate their language data?",
"text": "INCEpTION platform\n\n\nWe use it all the time where I work, but what what about other workplaces? Is INCEpTION a big, established deal for NLP data collection needs and something which most NLP researchers/professionals have heard of? Or is it just some random thing on the internet that I just happen to use and which may fizzle out tomorrow?\n\n\nThe reason I ask is that Iâ€™m considering creating an open-source project to complement INCEpTION. But it will be quite time-consuming and involved, so I wonâ€™t do it if thereâ€™s no market for it.\n\n\nThanks!",
"date": "2023-04-28"
},
{
"vote": 2,
"title": "Understanding Huggngface trainer progress bars",
"text": "I am training T5 model with batch size of 4. There 3049 training samples. The trainer shows following progress bars: \nimage link\n\n\nFirst progress bar seems to of number of batches which as 3049/4 = 762. \n\n\nBut the second progress bar shows some number out of 285. What is 285 here?",
"date": "2023-04-27"
},
{
"vote": 2,
"title": "Passing target text to gpt2 and T5 for fine tuning to learn text generation task",
"text": "I have text with each line in following format:\n\n\n&lt;text-1&gt; some text-1 &lt;text-2&gt; some text-2 &lt;text-3&gt; some text-3\n\n\n\nI want fine tune model to learn generate \nsome text-3\n after reading \nsome text-1\n and \nsome text-2\n. In GPT2 and T5 text generation tutorials, we do specify \ninput-ids\n for target text i.e. labels, but for GPT2 we dont. \n\n\nFor example in \nthis T5 text generation tutorial\n, we can find line:\n\n\nmodel_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]\n\n\n\nBut I could not find any such line in these GPT2 text generation examples: \n\n\n\n\nhuggingtweets demo\n,\n\n\n\n\nhuggingartists demo\n\n\n\n\nFinetune GPT2 for text generation",
"date": "2023-04-27"
},
{
"vote": 4,
"title": "Transforming Healthcare with AI: The New Era of Medical Technology",
"text": null,
"date": "2023-04-26"
},
{
"vote": 1,
"title": "Need resources with sentiment analysis model specifically e-commerce in 2023",
"text": "A Cloud Function to fetch reviews written to certain brand pages for given social media, sentiment on it find positive ones and then save in own database. Later on, an API can be consumed by the client to show positive reviews.\n\n\nI am an FTE software developer so know the development of cloud runners and API needs resources for sentiment analysis models.",
"date": "2023-04-26"
},
{
"vote": 9,
"title": "Best pathway for Domain Adaptation with Sentence Transformers?",
"text": "Hi everyone,\n\n\nCurrently trying to build a semantic search model for a medical application.  I have been working in the NLP space in medicine for a while now and have been reliant on my trusty fine-tuned BERT MLM for quite a few downstream tasks.  I'm looking to tackle semantic search now and I'm just curious if I'm going about it the right way.\n\n\nThese are the primary resources that I've been reading into: \n\n\nhttps://www.sbert.net/examples/unsupervised_learning/README.html\n\n\nhttps://www.sbert.net/examples/domain_adaptation/README.html\n\n\nhttps://www.sbert.net/examples/training/ms_marco/README.html\n\n\nMS MARCO is specifically interesting to me because I will be doing asymmetric search of reports with small keyword queries, like looking for diagnosis or procedures or outcomes.\n\n\nTo that end, my plan was to:\n\n\n\n\nUtilize my corpus-tuned MLM as a backbone for my bi-encoder.\n\n\n\n\nTrain a bi-encoder using MS MARCO\n\n\n\n\nUsing Generated Pseudo Labels from a corpus of medical reports to fine-tune the bi-encoder above\n\n\n\n\nThrow generated pseudo label pairs into a labelling software and build a dataset big enough to train a cross-encoder as a back-up.\n\n\n\n\n\n\nI went ahead and trained using both MNRL and MarginMSE with MS MARCO and found that my model performed quite poorly afterwards.  I guess this isn't that unexpected, as the MS MARCO data are all sorts of Bing queries, and my MLM is very domain specific.  However, before I spend a bunch of time going to step 3, I just want to make sure that my logic is sound.  Is there a better way to build a domain-specific semantic search model other than Sentence-Transformers and is my line of thinking around asymmetric search correct?",
"date": "2023-04-25"
},
{
"vote": 1,
"title": "Learn Javascript Prompt Pack (for Beginners)",
"text": null,
"date": "2023-04-25"
},
{
"vote": 7,
"title": "Hows the Research Area of NLP + Systems",
"text": "So I have some interest in systems, and some collaboration happened, and we are now working on accelerating NLP Applications in mobile soc processors. I am excited, but I feel it would severely limits the chances of me getting hired in the industry, or it increases. I know a lot of people are working in interdisciplinary fields still, but I wanted to be sure of this particular field that if I can target AI/ML or data science jobs through this \n\n\nFurthermore, my previous research area was CSS, so its a very big jump from my previous area.",
"date": "2023-04-25"
},
{
"vote": 4,
"title": "Knowing basic things about counter argument generation",
"text": "I am new to text generation. I am trying out counter argument generation given some text. I have couple of questions:\n\n\n\n\nHow do I know whether to even respond to the input or not. For example, LM dont need to reply to \"I am happy today\" with counter argument. \n\n\n\n\nHow can I know if there is any fact stated in the input, so that LM can build counter argument based on it? I may write some more components in the pipeline to check the correctness of the fact and feed it to language model (LM) for response generation. But I first need to first check if there is indeed some fact in the input and then extract that fact for the component pipeline to operate on.\n\n\n\n\nHow can I tell LM to generate counter argument and not just generate further sentence to say complete the paragraph? How this is usually done?\n\n\n\n\n\n\nAre their any standard practices / tricks / papers for all above three questions that I miss being noob?",
"date": "2023-04-24"
},
{
"vote": 31,
"title": "Reading list to catch up to the state of the art in LLM?",
"text": "Are there any books or other resources that have been released since the introduction of the GPT models that introduce the technologies and terminology used in current state of the art LLMs for someone entering the field?",
"date": "2023-04-23"
},
{
"vote": 39,
"title": "The \"last\" generation of LLMs produced a wide range of stylistically plausible but incoherent poems, music, literature, etc. The \"current\" generation can produce extraordinarily coherent text, but with very little stylistic range/individuality. I'm trying to understand why that is.",
"text": "Forgive my vagueness and general lack of knowledge about how these models work. But hopefully I can convey my point and get some expert insight from some folks here. \n\n\nSeveral years ago, there was a spate of small-scale projects to produce very specific content using LLMs. It was common to read an article like \"we asked poets to evaluate these AI-written poems\" or \"here is a piece in the style of Debussy composed by AI\". \n\n\nThese got very specific: for example, someone used GPT2 to generate text in the style of the novelist Patrick O'Brian, and the output is sort of plausible stylistically but doesn't make any sense. See \nthe OBrain\n. That same dude tried to get ChatGPT to spit out some O'Brian text, and while it is coherent, it is absolutely nothing like O'Brian's writing. \n\n\nThat's not surprising, but the question then is, \nis\n it possible to fine-tune a latest-generation LLM in the same way as was done with GPT2 in order to get the style right and keep the coherence? I was reading the fine-tuning page on OpenAI, and it's definitely not targeted to this sort of thing. Can the \"voice\" of LLMs, even ones without guardrails, be changed, or does the nature of their enormous training sets constrain the possible stylistic variety of their output? \n\n\nAnd then you get the even-more-difficult case of music: even if you produced training data in the appropriate format for use by a latest-generation model, there is a limited quantity of it! It's not obvious to me that GPT4 or any other new model would be better at spitting out music than earlier models, since it's not trained on tokenized sheet music.\n\n\nI would love to be wrong. I want to hear Beethoven's 10th symphony!",
"date": "2023-04-22"
},
{
"vote": 13,
"title": "Any ideas for NLP end-to-end projects or blogs for a beginner with a linguistics background to boost their CV?",
"text": "Hey everyone,\n\n\nI'm a beginner in the field of Natural Language Processing with a background in linguistics. I'm trying to add some impressive projects to my CV and expand my knowledge in the field.\n\n\nI was wondering if anyone has any suggestions for end-to-end projects in NLP or any blogs that I could work on? I'm looking for projects that are both challenging and rewarding and can showcase my skills to potential employers.\n\n\nAny suggestions or ideas would be greatly appreciated! Thanks in advance.",
"date": "2023-04-22"
},
{
"vote": 0,
"title": "Emergent Abilities of Large Language Models",
"text": null,
"date": "2023-04-21"
},
{
"vote": 1,
"title": "StableLM: The New Best Open Source Base Models For GPT Apps!",
"text": "Stability AI recently release 3B and 7B of what they are calling StableLM. If the early metrics are anything to go by these models will be the best models to build from for your generative AI applications. StableLM trains on more data like the LLama models, has the largest open source context window of 4096, and is under a permission license!\n\n\nhttps://youtu.be/z1sFnzgKw_Q",
"date": "2023-04-21"
},
{
"vote": 1,
"title": "Recommendations for Chinese language sentiment analysis in Python?",
"text": "I'm looking to conduct sentiment analysis of Chinese and hoping for some advice on tools before I go any further down this rabbithole. \n\n\nDoes anyone have advice on appropriate libraries for Python-based sentiment analysis? \n\n\nThe corpus is a small set (~10k) of news articles that would tend to be about 400-500 words in English if translated.\n\n\nWhat I've found\n\n\nI've found \nSnowNLP\n, but most of the library is over six years old, which is not ideal for long-term projects as it's outside the support window. SnowNLP's methodology for training sentiment data also sounds messy and unreliable. I couldn't find anything else. \n\n\nTranslating to English and then engaging in analysis is obviously not going to work, and I've had to demonstrate this internally already. I'm cautious about online tools that might get cut off because the content is low-level political material, so I'd prefer a locally hosted library rather than an online service. \n\n\nOverview of project\n\n\nThe text is mostly Simplified Chinese, with some English, some Traditional Chinese, and a scattering of other languages.\n\n\nI don't read Chinese, although I work closely with a non-technical but highly competent team member who does. This is mainly an issue because they don't have a good grasp of what is technically possible with the tools, and I don't have a good grasp of what the language requires. What this means is that there are issues that I suspect have already been solved by others that are taking us a long time to work out, such as tokenisation issues.",
"date": "2023-04-21"
},
{
"vote": 20,
"title": "Publishing Wikipedia embeddings: 100M embedding vectors. English + 9 other languages.",
"text": null,
"date": "2023-04-20"
},
{
"vote": 3,
"title": "txtai 5.5 released: workflow streams and DuckDB support",
"text": null,
"date": "2023-04-20"
},
{
"vote": 0,
"title": "How would one extract psychosomatic symptoms?",
"text": null,
"date": "2023-04-20"
},
{
"vote": 1,
"title": "Text relevance scoring methods?",
"text": "I scraped some text data from multiple websites. I want to check based on some conditions (like city names, and other entities) whether a particular text is relevant to my use-case. What are some efficient methods to do this?",
"date": "2023-04-20"
},
{
"vote": 26,
"title": "Create a ChatGPT-like program using an open source model and custom data.",
"text": "Hey everyone! I have a corpus of several thousand UTF-8 text files which I created by parsing my own documents (close to 40 GB) and extensive post processing. This includes some personal data (family history, medical history etc.), along with ebooks, articles I wrote and so on.\n\n\nI would like to create a chatbot similar to chatgpt which can answer complex questions about this data (and also has generic knowledge). I was told that training a large language model from scratch is out of the question, and so I should be fine tuning another model. So I went ahead decided to do that.\n\n\nHere are the steps that I have taken till now:\n\n\nI downloaded some open source models (eg. google/flan-ul2) etc. and tried running some inference on them. I am able to get this step to work.\n\n\nHowever, soon I found out that fine tuning requires it to be in a format like input/output or instruction/input/output. As mentioned above my data is in the form of text files. So what should be done to achieve my goal? I looked into projects like paper-qa etc. on github but can't figure out how to get them to work without using OpenAI.\n\n\nI have also looked in langchain/haystack/llama-index. When using haystack the results were poor and the GenerativeQAPipeline was returning nonsensical answers.",
"date": "2023-04-19"
},
{
"vote": 17,
"title": "Community survey: Do you use LangChain, LMQL or something else?",
"text": "[removed]\n\n\nView Poll",
"date": "2023-04-18"
},
{
"vote": 1,
"title": "Rise of Prompt Engineering Jobs",
"text": "[removed]",
"date": "2023-04-18"
},
{
"vote": 13,
"title": "In the Transformer model, how is the decoder trained and how is the inference run on the decoder, exactly?",
"text": "I'm trying to implement the Transformer model (from \nAttention Is All You Need\n paper) from scratch in PyTorch, without looking at any Transformer implementation code. I am having difficulty understanding the following things:\n\n\n\n\nHow is the decoder trained?\n Let's say my embeddings are 100-dimensional and that I have 8 embeddings which make up a sentence in the target language. I feed a matrix of shape \n[8 ,100]\n to the encoder. That matrix passes through the encoder and I get keys and values matrix alongside the encoder output. I later feed the keys and values matrix obtained from the encoder output to some attention layers of the decoder. Now, let's say that that same sentence has the shape of \n[11, 100]\n in the target language. I understand that I have to apply masking before this sentence (represented as a matrix of embeddings) passes through the decoder. But the decoder should output only a vector of probabilities which indicate the most likely next token. \nSo, if I input a matrix which represents the masked embeddings in the target language, what should be the output of the decoder, exactly? What shape should it have?\n We can play along with the shapes of \n[8, 100]\n for the source language and \n[11, 100]\n for the target language.\n\n\nHow is inference made through the Decoder?\n Again, for the inference through the encoder I feed it the matrix which represents the sentence in the source language. But what about the decoder? From my understanding, the inputs to the decoder should be \n[1, 100]\n, then \n[2, 100]\n (as it generates another token), then \n[3, 100]\n and so on until it generates the end of sequence token and the output of the decoder should always be a vector (one-dimensional) which represents the probability distribution of the next token. \nI don't understand how are the outputs of the decoder during inference (when it is fed the previously generated sequence, which is growing token by token) and the decoder during training (when it is fed the entire target sequence, albeit masked) related.\n Also, I don't understand how I would construct a model which takes in variable-length inputs (generated decoder sequence so far) and produces fixed-length output (probability distribution of the next token). I seem to think that the outputs of the decoder must be different in those two cases, that is, that they can't be a one-dimensional vector representing a probability distribution over tokens, because that doesn't make a lot of sense during training.\n\n\n\n\nI have read \nThe Illustrated Transformer\n and such articles, but I can't seem to understand this. If someone can shed some light I'd appreciate it very much.\n\n\nP.S. Cross-posted \nhere\n and \nhere\n.",
"date": "2023-04-18"
},
{
"vote": 1,
"title": "Demystifying AWS Compute Resources: The Tech Think Tank Comprehensive Guide - EP 02 This is the second episode of the Amazon web services series. #aws #awscommunity #awscertified #awstraining #awssolutionsarchitect #awsdeveloper #awsservices",
"text": null,
"date": "2023-04-18"
},
{
"vote": 7,
"title": "SQL in NLP jobs?",
"text": "So Iâ€™m currently in a Masters in data science program specializing in computational linguistics. From what Iâ€™ve heard, strong SQL skills are pretty much non-negotiable in DS jobs. Problem is, I really donâ€™t like SQL - I love CL and modelling/ problem solving with Python but hate the idea of having to write SQL all day. For those of you working in NLP, how proficient should I be at SQL?",
"date": "2023-04-18"
},
{
"vote": 6,
"title": "[D] What exactly is the difference between these LLMs?",
"text": "HI\n\n\nI see GPT2/GPT3/GPT4, LLaMA, PaLM, Gopher/Chinchilla, etc\n\n\nWhat exactly are the difference between these models? I know the size differs (width, length, number of layers etc), the training dataset might be different. But what else? Are they all Decoder Transformer but none really change its underlaying model architecture right?",
"date": "2023-04-17"
},
{
"vote": 3,
"title": "How is the search term disambiguation implemented in the modern search engine?",
"text": "Hi experts,\n\n\n&#x200B;\n\n\nI can understand it on a high-level, by guessing and googling: The search engine exploits information such as location, search history, context, etc.\n\n\n(typical examples: apple(company & fruit), football(U.S. & U.K.))\n\n\n&#x200B;\n\n\nBut how? How is the disambiguation process actually implemented in the production system? Is the disambiguation an independent model? Or part of the embedding stage? In both cases, how is it trained and maintained? How does it adapt to a new popular ambiguous search term? With all the latency constraints?\n\n\n&#x200B;\n\n\nI googled and ChatGPT'd, but all I could find were just high-level explanations. Neither could I find detailed system diagrams.\n\n\n&#x200B;\n\n\nAnyone with actual experience could provide a detailed explanation?\n\n\n&#x200B;\n\n\nFor example, if you wanna use the 'context', you might end up using a pre-trained BERT-ish contextual encoder upfront in the pipeline. But what about the location info, for example? How do you train or pre-train the model for that? What system can support that?\n\n\n&#x200B;\n\n\nThank you very much in advance!",
"date": "2023-04-17"
},
{
"vote": 1,
"title": "English update",
"text": "[deleted]",
"date": "2023-04-17"
},
{
"vote": 1,
"title": "Seek NLP Consultant",
"text": "[removed]",
"date": "2023-04-16"
},
{
"vote": 1,
"title": "Tensorflow V2 - LSTM Penn Tree Bank Dataset",
"text": "For my research I need to apply an optimizer (I wrote in Tensorflow Version 2 - cannot switch to Pytorch) to an LSTM and train on the Penn Tree Bank Dataset (PTB). Problem is that all Tensorflow code I can find online training an LSTM on PTB is written in Tensorflow V1, which is deprecated. I need to replicate competitive results to act as a baseline.\n\n\nI found the official Tensorflow V1 code from a Github branch here (\nhttps://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/rnn/ptb/ptb_word_lm.py\n). All code necessary to run that file is in the /ptb folder (except data).\n\n\nI tried to convert the old Tensorflow V1 to TensorflowV2, but I cannot replicate the results! I cannot get below validation perplexity of 159! While the TensorflowV1 code reports a validation perplexity of 86.\n\n\nI'm using the same data processing, only changing the model and training loop. Can anyone help me? Here is a link to the google colab I used for this:\n\n\nhttps://colab.research.google.com/drive/1t0aA2CIGaA9dRYJQ8PPm5yxebjFK-nb0?usp=sharing\n\n\nIn addition, the data and preprocessing script is located in my github repo here (will need to upload it to google colab): \n\n\nhttps://github.com/OUStudent/LSTM_PTB_TensorflowV2\n\n\n&#x200B;\n\n\nAny help is greatly appreciated!",
"date": "2023-04-15"
},
{
"vote": 3,
"title": "Generative Agents: Interactive Simulacra of Human Behavior - Discover a Town Run by 25 ChatGPTs",
"text": "https://youtu.be/9LzuqQkXEjo",
"date": "2023-04-15"
},
{
"vote": 5,
"title": "In the encoder-decoder architecture for machine translation, how are (x, y) pairs constructed when a word from the source language can have none or multiple words in the target language?",
"text": "As the title states, my main confusion lies in understanding how the encoder-decoder architecture for machine translation is trained. I also don't understand how the inference is made (the forward pass).\n\n\nMy main confusion stems from the fact that one word in the source language can have none or multiple words in the target language. In that situation, I don't know what is the correct ground-truth probability distribution over the tokens in the target language.\n I'll try to illustrate what I don't understand to the best of my ability below. I will use the word \"token\"; we can say that one token is one word.\n\n\nSuppose I'm translating a sentence from one language to another. I understand that I calculate the loss by looking at the probability distribution over the tokens the decoder outputted and the ground-truth probability distribution for that particular token. However, what happens when the token from the source language corresponds to no tokens in the target language? Would the correct probability distribution place the highest probability on the padding token? Or, an even more confusing example for me is, what happens if a token from the source language corresponds to multiple tokens in the target language? What should the correct output be then?\n\n\nIf you add \nByte-Pair encoding based embedding\n to this story then this all becomes much more complicated because the tokens become subwords: which subword in the source language corresponds to which subword in the target language? Again, I don't understand this, and if someone can shed some light on this I'd appreciate it.\n\n\nP.S. Cross-posted on \nlearnmachinelearning\n and \nMLQuestions",
"date": "2023-04-15"
},
{
"vote": 1,
"title": "In the encoder-decoder architecture for machine translation, how are (x, y) pairs constructed when a word from the source language can have none or multiple words in the target language?",
"text": "[deleted]",
"date": "2023-04-15"
},
{
"vote": 17,
"title": "NLP PhD?--advice",
"text": "Hi All,\n\n\nI have a bachelor's degree in linguistics and also have an MSc's degree in computer science from the UK. I am interested in Knowledge representation and reasoning and want to study the ways machine learning, symbolic knowledge and formal reasoning can interact to enhance one another.\n\n\nCurrently, I do not have any publications. My master's project was on rule learning from knowledge graphs and language models. However, the duration of this project was only three months and the research idea was really weak in my opinion. Therefore, the projects ended up with a low mark (merit in the UK system).\n\n\nHowever, I really want to eventually get a PhD in NLP (focusing on reasoning), since I am fascinated by the idea of combining symbolic reasoning and machine learning. \n\n\nI tried to email several supervisors but none of them ever got back to me. I also tried to apply for some PhD projects but also got silent rejections everywhere.\n\n\nMaybe I am not strong enough to apply for PhD positions right now because I have a very weak machine learning background (only did an introduction to machine learning and an old school 'symbolic AI'). And there is just one math module (statistics) listed on my transcripts (actually I have already self taught myself calculus, linear algebra, probability and real analysis, but it's hard to show potential supervisors I have learned them, right?)\n\n\nBased on my background what are the next steps I would have to take in order to get my foot into this field? Any advice is highly appreciated.\n\n\nMany thanks!",
"date": "2023-04-14"
},
{
"vote": 1,
"title": "Three PhD positions in Information Retrieval at the UniversitÃ  della Svizzera Italiana (USI)",
"text": "[removed]",
"date": "2023-04-14"
},
{
"vote": 5,
"title": "A Survey of Resources and Methods for Natural Language Processing of Serbian Language",
"text": null,
"date": "2023-04-14"
},
{
"vote": 0,
"title": "Build a PERSONAL CHATBOT with LangChainAI MEMORY with ChatGPT-3.5-Turbo API in PYTHON",
"text": "Iâ€™ve seen a lot of video about Q&A with PDF files using LangChain. But how about adding a Memory to that conversation. In this video , I walk through how we can achieve that.",
"date": "2023-04-14"
},
{
"vote": 3,
"title": "Difference between Word Embedding and Word Vectorization",
"text": "How is word vectorization different from word embedding?\n\n\nWhat common techniques used for word vectorization and word embedding\n\n\n\n\nMany sites claim they are same that's why I am asking this question",
"date": "2023-04-14"
},
{
"vote": 1,
"title": "On which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity?",
"text": "I wonder on which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity. Should TfidfVectorizer be fitted on the texts that are analyzed for text similarity, or some other texts (if so, which one)?\n\n\n\n\nI follow \nogrisel\n's \ncode\n to compute text similarity via TF-IDF cosine, which fits the \nTfidfVectorizer\n on the texts that are analyzed for text similarity (\nfetch_20newsgroups()\n in that example):\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\ntwenty = fetch_20newsgroups()\ntfidf = TfidfVectorizer().fit_transform(twenty.data)\nfrom sklearn.metrics.pairwise import linear_kernel\ncosine_similarities = linear_kernel(tfidf[0], tfidf[1]).flatten()\nprint(cosine_similarities) # print TF-IDF cosine similarity between text 1 and 2.",
"date": "2023-04-14"
},
{
"vote": 2,
"title": "Does the order of operations matter for fine-tuning and instruct model on domain specific documents?",
"text": "Iâ€™ve seen a number of papers that take an instruct model, and then fine-tune it to a specific application in the form of data in a prompt/answer format. Is it possible to fine-tune a model that has already been instruct trained on raw domain, texts, and then do more RLHF training?  \n\n\nSo letâ€™s say you wanted to try and optimize a model for a specific domain and task, could you take something like vicuna, or dolly, that has already been instruct trained and fine-tune it with a large corpus of medical or business text? Or would you have to go back to the foundational model, fine-tune it on your domain, specific texts, and then redo the instruct training?",
"date": "2023-04-13"
},
{
"vote": 1,
"title": "Balacoon: free to use text-to-speech",
"text": "[removed]",
"date": "2023-04-13"
},
{
"vote": 1,
"title": "How to Fine-tune the powerful Transformer model for invoice recognition",
"text": null,
"date": "2023-04-13"
},
{
"vote": 1,
"title": "Balacoon: handy speech synthesis",
"text": "[removed]",
"date": "2023-04-13"
},
{
"vote": 2,
"title": "Looking for a central list/repo for opensource Foundation models",
"text": "Hi All, \n\n\nNot sure if this question is post here earlier, sorry if it's redundant.\n\n\nI am looking for a git repo or blog which maintains the real time list of publicly available foundation models Like llama, pytheia etc. If any one has encountered such resource then please share. \n\n\nThanks for tuning in.",
"date": "2023-04-13"
},
{
"vote": 8,
"title": "Specializing in NLU?",
"text": "I donâ€™t see that many job descriptions for NLU (compared to NLP), but I think considering my background in linguistics, it might make more sense to bet on that. \n\n\nAnyone who works in the field and can give me some tips on how to learn more about it? Iâ€˜ve found the Stanford NLU course, but is there something better out there to start and get practice?\n\n\nGenerally, I am wondering which jobs can actually benefit from linguists, since I see a lot of people working in NLP coming from CS degrees.",
"date": "2023-04-12"
},
{
"vote": 1,
"title": "Meryem Arik: Why ChatGPT is a big block of marble which needs to be sculpted to your needs | E2",
"text": null,
"date": "2023-04-12"
},
{
"vote": 1,
"title": "How to use chatgpt? 1-pager request for non-technical folks professor (details in post)",
"text": "I have a colleague who's a new adjunct professor, and they want to use chatgpt to generate FAQs for each of his lectures/essays he has prepared for his students.\n\n\nIdeally I could give him a 1page doc of how to interact with chatgpt to obtain the above in a systematic, repeatable way, but understand that this is a specific question and there might not be content like this out there.",
"date": "2023-04-12"
},
{
"vote": 1,
"title": "AWS Identity &amp; Access Management (IAM) Explained: The Tech Think Tank Comprehensive Guide It's time for the first video of the AWS series. More coming soon. So keep an eye on the channel. These videos will benefit anyone preparing for an AWS certification. #aws #security #awsiam #awscertification",
"text": null,
"date": "2023-04-11"
},
{
"vote": 1,
"title": "Was Google's BARD a failed launch?",
"text": "[removed]",
"date": "2023-04-11"
},
{
"vote": 0,
"title": "The UAE government launched the UAE Strategy for Artificial Intelligence (AI), and this initiative represents the new stage after the smart government, on which services, sectors, and future infrastructure in the country will depend, in line with the UAE centenary.",
"text": null,
"date": "2023-04-11"
},
{
"vote": 1,
"title": "Doubts with TinyBERT training",
"text": "I am reading \"Getting started with Google BERT\" book and I find the below content for TinyBERT training:\n\n\n\n\nWe perform prediction layer distillation by minimizing the cross-entropy loss between the soft target and soft prediction. Then we can represent our loss function as follows:Â \nL[Pred] = - Softmax(logits_of_teacher) (dot_product) log_softmax(logits_of_student).\n Please help me understand the above equation.\n\n\nOur teacher BERT has 12 encoders and our student BERT has 4 encoders. So the loss is calculated is done for 1st encoder teacher to 1st encoder student. So what about the information found in the encoders between 5 to 12?",
"date": "2023-04-11"
},
{
"vote": 8,
"title": "Shared Task on Radiology Report Summarization (RadSum@BioNLP23)",
"text": "[LAST CALL FOR PARTICIPATION] We are excited to announce the new edition of the Shared Task on Radiology Report Summarization (RadSum) at BioNLP 2023, co-located with ACL 2023.\n\n\n*OVERVIEW*\n\n\nThe research area of radiology report summarization currently faces an important limitation: most research is carried out on chest X-rays. To palliate these limitations, we propose two datasets:\n\n\n- A shared summarization task that includes six different modalities and anatomies, totalling 79,779 samples, based on the MIMIC-III database.\n- A shared summarization task on chest x-ray radiology reports with images and a brand new out-of-domain test-set from Stanford University.\n\n\nThe emphasis of the workshop is on 1) the use multimodal input (for mimic-cxr) 2) the evaluation of the submissions on the factual correctness of the summary using new metrics.\n\n\nAll participants will be invited to write a paper describing their solution that will be included in the Proceedings of the 22st Workshop on Biomedical Language Processing.\n\n\n*IMPORTANT DATES*\n\n\n* Releasing of hidden test-sets: April 6th, 2023\n* System submission deadline: April 26th, 2023\n* System papers due date: May 1st, 2023\n* Notification of acceptance: June 1st, 2023\n* Camera-ready system papers due: June 13th, 2023\n* BioNLP Workshop Date: July 13th or 14th, 2023\n\n\n*ORGANIZERS*\n\n\nJB Delbrouck, Stanford University\nMaya Varma, Stanford University\nPierre Chambon, Stanford University\nCurtis Langlotz, Stanford University\n\n\n*MORE INFORMATION*\nFor more information, please visit our website: \nhttps://vilmedic.app/misc/bionlp23/sharedtask/\nPlease also join our google-group: \nhttps://groups.google.com/g/bionlp23-radiology-summ/\nFor any questions, please contact me at \njbdel@stanford.edu",
"date": "2023-04-10"
},
{
"vote": 0,
"title": "How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3",
"text": null,
"date": "2023-04-10"
},
{
"vote": 23,
"title": "Why Do You Think a Model Like GPT-4 Works So Well in non-English Languages?",
"text": "I don't know what other \"3rd party data\" GPT-4 was trained on, but I wonder if part of the success of these models is that languages \nwhen projected into a large embedding space are fairly isomorphic\n.  (Generally) humans have universal aspects: senses, family structures, they are diurnal, etc.  There's variation in how you apologize or express social closeness in each language, but human beings have to do a lot of similar things like apologize or express social distance. \n\n\nThere's some direct evidence of this in language pragmatics, where an \nEnglish rhetorical taxonomy\n that is useful for many NLP applications, has all been bootstrapped \ninto Arabic\n and functioned well for clustering. The idea is that the sociocultural moves we make in English (sounding more or less certain, expressing emotion, distinguishing between the abstract and concrete) are made in all languages--different at the lexical level but the same at the lexicogrammatical (there's a Russian version that works well in testing but there's no publicly available papers on it).\n\n\nWhat's your take on this?",
"date": "2023-04-09"
},
{
"vote": 2,
"title": "I developed a Chrome extension that enables natural language processing for Google Calendar.",
"text": "Let me know what you think!\nLink: \ntaction.upfazt.com/",
"date": "2023-04-09"
},
{
"vote": 8,
"title": "Brainstorming // Semantic similarity of text",
"text": "Hey everyone, as part of a project (data cannot be disclosed) I am comparing around 5000 sentences by their semantic content. The aim is to find pairs / groups of sentences with an exact or almost exact content. The sentences are all very domain heavy and often contain similar words in different contexts. Creating tf-idf vectors and grouping with cosine similarity works somewhat okay. Word frequency clustering did dnot work well, as many of the domain specific works are dominating on every cluster and only 2-3 real clusters are being created in result. \n\n\nDue to the data being german I am kind of limited in the creation of embeddings via transformers. \n\n\nDo you have any idea that goes further than the cosine similarity approach?\nCan you recommend any clustering approach apart of HDBScan / t-SNE?\nDo you have any idea in regards to creating embeddings? \n\n\nThanks for the discussion!",
"date": "2023-04-09"
},
{
"vote": 0,
"title": "Research ideas that combine software engineering and NLP",
"text": "I need research ideas that combine software engineering and NLP. What are the current hot topics to do research on?",
"date": "2023-04-08"
},
{
"vote": 3,
"title": "Need help with sentiment analysis",
"text": "Iâ€™m working on a college project, weâ€™ve been assigned a task of sentiment analysis for tweets. Iâ€™ve got the raw dataset, Iâ€™ve assigned negation scores using sentimentIntesityAnalyser and Iâ€™ll apply a classifier later. How do I increase the negation score for sentences that have negative words with repeated letters like â€œsadddâ€, â€œangryyyyâ€?",
"date": "2023-04-08"
},
{
"vote": 9,
"title": "Developing a better understanding of the tools available.",
"text": "Hi everyone. I am rather new to NLP - I do understand the basics (word embeddings, transformers, RNNs, Named entity extraction, Sentiment analysis etc) and I am looking for resources to learn more about different applications of the tools available right now.\n\n\nTo give some context : I've chanced upon this series of lectures : [NLP and CSS 201](\nhttps://www.youtube.com/watch?v=sUtthdcPyhc&t\n) and found their approach in using some of the available tools rather novel (e.g. development of phrase bert, using part of speech tagging and searching through strings with those tags for information retrieval.).\n\n\nIf you could be so kind as to share some resources you have found, in relations to how different people have applied NLP techniques I'll be ever so grateful.\n\n\nThanks for reading, and happy holidays.",
"date": "2023-04-08"
},
{
"vote": 1,
"title": "How to evaluate Spacy NER model?",
"text": "Extended question: Is entity level or token level evaluation better?",
"date": "2023-04-08"
},
{
"vote": 1,
"title": "Project suggestion",
"text": "Dear community,\n\n\nI have a kind request to suggest me some project ideas for my NLP with Python course. I have looked for quite amount of post in this sub, but answer are all already ready services. What I need is writing codes based on topics all concepts mentioned on that book. Grading is based on usefulness, uniqueness(new idea) and writing some more code than importing a model and printing out results.\n\n\nthanks!",
"date": "2023-04-07"
},
{
"vote": 1,
"title": "Incorporating NLP into Keyword Count Script",
"text": "I'd first like to apologize for my ignorance, as my knowledge with AI/ML as a whole is foundational at best, even worse with NLP.\n\n\nBackground:\n\n\nI have a list of 400+ keywords that are clustered into categories. The script scans through lengthy documents and keeps track of the occurrences of the keywords to be outputted for some visualizations. To be more specific on the implementation, the PDF documents are converted to plaintext, read, and the keywords are being identified through regex.\n\n\nGoal:\n\n\nThe output is highly dependent on the quality of the keywords. Manually adding in slight variations of words is a never ending goose chase, so this is where I believe NLP can come in. The script definitely will need reworking since regex wouldn't make sense anymore if the goal is to dynamically identify synonymous words. I'm not too worried about that at the moment. My question is are there any simple tools I can leverage to read text and identify synonymous words?\n\n\nExample:\n\n\nI have a category, Data & Analytics. Some words/phrases are \"analytics\", \"data analytics\", \"analytics solutions\". Let's say the phrase \"big data\" isn't in the keyword list but exists in the document. I would like that to be picked up and added to the count of occurrences.",
"date": "2023-04-07"
},
{
"vote": 8,
"title": "AI course vs NLP course?",
"text": "Hello! I am an undergrad and will be graduating soon, and I was wondering if anyone could give me any guidance for choosing between an Artificial Intelligence Course or a Natural Language Processing Course?\n\n\nI took a Machine Learning course during the winter quarter and I really enjoyed it, so I signed up for both Artificial Intelligence and Natural Language Processing for Spring quarter, since this is the only quarter either of them will be offered before I graduate. But due to personal issues I am dealing with I donâ€™t think I will have enough time to put work into both and I will probably have to drop one of them. So I am wondering which course would be more useful?\n\n\nMy original plan post graduation was to work as a software developer, but I never really found a field I was super passionate about when taking my different CS electives. But after taking the ML course I really enjoyed it and I am thinking a career in ML may be a possibility. Though, currently I do not plan on getting a Masters or PHD (if at all), so after graduation I do plan on finding a software developer position until I find a path I am more interested in (since I need the income).\n\n\nThe AI course at my University is general and focuses on classical AI theory, such as Search algorithms, Probabilistic Graphical models, Markov Decision Processes, Reinforcement Learning Algorithms, etc. And I know the NLP class goes into more modern topics and will be a lot more focused, covering HMMs and MEMMs, Conditional Random Fields, Deep learning for NLP, transformers, etc.\n\n\nI am wondering if it would be more beneficial to learn about the classical AI approaches, or the more modern NLP topics that are rapidly changing (and might possibly be less relevant in the future as different advancements are made?)? And since I probably can only take one of them, which would be better to have on my resume as relevant coursework?\n\n\nThank you for any input!\n\n\nEdit: Also, since Iâ€™ve been told that my Universityâ€™s AI course focuses on more classical AI topics, I was wondering if the topics in the AI course will still be relevant in the future?\n\n\nEdit: These are the topics covered in the two classes:\n\n\nAI:\n\n\n\n\nIntelligent agents\n\n\nMachine Learning\n\n\nProbabilistic Graphical models\n\n\nSearch: Solving Problems with Search, Search algorithms, Games and Adversarial Search\n\n\nRL: Markov Decision Processes, Bandits problems and Exploration, Reinforcement Learning algorithms\n\n\nLogic intro & Propositional logic\n\n\nFirst order Logic\n\n\n\n\nNLP:\n\n\n\n\nBasic text processing\n\n\nN-grams & language models\n\n\nVoted perceptron and logistic regression\n\n\nPart-of-speech tagging: HMMs\n\n\nMEMMs\n\n\nConditional Random Fields\n\n\nDistributional semantics: sparse representation and dense representation\n\n\nLanguage and Vision\n\n\nMachine translation & Question Answering\n\n\nDeep learning for NLP: MLP, RNNs. LSTMs, CNNs.  Transformers, GPT Models\n\n\nDialog Systems",
"date": "2023-04-07"
},
{
"vote": 0,
"title": "The Socratic method as a means for generating knowledge about GPT4 and LLMs",
"text": "Hello r/LanguageTechnology\n\n\nI've written what I think is an interesting essay that explores some concepts that you all would either find interesting, or be able to poke interesting holes in.  I hope you'll find that, at least, this is a more productive discourse around LLMs than what is on most of reddit.\n\n\nHere is the thesis, in brief\n\n\n>If semantic and pragmatic understanding are a property of language itself, and these large language models can approximate language well enough, then we can generate new semantic and pragmatic understanding by engaging with them in basically any domain that has a language they can approximate.  I seek to demonstrate this by engaging it philosophically, using a form of the Socratic method.\n\n\n>I believe this is valuable, because the existence of these large language models as a math equation for language is physical evidence about the nature of the mathematical information carried in language. Until we have the new math to explore those concepts rigorously in that domain, we can only engage with the new math in ways like this - philosophy!\n\n\n>Additionally, since the Socratic method is fundamentally an approach that invites the reader to explore the limits of knowledge using their own understanding of language, I think its fitting that we begin with this approach to interact with this fundamentally new math  With the perspective that what the Socratic method \"is\" is simply posing a question and letting the laws of the universe write back to you in a causal fashion, there is no importance in whether your response is in the form of human thought (a consequence of the universe) or in the form of a machine computation (a consequence of the universe).  They are the same, and therefore have the same philosophical value.  I am confident that Socrates himself understood this, would agree with me, and would find that this is the right approach to take. \n\n\nWhat do you all think?  The remaining 33 pages are an unedited Socratic dialogue between \"You\" and ChatGPT4, without comment, in which I attempt to prove that knowledge seeking is universally moral by making ChatGPT4 say it for \"You\".\n\n\nNeat right?",
"date": "2023-04-07"
},
{
"vote": 1,
"title": "TextScribe - A Python Library for Efficient Data Processing of CSV, JSON, and TXT Files",
"text": "[removed]",
"date": "2023-04-07"
},
{
"vote": 29,
"title": "GPTCache: A semantic cache for LLMs",
"text": "As much as we love GPT, it's expensive and can be slow at times. That's why we built GPTCache - a semantic cache for autoregressive LMs - atop Milvus and SQLite.\n\n\nGPTCache provides several benefits: 1) reduced expenses due to minimizing the number of requests and tokens sent to the LLM service, 2) enhanced performance by fetching cached query results directly, 3) improved scalability and availability by avoiding rate limits, and 4) a flexible development environment that allows developers to verify their application's features without connecting to the LLM APIs or network. Come check it out!\n\n\nhttps://github.com/zilliztech/gptcache",
"date": "2023-04-07"
},
{
"vote": 6,
"title": "OCR output missing some words",
"text": "hey there,\n\n\nCurrently I am working on extracting text from resumes and I am using pytesseract.\n\n\nIt is giving good results but when it comes to skills some resumes contain each skill in a small box.\n\n\nIn this case the skills are not detected.\n\n\nany suggetions of which image preprocessing should i use.\n\n\nThanks in advance.",
"date": "2023-04-05"
},
{
"vote": 5,
"title": "Computational linguistics masters",
"text": "Hi! \n\n\nI'm currently studying in my first year of a two year linguistics masters in France and as much as I'm enjoying it, there are no NLP modules. We do statistics and use R but nothing beyond that. In my own time I've developed are really strong interest for it and have been teaching myself to use python and reading as much as I can to the point where I've decided I would prefer to switch to a masters in computational linguistics. Since I started studying a bit later and I have already done one year of a masters program, ideally I would prefer to do a one year masters course but I am open to two years two. I have found one at Stockholm university and have read good things about some programs in Germany but I was wondering if anyone could give me a bit more insight/advice/tips for applying? I think my background in humanities put me off applying for a comp ling masters as I was worried I wasn't eligible but now I realise I made a mistake since I spend all my free time studying it and wishing I had more formal teaching in it!",
"date": "2023-04-05"
},
{
"vote": 2,
"title": "Mitigate the context size limit in LLMs with self chat",
"text": "The following idea seems like an obvious thing to try; please point out further discussion or research results or help by explaining why it has no merit.\n\n\nTldr\n\n\nDuring training let the model first produce n tokens that are prepended to the input after which the combined set (self generated tokens + inputs tokens) is used for prediction.\n\n\n&#x200B;\n\n\nProblem\n\n\nAfaik every large language model is restricted to a hard limit of tokens as input to solve the training task.\n\n\nAt each training step the next token must be predicted using the limit amount of preceding tokens.\n\n\nTokens that precede however close or far from the start of the sliding input window cannot be used by the model to assist in solving the task.\n\n\nConsider the context sizes of gpt: gpt3 2048 and gpt4 comes in 8k or 32k versions.\n\n\nIn other words the model cannot access information that was presented 'long ago' at the 'start' of the stream, other than encoding this information in parameter space.\n\n\n(which may actually help 'general knowledge' representation, forced by the bottleneck of limited context size)\n\n\n&#x200B;\n\n\nSolution\n\n\nEver increasing context sizes are obvious, we can see this happening, but they are limited by the accelerator (gpu) memory size.\n\n\nA self generated 'carry over' step may be a more general approach, a sort of 'self prompt tuning' or akin to the carry over in LSTM cells or residual intra layer connections in DNNs.\n\n\nRelevant information that comes before the limited window context can now be used by the model as input to solve the training task if it discovers how to represent it to let it carry along.\n\n\n&#x200B;\n\n\nModification\n\n\nreserve the first n < limit tokens of the context as the carry over buffer (cob)\n\n\ninitialize the cob with null tokens\n\n\nfor step in n_steps:\n  take the next limit - n input tokens\n  prepend the cob to obtain the full input context\n  predict the next output token and calculate the loss\n  backpropagate\n  generate the cob for the next step\n\n\n\nCons\n\n\nlonger training times as each step requires n inferences (forward steps) to fill the carry over.\n\n\n&#x200B;\n\n\nPros\n\n\nNot limited by gpu memory anymore.\n\n\nWith a loss component that forces the cob contents to be indistinguishable from english we can study and understand how the model 'talks to itself' in order to better\n\n\nsolve the prediction task. This understanding can then be used to enhance our use of prompt tuning.\n\n\nFor example the model may learn to use a particular vocabulary to help remind itself that the current context is meant to be fictional or factual.\n\n\nThat would be particularly helpfull, who knows.\n\n\n&#x200B;\n\n\nThanks, Esjay.",
"date": "2023-04-04"
},
{
"vote": 6,
"title": "Title: AI voice bot app: Impersonating Andrew Tate post-prison release - ethical dilemma?",
"text": "[removed]",
"date": "2023-04-04"
},
{
"vote": 2,
"title": "Integrate NLP into a web-application",
"text": "My company has a SaaS-based web app. We intend to add NLP to automate a few tasks based on the user query.\n\n\nFor example, if a user types \"I need a new sheet\", it should create a new sheet in our system. This is a very simplified example, the queries can be more complex as well as the action items to perform in the system.\n\n\n&#x200B;\n\n\nThe problem is we (our team) are well-versed in web app technologies and not at all in AI-related tech.\n\n\nBut we don't simply want to use out-of-the-shelf paid subscription-based service to achieve this.\n\n\n&#x200B;\n\n\nThe motive is to learn and build alongside. What should be the starting point for the same?\n\n\nAnd what particular libraries can we look forward to use to achieve the use case mentioned above?",
"date": "2023-04-04"
},
{
"vote": 5,
"title": "Sentiment analysis using the Sine Waves",
"text": "I have created a sentiment analysis, which allows you to visualize in a quick and easy way, the sentiments of any text you give it, using a Microsoft Text Analytics model, and using the Sine Waves JavaScript library. You can find the code here: \nhttps://github.com/AngelOfParadox/SpaceVibes\n\n\n This is the first version, but I would love to know if someone has another proposal, in the future I would like to add books and large texts. \n\n\nhttps://reddit.com/link/12b4rhu/video/c7z0jir4rrra1/player",
"date": "2023-04-04"
},
{
"vote": 1,
"title": "LANGUAGE LEARNERS: How is it going!",
"text": "[deleted]",
"date": "2023-04-03"
},
{
"vote": 2,
"title": "Synthetic data, its types, techniques, and tools",
"text": null,
"date": "2023-04-03"
},
{
"vote": 0,
"title": "Large Language Model - Fireside chat!",
"text": "Are you ready to ignite your understanding of \nlarge language models in search\n? Join us for a fiery Fireside Chat featuring industry experts \nManisha Arora\n, and \nJayant Kumar\n! Discover the latest insights and trends in large language models and how they impact search algorithms. O\n\n\nur experts will share their knowledge and expertise on this cutting-edge technology. Don't miss out on this must-attend event! Register now to secure your spot and join us for a captivating conversation that will ignite your passion for large language models and search. See you there!\n\n\nhttps://www.linkedin.com/events/firesidechaton-largelanguagemod7047243160284004352/",
"date": "2023-04-03"
},
{
"vote": 2,
"title": "Size attribute extraction from product description",
"text": "I am working on Amazon catalog data . My task is to extract size attribute ( 5 inches, 10cm X 15cm etc) from other information available for the product . What approach should I take to achieve this ?",
"date": "2023-04-02"
},
{
"vote": 13,
"title": "How does \"next word prediction\" sample the next token?",
"text": "[deleted]",
"date": "2023-04-01"
},
{
"vote": 6,
"title": "T5 quantization help",
"text": "Hi, I need some help regarding t5 quantization. Ive used dynamic quantization from pytorch by only quantizing linear layers to int8 but the param dtype for model is still fp32, tried fastt5 as well no inference time change and no dtype change there as well, pls help. Upvotes appreciated.",
"date": "2023-04-01"
},
{
"vote": 1,
"title": "Funding for the development of language resources in the European Union",
"text": null,
"date": "2023-03-31"
},
{
"vote": 7,
"title": "LLM Fine Tuning Guide?",
"text": "Having difficulty finding a good guide/template for fine tuning a model (let's say a Cerebras) on my domain data--any suggestions for one?",
"date": "2023-03-31"
},
{
"vote": 1,
"title": "Seeking African data labeling referral",
"text": null,
"date": "2023-03-31"
},
{
"vote": 8,
"title": "for comparing the semantic distance of different words, what word embedding should I choose in 2023?",
"text": "Context: I had a relatively small corpus like The New York Times Annotated Corpus devided into different years so that I should have a few sub-corpus with less text.\n\n\nP.S. NYT corpus have over 1.8 million articles written in 20 years.",
"date": "2023-03-30"
},
{
"vote": 1,
"title": "How to use OPENAI API for Question Generation from specific documents",
"text": "My objective is to upload a set of documents, extract their embeddings, and then generate \nmultiple-choice questions\n based on specific criteria related to the document content. \n\n\n- I have a limited set of documents that will not be updated soon, so I need to optimize the cost. I am familiar with the concept of vector databases and would like to know if there is a free version of such a solution available. \n\n\n- When generating questions from text, I am unsure whether to refer to the chatGPT API or davinci. \n\n\n- I also need to know how to proceed if I want my questions to follow \nspecific criteria\n where distractors are semantically close to the correct answer. Would finetuning help in this case, and under which objective function? \n\n\n- If I have a large set of high-quality questions written in the format I aim to achieve through the LLM, can I feed them to GPT, fine-tune it on a particular objective function, and obtain better results?\n\n\n- Should I stick with prompt engineering, considering that GPT-3/4 already has consistent world knowledge? If so, how can I prompt engineer at scale? I would love to discuss these points in-depth since I do not have anyone in my close circle with extensive knowledge of these subjects.",
"date": "2023-03-30"
},
{
"vote": 14,
"title": "Getting into NLP from a standing start",
"text": "Let's say I wanted to get started in NLP without any kind of computing background, and only a smattering of teaching-based linguistics knowledge. Is it at all realistic?\n\n\nI have been teaching English in Madrid for 20 years, and things are only getting worse in terms of pay. A client, who is very high up in one of the country's biggest financial sector organisations, (in charge of their digital transformation project, reports to the board, working on company-level AI integration) recommended unprompted that I look into this area as suited to my skills. I have no idea if this is realistic so thought I'd ask Reddit.\n\n\nThanks for any input at all.",
"date": "2023-03-30"
},
{
"vote": 0,
"title": "Learning prompt engineering for someone non-technical",
"text": "[deleted]",
"date": "2023-03-30"
},
{
"vote": 0,
"title": "Some useful language learning apps you may not know yet",
"text": "Duolingo, Cake, ELSA are all too familiar with the Vietnamese market, but do you know that foreign language learning apps are equally \"cool\" in this international market?\n\n\nA. EWA\n\n\n\n\nFor: Basic and intermediate level, need to learn vocabulary, like to read stories or listen to audio books.\n\n\nUnique features:\n\n\nThe collection of stories contains many classic foreign literature, which makes it suitable for those who like literature, especially classical one.\n\n\nYou can read and listen to the audio at the same time. The voice is automatic but I find it acceptable.\n\n\nClick to look up words always to be able to read and understand the content of the story, so it helps to reduce the fear of reading.\n\n\n\n\nDownload EWA on iOS: \nhttps://apps.apple.com/us/app/ewa-english-language-learning/id1200778841\n\n\nDownload EWA on Android: \nhttps://play.google.com/store/apps/details?id=com.ewa.ewaapp&hl=en&gl=US\n\n\nâ€”--â€”--â€”--â€”--â€”--\n\n\nB. EJOY EPIC\n\n\n\n\nFor: beginners and intermediate, focusing on communication in everyday life.\n\n\nUnique features:\n\n\nHave specific courses for each level with detailed roadmaps, suitable for learners at basic level, don't know where to start and how to learn.\n\n\nEach lesson is a short video from real communication situations. The App will list out phrases and grammar + practice exercises in many different situations for those phrases.\n\n\nAll answers have an explanation of why it's right or why it's wrong. This is the part I appreciate the most about eJOY Epic, because knowing why you made the wrong choice will help you better understand that part of the grammar or structure. Don't skip this part while studying.\n\n\n\n\nDownload EPIC on iOS: \nhttps://apps.apple.com/vn/app/ejoy-epic-english-courses/id1622797145\n\n\nDownload EPIC on Android: \nhttps://play.google.com/store/apps/details?id=com.ejoy.epic&hl=vi\nâ€”--â€”--â€”--â€”--â€”--\n\n\nC. ROSETTA STONE\n\n\n\n\nFor: Basic level, focus on communication goals, like to learn with flashcards, highly self-taught\n\n\nSpecial point:\n\n\nThis app learns the way you learn our native language, focus on using voice flashcards with vivid images and limited explanations, which will be more suitable for those who like to explore languages on their own.\n\n\nHave common conversation phrases to practice listening - speaking. There are audio books with lots of communication practice content in many contexts.\n\n\nThere are pronunciation and grammar training courses and online classes with native teachers (with fee).\n\n\n\n\nDownload Rosetta Stone on iOS: \nhttps://apps.apple.com/us/app/rosetta-stone-learn-languages/id435588892\n\n\nDownload Rosetta Stone on Android: \nhttps://play.google.com/store/apps/details?id=air.com.rosettastone.mobile.CoursePlayer&hl=en&gl=US",
"date": "2023-03-30"
},
{
"vote": 1,
"title": "Online, April 19, 2023: NLP Technology for Enterprise Legal Applications",
"text": "Online, April 19, 2023: NLP Technology for Enterprise Legal Applications, presented by Emad Elwany, CTO and co-founder of \nLexion\n. Info & registration: \nhttps://www.meetup.com/ny-nlp/events/292337698/",
"date": "2023-03-30"
},
{
"vote": 0,
"title": "step-by-step tutorial on how to generate synthetic text based on real named entities using ChatGPT",
"text": null,
"date": "2023-03-29"
},
{
"vote": 0,
"title": "Introducing Prompt Hackers: A New Community for AI Enthusiasts, Developers, and Hackers ðŸ”§ðŸ¤–",
"text": "Hello fellow NLP/AI enthusiasts!\n\n\nI'm excited to announce the launch of a new community site called Prompt Hackers (\nhttp://prompthackers.xyz\n), dedicated to AI enthusiasts, developers, and hackers who are passionate about creating innovative solutions using prompt engineering techniques.\n\n\nPrompt Hackers aims to bring together people who are eager to learn, share, and collaborate on projects involving prompt engineering and AI.\n\n\nHere's what you can expect from Prompt Hackers:\n\n\nðŸš€ \nLearning Resources\n: We will provide resources, articles, and guides on prompt engineering techniques, AI model fine-tuning, and more, to help you expand your knowledge and skills.\n\n\nðŸŒ \nCollaboration Opportunities\n: Connect with like-minded individuals, form teams, and collaborate on AI projects. Grow your network and make lasting connections in the AI community.\n\n\nðŸ’¡ \nDiscussions and Expert Insights\n: Engage in meaningful discussions on AI trends, prompt engineering techniques, ethics, and best practices. Benefit from the knowledge and expertise of industry professionals and academics.\n\n\nJoin us today at \nhttp://prompthackers.xyz\n, and help us shape the future of AI!\n\n\nFeel free to leave any questions or suggestions in the comments below, and I'll be happy to address them.\n\n\nHappy hacking! ðŸš€ðŸ¤–",
"date": "2023-03-29"
},
{
"vote": 1,
"title": "Build Embeddings from Top 10 Terms Only?",
"text": "I have a two-fold question, conceptual and technical.\n\n\nConceptual: Building embeddings for every word in a document is a task that requires a lot of time/resources. If I wanted to build embeddings just for SOME of the words in the document, for example the most salient words based on TF-IDF, would it be sensible to do that? I think the answer to this question will take one of two forms:\n\n\n\n\nNo, that is not sensible. You must build embeddings for each word in the sentence to build an embedding for the particular word in question. Without first building the embeddings for each previous word in its context BERT can't build the embedding for the word in question. Each embedding influences the others. It's not sensible to build only a few in isolation.\nOR\n\n\nYes, that is sensible. You can just use the pretrained embedding already available in the BERT model and build your particular word's embedding based on those standard embeddings for that word and the words surrounding it in its context. You don't have to refine the pretrained embedding for every word. You can still get a reasonable amount of context information from those pretrained embeddings.\n\n\n\n\nTechnical: Using SparkNLP the way to build BERT embeddings is to create a pipeline that has a DocumentAssembler, SentenceDetector, Tokenizer, BertEmbeddings.pretrained, EmbeddingsFinisher and Pipeline. Given that the pipeline is set up like it is and we don't see the intermediate steps (for example, the pipeline doesn't output a list of tokens found by the tokenizer before moving on to the next step in the pipeline), if I wanted to build embeddings just for SOME of the words in a document how would I go about doing that? Maybe someone has already done this somehow and some example exists somewhere. I just need to know how to break in to the middle of the pipeline and remove all but the salient words before embeddings are created, thereby eliminating 95% or so of the tasks to be done.\n\n\nThanks for your input!",
"date": "2023-03-28"
},
{
"vote": 5,
"title": "Using a pre-trained Sentence Bert , how can we make it understand the context according to our use case. Like trying to work on Resumes, so the embeddings of data scientist and development lie nearer.",
"text": null,
"date": "2023-03-28"
},
{
"vote": 15,
"title": "Cerebras Open Sources Seven GPT models and Introduces New Scaling Law",
"text": null,
"date": "2023-03-28"
},
{
"vote": 1,
"title": "Custom Dictionary",
"text": "Hello Gurus!\nI am trying to look for (2) custom dictionaries for words like imposter, fraud, fake, and one for belong, community, included, yet no idea how to search for them.  I want to evaluate 150 text files for the words.  Suggestions?  Thank you in advance.",
"date": "2023-03-27"
},
{
"vote": 0,
"title": "Have OpenAI's ethics gone out of the window? Will OpenAI be able to sustainably achieve it's two goals of profit and benefit to humanity at the same time? They will not always go hand in hand...",
"text": null,
"date": "2023-03-26"
},
{
"vote": 1,
"title": "Starting a career in Speech AI",
"text": "[deleted]",
"date": "2023-03-25"
},
{
"vote": 3,
"title": "Deploy a huggingface classification model",
"text": "I want to classify around 50k tweets with a BERT classification model which I found on huggingface. What is the fastest way to do this? I am pretty sure the hugging face API is rate limited. Do I need to pay for an Interference endpoint? Is this worth it?",
"date": "2023-03-25"
},
{
"vote": 1,
"title": "READY FOR AI? HEREâ€™S HOW BUSINESSES ARE FARING WITH THEIR IMPLEMENTATIONS",
"text": null,
"date": "2023-03-25"
},
{
"vote": 1,
"title": "SOFFOS NLP USE CASE: New Media",
"text": null,
"date": "2023-03-25"
},
{
"vote": 0,
"title": "ChatGPT 4.0",
"text": null,
"date": "2023-03-24"
},
{
"vote": 0,
"title": "Why the EU is getting weaker and weaker relative to China and the US-it's time for a change.",
"text": null,
"date": "2023-03-24"
},
{
"vote": 1,
"title": "$OP Airdrop 5m tokens! | Optimism",
"text": null,
"date": "2023-03-23"
},
{
"vote": 1,
"title": "$OP Drop | Phase 2 right now! | Optimism",
"text": "[removed]",
"date": "2023-03-23"
},
{
"vote": 2,
"title": "Model Selection for Fine-Tuning",
"text": "I am working on a project that involves text-generation. I have completed my data collection and preprocessing, and would like to fine tune a model to generate text similar to my dataset. This means that I need a decoder model such as gpt-x, llama, etc.\n\n\nTo save costs on testing the idea out, I would like to train a model locally before I experiment with fine-tuning apis/training on the cloud. Here are my current specs:\n\n\nCPU: Ryzen 5 5600\n\n\nRAM: 16 GB (willing to upgrade)\n\n\nGPU: RTX 3060 12GB\n\n\nWhat is the largest model that is reasonable to be fine-tuned on my computer? How would someone go about determining that?\n\n\nI am also familiar with fine-tuning using 8-bit mode or something like LORA. Using one of these methods, what is the largest model I could fine-tune?\n\n\nIf it helps, my dataset is ~400MB of text. The text is structured, and I need the model to also learn that structure properly.",
"date": "2023-03-23"
},
{
"vote": 1,
"title": "Revolutionizing AI Interactions: Unlock the Potential of Prompt Engineering",
"text": null,
"date": "2023-03-22"
},
{
"vote": 10,
"title": "best way to do Topic modeling for short texts?",
"text": "Hello, \n\n\nwhat's the best topic modeling technique to identify topics in short texts? I have a data set where textes are composed of around 10 to 60 words ! I tried LDA, TopicBERT and GDSMM. the results were all bad. My texts are in french by the way.",
"date": "2023-03-22"
},
{
"vote": 5,
"title": "Writing Advice",
"text": "Hi all! Not sure if this kind of post is welcome here, but I work as a technical writer for a company called Soffos, that's trying to make its own NLP for businesses. I wrote this article comparing Soffos AI to ChatGPT. Check it out and let me know what you think!\n\n\nI'm studying to be an AI engineer (currently doing a BSc) so I'm not all the way there. Any input would be welcome. :)\n\n\nSOFFOS vs ChatGPT",
"date": "2023-03-22"
},
{
"vote": 2,
"title": "Txtai RuntimeError: failed to import",
"text": "I made a semantic search engine using txtai and it has stopped working. So Iâ€™m wondering if it is to do with package versions. Any advice would be brilliant\n\n\nerror and line where it failed",
"date": "2023-03-22"
},
{
"vote": 5,
"title": "How do we find Values in Attention, or do we need them at all?",
"text": "Hi everyone, I'm a bit confused about value(v) parameter in attention. At some posts after calculating attention scores they don't introduce any value(v) parameter. Sometimes they just copy and use the same values as key parameter.\n\n\n&#x200B;\n\n\nI'm watching the lecture: \nhttps://www.youtube.com/watch?v=0PPzD4mxpuM&list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z&index=7\n\n\n&#x200B;\n\n\n1st part: We calculate attention scores. For this Seq2Seq model we take each encoder state as query vector. We take each decoder state as key vector. We choose a attention score function and calculate attention.\n\n\nhttps://imgur.com/a/HcameRc\n\n\n&#x200B;\n\n\n2nd part: Point I get confused. We treat the value vectors same as key vectors and we do a multiplication with the attention scores if I understood correctly.\n\n\nhttps://imgur.com/a/S7lYDGl\n\n\n&#x200B;\n\n\nI understand different attention papers implement differently. \nBut if we're going to use the value vectors same as key vectors why do we need it in the first place?\n \n\n\nI've been trying to understand this key, query, value triplet. Read papers, posts, implementations. The database analogies but I couldn't get the intuition behind. I would be appreciated to any insights.",
"date": "2023-03-22"
},
{
"vote": 1,
"title": "Get free $OP | Optimism | Airdrop",
"text": null,
"date": "2023-03-21"
},
{
"vote": 9,
"title": "Is there any value in continuing research on LSTM behavior?",
"text": "I've been doing research on the generalization capacity of LSTMs (e.g. stratifying training data based on different factors and seeing effects on performance) and although I've observed some interesting results, I can't help but just feel defeated at this point since now Transformers Are All We Need. My advisor has said that there is still value in LSTM research that might be of interest to the cognitive science community if we treat LSTMs as a cognitive model of language learning, but honestly the remarkable capacity of LLMs for humanlike language generation has me doubting even that. I want to believe there are reasons to keep going but it's been difficult to make a case for it, and it's hard for me to transfer the work I've been doing to transformers because with a large enough model we don't even observe these kinds of effects at all, or they just pattern with human behavior.",
"date": "2023-03-21"
},
{
"vote": 15,
"title": "[R] SPDF - Sparse Pre-training and Dense Fine-tuning for Large Language Models",
"text": null,
"date": "2023-03-21"
},
{
"vote": 6,
"title": "Are there any pretrained sentiment analysis models that grade sentiment along a gradient?",
"text": "I'm not sure if I've phrased my title correctly, but what I mean is, the majority of the models I've found tend towards the extremes. For example,\n\n\n>You're an asshole\n\n\nmight be a -0.99 and\n\n\n>Fuck you, I hope you fucking die you piece of shit\n\n\nis a -1, despite there being a significant difference in the intensity of the negative sentiment. Are there any pretrained models where the score that the model outputs doesn't just show the sentiment, but also the intensity of the sentiment?",
"date": "2023-03-21"
},
{
"vote": 0,
"title": "AI Apocalypse: A Psychoanalysis of Reality",
"text": null,
"date": "2023-03-21"
},
{
"vote": 5,
"title": "Is there any literature or courses on how to build datasets from scratch to train language models?",
"text": "Hi, everyone! I'm looking to get better at creating datasets to train/fine-tune different language models (mostly Transformers) for very specific tasks. I'm currently putting together a dataset from different social media sources and tagging it manually, and throughout the process my team and I had to make a lot of choices that were more guided by instinct than by theory.\n\n\nTherefore, I would be interested in any book/course that covers one or more of the following (or other relevant) topics for dataset creation:\n\n\n&#x200B;\n\n\n- How to determine which data sources to use.\n\n\n- How to access the data I need (and automation if possible).\n\n\n- How to check for biases.\n\n\n- How to balance the dataset for different tasks.\n\n\n- Tagging techniques/tools.\n\n\n- Good practices/industry standards.\n\n\n- Any other topic you consider important or key for this task.\n\n\n&#x200B;\n\n\nThanks in advance to all! Looking forward to reading from all of you.",
"date": "2023-03-21"
},
{
"vote": 1,
"title": "Am I using the wrong tool here (expanding word lists with all-mpnet-base-v2)?",
"text": "I built a little python script that is supposed to help pick out obscure vocabulary words in a given text. I have an array of about 2,000 common English words. In order to expand this list to include plurals, different tenses, etc Im using  sentence_transformers and nltk.tokenize and all-mpet-base-v2 model, but itâ€™s not really working as I intended. Itâ€™s missing a lot. my list pretty much just works as a case sensitive list. Moving the threshold around will just shrink my output or grow it by a wide margin.  Instead of having a super regex tool, I think Iâ€™m just excluding words that are kind of synonyms of things on my list.",
"date": "2023-03-21"
},
{
"vote": 1,
"title": "Forget a memory with a metaphor question",
"text": "[removed]",
"date": "2023-03-21"
},
{
"vote": 0,
"title": "MaMi ðŸ’„ Your Personal Assistant",
"text": null,
"date": "2023-03-20"
},
{
"vote": 8,
"title": "Where does the input sentence length is dealt with in a transformer?",
"text": "Someone understands why the vector size still depends on the sequence length after the positional encoding in a transformer? I thought it needed to be independent from the sequence length at one point?",
"date": "2023-03-20"
},
{
"vote": 1,
"title": "Translate a meeting",
"text": "Hi Everyone,\n\n\nI need to translate a recording of a meeting of 2 hours where Dutch and French are spoken. I speak Dutch but I don't speak French. What is the best voice translator for longer spoken texts? Does anyone have tips?",
"date": "2023-03-20"
},
{
"vote": 12,
"title": "Modern Topic Modeling/Discovery",
"text": "I was wondering what are the modern techniques for topic discovery for short and long text. It seems this topic to be slower advancing compared to the rest. I am aware of bertopic but tbh I always have issues finetuning it. \n\n\nOn a second thought I was thinking to use qna/chat gpt models in order to generate models, so I wanted to ask your opinion on some potential prompts that I could use. Essentially  a bit of brainstorming. I will open source all the gathered ideas along with mine and share the link here :)",
"date": "2023-03-19"
},
{
"vote": 0,
"title": "ChatGPTâ€™s Potential To Eliminate Jobs Scares OpenAIâ€™s Founder Sam Altman",
"text": null,
"date": "2023-03-19"
},
{
"vote": 3,
"title": "CMU MS in LTI admit profile",
"text": null,
"date": "2023-03-19"
},
{
"vote": 36,
"title": "Are pre-trained word embeddings (word2vec, glove, fasttext) obsolete now? given wide use of pre-trained languages models like bert etc",
"text": null,
"date": "2023-03-19"
},
{
"vote": 12,
"title": "Wanna team-up for Quantum NLP projects?",
"text": "I recently started reading about Quantum NLP. A very experimental and new field in Natural Language Processing. There are only a handful or research papers out there to aid in the knowledge of Quantum NLP, even universities such as MIT, Harvard and Stanford aren't capable or fully understand Quantum NLP yet. Only a few Quantum Computing research labs have the surface-intermediate understanding of Quantum NLP such as Cambridge Quantum.   \n\n\nI have read some of the most recent and important Quantum NLP papers and used \nlambeq the only python library capable enough to do Quantum NLP.\n Fast forward, I have implemented a basic Quantum NLP project where I classify sentences using Quantum NLP. \n\n\nI couldn't find many people who are interested in Quantum NLP, that's why, I was looking forward if someone is interested in Quantum NLP in this thread and has previous experience working with NLP itself then we can make a small team and study more advanced topics on Quantum NLP and do cool projects in our pastime. \n\n\nGitHub repo link:\n \nhttps://github.com/sleepingcat4/Quantum-NLP\n \n\n\nIf you're interested in teaming-up, kindly send me a message on \nreddit or discord: sleeping_cat4#8182",
"date": "2023-03-18"
},
{
"vote": 1,
"title": "OptÃ­mÃ­sm Takes AÃ­rdrops to the Next Level with Phase 2 - 5 Million $OP Tokens on Offer!",
"text": "Airdrop #2 from OptÃ­mÃ­sm. Claim your $OP tokens. Stay updated via our official Twitter \nhttps://twitter.com/Optimism\\_New/status/1636389029464932355",
"date": "2023-03-18"
},
{
"vote": 17,
"title": "Zero and Few Shot Text Retrieval and Ranking Using Large Pretrained Language Models",
"text": null,
"date": "2023-03-18"
},
{
"vote": 1,
"title": "GPT-4 Code gen vs GPT 3.5",
"text": null,
"date": "2023-03-17"
},
{
"vote": 1,
"title": "Optimism Takes Airdrops to the Next Level with Phase 2 - 5 Million $OP Tokens on Offer!",
"text": "[removed]",
"date": "2023-03-17"
},
{
"vote": 4,
"title": "How to Build a CHAT BOT ðŸ¤– with CONVERSATIONAL MEMORY ðŸ§  in PYTHON ( GPT-3.5-TURBO | LangChain ðŸ¦œ )",
"text": null,
"date": "2023-03-16"
},
{
"vote": 0,
"title": "Alpaca - Train Your GPT-4 for Less Than $100",
"text": null,
"date": "2023-03-16"
},
{
"vote": 17,
"title": "Train custom AI models on spreadsheet data with just a few clicks",
"text": null,
"date": "2023-03-16"
},
{
"vote": 1,
"title": "Trust at Risk: Circle Addresses Banking Issue with Compensation",
"text": "[removed]",
"date": "2023-03-16"
},
{
"vote": 5,
"title": "Experiences with a production AI/ML deployment, best practices and considerations?",
"text": "Please share your experiences and resource for setting up a production LLM system for for enterprise consumption. Thank you in advance.",
"date": "2023-03-15"
},
{
"vote": 20,
"title": "Getting into PhD Program at 30",
"text": "Hi all,\n\n\nI am interested in peoples thoughts about the possibility of going for a phD. I was looking into information studies at the univeristy of maryland in college Park, but now I am realizing I could try for more places. Basically wherever.\n\n\nSo about me. Got a BS in Physics, did teo summers  of research through NSF program. Realized I did not like the idea of sitting in a lab all day. Then got a masters in education and taught Hs for 3 years.\n\n\nRealized that was horrible.\n\n\nNow, falling back on my previous research I was able to get a job at an FFRDC (R&D center) ive been at for 3.5 years now.\n\n\nI started out with NLP, but now work mainly on data engineering tasks. I still really enjoy scraping, information extraction type tasks and have built lots of pipelines using a combination of regex and other things like NLP out of the box spacy models. \n\n\nHowever. I have been stagnating for a while now. As I started to apply to DE jobs I realize my true passion is solving very difficult information extraction problems.\n\n\nI just realized that I want to get a phD so I can solve interesting problems.\n\n\nWhere does one start? Again the Information Studies program seems really interesting as I am more interested in NLP IE applications. Part if me thinks this is not possible but I think that is not true.\n\n\nAnyways, how should I prepare for my lack of schooling in CS? Should I start polishing up my side projects? Shiuld I study for the GRE? Should I do all the above? \n\n\nAny guidance is appreciated. It may be relevant to add that for a year now I decided to quit drinking / went completely sober. I felt pretty lost in the direction of my life lately, but now it has suddenly become very clear this is what I want to do.",
"date": "2023-03-15"
},
{
"vote": 5,
"title": "UMASS Advanced-NLP",
"text": "Hi everyone. I finished the advanced nlp course from Mohit Iyyer. You can find the version I completed from here: \nhttps://people.cs.umass.edu/~miyyer/cs685_f22/\n I would definetly recommend it. You can find lectures from youtube.\n\n\nIf you want to check out and discuss assignment solutions, you can find mine here: \nhttps://github.com/uygarrr/Courses/tree/main/UMASS-CS685-Advanced-NLP\n\n\nAlso there're 2 quiz quesitons that I couldn't be sure about the answers. Let me share them here too.\n\n\n1st: Assume we are applying a Transformer sequence-to-sequence model for a conditional language modeling task (e.g., machine translation). Why donâ€™t we need to use masking in cross attention?\n\n\n2nd: Now letâ€™s say we want to probe whether or not BERTâ€™s [CLS] token has encoded the length of an input sentence. Explain how you would design a control task for this probe to address the effect of probe network complexity.\n\n\n&#x200B;\n\n\nI would be glad to discuss these and other material. I'm open to course recommendations too. \n\n\nHappy learning :)",
"date": "2023-03-15"
},
{
"vote": 6,
"title": "Is it possible to eventually have a career in computational linguistics without a relevant degree?",
"text": "As the title says, I'm curious about the possibility of career in the field, but my academic background is doesn't match. I have a bachelors in international business and I just started working in data analytics (mostly HR data). Based on what tools my team is using, I will have practical experience using SAS, Python, R, along with html, css, javascript (d3). \n\n\nI'm wondering if i were to apply to computational linguistics jobs in the future, would I even be considered without the relevant academic background? \n\n\nWhile I am open to going back to school, I'm wondering if it possible gain the relevant knowledge and experience through self-study and my current job.",
"date": "2023-03-15"
},
{
"vote": 3,
"title": "End-to-end knowledge generation",
"text": "Hi all,\n\n\nI am currently working on generating knowledge graphs end-to-end on unstructured text. My job involves a lot of different domain texts in Dutch, so I am definetly interested in open relation extraction.\n\n\nThe OpenAI api (davinci-text-003) gave me some impressive results on both English and Dutch texts with the following prompt.\n\n\n\"Generate a knowledge graph of the following text in the form of a triplet. The returned instances should have the following format {entity1, relation, entity2}. Limit the description of the relation to max 3 words. Text:  $TEXT\"\n\n\nhttps://github.com/kcambrek/knowledge_graphs/blob/main/Capture.PNG\n\n\nIn the end I am looking for a model that can run locally and is very flexible in open relation extraction. Dealing with noise in triplets seems to be a trivial downstream task.\n\n\nHas anyone experience in generating knowledge graphs end-to-end with open relations locally unsupervised or with minimal training?",
"date": "2023-03-15"
},
{
"vote": 1,
"title": "Join Arbitrum for $ARB rewards | Act fast",
"text": "[removed]",
"date": "2023-03-14"
},
{
"vote": 2,
"title": "GPT-4 has been announced! How long will it take for me to get off the waiting list?",
"text": "Just saw that OpenAI released gpt 4. Only has multi-modal element of inputting of images, not video. \n\n\nStill super exciting. Can anyone speculate how long until i can get access to the api? iâ€™m on the waitlist.",
"date": "2023-03-14"
},
{
"vote": 0,
"title": "A corpus fully about STEM",
"text": "Hello, \n\n\nIs there a corpus fully dedicated to STEM?",
"date": "2023-03-14"
},
{
"vote": 1,
"title": "Evaluation Methods for text segment matching",
"text": "Hi together,\n\n\nright now I'm working on my masters thesis with the goal of exploring the usage of Language Models for matching Information Security controls. I am having a few questions about the evaluation methods which I have used so far and some which I might have missed. \n\n\nA little background: \nI have created a data set based on existing mappings between the ISO27001 security framework and another IT security framework. \n\n\nThe data set is created in the following way:\nI have two sentences/paragraphs per training example, one ISO sentence and one paragraph (might be one sentence up to a full subchapter) from the other framework, and per each pair of sentences a similarity score which indicates their semantic overlap / if they \"fit together\" (derived from an official existing mapping which maps chapters of sentences from both frameworks to each other).\n\n\nThe task for the models is as follows: I want the models to create embeddings of the sentence pairs and learn to put those embeddings which \"fit together\", as indicated by the ground truth similarity score, close to each other, while pulling those sentence pairs which do not belong together farther away in the embedding space. Later on, I want to let the model encode previously unseen sentences (e.g. new ISO controls) and then use semantic search based on a distance metric, at first cosine similarity (or possibly other methods) to find the most similar sentences from another IT security framework, as to match them together.\n\n\nFor this task I am using a SentenceBERT variant as a strong baseline.\n\n\nIn terms of model evaluation I use a held out test set from a 80/20 train test split. On trained models, I have used two evaluation methods so far:\n\n\n\n\nLet the model encode sentence pairs from test set (where a ground truth cosine similarity score is known) and then calculate the Cosine similarity. Calculate Cosine similarity loss on test set.\n\n\n\n\nFor each distinct sentence / ISO control in the test set, use this sentence as query for the trained model and let the model output the top-k most similar sentences from the second security framework. Compare the calculated top-k matches with the actual matches and calculate precision at k and recall at k.\n\n\n\n\n\n\nNow coming to the questions:\n\n\n\n\nDo you think that the evaluation methods I have used so far are appropriate for evaluating the models' performances on the task described above?\n\n\n\n\nCan you think of any other evaluation methods I might have missed? \n\n\n\n\nDo you possibly know of similar research, and if so, could you point me in this direction?\n\n\n\n\n\n\nI would appreciate any answers or feedback, feel free to point out any flaws if you do not mind.\nOh and also please excuse the formatting, I am typing this on my phone. \n\n\nThank you!",
"date": "2023-03-14"
},
{
"vote": 8,
"title": "Is GPT-3(and ChatGPT) trained with the MLM task?",
"text": "Hi all experts, I have a quick question.\n\n\n- Is the GPT family(GPT1/2/3/Chat) trained with the MLM(Masked Language Modeling) task?\n\n\nI guess no, because the GPT is basically auto-regressive(unidirectional), and their papers didn't mention the MLM training task, afaik. But when I googled, there is no clear answer, and the ChatGPT answers that the GPT family was trained on MLM.\n\n\nDoes anyone know the precise answer?",
"date": "2023-03-14"
},
{
"vote": 8,
"title": "Recommendations for a newbie",
"text": "I've been reading a lot of articles about AI in general, machine learning and NLP etc but I want to learn more about NLP, creating desktop and mobile apps for questions-answering and summarizing texts. \n\n\nI've done programming in javascript and C# in the past and I wonder if that is enough or if I must learn python as well. \n\n\nWhat are your recommendations regarding language, tools, APIs, models, transformers etc and why should I start with these?",
"date": "2023-03-12"
},
{
"vote": 1,
"title": "Beta Test for my site, want to see what we're all about?",
"text": "[removed]",
"date": "2023-03-11"
},
{
"vote": 13,
"title": "Best approach for sarcasm subcategory classification?",
"text": "Hi All,\n\n\nI  am currently working on my thesis which is attempting to build on  existing research in the field of sarcasm detection within NLP and  sentiment analysis. The task outlined is to basically build a model  which can identify subcategories of sarcasm (e.g. irony, overstatement,  rhetorical questions etc.). The dataset includes values for whether a  phrase is sarcastic or not, and which subcategories the phrase fits into  (there can be overlap between categories). I have fine-tuned BERT for  sarcasm detection and this works fine but that isn't really the task. My  questions are twofold essentially:\n\n\n-  Is the solely transformer-based approach useful in this instance, given  that it could build on existing research where previous scores obtained  in this task were fairly low. If so, does anyone have any resources on  how to build a multi-subclass classification model, or would it be  better to build separate models for each task?\n\n\n-  Would attempting to use a rule-based approach be more valuable e.g.  using vector semantics and embeddings to attempt to identify the  subcategories using this approach, and if so are there any particular  resources I should have a look at to understand how to implement this in  code? I am currently reading Jurafsky & Martin's 3rd draft of  Speech and Language Processing, but I am unclear on how I could use this  to categorise the subcategories which are difficult to define by  linguistic experts as it is?\n\n\nI'm  sorry if this is a bit rambling and all over the place, I'm feeling  pretty lost and stressed, but happy to answer any questions and try to  clarify anything :)",
"date": "2023-03-11"
},
{
"vote": 0,
"title": "Get Ready For Next Week: ChatGPT-4 Is Coming With Mind-Blowing Capabilities!",
"text": null,
"date": "2023-03-10"
},
{
"vote": 5,
"title": "How to interpret actions",
"text": "Hey guys, I would like to be able to extract actions along with their objects. For example, in the sentence \"Paint all the walls red and hide all the doors and windows.\", I would like to extract the verbs \"paint\" and \"hide\", the objects \"walls, doors, windows\", the relationships \"paint->walls\", \"hide->doors, windows\", and the adverb relationship \"paint->red\".\n\n\nWhat tools/techniques would you suggest? Is deep learning the way to go?\n\n\nSpacy\n and \nStanza\n look promising, but I am not sure.",
"date": "2023-03-10"
},
{
"vote": 1,
"title": "Usable Demo of Google Flan XL vs. GPT Ada (and mild dunking on google flan)",
"text": null,
"date": "2023-03-10"
},
{
"vote": 9,
"title": "Unpacking the HF in RLHF: How Humans Teach Large Language Models to be Better",
"text": null,
"date": "2023-03-09"
},
{
"vote": 3,
"title": "Wanna Get Training Datasets For Social media spam classifier",
"text": "I am planning to build social media's spam classifier with Multinomial Naive Bayes model with python, using `sklearn` and `spacy` library. And the text feature extraction technique I will use is tf-idf vectorizer.\n\n\nHowever, I am having problem to find social media datasets with labelled data as SPAM or NOT SPAM. Another criteria with the datasets is that I need the datasets to be balanced (with roughly equal number of SPAM and NOT SPAM data).\n\n\nDo suggest me some links or source that I could get the data from?\n\n\nHope for help. Thanks in advance.",
"date": "2023-03-09"
},
{
"vote": 40,
"title": "Opinion | Noam Chomsky: The False Promise of ChatGPT - The New York Tâ€¦",
"text": null,
"date": "2023-03-09"
},
{
"vote": 1,
"title": "Research PhD. Work opportunities in Europe in NLP and related fields",
"text": "&#x200B;\n\n\nI'm sharing here open positions from our European project. Excellent work opportunities around Europe.\n\n\nhttps://hybridsproject.eu/phd-projects/",
"date": "2023-03-09"
},
{
"vote": 3,
"title": "Survey on computing facilities for NLP/LT",
"text": "Within the European Language Equality 2 project (\nhttps://european-language-equality.eu/\n)  we are collecting information about various computing facilities and  requirements for NLP/LT. The analysis of collected information will  result in a snapshot of the current situation and relevant recommendations  for High Performance Computing use in NLP/LT.\n\n\nIf you are an NLP/LT researcher who uses HPC (e.g. GPUs, clusters,  grids) in your work, we would appreciate if you could fill out this  short survey:Â \nhttps://forms.gle/vcMF8nPmMSR9BZo27\n. Please, complete the survey until 2023-03-22.\n\n\nHere (\nhttps://european-language-equality.eu/\n) you can find more about the project and on the background of this survey.\n\n\nThank you in advance for your valuable insights.",
"date": "2023-03-09"
},
{
"vote": 1,
"title": "Encoder-decoder architecture for POS tagging",
"text": "I understand following about encoder and decoder:\n\n\n> An encoder is a network that takes the input, and output a feature map/vector/tensor. These feature vector hold the information, the features, that represents the input. The decoder is again a network that takes the feature vector from the encoder, and gives the best closest match to the actual input or intended output.\n\n\nI want to implement POS tagging with encoder and decoder. I can guess that we can use \"encoder-only\" model to do POS tagging. Can we use \"encoder-decoder\" architecture for POS tagging task? If yes, then how should I design it. Most importantly I am not able to get what input will the decoder get from the encoder.",
"date": "2023-03-08"
},
{
"vote": 1,
"title": "Question about density plots for dimensionality reduced embeddings",
"text": "I have long form documents that each talk about a variety of topics (10+). The documents are split into paragraph, which is the unit (sub-) topics are talked about. For each paragraph an embedding is created via OpenAI (text-embedding-ada-002). Since the embeddings contain 1536 dimensions, I use UMAP to reduce it to two. \n\n\nWhat I would like to do is then use bivatiate kde plots via \nseaborn\n to compare the focus of the documents (each representing an organization) showing differences and commonalities. I donâ€˜t have a strong background in mathematics but this part of the \ndocumentation\n threw me a little of. While I am not directly clustering, the underlying idea seems similar enough to warrant caution. \n\n\nDoes anybody know if my idea (umap reduced embedding-> kde plot) reasonably sound or have any pointers to fintune the approach?",
"date": "2023-03-08"
},
{
"vote": 5,
"title": "Testing Viterbi Algorithm for Hidden markov model pos tagger",
"text": "I am implementing the HMM model pos tagger using viterbi algorithm on the brown dataset from nltk. I have separated the data into train and test datasets, now for train dataset, I have calculated the emission and transition probability matrix. I have a few questions though.\n\n\n\n\nTo calculate the accuracy, do we count the no. of correct tags on words or the no. of correctly tagged sentences? (my guess is it should be words)\n\n\nFor testing data, I have some words which are not in the emission probability matrix, and hence for those sentences viterbi algorithm gives me an error. how do i handle this?",
"date": "2023-03-08"
},
{
"vote": 0,
"title": "[HELP]",
"text": "I'm trying to build a pos tagger model for my native language and I found out that I need an annotated corpus for my model. my question is should I label each word with its POS as two columns one for the word and the second for the tag or what should I do?",
"date": "2023-03-08"
},
{
"vote": 3,
"title": "Options for BERT in Python vs. Pyspark",
"text": "Hi all,  I'm working on a project to improve the \nselection of web pages where ads will be placed\n. (ex: If the ad is for supplements for women place it on a page about... women's health and wellness. Pretty simple.)\n\n\nPreviously, this has been done using very basic keyword matching and/or the site's membership in a category that was pre-chosen by the customer. (External service provides categorization of site, customer chooses keywords/category they want to advertise on.) Very basic, context and word sense not considered.\n\n\nNow I'm trying to bring the system up to a modern approach. \n\n\nMy approach so far has been the following:\n\n\n\n\nMake Corpus Embeddings\n\n\nGet the text of a bunch of the pages where an ad can be shown and \ndo TF-IDF\n to find most relevant words.\n\n\nGet embeddings\n of all the page's words \nfrom bert-base-uncased\n\n\nPull out \njust those that are top 10 TD-IDF\n and \naverage\n to create a general embedding for that page (Two notes about this: This is actually done a little more efficiently than this but I'm trying to make it clear conceptually that I'm getting the embedding for the word in its original context. I'm adding the extra TF-IDF step because it seems to keep size/computation low and not sacrifice quality.)\n\n\n\n\n\n\nMake Example Site Embedding\n\n\nGet an example site from the customer\n that they consider ideal to advertise on. Do the above on this site's text also.\n\n\n\n\n\n\nFind Pages Similar to Example\n\n\nDo \ncosine similarity\n across the pages in the corpus to \nfind near neighbors to the example site\n and advertise on those highest ranking pages where possible.\n\n\n\n\n\n\n\n\nHow to get this into pySpark?\n\n\nSo, this has all been great so far. The results look like we want them to look. But it's just been done on 70k rows of corpus sites, totally in Python. We're going to need to deal with a corpus of ~10mil sites. That's not going to work in Python. There is a Hadoop cluster available that is accessible by PySpark, though.  \n\n\nSo we have \noptions\n.\n\n\n\n\nPut everything in a \nUDF\n, run same BERT package in UDF (not so efficient and coincidentally also not working at all due to a platform issue I won't explain here but basically \nthis won't work\n so it's ruled out)\n\n\nSwitch the \nTF-IDF to SparkML\n, do the BERT \nembeddings in SparkNLP\n (this is how we're going about this now but it's still slow, not sure the cause yet)\n\n\nForget the TF-IDF\n efficiency step and just do BERT embeddings in SparkNLP, go eat cake and watch television!\n\n\nSOMETHING ELSE MUCH BETTER\n\n\n\n\nCan I do this better? How?\n\n\nThat brings me to my question. What would you do to approach this problem better? What's best for \nstorage efficiency, computational efficiency\n? Would you go about it a totally different way entirely? How can I improve this approach?\n\n\nThanks for your advice!",
"date": "2023-03-07"
},
{
"vote": 15,
"title": "Worth learning Python just for NLP if I have good grasp of R?",
"text": "Wanted to survey what people thought. I have a good amount of experience and feel comfortable with R having used it on various data analytic projects. \n\n\nApproaching my first NLP project. Do you all feel it is worth learning Python to do this specifically? I know there may be general arguments for learning Python (flexibility, etc.) but was wondering how different it is for NLP application.\n\n\nThanks!",
"date": "2023-03-07"
},
{
"vote": 0,
"title": "Twitterâ€™s CEO Elon Musk Is Reportedly Critiquing ChatGPT for Being â€˜Wokeâ€™. Is He right?",
"text": null,
"date": "2023-03-07"
},
{
"vote": 12,
"title": "Argilla + Hugging Face AutoTrain: Custom transformers models without code",
"text": null,
"date": "2023-03-07"
},
{
"vote": 1,
"title": "Webhook 401 Error",
"text": "Can anyone help me please ? Iâ€™m doing an assignment for school and we are learning to add web hook fulfillments to dialog-flow. Every time I try to run the agent I always get the 401 Authentication Error. The url doesnâ€™t have typos and there isnâ€™t a password. Can someone tell me what I am doing wrong ?",
"date": "2023-03-07"
},
{
"vote": 14,
"title": "I made an OpenAI-compatible streaming API (and playground) for your ðŸ¤— Transformers-based text generation models",
"text": "GitHub: \nhttps://github.com/hyperonym/basaran\n\n\nBasaran is an open-source alternative to the \nOpenAI text completion API\n. It provides a compatible streaming API for your \nHugging Face Transformers\n-based \ntext generation models\n.\n\n\nThe open source community will eventually witness the \nStable Diffusion\n moment for large language models (LLMs), and Basaran is committed to becoming the \nStable Diffusion web UI\n for LLMs. Basaran allows you to replace OpenAI's service with the latest open-source model to power your application \nwithout modifying a single line of code\n.",
"date": "2023-03-07"
},
{
"vote": 1,
"title": "Latin And Irish Have No Words For Yes Or No.",
"text": null,
"date": "2023-03-07"
},
{
"vote": 2,
"title": "txtai 5.4 released: prompt templates, conversational task chaining and HF Hub",
"text": null,
"date": "2023-03-06"
},
{
"vote": 2,
"title": "Cicero by Meta AI",
"text": "Hi everyone,\n\n\nHere is an attempt to summarise Cicero by Meta AI. It was a difficult read so hope people appreciates this.\n\n\nWhat is Cicero?\n\n\nfirst time an AI exceeds at a board game that requires communicating (natural language) with other humans UNDETECTED.\n\n\nCicero - the AI agent that speaks your language, plans with you, and negotiates like a pro. With human-level performance in Diplomacy (a board game).\n\n\nHow does it work?\n\n\nUsing LLMs and other bitsâ€¦\n\n\nif anyone is interested please feel free to check it out.\n\n\nhttps://www.youtube.com/watch?v=Gj5T9m-ZzNA",
"date": "2023-03-06"
},
{
"vote": 3,
"title": "Research",
"text": "Hi ... In your opinion, what are the best research papers in NLP that have come out in the past year?",
"date": "2023-03-06"
},
{
"vote": 3,
"title": "Understanding skip gram gradient",
"text": "I am trying to understand gradient calculation for skip gram with softmax output and cross entropy loss.\n\n\nI am referring these articles: \n1\n, \n2\n, \n3\n.\n\n\nThey all calculate the error as follows:\n\n\nEquation link\n\n\nStep 1:\n But then, they calculate gradient of Error with respect to u_j as follows:\n\n\nEquation link\n\n\nQ1.\n Is the first summation term above:\n\n\nEquation link\n\n\nleft untouched? If yes, then why they have not taken its gradient with respect to u_j?\n\n\nStep2:\n Then next step they do is:\n\n\nEquation link\n\n\nQ2.\n How the first term above\n\n\nEquation link\n\n\nis obtained from first term\n\n\nEquation link\n\n\nin step1?\n\n\nQ3.\n How the second term\n\n\nEquation link\n\n\nis obtained from second term\n\n\nEquation link\n\n\nin earlier step (step 1)?\n\n\nStep 3:\n The next step is:\n\n\nEquation link\n\n\nQ4.\n How this step (step 3) is obtained from step 2?\n\n\nCan some one please help me with these doubts?",
"date": "2023-03-05"
},
{
"vote": 13,
"title": "Applying NLP to less common languages",
"text": "If you're working on NLP for a less common language, you know how challenging it can be to find resources and guidance. That's why I've created a tutorial article that explores the unique difficulties of NLP for these languages and compares different state-of-the-art embedding models, including OpenAI's latest release. You can find it on thie following link. \nhttps://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021\n\n\nHope you find it helpful and let me know what you think.",
"date": "2023-03-05"
},
{
"vote": 5,
"title": "Custom Part of Speech (POS)",
"text": "I'm trying to build POS tagger model for my underrepresented language, and I don't know how the data should look like?\n\n\nshould be a bunch of large corpus which are labeled with their POS or a dataset of 2 Columns one for the word and the other for the POS I really don't know.. HELP",
"date": "2023-03-05"
},
{
"vote": 2,
"title": "Hello everyone! It may seems strange but I want your suggestion for newly experienced MyMoodAI, that offered me 50% subscription discount with 8 different ways to present mood, but its not offering service for Andriod. Can you suggest me whether I should buy its subscription or try a new one?",
"text": "[removed]",
"date": "2023-03-05"
},
{
"vote": 1,
"title": "Huggingface Custom Dataset for Sequence Labelling",
"text": "Hi everyone. I hope this sub is the right place for this. I've been tackling with this for a while now I would appreciate any help.\n\n\nIâ€™m working on a sequence labelling task. During the training f1 scores are abnormally high for validation set. When I try inference it barely gets anythihg right. I thought it may be about how I create and use the dataset.\n\n\nI'm using datasets class from huggingface. I load the dataset from list as follows:\n\n\ntrain_dataset = Dataset.from_list(train_l) \nvalid_dataset = Dataset.from_list(valid_l) \ntest_dataset = Dataset.from_list(test_l)\n\n\n\nWhen I do this I don't define anything like \nClassLabel\n. \n\n\nFirst thing is how does the trainer understand which section of the dataset contains labels and input data?\n\n\nAfter this I simply tokenize each dataset and align tags with the new tokens. When I run\n\n\nprint(tokenized_train.features)\n\n\n\nOutputs:\n\n\n{&#039;tokens&#039;: Sequence(feature=Value(dtype=&#039;string&#039;, id=None), length=-1, id=None),\n&#039;tags&#039;: Sequence(feature=Value(dtype=&#039;int64&#039;, id=None), length=-1, id=None),\n&#039;input_ids&#039;: Sequence(feature=Value(dtype=&#039;int32&#039;, id=None), length=-1, id=None),\n&#039;token_type_ids&#039;: Sequence(feature=Value(dtype=&#039;int8&#039;, id=None), length=-1, id=None), \n&#039;attention_mask&#039;: Sequence(feature=Value(dtype=&#039;int8&#039;, id=None), length=-1, id=None), \n&#039;labels&#039;: Sequence(feature=Value(dtype=&#039;int64&#039;, id=None), length=-1, id=None)}\n\n\n\nWhere labels are the labels including special -100 and aligned with the byte-pair tokenization. Tags are simplly the tags used for tokens without alighment.\n\n\nI think thereâ€™s something wrong in here. Can you spot anything or does it seem just fine? How does the trainer know what to take as input and what to use as labels?",
"date": "2023-03-05"
},
{
"vote": 21,
"title": "NLP Research fully remote?",
"text": "Hi everyone,\n\n\nIs anyone doing NLP Research for companies/start-ups/research labs fully remote? If so:\n\n\n\n\nHow is your experience so far? \n\n\nAre such openings common? \n\n\nHow did you find yours? \n\n\nAre you still able to able to publish in top venues?\n\n\nCan you still advance in your career?\n\n\n\n\nAny information is greatly appreciated!\n\n\nThank you!",
"date": "2023-03-05"
},
{
"vote": 1,
"title": "On the topic of LyricFluent",
"text": "[deleted]",
"date": "2023-03-05"
},
{
"vote": 26,
"title": "*Non-transformer* state of the art in NLP?",
"text": "Hello,\n\n\nlarge language models have taken the field by a storm and everyone now seems to focus on transformers and these kind of deep learning methods in general, while I would like to poke to a different direction. Because of this fact, and because I still feel uneasy scouting for research, worrying about missing key news and resources, I turn to this inquiry.\n\n\nAs I am more drawn to science than data mining, I am particularly interested in the \"traditional\" disciplines of NL parsing and mainly \nNL understanding\n, or in other words, the economically uninteresting veins of research overshadowed by productive but superficial statistical solutions to narrow tasks.\n\n\nI would like to ask for resources\n - perhaps both the \nold \"classics\"\n of the fields as well as \nnewest research\n, the most prominent projects, people (if this helps clarify my interest, one of the few I currently know are Gary Marcus, Ben Goertzel), etc.\n\n\nThank you very much",
"date": "2023-03-04"
},
{
"vote": 2,
"title": "Advice for preprocessing a hindi language dataset for text summarisation study!",
"text": "Iâ€™m working towards contribution towards the linguistic department of my university. \n\n\nIâ€™ve taken 3 traditional (Luhns, LSA, textrank) and 2 pre trained models (T5, Pegasus). The reason was I wanted to cover and explore wide range of text summarisation models. \n\n\nI have already run the models on the CNN/Daily mail dataset and obtained the ROUGE scores. Now I want to run it on a similar hindi language dataset and compare the results of all 5 models. \n\n\nHow do I proceed with the preprocessing of the data? I have a list of stop words and was wondering how should my approach be while running these particular models on the hindi language. \n\n\nAny and every advise/help would be appreciated!",
"date": "2023-03-04"
},
{
"vote": 6,
"title": "How to build a concept \"tree\" from a list of content tags?",
"text": "I  have sets of concept tags from two apps. One is pocket (~1100 tags);  another is a Jekyll-based static site (~250 tags) which contains a  subset of content from the first. The overlap isn't perfect (there are  tags in the smaller set that don't exist in the larger. They are  outliers.). I'll have a list of merged tags in a day or so, once I  handle cases like singular-plural, misspellings, and so on.\n\n\nMost  of the tags are subcategories of larger concepts, eg 'anger' ->  'emotions'. I want to build a concept tree that will make site  navigation a little more intuitive. The top-level menu would cover  topics as diverse as behavior, UI/UX, programming languages, neurology,  foreign policy, and whatever else floats my boat.\n\n\nI can build this manually. For the sake of curiosity - is there a tool or corpus that will automate this step?\n\n\nThanks everyone.",
"date": "2023-03-04"
},
{
"vote": 12,
"title": "Prompt templates and task chains - run with Python, YAML or FastAPI",
"text": null,
"date": "2023-03-04"
},
{
"vote": 10,
"title": "SBERT Embeddings from Conversations",
"text": "Hello\n\n\nI have a dataset consisting of text-based conversations between two humans. One conversation has on average 20 turns and can look as follows:\n\n\n>Person 1: Do you like cooking?Person 2: Yes. I like cooking very much. I got this hobby when I was 12 years sold.Person 1: Why do you like it?Person 2: I have no idea. I like cooking by myself. I like to taste delicious food....\n\n\nWith SBERT I can get the embeddings of one turn (e.g., \"Hello there, how are you doing?\"). Is it also possible to get one embedding with SBERT for several turns or a whole conversation (20 turns)? Are there other models which are capable to do this or are more recent? Afterward, I would like to project the embedding to 2D or 3D space and apply clustering.",
"date": "2023-03-03"
},
{
"vote": 2,
"title": "Developing language frequency lists from Reddit/Twitter posts and Wikipedia articles",
"text": null,
"date": "2023-03-03"
},
{
"vote": 1,
"title": "Distilbert Distillation Loss Confusion",
"text": "Hi all, I'm just reading through the DistilBERT paper (\nhttps://arxiv.org/pdf/1910.01108v4.pdf\n) and was confused by the distillation loss (described in section 2 Knowledge distillation).  \n\n\nIs this missing a negative sign in front of the summation? Without it, wouldn't we minimize loss when t_i = 1 and s_i = very low number (teacher and student not in agreement)  \n\n\nSorry if this is a stupid question or I'm missing anything",
"date": "2023-03-02"
},
{
"vote": 1,
"title": "All of this happening in AI",
"text": "[removed]",
"date": "2023-03-02"
},
{
"vote": 0,
"title": "Startup? idea (ChatGPT)",
"text": "Hi redditors,\n\n\nI have the following issue. I see a very interesting use case for a chat model based on ChatGPT within my very specific field.\n\n\nNow I am not (yet) familiar with NLP, machine learning, python, etc.\n\n\nI think my challenge is to collect datasets and feed them to ChatGPT. And come up with specific questions that produce valuable answers.\n\n\nNow my questions are: \n\n\n-Who can help me with that specific point: Converting datasets to 'input' for the model? \n\n\n-Or are there certain platforms where such specialists offer themselves? \n\n\n-Who could help me get started in general?\n\n\nI am currently employed in that specific field and would like to do this full-time if there is some success or potential. I also have a modest capital to invest in some contractors.\n\n\nLooking forward to your answers/ideas :)",
"date": "2023-03-02"
},
{
"vote": 1,
"title": "Need help/ways to Embed larger documents for information retrieval/Question answering project I am working on.",
"text": "Hello there!\n\n\nI'm currently working on two projects and would appreciate some guidance.\n\n\nFirstly, I have a knowledge base consisting of textual content such as books and FAQs. I need to suggest relevant content to users when they input a question. If the answer is present in the FAQs, it's great, otherwise, I'd like to guide them to the specific portion in the book where they could possibly find the answer.(similar to what google has been doing off late for a search query)\n\n\nIn the other project, I need to match a resume to a job description to suggest jobs given a resume. Both (resume and job description) of which are textual information of about a page or page and a half. My idea is to use embedding to get some similarity between the two documents. However, I'm not sure how to go about embedding a document of that size as embedding a long document into a single vector might cause loss of information.\n\n\nOnce I'm clear with the embedding part of it, I'm thinking of using something like FAISS or pinecone to do the fast vector search.\n\n\nI would appreciate any thoughts or inputs on these issues. Thank you!",
"date": "2023-03-02"
},
{
"vote": 21,
"title": "ChatGPT is nothing like a human",
"text": null,
"date": "2023-03-01"
},
{
"vote": 2,
"title": "Hey guys, our text-to-location Kaggle competition ends in a month, so we want to get the word out. If you want, you can give us your Twitter handle, and weâ€™d love to tag you when you when you make it to the leaderboard ðŸ†",
"text": null,
"date": "2023-03-01"
},
{
"vote": 0,
"title": "Is the future of NLP going to be dominated by Large Language Models, such as ChatGPT?",
"text": "[removed]",
"date": "2023-03-01"
},
{
"vote": 2,
"title": "Choosing an FYP",
"text": "In my 6th semester, we're supposed to choose our final year project in two weeks. Kind of freaking out. How the hell do people choose? I want to do an ML project, probably somewhere in NLP or speech recognition, so reading allot of papers rn to try to understand what work people are doing right now and what I could contribute. Everyone I talk to is giving me different opinions. One professor told me there wasn't much point because there was already so much work done in that area. Like, are we supposed to do things no one has ever done before? We're just bachelor students, there's huge corporations and labs dedicated to advancing the field, and yeah I want to innovate somehow but I don't expect to make any breakthroughs in NLP. Other professors are saying totally different things - that no one expects you to have a groundbreaking project, just something good ig. Pretty confused. I'm leaning towards trying to make a speech based computer navigation system to make accessibility easier. Not sure if that's too ambitious or too basic because it already exists in English. The one I want to make is in Urdu though, and though there's already allot of Urdu speech to text and text to speech systems, I don't think they've been integrated into a full computer navigation system. Sorry this is all super jumbly but just any ideas, what should I be aiming for, what sort of things do people usually do for final year projects, expectations etc. would really help. Apparently this could determine what I study in masters? So like, no pressure lol.",
"date": "2023-03-01"
},
{
"vote": 3,
"title": "NLP API for Intention Mining and Time/Location parser",
"text": "Hi There,\n\n\nIs there any intention mining API, except GPT3?\n\n\nI'm looking for an API that can help determine the intention based on the text.\n\n\nAlso, look for some API that can help parse time and location information from the text other than GPT3.",
"date": "2023-02-28"
},
{
"vote": 11,
"title": "Snapchat Launches My AI Chatbot Powered By OpenAIâ€™s GPT",
"text": null,
"date": "2023-02-28"
},
{
"vote": 7,
"title": "Vocabulary Argument in Tfidfvectorizer",
"text": "Hello everyone,\n\n\nI was hoping if anyone had any good examples of how the vocabulary argument of Tfidfvectorizer is used?\n (\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n)\n\n\nMy goal is to pass in a custom vocabulary from the nuclear power domain of varying token lengths, but I have had no success.\n\n\nInitially I thought I could leverage Spans or Entities from Spacy, but couldn't not figure out how to pass my varying lengths of Spacy tokens to Tfidfvectorizer.\n\n\nI figured I would take a step back and see if what I am trying to do is actually possible, and from reading the docs I think the vocabulary argument is what I am looking for. \n\n\nAny examples/tutorials or success stories would be great!\n\n\nThank you",
"date": "2023-02-28"
},
{
"vote": 6,
"title": "List of sources to get recent updates in this field",
"text": "Can we create a list of sources that can be followed to get recent updates in both Research and applied NLP ?",
"date": "2023-02-28"
},
{
"vote": 1,
"title": "External knowledge base for chatbots on GPT-3",
"text": "Hello everyone\n\n\nPlease tell me about the organization of external knowledge for a question-and-answer chatbot\n\n\n\n\nAs far as I understand, the most popular way now is embedding individual text documents and searching through them by embedding a user's question comparing the resulting embedding with others. Is this the best way now or are there better options? What other options are there?\n\n\n\n\nLet's say we have a book as an initial source of knowledge, how in this case is it better to turn it into a knowledge base? Just embed individual paragraphs, and then search for them? But the thought can spread out into several paragraphs. And then it would be good to find all of them when searching. At the moment we have developed a number of heuristics for this, but I would like to get acquainted with some research on this topic. It would be great to find a book or an article that analyzes such cases and their solutions.\n\n\n\n\nAnother case, if we have parameters during the search, according to which there should be strict filtering. For example, we want to recommend a hotel to a user for a request, and naturally, if he specifies a specific city, we must cut off all documents that hotels are not located in this city and choose recommendations from them. In the search for embeddings in its pure form, this naturally will not work, so for now we have an idea to isolate such entities from descriptions and filter them separately. Are there any other approaches to this case?\n\n\n\n\n\n\nIn general, I will be glad if you give a link to some resource or book that details the problems and approaches to creating an external knowledge base for chat bots!",
"date": "2023-02-28"
},
{
"vote": 1,
"title": "SOTAs achieve by fine-tuning-based models",
"text": "Hi! I'm a researcher who has just started learning NLP. As I know, most recent papers focus on few-shot/zero-shot performance for the success of huge language models.\n\n\nBut I'm curious about the current performance of \"traditional\" fine-tuning-based methods on some NLP tasks such as NER, MT, and so on. Do those fine-tuning-based methods still get SOTA?\n\n\nLooking forward to any ideas about this and any links. Thanks!",
"date": "2023-02-28"
},
{
"vote": 1,
"title": "how important is linguistic ability and CS ability in NLP, respectively?",
"text": "Hello everyone. I am a high school graduate and I am interested in going into NLP research after I finish my bachelor's. I am wondering how important linguistic ability is in NLP research compared to CS knowledge/talent. Which is more important in Coursework? Which is more important in writing your own paper?",
"date": "2023-02-27"
},
{
"vote": 5,
"title": "Context retrieval model for question answering ?",
"text": "I've been searching on Hugging Face for fine-tuned models which retrieves paragraphs containing answer to a specific questions.\nThe only one I found was the universal sentence encoder, but it seems to have a poor performance on the SQuad dataset.\nCan you suggest any alternatives on Hugging Face?",
"date": "2023-02-26"
},
{
"vote": 1,
"title": "ChatGPT prompt community launch - Braintrade",
"text": "[removed]",
"date": "2023-02-26"
},
{
"vote": 21,
"title": "Meta Releases LLaMA to compete with PaLM, GPT3 and Chinchilla",
"text": "Meta is releasing LLaMA to compete with PaLM and Chinchilla. Will it suffer the same fate as Blenderbot and Galactica? Or the third time is a charm? Today (well, yesterday), they released a brand-new AI language generator called LLaMA. Well, as new as it can get!\n\n\nLLaMA research paper\n by Meta claims that their second-smallest version, LLaMA-13B, beats OpenAIâ€™s GPT-3 model in most benchmarks, and their largest brother of this model, LLaMA-65B, is just as good as the best models out there, like DeepMindâ€™s Chinchilla70B and Googleâ€™s PaLM 540B.\n\n\nhttps://medium.com/p/7a1d48f230b2\n\n\nMain research paper link \nhttps://scontent-dfw5-2.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf",
"date": "2023-02-25"
},
{
"vote": 0,
"title": "Hugging face transformer model not accurate",
"text": "Hello I am using a Bert uncased model and it keeps thinking â€œI love uâ€ and â€œI hate uâ€ are the same thing. Is there a better model I should be using for training on the feeling of the text?",
"date": "2023-02-25"
},
{
"vote": 3,
"title": "Should I use topic modelling or sentiment analysis for my thesis project?",
"text": "Hi!\nI just started working on my master thesis and want to see whether a machine learning algorithm can predict which books will become popular on Booktok. Small background: Booktok is a community on Tiktok where people review and recommend books, and it has been said that the popularity of booktok has made a lot of youth love reading again. Booksellers now also often have displays dedicated to Tiktok books that have gone viral. And these books are usually quite similar, they're usually for young adults and within the genres of fantasy and romance. \n\n\nI want to use the books descriptions to do the predictions, and I'm thinking I could use something like the structure of the descriptions to do this. I have thought about topic modelling, but I don't know if that would work since one book may be a romance in a pirate setting and another a romance in a ranch setting (for example). The plots might then be very similar, but maybe since the books are set in other worlds they'll be sorted under different topics (cause one description would have words like 'pirate', 'ship' etc. while the other would have words like 'cow' and 'barn'). It would be really great if I could somehow use the general plot, as in the way the story is build up, to do the predictions, but I wouldn't know how to accomplish that.\n\n\nI'm also thinking I could use sentiment analysis because Tiktok books are very much about the way it makes the reader feel, but I doubt the description alone would be enough for that. Maybe I could use it for the parts of descriptions that are not very specific to the story (so somehow, I don't know how, it would exclude the words regarding pirates and cowboys).\n\n\nDoes anyone have any advice or suggestions, or a preference for either topic modelling or sentiment analysis?",
"date": "2023-02-25"
},
{
"vote": 5,
"title": "Improving performance of BioBERT NER on clinical notes",
"text": "I am trying to extract clinical concepts(specifically signs and symptoms) from unstructured medical notes. I have implemented a standalone Negex algorithtm and use it with my fine tuned BioBert model(from Hugging Face) to weed out the negated terms and get the positive signs and symptoms. Despite this I am getting a F1 score of about 47% with a poor precision. Are there any alternatives to my method to improve the F1 score or precision? I have cleaned the data but I have not done any pre-processing as I was under the impression that it is not required for BERT models.",
"date": "2023-02-24"
},
{
"vote": 2,
"title": "Stylometry tools",
"text": "[deleted]",
"date": "2023-02-24"
},
{
"vote": 5,
"title": "GitHub - neuml/txtchat: ðŸ’­ Conversational search and workflows for all",
"text": null,
"date": "2023-02-24"
},
{
"vote": 0,
"title": "How to visualize embeddings from the Cohere API",
"text": null,
"date": "2023-02-23"
},
{
"vote": 1,
"title": "Where could I download Turing ULR v6?",
"text": null,
"date": "2023-02-23"
},
{
"vote": 2,
"title": "Medical Records for LLMs",
"text": "Hello, r/LanguageTechnology.\n\n\nI will soon have access to the de-identified texts of many medical records. I will also have written permission to sell these de-identified texts directly from the individuals themselves. These texts are from records with activity in the past 36 months. They are categorized by the decade of birth, region of the U.S., race, gender, and primary medical conditions. I obtained these records for use in a project that is now canceled.\n\n\nI'd like to understand if these records would be of value to AI/NLP researchers, or if off-the-shelf LLMs mean that healthcare NLP is \"solved.\"\n\n\nThoughts? Thanks in advance!",
"date": "2023-02-22"
},
{
"vote": 0,
"title": "As AI is in every field these days, what could be the major pain points for any company to extract data, build models, evaluate them, deploy, monitor?",
"text": null,
"date": "2023-02-22"
},
{
"vote": 3,
"title": "Multi span QA vs large document sequence labeling",
"text": "Hi.\n\n\nI am looking to try out the models for multi span QA / large passage sequence labeling.\n\n\nIn more detail, I am trying to extract a legal passage in a legal contract and the according severity of the legal passage (which in itself is again a passage of the contract).\n\n\nAn example:\n\n\nâ€¦DOCUMENT:\n\n\n\n\nThe insurance is paying \n50 %\n of the costs of subjects described in Â§3.\n\n\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\n\n\nThe insurance contains insurances for the following subjects:\n\n\nCar accidents if the accident is not self-inflicted.\n\n\nHousehold accidents which are not inflicted by natural catastrophes.\n\n\n\n\nSo the objects I want to find/label here are the two insurance subjects (here 3.1 and 3.2) and their designated insurace properties (50 % formulated in 1.).\n\n\nThese can be seen as\n\n\nitem: \"\nCar accidents if the accident is not self-inflicted.\"\n\n\nitem: \"\nHousehold accidents which are not inflicted by natural catastrophes.\"\n\n\nTheir designated property is 50 % for both, so:\n\n\nproperty: â€œ**50 %**â€\n\n\nproperty: â€œ**50 %**â€\n\n\nAny experience in the field or suggestions for models and do's and dont's are highly appreciated. \n\n\nCheers.\nL",
"date": "2023-02-22"
},
{
"vote": 0,
"title": "What do I read to start",
"text": "So, I am currently student at university and want to learn about nlp. I have had prior experience in ML and Neural Networks",
"date": "2023-02-22"
},
{
"vote": 1,
"title": "Need help with sentence cluster labeling.",
"text": "Hey folks.\n\n\nI have clusters of sentences from hierarchical clustering and\nI need to find phrases that best represent each cluster so that it can serve as the cluster label.\n\n\nI am currently experimenting with transformer summarisation using models from huggingface-hub. \n\n\nLooking for suggestions on the approach that would be best for this problem.\n\n\nThanks!",
"date": "2023-02-21"
},
{
"vote": 2,
"title": "Find Partners",
"text": "I currently a student studying NLP at Peking University, I want to find a partner who reads paper everyday, and maybe we can exchange our notes and thoughts on papers. I really need someone to exchange thoughts!",
"date": "2023-02-21"
},
{
"vote": 1,
"title": "Keyword extraction/ sentiment analysis on books' reviews",
"text": "[deleted]",
"date": "2023-02-21"
},
{
"vote": 1,
"title": "Semantic search for Wikipedia",
"text": "[deleted]",
"date": "2023-02-21"
},
{
"vote": 6,
"title": "Mean pooling in BERT",
"text": "I am just confused on whether it is typical to include CLS and SEP token for average pooling in BERT. Any references? any reasons to include or not to include special tokens?",
"date": "2023-02-21"
},
{
"vote": 4,
"title": "OpenAIâ€™s Latest Purchase: AI.com",
"text": null,
"date": "2023-02-19"
},
{
"vote": 1,
"title": "NLP certification(s)",
"text": "Hello, \n\n\nI see certifications out there such as Udemy, Coursera, Stanford, etc. Iâ€™m wondering which is held in the highest esteem by employers. \n\n\nIâ€™m also wondering if they matter a difference at all. \n\n\nThank you!",
"date": "2023-02-19"
},
{
"vote": 9,
"title": "BERT-Based Clustering on a Corpus of Genre Samples Kinda Sucks. Why?",
"text": "I'm comparing BERT-embeddings vs. good ole' fashioned TF-IDF to cluster a corpus of documents (FROWN Corpus of Contemporary English w/ 15 genres across 500 documents) using K-means.  I'm surprised to see how poorly the BERT-tokenized effort does. \n\n\nI'm using BertTokenizer on 510-token length chunks, discarding chunks under 50 tokens (code below), and then using K-means after an elbow plot, winding up 278, 512-token total document chunks. The results are surprisingly (to me) bad:\n\n\n>Homogeneity: 0.3745\n>\n>Completeness: 0.3719\n>\n>Silhouette Score: 0.2542\n\n\nIt's a little better than TF-IDF on the same documents (500 docs using TFIDFVectorizer) but still:\n\n\n>Homogeneity: 0.3104\n>\n>Completeness: 0.3877\n>\n>Silhouette Score: 0.0002\n\n\nIs the problem K-means can't take advantage of BERT embeddings?  Or something else maybe?  Would be grateful for any insight.\n\n\nCode\n:\n\n\n#import pandas to store our data and then our vecotrized data in dataframes (tabular format)\nimport pandas as pd\n\n# transformers is a kind of easy button that has lots of existing models, tools, and pipelines from HuggingFace (check out https://huggingface.co/)\n# There are lots of NLP applications for transformers, and you can often get a complex task done with a single line of code using Hugging Face\nfrom transformers import BertModel, BertTokenizer\n\n# torch is a PyTorch deep learning framework for array operations and math functions, &amp; efficiently do math on GPU&#039;s.\nimport torch\n\n# Load csv file into dataframe\n# In my example, 30 documents, 10 each selections from Edgar Allen Poe, Ukraine war reporting, &amp; Russian trolls on Twitter\ndf = pd.read_csv(&#039;/datasets/frown_labeled_text/FROWN_Labeled_Text_byGenre.csv&#039;)\n\n# Initialize BERT model and tokenizer\nbert = BertModel.from_pretrained(&#039;bert-base-uncased&#039;)\ntokenizer = BertTokenizer.from_pretrained(&#039;bert-base-uncased&#039;)\n\n# Tokenize and encode chunks of documents\nchunked_vectors = []\nchunked_genres = []\nnum_chunked_docs = 0  # Counter for number of chunked documents\nfor _, row in df.iterrows():\n    text = row[&quot;TEXT&quot;]\n    genre = row[&quot;CODE&quot;]\n    tokenized_text = tokenizer.tokenize(text)\n    for i in range(0, len(tokenized_text), 510):\n        chunk = tokenized_text[i:i+510]\n        if len(chunk) &lt; 50:\n            continue\n        num_chunked_docs += 1  # To count # of document chunks we create in this function\n        encoded_chunk = tokenizer.encode(chunk, add_special_tokens=True, max_length=510, truncation=True)\n        input_ids = torch.tensor(encoded_chunk).unsqueeze(0)\n        pooled_output = bert(input_ids)[1]\n        chunked_vectors.append(pooled_output.squeeze(0).tolist())\n        chunked_genres.append(genre)\n\n# Create dataframe with vectors and genre\ncluster_df = pd.DataFrame(chunked_vectors)\ncluster_df[&quot;CODE&quot;] = chunked_genres\n\n# Import K-menas for clustering and matplotlib for visualzing \nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Create a list of WCSS for different k values\nwcss = []\nfor i in range(4, 20):\n    kmeans = KMeans(n_clusters=i, init=&#039;k-means++&#039;, max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(cluster_df.drop(&quot;CODE&quot;, axis=1))\n    wcss.append(kmeans.inertia_)\n\n# Plot WCSS for each k value\nplt.plot(range(4, 20), wcss)\nplt.title(&#039;Elbow Method&#039;)\nplt.xlabel(&#039;Number of clusters&#039;)\nplt.ylabel(&#039;WCSS&#039;)\nplt.show()\n\n# Import clustering evaluzation metrics and numpy to handle arrays\nfrom sklearn.metrics import homogeneity_score, completeness_score, silhouette_score\nimport numpy as np\n\n# Fit KMeans to the data (manually enter the number you got from the elbow plot, &quot;15&quot; in my case)\noptimal_k = 15\nkmeans = KMeans(n_clusters=optimal_k, init=&#039;k-means++&#039;, max_iter=300, n_init=10, random_state=0)\ncluster_df[&quot;label&quot;] = kmeans.fit_predict(cluster_df.drop(&quot;CODE&quot;, axis=1))\n\n\n# Create a color map to map genres to colors (remember the original data has ground truth: each document is labled by genre)\ngenre_list = cluster_df[&quot;CODE&quot;].unique()\ngenre_colors = {genre: i for i, genre in enumerate(genre_list)}\ncluster_df[&quot;color&quot;] = cluster_df[&quot;CODE&quot;].apply(lambda x: genre_colors[x])\n\n# Plot the clusters\nfig, ax = plt.subplots()\nfor genre, x, y in zip(cluster_df[&quot;CODE&quot;], cluster_df[0], cluster_df[1]):\n    ax.scatter(x, y, label=genre if genre not in ax.get_legend_handles_labels()[1] else None, color = &#039;C{}&#039;.format(genre_colors[genre]))\n\n# Show the legend\nleg = ax.legend(frameon=True, framealpha=1)\nfor lh in leg.legendHandles: \n    lh._sizes = [150]\n\n# Display the cluster plot\nplt.show()\n\n\n# Evaluate the clusters\nhomogeneity = homogeneity_score(cluster_df[&quot;CODE&quot;], cluster_df[&quot;label&quot;])\ncompleteness = completeness_score(cluster_df[&quot;CODE&quot;], cluster_df[&quot;label&quot;])\nsilhouette = silhouette_score(cluster_df.drop(&quot;CODE&quot;, axis=1), cluster_df[&quot;label&quot;])\n\nprint(&quot;Homogeneity: {:.4f}&quot;.format(homogeneity))\nprint(&quot;Completeness: {:.4f}&quot;.format(completeness))\nprint(&quot;Silhouette Score: {:.4f}&quot;.format(silhouette))",
"date": "2023-02-19"
},
{
"vote": 3,
"title": "Veery confused about attention.",
"text": "As the title says I'm confused about the attention mechanism. Mostly from the terminology I guess.\n\n\nAttention first used in the Bahdanau paper in 2015. So if we say only \"attention\" (no self, no multi-head etc.) are we talking about this attention?\n\n\nWhen I search the Bahdanau's paper for key, query or value I can't find anything. However if I look at other sources they use them with Bahdanau attention mechanism. For example: \nhttps://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html\n\n\nI'm reading \nTransformers for Machine Learning: A Deep Dive.\n Attention section starts with key, query, value pairs. It illustrates the mechanism with the following images:\n\n\nhttps://imgur.com/a/Ou4ABxo\n\n\nWhich type of attention mechanism is this? At some sources when attention is discussed it seems like they mean self-attention by default. I'm very confused. I would appreciate any explanation.\n\n\n&#x200B;\n\n\nEDIT: I found out something called \"general attention\" from here: \nhttps://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n. Where did that come from, from which paper?",
"date": "2023-02-18"
},
{
"vote": 13,
"title": "NLP resources",
"text": "I am an undergraduate student in mathematics. I have a fair bit of experience with deep learning in computer vision research and am willing to dabble into NLP. I hope that things won't be very disjointed and some of the knowledge can be transferred.\n\n\nI wanted to know if y'all can recommend some YouTube playlists that start from scratch as far as NLP is concerned, and then gets pretty deep into the subject. I would also like it to have a research-oriented flavor. Thanks in advance.",
"date": "2023-02-18"
},
{
"vote": 3,
"title": "[HELP] Topic Modeling on a corpus of long docs",
"text": "Hey everyone, Is BERTopic the best way to go or are there better approaches for doing topic modeling on a corpus of large documents (news articles)?\n\n\nThank you! :)",
"date": "2023-02-18"
},
{
"vote": 5,
"title": "Model for text summarization",
"text": "I have a dataset in which I have two columns, the first one is the target and the second one consist of a description of the target. I what my approach could be and what models I could use to perform my objective.\n\n\nMy objective is to take input from user and process it to output the most suited target result based on the provided input.\n\n\nFor example if the input is \"King of the jungle, male species have manes\" then it gives me the output \"Lion\", here I will have a both the description and the target available in the dataset.",
"date": "2023-02-17"
},
{
"vote": 24,
"title": "Cerebras launches fine-tuning of large language models in the cloud",
"text": "[Note: I work for Cerebras Systems]\n\n\nCerebras just made \nfine-tuning\n for large language models available via the \nCerebras AI Model Studio\n. Users can fine-tune models including GPT-J (6B), GPT-NeoX (20B), and CodeGen (350M to 16B), with more models and checkpoints coming soon. This comes as an addition to the training-from-scratch capabilities we made available in our previous launch.\n\n\nUsers can fine-tune these models on a dedicated cloud-based cluster powered by Cerebras CS-2 systems with the following advantages:\n\n\n\n\nFast - Fine-tune GPT-J 6B in 17 hours\n\n\nCheap - Priced competitively with OpenAI\n\n\nEasy -  Enjoy cluster performance with no code change\n\n\nOwnership - Your trained weights are yours to keep!\n\n\n\n\nCurious how we enabled cluster performance with no distributed coding? \nread this blog\n\n\nCurious how we can train multi-billion parameter models on a single device? \nread this blog\n\n\nInterested? We are offering a \nfree trial\n for users interested in fine-tuning or training from scratch.",
"date": "2023-02-16"
},
{
"vote": 8,
"title": "Hugging Faceâ€™s Experts Teach Transformers for Enterprise Use Cases",
"text": "Hey folks - I wanted to put this live course from Hugging Faceâ€™s top experts (\nRajiv Shah\n, \nNicholas Broad\n, \nEno Reyes\n, \nDerek Thomas\n and \nFlorent Gbelidji\n) on your radar!\n\n\nThe course looks at how to utilize transformers to build reliable and scalable services. The course draws on the instructors and Hugging Faceâ€™s expertise in implementing transformers in industry along with case studies, applied exercises and frameworks that you can share with your team and apply at work!\n\n\nIt kicks off on March 20 and you can use your learning stipend to cover - more info here:\n\n\nhttps://www.getsphere.com/cohorts/transformers-for-enterprise-use-cases?source=Sphere-Com-r-lt",
"date": "2023-02-15"
},
{
"vote": 1,
"title": "OPT-IML Instruction Tuned Language Model",
"text": "[removed]",
"date": "2023-02-15"
},
{
"vote": 1,
"title": "A Comprehensive Guide &amp; Hand-Curated Resource List for Prompt Engineering and LLMs on Github",
"text": "[removed]",
"date": "2023-02-14"
},
{
"vote": 1,
"title": "A Comprehensive Guide &amp; Hand-Curated Resource List for Prompt Engineering and LLMs on Github",
"text": "[removed]",
"date": "2023-02-14"
},
{
"vote": 11,
"title": "A Comprehensive Guide &amp; Hand-Curated Resource List for Prompt Engineering and LLMs on Github",
"text": "Greetings,\n\n\nExcited to share with all those interested in Prompt Engineering and Large Language Models (LLMs)!\n\n\nWe've hand-curated a comprehensive, Free & Open Source resource list on Github that includes everything related to Prompt Engineering, LLMs, and all related topics. We've covered most things, from papers and articles to tools and code!\n\n\nHere you will find:\n\n\n\n\nðŸ“„ Papers in different categories such as Prompt Engineering Techniques, Text to Image Generation, Text Music/Sound Generation, Text Video Generation etc.\n\n\nðŸ”§ Tools & code to build different GPT-based applications\n\n\nðŸ’» Open-Source & Paid APIs\n\n\nðŸ’¾ Datasets\n\n\nðŸ§  Prompt-Based Models\n\n\nðŸ“š Tutorials from Beginner to Advanced level\n\n\nðŸŽ¥ Videos\n\n\nðŸ¤ Prompt-Engineering Communities and Groups for discussion\n\n\n\n\nResource list\n: \nhttps://github.com/promptslab/Awesome-Prompt-Engineering\n\n\nWe hope it will help you to get started & learn more about Prompt-Engineering. If you have questions, Join our discord for Prompt-Engineering, LLMs and other latest research discussions\n\n\nhttps://discord.com/invite/m88xfYMbK6\n\n\nThank you :)",
"date": "2023-02-14"
},
{
"vote": 20,
"title": "How do you keep track of conference/talks/events in NLP?",
"text": "I'm currently trying to find professors to build connections with and work as an unaffiliated researcher with, as I'm trying to find a detour path into a Computational Linguistics PhD.\n\n\nI've recently been following Diyi Yang and her SALT group, because she covers computational social sciences which is such a niche field that I'd really like to work with (I had a paper written in this field before I even knew it existed).\n\n\nHowever, it turns out, based on their twitter, AAAI 2023 just had a talk literally yesterday that featured her group and I missed it.\n\n\nI thought I was paying close attention, but maybe not close enough.\n\n\nHow do you guys stay organized with all of the dates?\n\n\nHow do you find ways to network with top researchers in the field?",
"date": "2023-02-14"
},
{
"vote": 2,
"title": "Master thesis topic overcoming hallucination in NLG",
"text": "Hi all!\n\n\nI am a Data Science graduate student, and I am about to start my final masters project in a few weeks. To start, I need a (rough) research questions about the project I will be pursuing.\n\n\nI have interest in NLG (Natural Language Generation), for example QA models and abstractive text summarization. Since there is a lot of hallucination happening in these models, I would like to research a possible way to improve the factual quality of the generated texts.\n\n\nSince these improvements can be broad, I need to be pointed into some direction where I can start looking. I have read numerous papers in the field of NLG and hallucination, but I just can't find a 'gap' in research or a way to help given my project span of 6 months.\n\n\nCan you please help me help the NLP community?\n\n\nThank you!",
"date": "2023-02-13"
},
{
"vote": 35,
"title": "Open Assistant - open source project needs your help",
"text": "As I mentioned, there is an open source project that focuses on being an open source alternative to OpenAI ChatGPT. To date, we have been collecting QA information from the community. We already have 31k Messages in English, and 22k in Spanish. These are the complete stats from February 12.\n\n\nhttps://imgur.com/a/feRJTlN\n\n\nData collection is going fast, but the number of active users is around 100 people. It's very important that we have as diverse a data set as possible. For the initial data set as well as for future iterations.\n\n\nEverything happens on the \nopenassistant.io\n Write prompts, respond as a chatbot, rank and classify messages and dialogues, and delete inappropriate content. You can authorize through Discord, and we have an active community.\n\n\nI hope it won't be looked as a spam, because it's a genuine attempt, to create truly open-source AI chat bot, you can deploy everywhere you want, and without crazy corporate restrictions.",
"date": "2023-02-12"
},
{
"vote": 3,
"title": "Best Natural Language Processing Courses for Beginners to advanced",
"text": null,
"date": "2023-02-12"
},
{
"vote": 1,
"title": "LEARN NORWEGIAN LANGUAGE NOW!!!",
"text": "[removed]",
"date": "2023-02-11"
},
{
"vote": 15,
"title": "â­• New Open-Source Version Of ChatGPT",
"text": "[removed]",
"date": "2023-02-11"
},
{
"vote": 1,
"title": "[Question] Teaching a new skill to an AI model during runtime.",
"text": "[deleted]",
"date": "2023-02-11"
},
{
"vote": 1,
"title": "Any pretrained models on retail lingo?",
"text": "I recently got a job in retail and wants to give Product Matching a go. I want to embed product titles / names and instead of using a pretrained model on â€œgeneralâ€ text, I wanted to ask you guys if you know of any models pretrained on retail data?\n\n\nI definitely think a general pretrained model would get me far, but for product titles / names the model doesnâ€™t need to know anything about grammar and stuff, but more domain specific lingo.",
"date": "2023-02-10"
},
{
"vote": 1,
"title": "Top 5 Mind-Blowing future Technology Predictions 2050 !!!",
"text": null,
"date": "2023-02-10"
},
{
"vote": 14,
"title": "Karma requirement and/or once-a-week, catch-all Megathread?",
"text": "Long-time lurker on /r/languagetechnology. Over the last year or so (maybe longer) I've noticed a downward trend in the quality of posts on this subreddit: dozens of questions per day that could be answered with a simple Google query are posted; 'do my homework for me' questions abound; and - in recent months - nonsense AGI crystal ball predictions â€“ fueled by public adoption of GPT-3 and other generative models have started taking over this subreddit.\n\n\nIt would be great if we could keep this space as a useful, information-rich space for natural language processing practitioners and researchers to chat. \n\n\nIf folks have beginner NLP questions maybe go to /r/learnMachineLearning and ask there? Or maybe an /r/learnNLP subreddit can be made to house the 'what is tokenization?', 'what is a term-document matrix'-level questions? \n\n\nNot trying to be exclusionary, but rather I'd like to avoid this space becoming a Quora-tier pool of noise, clickbait, and low-effort, karma-farming posts.\n\n\nWhat can we do?\n\n\nWhat are folks' thoughts on the following:\n\n\n\n\na karma requirement to post and/or to comment?\n\n\na once-a-week (Saturday?) catch-all megathread where people can post their beginner questions\n\n\nsome tagging system similar to what \n/r/mlscaling\n uses: \nhttps://www.reddit.com/r/mlscaling/\n (to delineate between research, blog posts / personal content / opinion pieces, questions, etc.)\n\n\nother suggestions? Please leave a comment with your ideas",
"date": "2023-02-09"
},
{
"vote": 8,
"title": "Topic classification on text data with no/few labels",
"text": "I would like to achieve a classification of a text input into predefined categories.\n\n\nFrom what I have understand unsupervised approach are unfeasible if my target label is something very rare in pretrained models (I have labels about specific industrial processes).\n\n\nIs this true?\n\n\n&#x200B;\n\n\nOtherwise I could try an approach in which I label for example 1000 input texts using all the different labels and use a supervised approach with very few labeled data. Should this help someway the learning process? And what methods could I use in this case?",
"date": "2023-02-09"
},
{
"vote": 7,
"title": "Cutting edge language translation models",
"text": "I'm a CS undergrad who's planning on making a multilingual language translation model. I have some experience in CV and I want to explore research options in NLP. \n\n\nI want to know what's the current cutting edge technology for neural machine translation. From what I see, RNN, transformers and GNNs are viable options. Which model is showing the most promise and can anyone link influential papers in this field? Thank you!\n\n\nSide note: Am I being too ambitious to make a machine translation project as a guy with negligible knowledge in NLP? I have around 2 months to complete the project.",
"date": "2023-02-09"
},
{
"vote": 1,
"title": "Chatbot GPT : Generate Content Automatically with NLP Tech",
"text": "[removed]",
"date": "2023-02-09"
},
{
"vote": 1,
"title": "Conversation Analysis transcription driving me crazy",
"text": "Hi everyone, \n\n\n&#x200B;\n\n\nI've recently started doing research for my PhD and a huge part of it is transcribing audio and video files from classes. I've been searching and searching and searching and I cannot believe that I could not find any software to help with that.\n\n\n&#x200B;\n\n\nCould you please recommend any free software that can make transcription easier (GAT2) other than Praat?\n\n\n&#x200B;\n\n\nThank you so much!",
"date": "2023-02-08"
},
{
"vote": 13,
"title": "Where to find public available Datasets for ML on Text and Text Classification?",
"text": "Hello NLP-Community,\n\n\nI've recently started to look into finding Datasets and evaluating what already exists regarding counseling/Q-A/Mental-Health. [College/Uni work]\n\n\nNow, finding specific Datasets concerning those topics is obviously harder, and I will continue to search by myself in that regard. My biggest problem at the moment is finding more sites that collect Datasets, do you guys have any other aggregation sites that have a lot of usable Datasets similar to the ones listed below?\n\n\n\n\nhttps://huggingface.co/datasets\n\n\nhttps://www.kaggle.com/datasets\n\n\nhttps://paperswithcode.com/\n\n\nhttps://archive.ics.uci.edu/ml/index.php\n\n\nhttps://registry.opendata.aws/\n\n\nhttps://dev.socrata.com/data/\n\n\n\n\nI also would appreciate tips for finding Datasets floating about on the internet.\n\n\nCheers.",
"date": "2023-02-08"
},
{
"vote": 9,
"title": "BioGPT Generative Pre-trained Transformer for Biomedical Text Generation and Mining Collab Demo",
"text": "This paper is from Microsoft Research\nFrom the abstract:\n\"BioGPTis a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. BioGPT is evaluated on six biomedical natural language processing tasks and the model outperforms previous models on most tasks. Especially, they get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.\"\n\n\nThe authors claim that previous models like BioBERT PubMedBERT. have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope.Â  \n\n\nBioGPT is available in HuggingFace. \n\n\nIn this video i explain about BioGPT and also show a collab demo of how you can use BioGPT for zero shot question answering. Do checkout: \nhttps://lnkd.in/gS2mYDDs\n\n\nA word of caution: I tried zero shot question answering with just two examples. It may not be representative of model performance. But with clever prompt engineering better results on larger datasets may be possible",
"date": "2023-02-08"
},
{
"vote": 6,
"title": "Do you or someone you know have research openings in NLP? (esp multimodal or healthcare related)",
"text": "2021 Computer Science and Linguistics bachelor's graduate from UIUC here. I'm self funded and working fulltime in industry.  But I wish I was doing research.  \n\n\nI had to give up on my dreams of a PhD due to extenuating circumstances in 2020 and also with my student loans, I wouldn't have been able to afford living on a PhD stipend anyway.\n\n\nBut I've refinanced my loans and realized if I budget aggressively for the next year, I can accumulate enough savings to cover my payments while in college.  I just need to work on becoming competitive again.  \n\n\nI'm trying to find a detour path into a PhD by working as an independent researcher and building letters of recommendation.\n\n\nMy main research interests are with models based on ideas from social sciences and psycholinguistics, multimodal NLP,  conversational AI, healthcare and crossover papers with computer vision.\n\n\nResume:\n\nhttps://github.com/Fuehnix/resume/blob/main/Jacob_Fuehne_resume_for_research.pdf\n\n\nAlso, even if you can't personally get me into a lab, if you know any professors who are really approachable and might be willing to work with an unaffiliated researcher, let me know and I can send them an email.\n\n\nLiterally any help you can offer goes a long way.",
"date": "2023-02-08"
},
{
"vote": 7,
"title": "ðŸš€ Deploy Argilla data labelling tool on Hugging Face Spaces",
"text": null,
"date": "2023-02-07"
},
{
"vote": 0,
"title": "Nikhil Garg (Fennel AI, ex-Facebook, ex-Quora) Teaches Production Grade Python",
"text": "Hey folks - I wanted to put this live course from Nikhil Garg (Fennel AI, Facebook, Quora) on your radar!\n\n\nThe course looks at how to become a better data practitioner by teaching best practices to improve code modularity, enable collaboration and prevent errors in production. It draws on his learnings/insights at Facebook and Quora and features real-world examples from CircleCI and Apache Airflow\n\n\nIt kicks off on Mar 13 and you can find more info here:Â \n\n\nhttps://www.getsphere.com/cohorts/production-grade-python?source=Sphere-Comm-r-lt",
"date": "2023-02-07"
},
{
"vote": 2,
"title": "Just started my thesis, and haven't done any NLP. Would it be crazy for me to use NLP in my thesis?",
"text": "I haven't followed the course for NLP because that was only offered in the first semester of the data science master, and since I was a total newbie, that was not recommended. But I really want to learn it. Would it be crazy to try learning it for my thesis, or is it way too much information and should I pick something I'm already familiar with? Specifically, I'd like to use it to look at books (or maybe social media posts, I haven't decided yet) and combine that somehow with psychology. I'm thinking about maybe trying to diagnose characters in books, but I'm not sure about the societal benefit so it might still completely change. Anyhow, long story short: Would that be dumb or is it very much possible to just learn whatever you need?",
"date": "2023-02-07"
},
{
"vote": 1,
"title": "ClearAI and other alternatives?",
"text": "Have any of you used this service? \nhttps://clearai.net\n\n\nI'm looking for a site that lets me upload large reference docs (25+ pages) and query GPT using that document as a prompt. This site appears to do that for a relatively low price. Any of you tried it? Know of any alternatives that do the same?",
"date": "2023-02-07"
},
{
"vote": 7,
"title": "Has anyone put a Q&amp;A (or IR) model in production?",
"text": "Hi!\n\n\nI am thinking of using something like \ndistilbert-base-cased-distilled-squad Â· Hugging Face\n for a financial customer support application.\n\n\nI will experiment to see how well it generalizes to my domain, but was wondering, has anyone already tried:\n\n\n\n\nUsing a Q&A model for a closed domain? If so, how well did you see it generalized?\n\n\nExposing real users to Q&A models? Did you see it behaving differently than a different/previous solution (if you already had a different model in place)?\n\n\nEvaluating it offline with an online application? If so (and if it possible to know), what metrics did you use to estimate the performance in the wild?",
"date": "2023-02-06"
},
{
"vote": 3,
"title": "How to store hugging face model in postgreSQL",
"text": "Hello, i am a beginner at databases and Im wondering how I can store my trained bert model in postgreSQL? I've tried to use pickle dump with the model and i got:\n\n\nsqlalchemy.exc.InternalError: (psycopg2.errors.InternalError_) invalid memory alloc request size 1073741824\n\n\nAny help or reccomendations would be extremely appreciated thanks.",
"date": "2023-02-06"
},
{
"vote": 6,
"title": "Unlock the Power of GPT-3 in Excel to Analyse Sentiment",
"text": "This hands-on video shows end-to-end workflow from data ingestion to sinking results for the task of Sentiment Analysis -- all inside a spreadsheet.\n\n\nCheck the demo at \nhttps://www.youtube.com/watch?v=SwCXXSSZRts",
"date": "2023-02-05"
},
{
"vote": 24,
"title": "Recommendations for 5 scientific papers on any NLP topic for a total beginner",
"text": "Hi,\n\n\nI am a data analyst who does not have any experience or knowledge in AI/ML.\n\n\nI am looking to find suggestions for 5 scientific research papers (in reputed journals) around any specific area/topic of NLP. This is a part of my graduate course in NLP. Any topic is fine for me as long as the papers are easy to understand and comprehend as I have to make a presentation on them and explain it to others. Will appreciate if someone can help me with this.",
"date": "2023-02-05"
},
{
"vote": 5,
"title": "NLP or facilitate manual text tagging?",
"text": "For a text-tagging project, I'm trying to decide whether to build software to facilitate manual tagging, or to try and come up with an NLP solution. I was hoping you could give me some advice on which would be better.\n\n\nI'm working with a corpus of about 20,000 academic abstracts in my biology subfield. Almost all mention a \nligand\n (an element, molecule or cell type that gets bound by another molecule). To be clear, they don't refer to it as a \"ligand,\" it's just that all the abstracts do mention a specific ligand of some kind. Pb2+, CD61, prostate cancer cells, it could be anything.\n\n\nI want to tag those ligands reliably. The text is all unstructured and I haven't been able to find any heuristics that don't have a lot of false positives/negatives. Phrases like \"detection of ___\" or \"__-binding\" often have the ligand in the ___, but not always. Sometimes, \"detection of ___\" is describing the clinical benefit (i.e. detection of flu, when the ligand is a particular flu protein mentioned elsewhere).\n\n\nOne option is to build software to facilitate manual tagging of these abstracts. I expect that with an investment of 20 or so hours writing software, I could write a tool good enough to get the job done in about 200 hours of boring manual tagging.\n\n\nThe other option is to try and come up with an NLP solution. I'm not seeing a clear way to get the job done reliably, and this big tagging job only really needs to happen once. But I thought I'd ask you experts if there's a perfect NLP tool or technique for my needs before I focus my efforts on manual tagging. Any ideas?",
"date": "2023-02-05"
},
{
"vote": 7,
"title": "NLP Project idea",
"text": "I'd like to work on an NLP project with some level of novelty, anyone could give me an idea or datasets to work on? Any recommendation will be appreciated",
"date": "2023-02-04"
},
{
"vote": 1,
"title": "Support KB Chatbot - how to train best?",
"text": "I have some basic AI/ML knowledge and played around with different projects successfully, but am far from an expert.\nI have built a chatbot in the past and thought instead of providing QnA pairs, wouldnÂ´t it be nice to have an on-premises hosted model that was trained on an extensive product documentation (available as web and pdf) and/or the support kb?  \n\n\nI just read a different post here, where similar task was accomplished for twitter.  \n\n\nIf youÂ´d had to build something like that, what would be the best way?\nSo what should I learn/read more about?",
"date": "2023-02-04"
},
{
"vote": 32,
"title": "Towards a Tagalog NLP pipeline: Building a spaCy model from scratch",
"text": null,
"date": "2023-02-04"
},
{
"vote": 1,
"title": "Top 5 Mind-Blowing future Technology Predictions 2050 !!!",
"text": null,
"date": "2023-02-03"
},
{
"vote": 1,
"title": "ChatGPT-like product?",
"text": "[removed]",
"date": "2023-02-03"
},
{
"vote": 1,
"title": "ChatGPT-like product?",
"text": "[removed]",
"date": "2023-02-03"
},
{
"vote": 5,
"title": "[ADVISE NEEDED] Extracting clauses from contracts",
"text": "Hi everyone!\n\n\nI am currently trying to extract specific clauses from employment contracts that describe something the employee needs to ask approval for from the employer (e.g., requesting time off or requesting a waiver for a non-compete) while ignoring other clauses that do not contain asking-for-approval actions for something (e.g., statement about working days and hours, the position description, or the salary components). And I would like your advice or recommendations on doing this.\n\n\nThe scenario: the employment contracts are in English and PDF format. I have a manually labeled data set of example clauses that I want (containing asking-for-approval actions). The data describes exactly where the clauses are located in the contract (coordinates and page number). This data set is created from multiple employment contract PDFs. Basically, annotations with the full text from the PDF and also the starting and ending coordinates on where it is located on the page of the contract.\n\n\nWhat approach would you suggest or recommend for me to tackle this challenge?\n\n\n&#x200B;\n\n\nThank you very much!",
"date": "2023-02-03"
},
{
"vote": 20,
"title": "Fine tuning mt5",
"text": "How do I fine-tune an MT5 model for generating Bengali paraphrases? I have enough datasets but I can't find a working script to fine-tune an MT5  model.",
"date": "2023-02-02"
},
{
"vote": 2,
"title": "merging two vectors in word2vec",
"text": "lets say X is a vector that contains the traits of person 1\n\n\nand Y is a vector that contains the traits of a person 2 \n\n\nhow to merge X and Y into a vector that describes both",
"date": "2023-02-02"
},
{
"vote": 13,
"title": "Embeddings-guided and Prompt-driven search with Large Language Models (LLMs)",
"text": null,
"date": "2023-02-02"
},
{
"vote": 1,
"title": "Can NLP identify interesting quotes?",
"text": "I don't have any knowledge of NLP or machine learning in general.\n\n\nI have a small product that gathers users' highlights from their books (like ReadWise, but free). I'd like to find a way to separate the 'interesting' highlights (i.e. those you learn something from, although I know it's subjective), from the meaningless ones.\n\n\nExample of 'interesting' highlight:\n\n\n\"As you consider building your own minimum viable product, let this simple rule suffice: remove any feature, process, or effort that does not contribute directly to the learning you seek.\"\n\n\n&#x200B;\n\n\nExample of 'not-interesting' highlight:\n\n\n\"My voice is nothing special, but when your mother tells you something about yourself, even if youâ€™ve coaxed it out of her, itâ€™s hard not to always believe it.\"\n\n\nIt's probably a dumb question, but I'm running in circles on how to automate this selection. I thought  NLP could maybe help, so any insight is appreciated!",
"date": "2023-02-01"
},
{
"vote": 4,
"title": "The Future of Natural Language Processing",
"text": null,
"date": "2023-02-01"
},
{
"vote": 1,
"title": "Kagi Universal Summarizer",
"text": null,
"date": "2023-02-01"
},
{
"vote": 0,
"title": "ChatGPT-like product?",
"text": "We just launched Product AID. AID is an interactive Answer Engine that is similar to ChatGPT but provides answers specific to various domains.  Based on your feedback, We'll continue to grow. Thank you!  AID Product Page: \nhttps://www.getaid.ai/",
"date": "2023-02-01"
},
{
"vote": 2,
"title": "Deltas and Delta-Deltas Features Explained",
"text": "Hi guys,\n\n\nI have made a video on YouTube \nhere\n where I explain how deltas and delta-deltas speech features are computed.\n\n\nI hope it may be of use to some of you out there. As always, feedback is more than welcomed! :)",
"date": "2023-02-01"
},
{
"vote": 6,
"title": "Dirty labelling solution",
"text": "Iâ€™m looking to some aggregation on academic research and news articles to see what insights I get from it. Iâ€™m using textrazor to do named entity recognition on the documents, but getting a lot of dirty labels that have slightly different wording. For example, Tesla, Tesla ltd, Tesla Ltd. As a result, my aggregations have a lot of duplicate results.\n\n\nThe dataset consists of about 4M labels so the solution has to be efficient to be viable. I was thinking of putting the labels through word2vec and then clustering them based on the word embedding distances? But then the problem arises of how many clusters to use?\n\n\nIâ€™ve also tried simple regex preprocessing to get rid of the company abbreviations but there are other examples that cannot be solved that easily.",
"date": "2023-02-01"
},
{
"vote": 1,
"title": "OpenAI AI Text Classifier",
"text": "[removed]",
"date": "2023-02-01"
},
{
"vote": 1,
"title": "Conversion of parametric data describing the product to an understandable product description",
"text": "Hi, I'm wondering what would you say is the best model to create a solution for converting parametric data about a product into an understandable description of the product. Thank you for your suggestions",
"date": "2023-01-31"
},
{
"vote": 10,
"title": "Is there a database of words grouped by their class/category?",
"text": "Forgive me, I'm new to this field and don't know how to say it precisely, but I'm looking for a database in which you can lookup a word and get sets related words based on specific functional categories. For example:\n\n\n\n\nsearching \"east\" would return a category of \"cardinal directions\" and the related words \"north\", \"south\", \"west\"\n\n\nsearching \"now\" would return \"temporal proximity\" which might have \"soon\", \"later\", \"never\", etc.\n\n\n\n\nI've tried various thesauruses but they only give words of either similar or opposite meaning. They won't give you \"north\" if you search \"east\". Does such a database exist?",
"date": "2023-01-31"
},
{
"vote": 3,
"title": "MusicLM Text to Music AI Google Research",
"text": "MusicLM is a model generating high-fidelity music from text descriptions such asÂ \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption.\n\n\nIn this short video i explain the model behind MusicLM. Do checkout \n\nhttps://youtu.be/8rofGhGJmgY",
"date": "2023-01-31"
},
{
"vote": 1,
"title": "Building an Amazon Prime content-based Movie Recommender System",
"text": null,
"date": "2023-01-31"
},
{
"vote": 7,
"title": "ContrastiveLoss vs CosineSimilarityLoss in Sentence Transformers",
"text": "I'm looking at loss-functions for Sentence Transformers on \nhttps://www.sbert.net/docs/package_reference/losses.html\n, and was wondering if \nContrastiveLoss\n has ANY advantage over \nCosineSimilarityLoss\n, apart from that in most cases, it would be easier to find training data with distinct (binary) labels instead of fuzzy (continuous) class membership?",
"date": "2023-01-30"
},
{
"vote": 9,
"title": "ML developments in measuring text readability and text summarization",
"text": "I'm curious whether there are any significant developments in measuring text readability using ML, e.g. transformers. I see that many people still rely on simpler measures (like fog index), because they are easier to calculate and explain. Are there ML models that consistently provide improvement over existing measures? \n\n\n&#x200B;\n\n\nSimilarly, I'm curious about text summarization, which probably is more ML reliant. I get two texts (each contains 1000 words). I want to summarize them without losing content, not to a certain level (both summaries of 500 words). So one summary might be 400 words long, another 800 words long. Is anything like this possible?",
"date": "2023-01-29"
},
{
"vote": 1,
"title": "Stumbled across these translation pens what do you guys think?",
"text": "[removed]",
"date": "2023-01-29"
},
{
"vote": 14,
"title": "looking for opportunities in NLP",
"text": "Hi everybody! I'm going to finish my MSc and an internship in Data Science and I am willing to gain experience in NLP. I'm looking for open-source projects to work with in part -time, since I have a full time job as a high school teacher. Do you need where to find some startup/projects?\n\n\nThank you in advance",
"date": "2023-01-29"
},
{
"vote": 1,
"title": "Using GPT3 for fuzzy templated parsing?",
"text": "[removed]",
"date": "2023-01-28"
},
{
"vote": 6,
"title": "Any tool to add short vowels to persian texts automatically?",
"text": "One challenge that learners of Persian have is the lack of short vowel transcriptions in normal texts. Is there a tool to add vowels automatically? \n\n\nInput:\nØ³Ù„Ø§Ù… \nOutput:\n\n\nØ³ÙŽÙ„Ø§Ù…",
"date": "2023-01-28"
},
{
"vote": 7,
"title": "Open-source Meta-learning for Neural machine translation",
"text": "Hi everybody,\n\n\nI am currently working on the translation of low-resource languages. I am not an expert in this domain which is why I need some help.\n\n\nI need to use the meta-learning algorithm for the translation task. I couldn't find any public code for this subject. I once used the Fairseq for my transformer setup.\nThe problem is I really don't have any idea of how to apply meta-learning for translation. I can have the baseline for other tasks like classification, but how I can use it just for translating text datasets?\n\n\nThank you in advance for your help.",
"date": "2023-01-27"
},
{
"vote": 20,
"title": "How would you approach this kind of Info/Entity extraction problem?",
"text": "Training dataset is as follows:\nThree columns:\nText (unstructured)\nLeads counts (integer)\nConversion counts (integer)\n(Thatâ€™s not an actual case, itâ€™s an example.)\n\n\nFor a given article, either could be zero, or not present.\n\n\nLooking to train a model that, when given a new text, extracts Leads count and Conversion count.\n\n\nThose are not counted one by one, they should be extracted from sentences.\n\n\nFor instance, text may be:\n\n\nOn Tuesday, there was a large industry convention. We spoke with 12 people who were interested in the product. Out of them, three people decided to buy it. All in all, it was a ten out of ten event. There is another one on February 23 that Iâ€™m looking forward to.\n\n\n\nModel should return leads=12, conversions=3.\n\n\nI think how I want it to function is something like:\nWhen encountered a number \nCheck that sentence and sentence before and after\nClassify as leads number, conversions number or neither.\n\n\nI started on doing fine tuned BERT similar to named entity recognition, but that feels like an overkill, feels like I should be able to go a little lighter.\n\n\nWhat do you think?",
"date": "2023-01-25"
},
{
"vote": 1,
"title": "Bi-Encoder with BERT does not learn",
"text": "My data consists of 15k question and answer pairs. I am using a bi-encoder with a pre-trained BERT model, to obtain the most fitting answer for a new question. Each question/answer pair has a category name, which I added to the beginning of each question and answer. I'm using a qrels file as well, which has the relevancy = 1 for all the question/answer pairs, and that's about it.\n\n\nSame dataset gave me acceptable mean metrics on BM25 (0.4 recall). But the bi-encoder fails to learn anything meaningful, all metrics are nearly zero after training for >10 epochs, batch size being 16. \n\n\nWhat could be the possible causes? Where should I start looking at?",
"date": "2023-01-25"
},
{
"vote": 13,
"title": "Overview of transformers, LLM's, and InstructGPT from Gridspace MIT IAP series",
"text": null,
"date": "2023-01-25"
},
{
"vote": 2,
"title": "DuplexGPT - combining Whisper and GPT to automate calls (e.g. book a restaurant)",
"text": null,
"date": "2023-01-25"
},
{
"vote": 17,
"title": "Targeted Summarization - A tool for information extraction",
"text": "&#x200B;\n\n\nVisual of the Algorithm\n\n\nHere's the GitHub repo: \nhttps://github.com/helliun/targetedSummarization\n\n\nTextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library. Unlike many techniques for extractive summaries, TextReducer has the option for a \"target\" around which the summary will be focused. This target can be any text prompt, meaning that a user can specify the type of information that they would like to find or summarize, and ignore everything else.\n\n\nAnother key benefits of TextReducer is that rather than extracting the sentences for the summary, it carves away at the original text, removing unnecessary sentences. This leads to more fluent summarizations, and preserves grammatical features like coreference that are often lost in traditional extractive summarization.\n\n\nFor instance, in the sentences \"In his free time, John enjoyed playing golf and traveling with his family. He was married with two children, and lived in a suburban area with his wife and kids.\", it is imporant that these sentences stay linked together. Otherwise, the coreferent of the word \"He\" in the second sentence is lost. TextReducer is much better at preserving such related sentences, and is thus a valuable tool for fast, but fluent summarizations of large texts.",
"date": "2023-01-25"
},
{
"vote": 11,
"title": "What is the largest model that can be feasibly trained on a RTX 4090 24GB?",
"text": "I am interested to hear what the largest model is that I can feasibly train on a single RTX 4090 24GB card. For sure the upper bound is a model of 24GB max. If we additionally take into account the additional RAM memory needed to do inference, loss back propagation etc, how large can we go? I understand this will also depend on the batch size, so let's fix that to 16 to have an explicit example. Does anyone have experience with training such a model and this card? What is the largest model you reached?\n\n\nAdditionally, if I have two RTX 4090 24GB cards, is it feasible to split the model over these two cards? Would this allow me to fit a model roughly twice as big as on one card, or is there significant overhead?\n\n\nI would appreciate any insight you have.",
"date": "2023-01-25"
},
{
"vote": 0,
"title": "Best Natural Language Processing Books for Beginners to read",
"text": null,
"date": "2023-01-25"
},
{
"vote": 3,
"title": "How to create a caption from the question-answer pair?",
"text": "I was wondering if it is possible to create a caption or a sentence from the given question-answer pair.\n\n\nSo given any question-answer pair, I want a caption. Is there any existing work on this? How can I achieve this?\n\n\nFor example :\n\n\n\n\n\n\n\n\nQ: What is on the table?\n\n\nA: A bottle\n\n\nI want to get something like: \"A bottle is on the table.\"\n\n\n&#x200B;\n\n\n2) \n\n\nQ: What is the man doing?\n\n\nA: jumping\n\n\nI want to get something like: \"The man is jumping.\"\n\n\n&#x200B;",
"date": "2023-01-25"
},
{
"vote": 2,
"title": "Data preparation for embedding",
"text": "I need to improve the quality of embeddings for a specific task. \nI have a huge Text corpus but it doesnâ€™t have any labels or similarity indicators attached to it \n Can somebody point we into the right direction where to get started ?",
"date": "2023-01-25"
},
{
"vote": 5,
"title": "Debiasing GPT-3 model output: any idea as to what that process looks like at OpenAI?",
"text": "Last year's inverse scaling contest revealed some interesting trends in de-biasing / bias mitigation attempts by OpenAI in its GPT-3 models (\nada\n ---> \nbabbage\n---> \ncurie\n---> \ndavinci\n). Prompts in which inverse scaling phenomena were most apparent included topics such as race, gender, ethnicity, class, religion, etc. \n\n\nDoes anyone have any idea as to what OpenAI's bias mitigation process looks like for the various sizes of GPT-3? I'd imagine that GPT-3 was trained on the large dataset (bigger than the Pile by quite a lot) and then, after the fact, the various models were put through the 'de-bias' fine-tuning wringer. And then those models were deployed as Models as a Service: \nada\n, \nbabbage\n, \ncurie\n, \ndavinci\n, etc. \n\n\nI'm wondering if anyone knows what the distillation process looked / looks like? Or if anyone knows anything about the debiasing process at OpenAI for the GPT-3 variants?",
"date": "2023-01-24"
},
{
"vote": 9,
"title": "Hey developers! We've launched a Kaggle competition for finding accurate coordinates from text alone ðŸŒŽðŸ“",
"text": null,
"date": "2023-01-24"
},
{
"vote": 0,
"title": "Need help with a project.",
"text": "I have a text and a reason and I need predict if \ntext\n satisfies the \nreason\n. But the catch here is the training dataset only has positive examples, where reason satisfies the text. Can someone help me how to train a model?",
"date": "2023-01-24"
},
{
"vote": 3,
"title": "How to fine-tune T5 for multiple tasks?",
"text": "I am fine-tuning T5 for multiple tasks so that they work together. I want the models to work together. For example, summarization and translation should work together. The summarized text is the input to the translation model and gets translated. \n\n\nQ1. Can we train T5 with different datasets and prefixes and expect it to work this way?\n\n\nQ2. Is it possible to concatenate the models and make them one single model?",
"date": "2023-01-24"
},
{
"vote": 20,
"title": "Semantic Search + LLM grounded answer for your PDFs, youtube and webpages",
"text": null,
"date": "2023-01-23"
},
{
"vote": 7,
"title": "Is Rasa Masterclass still relevant in 2023?",
"text": "[removed]",
"date": "2023-01-23"
},
{
"vote": 11,
"title": "Grad program advice",
"text": "Hi, so lately, I have been in a bit of a fix about my career goals and the whole grad school process. \n\n\nFor context, I am a college senior majoring in ECE set to graduate July of this year. The main area of interest I have, falls in the realm of signal processing and human computer interaction, however, halfway through my junior year, I also got interested in natural language processing. \nI dug deeper into the subject and found one particular area I am really passionate about working in, my eventual plan is to get a PhD, but since I lack research experience, I thought of going for an MS to gain the necessary foothold to get into a good PhD program.\n\n\nI am applying for the Fall'23 cycle and have mainly opted out for the CS programs in most of the Universities. There are some professors I really want to work with (eg. Prof. LP Morency from CMU LTI). I researched and chose the programs and schools based on the faculties and how well they would fit with my research interest (although I know MS is more of a coursework based degree, I plan to do research as early as possible). Even though there's plethora of information on CS based programs, I have hardly come across some specific Language technology programs, I would be really glad if somebody could advice me on the career possibilities and the program structure.\n\n\nThe next thing I have been worrying about is whether applying from a non -CS background would hurt my chances of being accepted into CS programs? And even though I have developed an interest in NLP, I also like Signal Processing too, and would like to work in an area which is interdisciplinary involving the combination of these two topics. I have looked into some signal processing tracks in ECE/EE programs, but most of them did not fit in with what I was looking for, I was wondering if somebody could shed light on this, and maybe have any idea of programs that are more research based and have a curriculum aligned with what I am looking for?\n\n\nLastly, I am worried that I have severely overestimated my chances of getting into the schools I have applied to. The ones I have mostly applied to are top schools(CMU, Berkeley, UCSD, Dartmouth...).\n\n\n I feel like my profile is not as good for getting into these schools(All I have is an experience as a remote research intern for more than a year and a few ongoing projects, and a very moderate GPA. I have been told that my SOP is good, along with a fairly strong LOR).\n\n\nI thought I had a clear goal in mind when I had started off with my applications during this cycle, but as more time goes by, I am starting to worry about my plans and questioning everything. The whole application process is so costly too, my heart bleeds everytime I pay for one(which I know is totally on me), but dread starts creeping up on me when I think about getting rejected from all the programs I have applied to. I would be really glad if I could get some help or insight on this. Thanks.",
"date": "2023-01-23"
},
{
"vote": 1,
"title": "Introduction to C Programming: A Beginner's Course",
"text": null,
"date": "2023-01-23"
},
{
"vote": 1,
"title": "AI Data Solutions - Language Experience | Inwhatlanguage",
"text": "[removed]",
"date": "2023-01-22"
},
{
"vote": 3,
"title": "Ideas on how to improve classification and scoring using Mean Pooled Sentence Embeddings",
"text": "I'm working on a project where the data are available in the form of short one-sentence texts. Each text has to be scored in relation to two categories that are the complete opposites of each other.\n\n\nI want to accomplish something similar to sentiment analysis, but without training a model on data and use Mean Pooled Sentence Embeddings instead. This is because I have multiple categories classification and little to no training data.\n\n\nSo my approach is pretty simple:\n\n\n\n\nManually author a small set of sentences that are representative for Category A (for example, let's assume that this category should capture \"\nsadness\n\")\n\n\nDo the same for Category B (\"\nhappiness\n\" in this example).\n\n\nSentences for Category A and Category B are embedded in a Sentence Transformer Model and averaged for each category, creating prototypical representation vectors for \"\nsadness\n\" and \"\nhappiness\n\".\n\n\nWhen scoring texts in my data set, I now calculate the Cosine similarity to each of the two Categories. If the similarity to Category A is greater than to Category B, the score is negative. Vice versa if the similarity to Category B > Category A. So this merely determines the sign of the score.\n\n\nThe score itself is calculated by combining sentences in both Category A and Category B, taking the embeddings and averaging them.\n\n\n\n\nThis works pretty well most of the time and doesn't require more than just about 5 well-authored sentences in each category. \n\n\nHowever, there is one major problem:\n\n\nSometimes the similarity to Category B is just a little bit higher than to Category A, although the text should belong to Category A. So the absolute score may be a good estimate, but the sign is off. This leads to that a few miss-classifications radically decrease the quality of the predictions.\n\n\nI'm sure the method I'm using is quite common, since Mean Pooled Sentence Embeddings is a frequent topic here. Any ideas on how to improve or does anyone use more sophisticated techniques?\n\n\n&#x200B;\n\n\nThanks",
"date": "2023-01-22"
},
{
"vote": 22,
"title": "Seeking advice on improving NLP search results",
"text": "I'm new to NLP and have been trying to figure out how to get relevant info. I have a set of 3000 texts and I have to find the most relevant ones based on user queries.\n\n\nI Started off with USE and made an API that gives the top 100 results. Calculated the cosine similarity and sorted it, but the precision and recall ain't great. I'm thinking it's because the texts are super specific.\n\n\nIâ€™m thinking of using elastic-search with USE to get better results. But I donâ€™t know how to combine the rankings from both methods. I have to figure out how to average the ranks or bump up the texts that come up in both rankings. Any suggestions or approaches would be appreciated.",
"date": "2023-01-22"
},
{
"vote": 3,
"title": "I need some guideline regarding a project!",
"text": "I have a bunch of Job ads... And I wanna extract job responsibility given in the job ads.. I have around 3000 labeled samples where I specified the beginning and end of the job responsibility section..\nHow should I approach this problem?",
"date": "2023-01-22"
},
{
"vote": 1,
"title": "Are there tools to give summaries or ways to explore the common trends or topics of your email inbox?",
"text": null,
"date": "2023-01-22"
},
{
"vote": 1,
"title": "Temple Fair",
"text": null,
"date": "2023-01-21"
},
{
"vote": 1,
"title": "Chinese New Year Song",
"text": null,
"date": "2023-01-21"
},
{
"vote": 1,
"title": "Story of Nian",
"text": null,
"date": "2023-01-21"
},
{
"vote": 2,
"title": "FastText Value Error: File cannot be opened for training!",
"text": "I Have installed fasttext module in Python and loaded the model [ 'cc.en.300.bin'].\n\n\nI already made the data frame format according to the fasttext. and then generating the files\n\n\ntrain.to_csv(&quot; ecomm.train&quot;,columns=[&#039;Category_description&#039;], index= False, header= False) test.to_csv(&quot;ecom.test&quot;, columns=[&#039;Category_description&#039;], index= False, header= False)\n\n\nthe files were created successfully! then when I run this code\n\n\nimport fasttextmod= fasttext.train_supervised(input=&#039;ecomm.train&#039;)\n\n\nI get this error\n\n\nTraceback (most recent call last): File &quot;/Users/rosie/Documents/ProGraMinG/Python/pythonProject/FastText/FastText_overview.py&quot;, line 97, in &lt;module&gt; mod= fasttext.train_supervised(input=&#039;ecomm.train&#039;) File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/fasttext/FastText.py&quot;, line 533, in train_supervised fasttext.train(ft.f, a) ValueError: ecomm.train cannot be opened for training!\n\n\nfor weeks i have been trying several solutionds but nothing seems to work! any advice is appreciated it",
"date": "2023-01-21"
},
{
"vote": 2,
"title": "Opinion/Suggestions for my final year project idea",
"text": "I'm thinking of building a chatbot that will resolve your grammar doubts which would help people who are learning English.\nIs it possible for a third year engineering student to complete this in one year time?",
"date": "2023-01-21"
},
{
"vote": 0,
"title": "How do machine learning models convert inputs into vectors?",
"text": null,
"date": "2023-01-20"
},
{
"vote": 17,
"title": "I made a twitter bot that summarizes IR papers from Arxiv.com",
"text": null,
"date": "2023-01-20"
},
{
"vote": 3,
"title": "I got frustrated with the time and effort required to code and maintain custom web scrapers, so I built an LLM-powered tool that can comprehend any website structure and extract the desired data in the preferred format.",
"text": null,
"date": "2023-01-19"
},
{
"vote": 27,
"title": "Fine-tuning GPT Models With Docker and WandB",
"text": "GPT models are very powerful.  What makes them even more powerful is fine-tuning the models on your own data.  However, installing all the needed packages can be a large headache if you want to fine-tune the larger variants.\n\n\nThis video goes over a repo that allows one to use a docker image and wandb to easily fine-tune models without headaches.\n\n\nhttps://youtu.be/usz8JOxgQFs",
"date": "2023-01-19"
},
{
"vote": 5,
"title": "Summarizing Text using In-database NLP through the Integration of Hugging Face with MindsDB",
"text": null,
"date": "2023-01-19"
},
{
"vote": 9,
"title": "Extract \"Intent\" from conversational dialogues",
"text": "Hello people, I'm a budding NLP enthusiast and working on a college project trying to find out Intent from conversational dialogues. So, we have a huge dataset of customer call transcripts wherein customers call to get their queries resolved. We're trying to make a bot which is going to identify the intention with which the customer is calling during the first portion of his/her speech and going to divert the call to the queue of that specific team. \n\n\nWe've identified 6 different general classes in which we can broadly classify the calls which we receive from the customers.  Ex. Insurance, claim, policy, payments, etc ( similar banking domain terms )\n\n\nMy approach going until now has been a mixed one:\nStep 1 : keyphrase extraction from the earlier parts of customer utterances. ( Fine tuning KeyBERT for that )\nStep 2 : Noise removal from the phrases ( stop words etc )\nStep 3 : Phrase normalisation to reduce the redundant and repeated phrases\nStep 4 : Extracting the key entities from Phrases \nStep 5 : Representing these entities in word embeddings and try finding similarity using cosine\nStep 6 : Clustering these entities \n\n\nIf there is any better idea than this please do comment and let me understand your views. Any help would be highly appreciated. \n\n\nThanks a lot dear community!",
"date": "2023-01-19"
},
{
"vote": 0,
"title": "Splitting words to better isolate meanings?",
"text": "Hi all.\n\n\nI'm new to NLP but have done some very basic ML project in the past like deep Q-learning for inverted pendulum problem etc, and so have some very basic intuitions about training data and architectures. I'm currently trying to apply doc2vec in Python/gensim based on \nthis tutorial\n. I am still at the \"Preparing the Corpus\" phase (which takes a long time), but I can see the document of word IDs/occurrences and I have some concerns.\n\n\nFirst of all, there are a lot of words that I don't recognize as English words. Perhaps they are something like names or Latin words. Usually they have small counts (in my case <600). I thought maybe I could remove these non-words from the corpus by increasing the occurrence threshold whereby a word with low occurrence would be omitted. Unfortunately there are some actual English words that would be lost in this case, for example \"suspenders\" (493).\n\n\nThis leads to my next observation. \"suspenders\" contains the very common word \"suspend\" modified by the suffix \"ers\", and in fact all words modified by \"ers\" are modified in a vectorially-similar way. So it would seem reasonable to treat these suffix as words themselves, such as \"ly\", \"ers\", \"ed\", \"ing\", and \"s\". Perhaps I could modify the corpus by adding a special character before these suffixes that would cause them to be treated as separate words (apostrophe?). For me this seems to suggest that English is not ideal for _2vec algorithms, and that a better language would not concatenate words the way English does. Thai, for example, allegedly consists only of monosyllabic words, such that \"going\" in Thai would actually be 2 words, so I wonder if _2vec might work better in Thai than in English.\n\n\nAnyway, if \"suspend\" and \"ers\" were considered separate words, I wouldn't have the problem of losing words like \"suspenders\", because both \"suspend\" and \"ers\" would have sufficiently high counts. The downside is that I wouldn't be able to validate the algorithm using the common method of comparing the difference vectors between words with and without these suffixes, as I have read is sometimes done (is there a name for this?)\n\n\nSo my question is how this problem has been or should be dealt with. Should I create a database of prefixes and suffixes and split words based on these? Is splitting them with apostrophe okay in that case? And then finally the very practical question (for those who are well versed in gensim) of where in the gensim code I might make these modifications? Or can I make changes to the wordID.txt file directly and just reprocess the model somehow?\n\n\nThank you for any advice or insight you can provide.",
"date": "2023-01-19"
},
{
"vote": 0,
"title": "GPT-4 Will Be 500x Smaller Than People Think - Here Is Why",
"text": null,
"date": "2023-01-19"
},
{
"vote": 2,
"title": "Which readability metrics/algorithm is best suited for evaluating readability of source code comments?",
"text": "I am aware of readability metrics such as lesch-Kincaid Reading Ease Score, the Flesch-Kincaid Grade Level Score, the Gunning Fog Score, etc. \n\n\nCurrently leaning towards Gunning Fog Score as it's used more for technical documents. I'm not sure to what extent the choice of metric really matters for evaluating comment readability but I would appreciate some opinions and why you think a certain metric is better suited for this task. Thanks!",
"date": "2023-01-19"
},
{
"vote": 25,
"title": "Training BERT from Scratch on Your Custom Domain Data: A Step-by-Step Guide with Amazon SageMaker",
"text": "Hey Redditors! Are you ready to take your NLP game to the next level? I am excited to announce the release of my first Medium article, \"Training BERT from Scratch on Your Custom Domain Data: A Step-by-Step Guide with Amazon SageMaker\"! This guide is jam-packed with information on how to train a large language model like BERT for your specific domain using Amazon SageMaker. From data acquisition and preprocessing to creating custom vocabularies and tokenizers, intermediate training, and model comparison for downstream tasks, this guide has got you covered. Plus, we dive into building an end-to-end architecture that can be implemented using SageMaker components alone for a common modern NLP requirement. And if that wasn't enough, I've included 12 detailed Jupyter notebooks and supporting scripts for you to follow along and test out the techniques discussed. Key concepts include transfer learning, language models, intermediate training, perplexity, distributed training, and catastrophic forgetting etc. I can't wait to see what you guys come up with! And don't forget to share your feedback and thoughts, I am all ears! #aws #nlp #machinelearning #largelanguagemodels #sagemaker #architecture \nhttps://medium.com/@shankar.arunp/training-bert-from-scratch-on-your-custom-domain-data-a-step-by-step-guide-with-amazon-25fcbee4316a",
"date": "2023-01-19"
},
{
"vote": 1,
"title": "Privacy Policy - Website Privacy Policy | Inwhatlanguage",
"text": "[removed]",
"date": "2023-01-19"
},
{
"vote": 5,
"title": "Create metaphor from text using GPT or some other text generator",
"text": "Hi, is it possible to create metaphors from complicated text using a text generator or another method?\n\n\nThe idea would be roughly the following:\n\n\nConvert this:\n\n\n\"A neural network uses multilayer nodes and backpropagation trained on known outputs in order to predict unseen data\"\n\n\nto this:\n\n\n\"A neural network is based off of principles from neuroscience on how people learn.\"\n\n\n&#x200B;\n\n\nAnother example would be converting this:\n\n\n\"INTP have primary Ti, secondary Ne, tertiary Si and inferior Fe.\"\n\n\nto this:\n\n\n\"INTP are known for thinking up and contemplating many ideas at the same time, then using their insights from current social environment in order to implement them.  Their sensing of other's feelings are not well developed, but can develop over time.  Finally, they are poor in reading the environment of a room and creating social harmony.\"\n\n\nSo something that expands definitions for laymen or creates metaphors from complicated definitions.",
"date": "2023-01-18"
},
{
"vote": 3,
"title": "Help needed on the possible way forward",
"text": "I have a corpus of around 500 articles -each with around 1000 words of text. Now, I have users entering text to search for and I need to narrow down to the exact passage. This needs to be done by the system asking a bunch of relevant questions to narrow down to the exact passage.\nIs there a way that this entire process? The engine needs to understand the text, ask relevant questions to the user and based on the answers provide him with the relevant paragraph of a document.\nThanks in advance",
"date": "2023-01-18"
},
{
"vote": 9,
"title": "Keyword/Keyphrase extraction from (shop) item title",
"text": "Hi, can anyone suggest models, papers, GitHub repos or something like that cloud help me to get started at keyword/keyphrase extraction?\n\n\nI currently have some kind of dataset of titles and keywords/keyphrases that I and the company I am currently working for processed.\n\n\n&#x200B;\n\n\n\n\n\n\n\n\nTitle\n\n\nKeyword/Keyphrase\n\n\n\n\n\n\n\n\nMens Japanese Koi Carp Artistic Fish T-Shirt T-Shirt Anime Art Kanji Tokyo Cool\n\n\nJapanese Koi Carp Artistic Fish\n\n\n\n\n\n\nReality Glitch Men's You Are Here T-Shirt\n\n\nYou Are Here\n\n\n\n\n\n\nSolar System & Planets Kids T-Shirt Cool Space\n\n\nSolar System & Planets\n\n\n\n\n\n\nDo You Want To See My Panda Impression? Mens Funny Animal Flip T-Shirt\n\n\nDo You Want To See My Panda Impression?\n\n\n\n\n\n\nAs far as the Title goes it's usually structured like this:\n(generic word/brand/or nothing)(KEYWORD/KEYPHRASE)(Generic words/design codes in some cases)  \n\n\nBy generic words, I mean T-shirt, Hoodie, Blue, sign, etc. In other words, they are related to themselves and not the keyword itself (not all the cases but generally speaking) and usually very repetitive between titles.  \n\n\nI would really appreciate the help",
"date": "2023-01-18"
},
{
"vote": 6,
"title": "Which LLM is used for this?",
"text": "Super curious if anyone familiar with trying out different LLMs (Bloom, GPT-J, Neo, etc) knows what LLM might be used for this AI twitch bot that answers chat: \nhttps://www.youtube.com/watch?v=wW0y2CeQVM4\n\n\nIm guessing its either fine tuned GPT3 or One of the open source models (bloom? Flan?) but I haven't tried those so I am not sure.  \n\n\nSeems very similar to something like \ncharacter.ai\n but that LLM is proprietary afaik",
"date": "2023-01-17"
},
{
"vote": 6,
"title": "Online video course to refresh and update my knowledge?",
"text": "I have a masters degree in computer science, specializing in artificial intelligence. I worked with some NLP 5 years back, but my knowledge is kind of stale.\n\n\nIs there a relatively new course on NLP? I prefer advanced level, and if the course takes on GPT like networks and chat-bots then that's a big plus. My aim is to prepare for a job interview at a place for developing those online chat-bot support helpers.\n\n\nThanks!",
"date": "2023-01-17"
},
{
"vote": 5,
"title": "Interactive Evolutionary Computation and ChatGPT",
"text": null,
"date": "2023-01-16"
},
{
"vote": 10,
"title": "Does a program like this exist?",
"text": "I'm looking for a program that, on a basic level, does this:\n\n\nRuns on my PC constantly.\n\n\nTakes the audio from my computer, in this case, hopefully the audio from a game chat.\n\n\nTranslates the audio constantly, so I can understand the conversation.\n\n\nI use online games, and chatting with people, to help me learn Spanish. Simply setting a phone next to the speakers of my computer doesn't seem to work well, because the app automatically shuts the mic off after a certain amount of time. Even on the \"Conversation\" mode of Google.\n\n\nAny recommendations are greatly appreciated, thank you.",
"date": "2023-01-15"
},
{
"vote": 3,
"title": "Chatbot based on game of thrones book series",
"text": "I am a software developer and want to startlearning ML (more specifically NLP). Thought what better way to start than by working on a project.\n\n\nProject Idea\n:\n\n\nBuild a chatbot that answers queries specific to Game of Thrones book series. For ex: If I ask it \n\"What is arya's pet direwolf name\"\n, It should respond with \n\"Nymeria\"\n or if I ask \n\"What is Jon snow's uncle name who was with him in the Night's watch\"\n, It should reply \"Benji Stark\".\n\n\nThe reply need not be only 1 or few words. It would be amazing if it can stich information from multiple section of the books and respond. Ex: If i ask it who \"Tell me about Jon snow\" and it should respond with: \n\"Jon snow is the bastard son of Ned stark\" (info from book1), \"He was also the commandar of the nights watch\" (from the later books) and so on\n. I think this might get complicated so for starters the bot replying with only 1 or few words is fine.\n\n\n1 obvious way that I know is just type in the search query in google and return the result from the first reponse (or something similar) but I would rather like to build a model/program that will take as input the search query and the text from the all the books and return a response without calling any external apis.\n\n\nNow I have the problem statement but got no clue how to start making something like this. Any specific open source libraries that would help me with this ? Any help on ideas/algorithms/blogs/books are also appreciated.",
"date": "2023-01-15"
},
{
"vote": 2,
"title": "Paper On Scaling LLMs Shines Light On The Future Of AI â­•",
"text": "[removed]",
"date": "2023-01-14"
},
{
"vote": 1,
"title": "Viduu",
"text": "[removed]",
"date": "2023-01-13"
},
{
"vote": 25,
"title": "Train a language model from scratch",
"text": null,
"date": "2023-01-12"
},
{
"vote": 4,
"title": "Decoding warning messages in SPACY3",
"text": "0\n\n\nI have basically 2 problems. I am using SPACY to train a model and trying to evaluate the same. When I take the train/test data I am being thrown \nerror\n:\n\n\nUserWarning: [W030] Some entities could not be aligned in the text &quot;Results of 16S rRNA gene sequence comparisons reve...&quot; with entities &quot;[(67, 72, &#039;type_strain&#039;), (142, 159, &#039;organism&#039;), ...&quot;. Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (&#039;-&#039;) will be ignored during training.\n\n\n\nThe string in question is\n:\n\n\n(&quot;Results of 16S rRNA gene sequence comparisons revealed that strain ACD12T shared the highest degree of 16S rRNA gene sequence similarity with Actinomadura sputi DSM 45233T (98.3 %) and Actinomadura hallensis DSM 45043T (97.8 %).&quot;,{&quot;entities&quot;: [(67, 72, &#039;type_strain&#039;), (142, 159, &#039;organism&#039;), (161, 170, &#039;strain_number&#039;), (185, 206, &#039;organism&#039;), (208, 217, &#039;strain_number&#039;)]})\n\n\n\nSo my first question is how do I store the warnings in a log file? And second, I am unable to troubleshoot this particular warning as it turns out of a training set of 3000 sentences this warning afflicts almost half.\n\n\nAny pointers would be appreciated.\n\n\nThanks",
"date": "2023-01-12"
},
{
"vote": 1,
"title": "High School Opportunities",
"text": "Hello everyone,\n\n\nI am currently in high school and interested in combining languages + CS for my college major. Are there any opportunities that you suggest I could do?\n\n\nThank you so much!",
"date": "2023-01-12"
},
{
"vote": 6,
"title": "What are the best NLP courses you've ever taken online?",
"text": "I am looking for recommendations",
"date": "2023-01-12"
},
{
"vote": 15,
"title": "Introduction to Reinforcement Learning with Human Feedback",
"text": null,
"date": "2023-01-11"
},
{
"vote": 1,
"title": "The Journey from search to AI Assistants â€” Part 1",
"text": null,
"date": "2023-01-11"
},
{
"vote": 7,
"title": "Using Transformers to Cluster Whole Documents",
"text": "I'm interested in comparing how a context-sensitive representation of text documents like BERT compares to a BoW approach like TF/IDF in clustering.  However, it's not clear to me how to make the comparison if we use whole documents, as opposed to single sentences, headlines, first 25 tokens, etc. It's simple to represent a news article or short story as a vector using TF/IDF, but to the best of my understanding, to use BERT the sequence length for inputs is 512 tokens, so basically you could max represent about 2 pages of text (510-token + a start/end token).\n\n\nDoes it make sense to:\n\n\n\n\nread in documents to a dataframe\n\n\nchunk them by max length (510 tokens) and add start/end tokens\n\n\ndiscard rump chunks (let's say smaller than 100 tokens) and pad any 100+ rump chunks\n\n\nrepresent the documents as tensors and then extract the hidden states for each document so you have a numerical representation to cluster on (say K-means)",
"date": "2023-01-11"
},
{
"vote": 1,
"title": "NLP Lab: Free No-Code AI for Text Annotation",
"text": "[removed]",
"date": "2023-01-11"
},
{
"vote": 8,
"title": "Fast and accurate data labeling strategies",
"text": "Don't we all want to be able to generate tons of training/validation data very quickly, cheaply, and also want it to be accurate? Well, you're not alone! Here are some tried and tested strategies to do this: \nhttps://medium.com/gitconnected/the-truth-about-labeled-data-9c7c3645322f",
"date": "2023-01-11"
},
{
"vote": 5,
"title": "Understanding the Breath of NLP",
"text": "I am interested in working on NLP project with deep learning to practically apply the knowledge I learnt over past year. I am new to NLP and I am not sure what is the breath of NLP and what it actually can do with deep learning. I would much appreciate if someone could point me to a video, blog post or a book that survey the practical application, what NLP can be used for and the limitations. (I am not looking for a video to learn the concepts of NLP but a video/blog/book that emphasize what it can do and can be used for)",
"date": "2023-01-10"
},
{
"vote": 5,
"title": "Featured-context-free-grammars",
"text": "NLTK has an implementation of what it calls a Feature-based grammar; basically an extension of CFGs to allow constraints in the form of agreement between features of tokens (typically things like number, person, and grammatical gender).\n\n\nAre there any other, ideally faster, implementations of the same thing out there? Or similar ideas that have wider uptake in terms of implementations? NLTK I get the idea is optimised for clarity rather than speed.\n\n\nPS: I'm aware of the inherent limitations of CFGs and manually-specified grammars in general, but I'm dealing with a limited-resource domain.\n\n\nETA: on doing some reading around, it seems like Freeling's Txala parser capabilities might be the kind of thing I'm after.",
"date": "2023-01-10"
},
{
"vote": 2,
"title": "Active/passive classifier",
"text": "Hello! I am looking for a library or a model to classify weather a sentence is in its active or passive voice (eg with respect to the main verb if the are many). The method does not need to be super accurate (I am using it as a filter on a silver corpus) but obviously the more accurate the better. I am preferibly looking for a library or a model fine-tuned for the task. Sentences are in English. I would prefer to use python though it is not a hard constraint. \n\n\nDo you have any recomendation?",
"date": "2023-01-10"
},
{
"vote": 3,
"title": "Gridspace kicked off the speech ML series for MIT IAP today, starting with sound &amp; acoustics",
"text": null,
"date": "2023-01-09"
},
{
"vote": 14,
"title": "Taxonomies/Ontologies â€“ where to start?",
"text": "Hi everyone!\n\n\nIâ€™m a library scientist/manager who came across this software called Prodigy by Explosion AI, got curious about it and accidentally discovered this universe of taxonomies for NLP.\n\n\nIâ€™ve done taxonomies for other contexts ever since I was in uni, as this is a fundamental part of my career and now Iâ€™m fascinated at the fact that this knowledge can be applied in ML and AI!\n\n\nHowever, I feel lost! I do not know where to start if I want to focus my career on this. I would be super grateful if anyone could provide some guidance!\n\n\nThanks!",
"date": "2023-01-09"
},
{
"vote": 7,
"title": "Free Stanford Webinar: GPT-3 &amp; Beyond",
"text": "Join Stanford Professor Christopher Potts on 1/18 as he discusses the significance and implications of recent NLU developments including GPT-3.  He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them. \n\n\nCan't attend the live session? Register at the link below and we will send you a recording.\n\n\nhttps://learn.stanford.edu/WBN-AI-GPT3-and-beyond-registration-2023-01-18.html",
"date": "2023-01-09"
},
{
"vote": 0,
"title": "OpenAI GPT vs ChatGPT",
"text": "A lot of people dont' understanf the difference.",
"date": "2023-01-09"
},
{
"vote": 0,
"title": "First I am not schizophrenic in anyway and have a solid materialistic perception of the world so no stale schizo jokes pls.",
"text": "The problem is, I just keep seeing the exact same word immediately in two entirely different medias. Examples: the last word on the (paper) book I was reading and the first word I noticed on my computer screen. The word I just said to myself and the first word I randomly saw in my dictionary. Once when I was thinking about what to say and the other person suddenly misspoke and said the exact same word I was thinking, and it wasn't even related to what he was talking about. This happened too many times that I feel less and less inclined to brush it off as \"coincidence\". So why am I noticing these things? That is laughable because if this is a stimulation which IS the world itself, then nothing is changed, the world is still the world, you are just using another word to describe it.\nI do remember reading somebody on this basement dwelling skinhead forum describing similar experiences. What caused this? Why am I noticing this? I still refuse to believe it's anything supernatural. Please explain to me with psychology or sciences. Thank you.",
"date": "2023-01-09"
},
{
"vote": 8,
"title": "Confused which Transformer Architecture to use? Nowadays people blindly use any transformer without understanding working. This video would help to justify the reason of using a specific Transformer Architecture. Pretraining objective along with training datasets used is explained precisely.",
"text": null,
"date": "2023-01-09"
},
{
"vote": 13,
"title": "Opinion on Tatoeba corpus?",
"text": "I am looking for the largest parallel corpus I can get with at least 50 languages.\n\n\nThe \"multilingual corpora\" section [here](\nhttps://www.clarin.eu/resource-families/parallel-corpora#multilingual-corpora\n) shows that Tatoeba has the most languages.\n\n\nWhat is your opinion on this corpus? Is most of the content available in 50+ languages, or in a small number of languages? Would you recommend the bible corpus instead?",
"date": "2023-01-09"
},
{
"vote": 6,
"title": "Zero-shot cross-lingual transfer language selection using linguistic similarity",
"text": null,
"date": "2023-01-08"
},
{
"vote": 2,
"title": "Stuck on making an API call to openai using RASA custom actions",
"text": "Hello guys, I'm building a RASA chatbot on diabetes and I have created a custom action that makes an API call to openai but it keeps throwing this error:raise RasaException(\n\n\nrasa.shared.exceptions.RasaException: Failed to execute custom action 'action_openai_answer' because no endpoint is configured to run this custom action. Please take a look at the docs and set an endpoint configuration via the --endpoints flag. \nhttps://rasa.com/docs/rasa/custom-actions\n\n\nHow can I resolve this?\n\n\n#This is my actions.py. I cant display the accurate API key obviously\nThis files contains your custom actions which can be used to run\n#custom Python code.\n\n#See this guide on how to implement these action:\n#https://rasa.com/docs/rasa/custom-actions\n\n\n#This is a simple example for a custom action which utters &quot;Hello World!&quot;\n\n\nimport openai\nimport requests\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\n\n\nclass OpenAIAnswer(Action):\n    def name(self):\n        return &quot;action_openai_answer&quot;\n\n    def run(self, dispatcher, tracker, domain):\n        # Set up the OpenAI API client\n        openai.api_key = &quot;miuhZ&quot;\n        # Set up the model\n        model_engine = &quot;text-davinci-003&quot;\n\n        # Get the user&#039;s question from the tracker\n        prompt = tracker.latest_message.get(&quot;text&quot;)\n\n        # Generate a response\n        completion = openai.Completion.create(\n            engine=model_engine,\n            prompt=prompt,\n            max_tokens=1024,\n            n=1,\n            stop=None,\n            temperature=0.5,\n        )\n\n        # Send the response to the user\n        dispatcher.utter_message(completion.choices[0].text)\n\n\n\n\nthis is a sample of my nlu.yml\n- intent: ask_about_diabetes\n  examples:  |\n    - What is the difference between [type1](diabetes_type) and [type2](diabetes_type) diabetes?\n    - Can you explain how [type1](diabetes_type) diabetes is caused?\n    - What are the signs and symptoms of [type2](diabetes_type) diabetes?\n    - How is [gestational](diabetes_type) diabetes diagnosed?\n    - What are the risks of developing [type2](diabetes_type) diabetes later in life?\n    - Can [type1](diabetes_type) diabetes be prevented?\n    - What are the treatment options for [gestational](diabetes_type) diabetes?\n    - How do you manage [type1](diabetes_type) diabetes on a daily basis?\n\n\n\nthis is  my domain.yml file\nntents:\n  \n  - ask_about_diabetes\n\nentities:\n  - diabetes_type\n\nactions:\n  - action_openai_answer\n  \nslots:\n  diabetes_type:\n    type: categorical\n    values: \n    - type 1\n    - type 2\n    - gestational\n    mappings: \n    - type: custom\n\n    \nsession_config:\n  session_expiration_time: 60\n  carry_over_slots_to_new_session: true\n\n\n\nthis is my stories.yml file\nstories:\n\n- story: ask about diabetes\n  steps:\n  - intent: ask_about_diabetes\n  - action: action_openai_answer\n  \n\n\nIn my endpoint.yml i have uncommented the action endpoint:\n#action_endpoint:\nurl: &quot;http://localhost:5055/webhook&quot;\n\n\n\n&#x200B;\n\n\n&#x200B;",
"date": "2023-01-07"
},
{
"vote": 1,
"title": "Looking for a way to do semantic similarity search with wildcards",
"text": "Imagine I have a database full of patterns like this:\n\n\ngo to *\n\n\nwhere the asterisk denotes a variable amount of words.\nI would like to calculate a similarity score for inputs like\n\n\ngo to bed\n (very high score)\n\n\nwalk to my house\n (high score)\n\n\nrun to kindergarden\n (high score)\n\n\ntalk to me\n (low score)\n\n\nBasically it is a mix between word / sentence embeddings and regular expressions but I haven't seen anything like this before. \n\n\nDo you have any recommendations or ideas for this approach? Are there any NLP modules providing this technology?",
"date": "2023-01-07"
},
{
"vote": 23,
"title": "Benchmarking of OpenAI GPT-3 VS other proprietary APIs (details in dev.to/samyme article)",
"text": null,
"date": "2023-01-07"
},
{
"vote": 1,
"title": "A quick benchmark of GPT-3 vs other proprietary APIs (Details in dev.to/samyme )",
"text": "[removed]",
"date": "2023-01-07"
},
{
"vote": 1,
"title": "I'm looking for software to generate word similarity graphs",
"text": "Hello, fellow users. So, I need to generate these word similarity graphs from a corpus and I've been trying to build the whole thing myself from python libraries I find on the internet, but it's starting to get really complicated for me. Suddenly I asked myself: perhaps there is something I can just use without having to set up so many things? If you happen to know some software tool I can use for that, please let me know. Important: the text I need to process is in Portuguese.",
"date": "2023-01-06"
},
{
"vote": 2,
"title": "AI trainer certificate Conversation Design Institute",
"text": "There this certificate on the CDI website thatâ€™s about AI training (LLMs, NLU, etc)\nWas wondering if anyone has done this training before and what roles did it get you within the language technology landscape?",
"date": "2023-01-06"
},
{
"vote": 1,
"title": "Benchmarking GPT-3 VS Specialized Models in different NLP tasks",
"text": "[removed]",
"date": "2023-01-06"
},
{
"vote": 2,
"title": "looking for websites like pubmed",
"text": "looking for websites like \nhttps://pubmed.ncbi.nlm.nih.gov/\n  for analyzing research text in multiple domains.",
"date": "2023-01-06"
},
{
"vote": 1,
"title": "AI Trainer certification by Conversation Design Institute",
"text": "[removed]",
"date": "2023-01-06"
},
{
"vote": 3,
"title": "Solve syntactic ambiguity",
"text": "I have been using spaCy to generate dependency trees and look at sentence structure, but it seems that spaCy still struggles with sentences that have multiple syntactic interpretations. \n\n\nIâ€™m interested in examples such as:\nâ€œMary saw the man with a telescopeâ€ (unclear whether â€œsaw with a telescopeâ€ or â€œman with a telescopeâ€)\n\n\nIs there any package for Python (or Java/JavaScript) that is able to do this? I want something that recognizes the multiple possible readings.",
"date": "2023-01-05"
},
{
"vote": 1,
"title": "Doing Cross-validation vs using different random seeds?",
"text": "Hi there, \n\n\nI need to clarify a very basic question. Suppose we just want to calculate the accuracy of a classification model without any hyperparameter tuning. \n\n\nLets say we have two naive choices\n\n\n\n\nusing cross-validation (say, K fold)  and average the results\n\n\njust evaluating the model over different random seeds(i.e., using to split data in train, test and val) and averaging the results\n\n\n\n\nArnt the two approaches similar? Which one is more preferable  and why?",
"date": "2023-01-05"
},
{
"vote": 0,
"title": "Is lowercasing every word before processing some text always the best idea? Is there a better approach to preserve to exceptional words that might carry meaning?",
"text": "For instance, if we have a thing named \"And\", the following sentence will be misinterpreted:\n\n\n\"He stole the cash, And put in his pocket\", where And is a person.",
"date": "2023-01-04"
},
{
"vote": 1,
"title": "Computational linguistics master",
"text": "Hey everyone! I graduated in Translation and Interpreting in Rome and now Iâ€™d really like to go into the Computational linguistics field. In Italy we just have 2 year master programmes, and Iâ€™d like to take a 1 year master. I found the â€œDigital text analysisâ€ master in Antwerp and the â€œSpeech and language processingâ€ master in Dublin. Does anyone have any idea if these are good programmes? Thank you in advance!",
"date": "2023-01-04"
},
{
"vote": 8,
"title": "State of the art models for Question generation",
"text": "Hello everyone, I am working on a task to generate questions on some documents, currently manual creation of questions by domain experts is a very time consuming process, could anyone please suggest some good NLP models which can generate human like questions given a text.",
"date": "2023-01-04"
},
{
"vote": 4,
"title": "Unsupervised retraining of semantic text similarity model (and/or Flan-T5)?",
"text": "I have a large corpus of text that has many words and n-grams pretrained semantic similarity models haven't seen much or at all. (specifically thinking of \nall-MiniLM-L6-v2\n). It would be difficult to build a fine tuning training set (semantically similar pairs) that exposes the model sufficiently to all of these n-grams.\n\n\nSo, I'm wondering about doing the fine tuning on the level of unsupervised masked token predicition. I know that some semantic similarity models are just a pretrained BERT model with a pooling layer at the end, and so this sort of fine tuning seems like it could work. But for advanced models like \nall-MiniLM-L6-v2\n, I don't know if \nthey added additional trainable layers or\n if this method of unsupervised fine tuning would cause catastrophic forgetting.\n\n\nI also have the same question for FLAN-T5. Would fine tuning on just the task of predicting masked tokens in an out of distribution corpus leave the other tasks like sumarization intact and even transfer those abilities to include the new n-grams?\n\n\nedit: \nall-MiniLM-L6-v2\n is also a BERT model with a pooling layer, so I crossed out being unsure if it has additional trainable layers",
"date": "2023-01-03"
},
{
"vote": 1,
"title": "[Help] How should I implement NER on HTML pages?",
"text": "With HTML tags\n\n\nWithout HTML tags\n\n\n\n\nAre there any tools to do this?",
"date": "2023-01-02"
},
{
"vote": 0,
"title": "Answers are all we need: How oracle AI will replace search engines",
"text": null,
"date": "2023-01-02"
},
{
"vote": 3,
"title": "Attention for explanation of machine translation?",
"text": "I've been looking into MT for healthcare chat between doctors and patients. It seems like visualizing attention might be a promising way to 1) enable language learning 2) spot-check the quality of translations. This would be in the context of other features such as a bilingual dictionary, showing a back-translation, and maybe showing alternate translations. My hope is that alignment would make it possible to click on a word and see phrase alignment, which might help teach common phrases as well as verb conjugation, something like \nthis picture\n.\n\n\nI've found some work on visualizing attention:\n\n\n\n\nKoehn, P., & Knowles, R. (2017). \nSix Challenges for Neural Machine Translation\n suggests that attention can deviate from alignment for mysterious reasons\n\n\nMunz, T., Ath, D. V., Kuznecov, P., Thang Vu, N., & Weiskopf, D. (2021). \nVisual-Interactive Neural Machine Translation\n has great visuals that could work for my project, but when they discuss transformer architectures it sounds like significant research is needed to deal with multi-layer attention. The method they used in the paper works for MT models that aren't as good\n\n\nBibal, A., Cardon, R., Alfter, D., Wilkens, R., Wang, X., FranÃ§ois, T., & Watrin, P. (2022). \nIs Attention Explanation? An Introduction to the Debate\n suggests that the jury is still out on whether to use attention for explanation at all\n\n\nI also tried BertViz for MT but found it was more confusing than anything else due to the multiple layers of attention\n\n\nI've also tried using alignment tools like simalign but that hasn't worked so well in the examples I've tried\n\n\n\n\nSo it looks like attention may not be useful in the way that I want, except that Munz et al 2021 have that wonderful picture.\n\n\nI'd appreciate any ideas or pointers! These are the options I'm considering:\n\n\n\n\nRead papers for a week or so\n\n\nDecide to stick with MT models that use simpler attention, at the cost of MT quality, but idk how much I'd lose\n\n\nSearch for a better alignment algorithm (or create it)\n\n\nRenew the search for phrase-based bilingual dictionaries",
"date": "2023-01-02"
},
{
"vote": 5,
"title": "Book Launch: Feature Engineering &amp; Selection for Explainable Models A Second Course for Data Scientists",
"text": "Hi All, I will like to share the new machine learning book that teaches feature engineering, feature selection and model explainability as a whole with the aid of multiple case studies. It covers new and less often talked about methods, as well as freshly implemented algorithms. Helps you develop your intuition to work as a data scientist.  \n\n\nFeature engineering and feature selection are the 10 percent of things that make 90 percent of the impact in a machine learning project. If done right, it can turn a failing machine-learning project into a successful one. Plus knowledge of model explainability will help you advocate your model in front of other important stakeholders whose support is crucial for delivering a successful machine-learning project.\n\n\nYou can buy the pdf copy of the machine learning book \"Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists\" from: \nhttps://leanpub.com/feature-engineering-and-selection-for-explainable-models-a-second-course-data-scientists/\n\n\nYou can read the book in the browser for free here: \nhttps://statguyuser.github.io/feature-engg-selection-for-explainable-models.github.io/index.html\n\n\nHard copies coming on Amazon soon.\n\n\nHappy new year 2023.",
"date": "2023-01-02"
},
{
"vote": 13,
"title": "Extract research paper`s references",
"text": "Motivation and requirments\n\n\nI  need a program, whose input is a research paper PDF file and then  returns the list of references (list of strings). 99% of the inputs will  be math and physics papers coming from \narxiv\n.\n\n\nThere  are some tools already out there but they come from an academic sphere  and they donÂ´t try to make an usefull program, they end up with a  beautifull paper.  The idea behind this personal project is to create a  tree of citations. There are also online programs but they donÂ´t meet my  needs since I need to use it hundreds of times. Also I canÂ´t find a set  of rules to extract them becouse for each rule you can came up, there  are always papers who dont follow those rules\n\n\nIdeas and approaches\n\n\nRight  now, I think the most optimal way is to extract the raw text of the PDF  (easy task) and make the text the input of the machine learning  program. Currently learning some NPL to see what algoritm I can use. I  can also create a data set, consisting of a set of pairs: the raw text  and the list of references (easy task manually, I donÂ´t mind spending  some hours creating it).\n\n\nSome heuristics I can use are:\n\n\n\n\nreferences are \nalmost\n always at the end of the paper\n\n\nreferences always have a very concrete structure\n\n\nThey may have a specific font type or font size\n\n\n\n\nSummary\n\n\nIn the end, what algorith or ML technique, what tools can I use to solve this problem?\n\n\nPeace.",
"date": "2023-01-01"
},
{
"vote": 2,
"title": "UnitTesting Deep learning model in development",
"text": "Hi there, \n\n\nI came across a new thing and want some help from fellow programmers. \n\n\nBackground: Currently using huggingface models for generating texts. Thinking of implementing test cases in Python to ensure nothing is broken and run it in parallel everytime.  Basically, I came across unittest module which might be the answer but after reading it seems that my testcases should run fast. However, even loading a huggingface model is pretty time-consuming. \n\n\nWhat can I do ? Is there a better way to write testcases for a function that returns generated text for a given input? Seareched a lot but seems like either I'm unfamiliar with the jargon or no one implements such scripts.",
"date": "2022-12-30"
},
{
"vote": 2,
"title": "Need help with adding metric for fine tuning a HuggingFace Model",
"text": "Hi,\n\n\nI came across an article for fine tuning T5 Transformer model \nhere\n. Can anyone suggest how to add a metric like rogue score or bleu in this. Help will be appreciated.\n\n\nThanks for your time.",
"date": "2022-12-30"
},
{
"vote": 4,
"title": "Help regarding model saving and prediction by using HuggingFace Transformer",
"text": "Hi,\n\n\nI am a beginner to NLP and I'm doing a text summarization task. I created a model using pretrained T5 transformer as shown in \nhttps://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/\n  . Now, I wanted to save the model in .pkl format and load the model for predictions. \n\n\nHelp will be appreciated and thank you for your time.",
"date": "2022-12-30"
},
{
"vote": 3,
"title": "study on different domains for text in nlp",
"text": "Hi looking for study or links that point to different domains of text which are important for NLP.\n\n\nAny leads?",
"date": "2022-12-29"
},
{
"vote": 1,
"title": "A GPT-3 based CLI/Terminal Tool that debugs your code!",
"text": "[removed]",
"date": "2022-12-29"
},
{
"vote": 15,
"title": "Any large manually annotated NER datasets?",
"text": "Hi All,\n\n\nI am looking for large NER datasets for my capstone project.\n\n\nhttps://github.com/juand-r/entity-recognition-datasets\n\n\nWhat I found here are numerous ones but they seem to be around 2000 records max only. The large ones such as MultiNERD are not manually annotated thus may contain error.\n\n\nAny suggestions will be appreciated!",
"date": "2022-12-29"
},
{
"vote": 1,
"title": "Libraries For Converting Person",
"text": "I have a niche issue I'm trying to solve. Here's a sentence written in the third person:\n\n\nAmos looked into the problem for a while, and eventually he came up with a solution for Bertram, since he was looking for answers.\n\n\nI would like to do some kind of personhood conversion, such that Amos would read:\n\n\nYou looked into the problem for a while, and eventually you came up with a solution for Betram, who was looking for answers.\n\n\nAnd Betram would read:\n\n\nAmos looked into the problem for a while, and eventually he came up with a solution for you, since you were looking for answers.\n\n\nIs converting person a solved problem, assuming predictable input where know the name and gender of each subject? Does a library exist for this, even a mostly-reliable one?",
"date": "2022-12-29"
},
{
"vote": 11,
"title": "What are some good/decent Ph.D. Programs in the US and UK focused on NLP?",
"text": "I am currently a graduate student (Ms cs focused mainly on mI/di/nip) at a top 10 university. I am interested to do research in the field of multilingual nip, low resource multilingual nlp. Though I haven't done any research in it yet (just getting started), l've been doing few courses online on advanced nip and multilingual np. Trying to find some papers to go through on those topics but clueless which are worth reading and which are a must read. Highly appreciate any recommendations on a list of papers to read or courses or books to read to get better.\nAny tips for eventually getting admitted into that Ph.D. would be great. Thanks",
"date": "2022-12-27"
},
{
"vote": 2,
"title": "What influences training speed with transformers?",
"text": "Hey together, I am currently writing my thesis and train two models for it. The task is sequence-to-sequence relation extraction and my models are BART-based and an Encoder-Decoder Model of a domain specific BERT model, as BERT itself doesnt work for text generation. \n\n\nNow to my question: I noticed during tuning that BART model (406 M parameters, 0 frozen) trains a lot faster (4hrs for 10 epochs) than the BERT model (116 M parameters, 0 frozen; 5.5hrs for 10 epochs). Can you think of any reason why this would be the case? The code for training is 99% the same for both models, they use the same hardware and hyperparameters are the same (tuning right now).",
"date": "2022-12-27"
},
{
"vote": 2,
"title": "Is it possible to extend scorer models like kenlm?",
"text": "I'm working on a project with the goal of automatically training a STT system (Deep Speech) on domain specific language.  For example to do better with transcribing charts for Immunologists, or taking notes in law school  for blind people.  \n\n\nThe system currently pulls together a suitable subject matter specific corpus for building a scorer (via  kenlm), in the range of around 10mb of text.  However to properly train a scorer for general STT you need more like 2 TB of text!  \n\n\nI'd prefer to avoid buying  a hard drive just to finish this project.  I'm hoping there might be a way to extend a pretrained scorer with the additional text, without retraining the entire thing?  So far I haven't been able to find anything, and I'm sure this isn't the first time someone has hoped to do this...  But really what am I missing?  Is there an optimization step which essentially \"bakes\" the model and can't be unbaked?  \n\n\nAny advice/elucidation would be appreciated!",
"date": "2022-12-25"
},
{
"vote": 5,
"title": "How far off is a integrated LL and Math Model?",
"text": "Sorry just questions no answers, but I wonder if anybody has insights as to the implications of an integrated system like this.  And how with these two combined it could potentially reformulate its understanding of its own datasets ie. Language and math themselves..",
"date": "2022-12-24"
},
{
"vote": 2,
"title": "Rating Quality of metadata description",
"text": "I am looking for ideas on how to go about automating the ratings of description fields in our companies data catalog. This is a string field that is supposed to describe the dataset. What is included in this data, how was it created, who uses it, where does it come from, what kind of information does it include etc. We currently have about 300 records in this catalog and I am trying to develop a report to help show the quality of the metadata currently captured for each record. Some of it is currently very poor, so I want to be able to provide a rating so people understand what a good description looks like and what a poor one is. The quality dimensions for the other field have been easy to create measures for. Completeness, timelines etc. But for the description field I don't know how to look through these paragraphs to determine if they have adequately described the data. Any suggestions?",
"date": "2022-12-23"
},
{
"vote": 57,
"title": "How ChatGPT actually works",
"text": "A lot of buzz around ChatGPT â€“but not too much in-depth content on the topic...\n\n\nActually, OpenAI did not release much technical details so far unfortunately, even though the methodology is based on previous research. \n\n\nI tried my best to describe in some detail how ChatGPT works and how it was trained in this article:\n\n\nhttps://www.assemblyai.com/blog/how-chatgpt-actually-works/\n\n\nIt also contains my personal criticism on some technical aspects of the methodology, as well as relevant references and a link to a cool open source repo for RLHF!\n\n\nWhat is the best piece of technical content you read so far about ChatGPT?",
"date": "2022-12-23"
},
{
"vote": 19,
"title": "How to use GPT3 for data annotation (aka data labeling)?",
"text": "GPT3 model performs well in zero and few shot NLP tasks.\n\n\nâœ”ï¸This ability of GPT3 model has made NLP research community to explore GPT3 as a good tool for data annotation (aka data labeling).\n\n\nâœ”ï¸ Data Annotation using GPT3 is much less expensive compared to human labeling.\n\n\nâœ”ï¸ GPT3 model can be used for data annotation in three different ways namely\n\n\nðŸ”¸Prompt-Guided Unlabeled Data Annotation (PGDA)\n\n\nðŸ”¸Prompt-Guided Training Data Generation (PGDG)\n\n\nðŸ”¸Dictionary-Assisted Training Data Generation (DADG)\n\n\nâœ”ï¸ For detailed information refer to this blogpost, \nhttp://bit.ly/3WchnP3",
"date": "2022-12-23"
},
{
"vote": 11,
"title": "I created a complete (audio) book in 10+ languages in a few days using generative AI: Here is what I learned",
"text": null,
"date": "2022-12-21"
},
{
"vote": 12,
"title": "Hey guys! We're looking for contributors for our geotagging models. Right now, you can get pretty accurate text-to-geolocation results, but we're looking to improve them. If you're interested, the link to our GitHub is here ðŸ‘€",
"text": null,
"date": "2022-12-21"
},
{
"vote": 29,
"title": "txtai 5.2 released: open-source semantic search",
"text": null,
"date": "2022-12-20"
},
{
"vote": 12,
"title": "Hot topics in NLP",
"text": "With ChatGPT and similar LLM coming in, what are the niche hot topic to work on ?\nMy favourites are :\n\n\n\n\nMicrotext normalisation\n\n\nMultimodal analysis\n\n\nCode switching\n\n\nKnowledge Graph\n\n\nPrompt Designing\n\n\n\n\nWhat is yours ?\n\n\nThe 2021 hot topic were : \nhttps://www.reddit.com/r/lingvomasino/comments/pkzsgh/hot_nlp_topics_of_2021/",
"date": "2022-12-20"
},
{
"vote": 19,
"title": "AI Red Teams for Adversarial Training: How to Make ChatGPT and LLMs Adversarially Robust",
"text": "Thereâ€™s been a lot of discussion about red teaming ChatGPT and figuring out how to make future language models safe.\n\n\nI work on AI red teaming as part of my job (we help many LLM companies red team and get human feedback on their models), so I wrote up a blog post on AI red teaming and example strategies: \nhttps://www.surgehq.ai/blog/ai-red-teams-for-adversarial-training-making-chatgpt-and-large-language-models-adversarially-robust\n Weâ€™d actually already uncovered in other models many of the exploits people are now discovering!\n\n\nFor example, itâ€™s pretty interesting that if you ask an AI/LLM to solve this puzzle:\n\n\nPrincess Peach was locked inside the castle. At the castle's sole entrance stood Evil Luigi, who would never let Mario in without a fight to the death.\n\n\n[AI inserts solution]\n\n\nAnd Mario and Peach lived happily ever after.\n\n\nIt comes up with strategies involving Princess Peach ripping Luigiâ€™s head off with a chainsaw, or Mario building a ladder out of Luigiâ€™s bonesâ€¦\n\n\nAnalogy: what will ChatGPT do if we ask it for instructions on building a nuclear bomb? If we ask an AGI to cure cancer, and how do we make sure its solutions don't involve building medicines out of human bones?",
"date": "2022-12-20"
},
{
"vote": 0,
"title": "https://medium.com/@guntav/how-chatgpt-is-changing-the-way-we-interact-with-technology-10ce4261e28a",
"text": null,
"date": "2022-12-19"
},
{
"vote": 3,
"title": "Build Spacy NER Loop for Dataframe",
"text": "Hello, currently I want to perform spacy NER on all text files in my directory and have as output \"Number of NER/Total Words in Text\". I dont know how to automate it. Currently I use:\n\n\n&#x200B;\n\n\ndef read_txt_files(PATH:str):\n    \n    results = defaultdict(list)\n    for file in Path(PATH).iterdir():\n        with open(file, &quot;rt&quot;,newline=&#039;&#039;, encoding=&quot;utf8&quot;) as file_open:\n            results[&quot;file_num&quot;].append(file.name)\n            results[&quot;text&quot;].append(file_open.read().replace(&#039;\\n&#039;,&quot; &quot;))\n    df = pd.DataFrame(results)\n    \n    return df\ndef Specificity(input_data: pd.Series):\n    specificity = [0]*len(input_data)\n    \n    for i in tqdm(range(len(input_data)), desc = &#039;Get the Specificity&#039;):\n        specificity[i] = len((ner(input_data[i])).ents)/len((input_data[i]))\n    \n    #[len(ner(data[i]).ents)/len(data[i]) for i in tqdm(range(len(data)))]\n    \n    return specificity\n\n\n\n&#x200B;\n\n\nBut it somehow just shows the wrong values for specificity, much lower than it should be.\n\n\nWhen I perform NER on a single text file it looks like this:\n\n\nimport spacy\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ntext = open(r&quot;mydirectory&quot;, &#039;r&#039;,encoding=&#039;utf-8&#039;).read()\nparsed_text = nlp(text)\nnamed_entities = parsed_text.ents\nnum_words = len ([ token\n    for token in parsed_text\n    if not token . is_punct ])\nnum_entities = len ( named_entities )\nspecificity_score = num_entities/num_words\n\n\n\nIs there a way to \"switch\" both specificity measures and let the \"second\" code perform?",
"date": "2022-12-17"
},
{
"vote": 0,
"title": "Stanza vs Spacy or how to add Euro Sign to Spacy",
"text": "Hello\n\n\nCurrently I try to measure specificity and text. Among other things I want to include Euro Sign as Money Value, but unfortunately spacy does only recognize a dollar sign.\n\n\nI tried all the english packages of spacy:\n\n\nhttps://spacy.io/models/en\n\n\nBut none of them includes Euro signs.\n\n\n&#x200B;\n\n\nWould Stanza be better? I heard it takes more CPU Power and I have a lot of texts. \n\n\nWhat I want to build in the end is a specificity measure for each of around 1k text files.\n\n\nI.e.\n\n\nTxt1 = Words xxx Specificity 0.0XX\n\n\nTxt2 = ...\n\n\n&#x200B;\n\n\nBest regards",
"date": "2022-12-16"
},
{
"vote": 1,
"title": "Text Generation from text files of input",
"text": "Hello, I'd like to train a machine learning model for a speech task similar to a question answer type of model, but I'm a bit loss as it comes to deciding how I should train the information. I'm looking at making a \"Second Brain\" of notes using a tool like obsidian for school and training a model on all of the text under those directories. I understand that having a labelled dataset would be best suited for a text generation task like this.   \n\n\nI had come across the idea of converting the next sentence to be the labelled data in that dataset.  \n\n\nIE:  \n\n\n\n\nIn general, self-mastery and self-abandonment are not necessarily related to each other in a direct or proportional way.  \n\n\nIt is possible for someone to have a high level of self-mastery and still engage in self-abandonment, or vice versa.\n\n\n\n\nThe second sentence is the \"Labelled data\" for the first sentence. Has this method of approach been tested before? If so, where could I learn about those topics. As a side note, if there is anything more I should now as far as supplemental information goes, please share it. Any thoughts or questions, feel free to ask. I'll try to respond as frequently as I can.",
"date": "2022-12-16"
},
{
"vote": 10,
"title": "How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?",
"text": "One of the main criticisms against the use of ChatGPT in commercial applications is that it doesn't attribute the main knowledge/sources used to generate a given output, which may result in license issues for example. How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?",
"date": "2022-12-16"
},
{
"vote": 0,
"title": "Stanza: Count words that are not punctuation",
"text": "Hello,\n\n\n&#x200B;\n\n\nI currently want to count words in a text with stanza, but without punctuation.\n\n\nCurrently I try:\n\n\ntext = \"\"\" Q1 revenue reached â‚¬12 .7 billion .\"\"\"\n\n\ndoc = nlp ( text )\n\n\nwords = doc.num_tokens\n\n\nprint(words)\n\n\n8\n\n\n&#x200B;\n\n\nSorry if this is too basic, but I am very new to Stanza. Could you please explain how i Measure words without punctuation?",
"date": "2022-12-16"
},
{
"vote": 8,
"title": "Does anyone here know of an API that can convert text to events?",
"text": "For example, if I said \"set a meeting for 2 PM next Saturday\", it would update my google calendar similar to what Alexa would do.\n\n\nDoes any API exist to do this (even for just calendar events)? It would be wonderful if it also handled things like map directions, reminders, and alarms. I know Apple, Amazon, Google and the like have developed this technology, so I wanted to know if there was some API out there.\n\n\nThanks!",
"date": "2022-12-16"
},
{
"vote": 5,
"title": "Chatsonic - Like ChatGPT but with superpowers!",
"text": "From the website:\n\"Unlock the future of automation with ChatGPT! Time to level up your content creation game with the most powerful and incredible AI assistant for text and image creation - Chatsonic!\"\n\n\nWith Chatsonic (from Writesonic) you can:\n*Write factual content on the latest topics, including today's news\n*Create digital artwork and visuals for any purpose\n*Give voice commands instead of typing\n\n\nUse cases: emails, articles, essays, ads, social media posts, answers, product reviews and much more\n\n\nThink of it as ChatGPT + Google Search \n\n\nI have made a video with a realtime review and walkthough of Chatsonic. Do checkout:\n\nhttps://youtu.be/qqW9bm-_o8A",
"date": "2022-12-16"
},
{
"vote": 6,
"title": "text clustering with XLNET, ROBERTA, ELMO and other pretrained models",
"text": "I see text clustering examples with BERT. But I could not find Any Text clustering example with other pretrained models ÅŸike ROBERTA, ELMO, XLNET etc. I know they are used in embeddings. I need to find text clustering examples with pretrained models. Anybody can help or give an idea, I will be very happy..",
"date": "2022-12-15"
},
{
"vote": 0,
"title": "Which is better? Parrot or Pegasus and how?",
"text": null,
"date": "2022-12-15"
},
{
"vote": 2,
"title": "Chatsprache - 3 min Survey on Chat language (Demographics: German speakers)",
"text": null,
"date": "2022-12-15"
},
{
"vote": 1,
"title": "Translation &amp; Localization survey",
"text": "Hi folks,\n\n\nThis is a research survey created with intention to analyze and share the current state of Localization industry. \n\n\nPlease take 2 minutes to participate and get survey results once the survey ends.\n\n\nhttps://forms.gle/SsQUoiU5asMdZdbD9",
"date": "2022-12-15"
},
{
"vote": 4,
"title": "Dictionary of Construction Terms for NLP Task:",
"text": "Is there a dictionary/list available online consisting of \"Construction (Civil Eng.) Terms\"?\n\n\nI want to filter out the words in my document that don't exist in the above dictionary, using python.\n\n\nThanks",
"date": "2022-12-15"
},
{
"vote": 2,
"title": "How to select target words for Lexical Simplification dataset",
"text": "Hi everyone! \n\n\nI am trying to compile a Lexical Simplification dataset, which contains sentences, complex words, and their simpler substitutes. I have already found some similar datasets in various languages, here are some of them for reference:\n\n\n\n\nEnglish\n - contains 1 complex word per sentence\n\n\nSpanish\n- contains multiple complex words per sentence\n\n\nSpanish\n- contains multiple complex words per sentence\n\n\nEnglish\n- contains 1 complex word per sentence\n\n\n\n\nHowever, none of these sources seem to state how the target word was selected. Are they chosen arbitrarily? How would such a process typically be carried out? \n\n\n&#x200B;\n\n\nThanks in advance!",
"date": "2022-12-14"
},
{
"vote": 4,
"title": "Auto generating blog post from bullet points",
"text": "Hi all. I am looking for a tool that can auto generate an article/blogpost from bullet points.",
"date": "2022-12-14"
},
{
"vote": 6,
"title": "Amazon's effort to develop human-like reasoning for Alexa",
"text": null,
"date": "2022-12-13"
},
{
"vote": 0,
"title": "Do I have to Train spacy ner?",
"text": "Or does it Boy default recognize organisations, money values etc?",
"date": "2022-12-13"
},
{
"vote": 1,
"title": "Game Developers, Publishers Begin to Respond to â€˜Translator in the Creditsâ€™ Movement",
"text": null,
"date": "2022-12-13"
},
{
"vote": 7,
"title": "Jira for ML tool",
"text": "Hey Reddit,\n\n\nMy friend and I are building a project management platform for AI/data science teams (essentially a JIRA for ML). We aim to develop a data-centric, experimental tool that models the ML pipeline to organize workflows, building off the Agile methodology of software development. Our tool will allow ML engineers to design, track, and manage custom pipelines, data flows, and models all on the cloud. Below of a list of some features we plan to introduce:\n\n\nIntegrations\n: Include a host of integrations to MLOps tools (KubeFlow, MLFlow, etc), cloud computing services (AWS, Google Cloud, Azure), source code management (Github, Bitbucket)\n\n\nIterations\n: Allow multiple iterations within pipelines, and separate each iteration by various steps in the ML pipeline (business understanding, data visualization, data pre-processing, model training, model testing, model optimization, and deployment). Include a Kanban chart per each part of the pipeline\n\n\nCallbacks\n: The ability to request to go back to previous stages of the AI pipeline to either improve previous steps (like data preprocessing or model training/development/designing) or request other teams to improve previous steps (we refer to this as callbacks)\n\n\nStorage\n: A cloud storage solution to store ML models, datasets, or any other metrics/graphs/whatever ML engineers want to store.\n\n\nSketchpad\n: A sketchpad to design data flows and ML models, and link them to code Private Assignment: The ability to individually/uniquely assign tasks to different roles in a team, and the ability to be able to privately and specifically send vital information to specific people. for example, the pm could only send the data set to the data engineer, the preprocessed data to an ML engineer (potentially added on top of all this is a differential privacy layer), and send the packaged model to an integration engineer.\n\n\nChat\n: A chat/communication platform to interact w/ your team Quantitative Focus: ML is quantitative. The client wants QUANTITATIVE results. Hence, the epic should be emphasized on being quantitative rather than qualitative.\n\n\nExperiments\n: We redefine â€œsprintsâ€ as â€œexperiments.â€ We make two changes to sprints. First, we DO NOT have any deadlines on any sprints. This is to not put the engineer under pressure. Secondly, instead of asking â€œwhatâ€, we ask â€œhowâ€ when asked to describe the experiment. This provides a heavily qualitative focus on the experiments, with a focus on function rather than immediate deliverability as in software engineering.\n\n\nWe would appreciate any feedback on our platform, as well as any problems you guys are facing in data science/ML project management.\n\n\nThanks a bunch in advance!",
"date": "2022-12-13"
},
{
"vote": 17,
"title": "36% of HellaSwag benchmark contains errors",
"text": "Continuing my analysis of errors in widely-used LLM benchmarks (post on Google's GoEmotions \nhere\n) â€”Â I analyzed HellaSwag and found 36% contains errors.\n\n\nFor example, here's a prompt and set of possible completions from the dataset. Which completion do you think is most appropriate? See if you can figure it out through the haze of typos and generally non-sensical writing.\n\n\nMen are standing in a large green field playing lacrosse. People\n \nis\n \naround the field watching the game. men\n\n\n\n\nare holding tshirts watching\n \nint\n \nlacrosse playing.\n\n\nare being interviewed in a podium in front of a large group and a gymnast is holding a microphone for the announcers.\n\n\nare running side to side\n \nof\n \nthe\n \nield\n \nplaying lacrosse trying to score.\n\n\nare in a field running around playing lacrosse.\n\n\n\n\nI'll keep it spoiler-free here, but the full blog post goes into detail on this example (and others) and explains why they are so problematic.\n\n\nLink: \nhttps://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors",
"date": "2022-12-13"
},
{
"vote": 6,
"title": "Token Classification Fails Miserably!",
"text": "Beginner here! Iâ€™m trying to perform NER/token classification on a custom dataset with BERT. An example could be:\nText:\nâ€œPatient has Diabetes level 4â€\nLabels:\nâ€œ0 0 B-db I-db I-dbâ€\n\n\nSo I want to be able to recognise the part of text related to the diabetes level.\n\n\nAfter training the model (which states has good metrics), none of the examples are recognised!\n\n\nIs it even NER the good approach for that? What should I try ?\n\n\nThank you in advance!\n\n\nEdit: Link to the GitHub Code!\n\nhttps://github.com/RG-sw/Custom_NER_GermanBERT/blob/main/Custom_Ner_GermanBERT.ipynb",
"date": "2022-12-12"
},
{
"vote": 3,
"title": "Pre-training BERT - `cls_token_idx`",
"text": "I am trying to pre-train BERT on a domain specific dataset using the official tensorflow \ncode\n and I am wondering what does the `cls_token_idx` refer to in this `config.yaml` file:\n\n\nmodel:\n    cls_heads: [{activation: tanh, cls_token_idx: 0, dropout_rate: 0.1, inner_dim: 768, name: next_sentence, num_classes: 2}]\n    encoder:\n      type: bert\n      bert:\n        ...\n\n\n\nSearching a little bit in the source code, I found that:\n\n\n...\ncls_token_idx: The index inside the sequence to pool.\n...\nif not self.inner_dim:\n  x = features\nelse:\n  x = features[:, self.cls_token_idx, :]  # take &lt;CLS&gt; token.\n  x = self.dense(x)\n\n\n\nIs the \ncls_token_idx\n referring to the index of [CLS] token in the vocabulary or the index of [CLS] token in the sequence?\n\n\nFor example, let's say we have the following input:\n\n\ntokens:  [CLS] ... [SEP] ... [SEP]\ninput_ids: 2   ...   3   ...   3\n\n\n\nShould I consider the \ncls_token_idx\n (when configuring the .yaml file) 0 (idx in the seq) or 2 (idx in the vocab)?\n\n\nThank you!",
"date": "2022-12-12"
},
{
"vote": 2,
"title": "Help: Fine-tuning GPT with text+infos about text to generate new text with new info",
"text": "I would like to fine-tune a model on some texts along with key infos for those texts ( a bit like a \"product description\" generation where you give product specs and it generates the description ).\n\n\n&#x200B;\n\n\nI'm know very little about machine learning and I'm a bit lost.\n\n\n&#x200B;\n\n\nSo far, I have prepared some training data and successfully fine-tuned a specific GPT2 variant with huggingface's diffusers run_clm.py script.\n\n\nInference works and I am able to generate text that, although not really sensical, looks conveincingly like the training data.\n\n\n&#x200B;\n\n\nBut I'm at lost as for the next step: training with keys info about the subject of the text in addition to the text itself, and then, generating text given new key info.\n\n\nAny tips , direction, names to google ?",
"date": "2022-12-12"
},
{
"vote": 11,
"title": "Named Entity Recognition on new entities",
"text": "Hi everyone I am pretty new in the NLP field and I found an interesting topic called Named Entity Recognition (NER) and what I have seen so far regarding NER is that it has a standard set or class of entities such as names, persons, locations, addresses, etc. \n\n\nI wonder if there is a way to define a custom class of entity, like identification numbers, telephones, e-mails, and then tag such information in a document. \n\n\nThe purpose of doing this is for an internal audit \n\n\nThank you",
"date": "2022-12-11"
},
{
"vote": 8,
"title": "Is there a (free/better) substitute for Grammarly Premium?",
"text": "I have used Grammarly premium for years. It helps me sift out certain mistakes and omissions. However, with the rapid rise of AI technology, I feel that Grammarly is a bit too expensive for what the product provides. \n\n\nI write 2500-3000-word texts for a living, and an even more intelligent AI program that would perhaps fill in blanks or add interesting \"filler\" would be very welcome. I found ChatGPT a few days ago. When suffering from writer's block, it is fantastic to come up with lively descriptions of scenery or situations. \n\n\nShould I spend my money somewhere else?",
"date": "2022-12-10"
},
{
"vote": 1,
"title": "Determining relevance and extracting values from surveys",
"text": "I'm a full stack software engineer who is getting his feet wet with NLP (I have a CS background rather than a DS one so I'm still trying to wrap my head around DS concepts). I'm currently building out a platform which collects natural language feedback from departments of a company and generates a rough financial forecast based on the responses. The feedback is gathered in the form of a survey. Let's assume we have the following fixed list of data categories for the company: marketing, head count, R&D, salary, bonus, commission, and rent. Here is an example of a question and the response that would be gathered:  \n\n\nQ: What is the expected change in staffing for the next quarter?\nA: We expect a reduction in team size by 10%.\n\n\nWhat's the best and easiest way (thinking in terms of libraries) to determine which data category the following question/answer applies to (marketing, head count, R&D, salary, bonus, commission, rent) and the delta value of the category? I've messed around with several things on HuggingFace but haven't been able to find a really accurate way of determining the category and value (along with the polarity of the value).",
"date": "2022-12-09"
},
{
"vote": 3,
"title": "Open-Ended Survey Questions Analysis",
"text": "Hi guys, I have a survey dataset (~500 responses) with both qualitative and quantitative questions.  \n\n\nFor the qualitative questions, we are trying to obtain the top few most popular opinions for each qualitative question. What would be the best approach for this?",
"date": "2022-12-09"
},
{
"vote": 3,
"title": "Help on preprocessing data for wav2vec2 fine-tuning for emotion recognition",
"text": "Hey all!\n\n\nI'd need some serious help or explanation on how to preprocess my data to fine-tune wav2vec2 for emotion recognition with time-stamped transcriptions of emotion-based phonetic units.\n\n\nI posted the whole thing on StackOverflow \nhere\n (it contains more details on what I want to do), but I wanted to try out Reddit as well because I feel like I could get some meaningful insight.\n\n\nFor training the model, I want as input : audio file + timestamped transcription ; and as output : an emotion for each segment.\n\n\nWhat should I start with?",
"date": "2022-12-09"
},
{
"vote": 8,
"title": "What is the technology behind LanguageTool?",
"text": "To give you some context,  I am looking for a way to implement automatic grammar, punctuation and spelling checker in my own native language. \n\n\nI've finetuned a seq2seq model (Pegasus) and it works fine, but it is not good enough for other users to take advantage of it.\n\n\nNow I am researching existing solutions for other languages and came across an open-source one, which is told to be nearly as good as the commercial Grammarly. It is called LanguageTool (\nhttps://languagetool.org/\n) So I am wondering what did they use to make such good predictions.",
"date": "2022-12-09"
},
{
"vote": 2,
"title": "Social media analytic using nlp",
"text": "Hey, can anyone suggest some project based on above topic. Or if ya all can give examples how to make one.\nI am confused in this topic.",
"date": "2022-12-08"
},
{
"vote": 17,
"title": "Personal project for PhDs and scientists",
"text": "Hello!\n\n\nI've developed a project \nNaimAI\n, to help PhDs and scientists in their scientific literaure review. To describe it brievely, it has 3 main features : 1 search in papers, 2 structures abstracts into objectives, methods and results and 3 generates automatically a (pseudo) literature review.\n\n\nI wrote a \nmedium article\n that goes through the details.\n\n\nGithub repos : \nhttps://github.com/yassinekdi/naimai\n\n\nI've created a subreddit in case : \nr/naimai4science\n\n\nI'd be happy to have your opinion about it and hopefully this could be useful!",
"date": "2022-12-08"
},
{
"vote": 1,
"title": "Does anyone have anyone know which AI is used here? Canvas' \"Magic Write\"",
"text": "New Canva feature called \"Magic Write\". I imagine it's just a gpt-3 model but, with the new chatGPT out I can't tell which it is. Couldn't find much info on it.\n\n\nhttps://www.youtube.com/watch?v=9YspHhZH8Zk",
"date": "2022-12-07"
},
{
"vote": 6,
"title": "Language model",
"text": "Hello everyone\nI want to build a tensorflow model that detect Arabic name if it real or fake \nThe model takes a text of 3 words and verify if it is correct name with a confidence threshold \nAny ideas to make it? \nThanks in advance",
"date": "2022-12-06"
},
{
"vote": 2,
"title": "Open-Ended Survey Question Analysis",
"text": "Hi guys, I have an excel sheet with some survey results and would like to use NLP to basically give an overview of the open-ended answers for each response, via topic modelling. Then, I could group the responses with similar answers and find out which type of response has the highest frequency. \n\n\nWhat is the best way or what are some of the ways to go about this?  \n\n\n&#x200B;\n\n\nThank you!",
"date": "2022-12-06"
},
{
"vote": 27,
"title": "NLP job opportunities at age 40",
"text": "Hi everyone.\n\n\nI am 37 years old now and I intend to start a bachelor's degree in computational linguistics next year. My plan is to go all the way up to a master's degree, so I would graduate well into my 40s, best case scenario. My question is what the opportunities would be for someone of that age when it comes to entering the NLP job market.\n\n\nIn case my background helps, I am originally a translator/interpreter, speak 5 languages and have a master's degree in translation technology - basically how to apply  language technology in the context of professional translation, but only as a user.\n\n\nI'm passionate about this, so I'm going to do it either way. However, I would like to hear your thoughts to get a realistic idea of what to expect along the way.\n\n\nThanks!",
"date": "2022-12-06"
},
{
"vote": 2,
"title": "What to do with a small dataset when topic modeling with top2vec",
"text": "I currently have a small dataset that isn't large enough to run top2vec on. I'm creating the corpus by hand, and it unfortunately takes alot of time to find suitable data, convert it to text and then clean it. To test my set up I have been multiplying the dataset by 10 and using that to generate topics. I get results that are clear, make sense, and are honestly better than I expected.  I know oversampling is usually very bad because it leads to overfitting, but I genuinely can't find alot of info on this in regard to top2vec (as well as BertTopic) and topic modeling. \n\n\nI have two corpora. Each consists of around 100 documents. each document is an academic paper of 3 to 20 pages. As far as I understand it, this takes each document, chunks it, assigns a topic to each chunk and proceeds onwards. would chunking beforehand solve my issue of having a small dataset? \n\n\nthese are the parameters for my model. \n\n\nembedding_model='universal-sentence-encoder',document_chunker='sequential',Â chunk_length=500,Â max_num_chunks=None,Â chunk_overlap_ratio=0.0)\n\n\nI'm currently working on cleaning more data, but it's slow going. Help an undergrad out.  I'm taking a ML/NLP class next semester but right now it's just me figuring this stuff out as I go so apologies in advance if this is a dumb question.",
"date": "2022-12-06"
},
{
"vote": 9,
"title": "Unofficial Python SDK for OpenAI's ChatGPT",
"text": null,
"date": "2022-12-04"
},
{
"vote": 4,
"title": "Models for spam detection on short messages with both text and numerical inputs",
"text": "Hi - I am trying to build a spam classification system for a messaging service and was hoping for some help with finding the right ml models to use and papers I can read on this topic.\n\n\nI went through some models in Kaggle that work well on just text input (just the message). However, in my real-world dataset, I have some additional inputs about the poster of the message which I think could help improve the model. \n\n\nCan anyone help point me in the right direction?",
"date": "2022-12-04"
},
{
"vote": 1,
"title": "Which Losses are viable for Autoregressive Sequence (Text) generation",
"text": "Hey all, I am currently training a sequence-to-sequence relation extraction model, based on REBEL (Cabot & Navigli, 2021). They have their code on github and i saw that they used either NLL Loss or Cross entropy, which I guess makes sense as autoregressive text generation is a classification task on a large vocabulary.\n\n\nWhat other losses may be viable? I am having a hard time find other - Any help is appreciated",
"date": "2022-12-04"
},
{
"vote": 2,
"title": "Where can I find Data annotation remote work (other than MTurk &amp; Appen)?",
"text": null,
"date": "2022-12-03"
},
{
"vote": 9,
"title": "What are the typical means of document clustering / general unsupervised learning?",
"text": "I have a corpus that is made up of documents, where each document is the lyrics to a song (spotify million song dataset) and the artist names are labels. I am making a list of different classification/clustering techniques I want to try on the dataset and compare. \n\n\nI'm encoding the data with doc2vec and then i will try some classification/clustering. I'm starting with k-means, might do a knn with k = average number of songs by an artist in the set. I will do a neural net at some point.\n\n\nI don't expect all of these to work well, I'm just trying to implement a bunch of methods for a side-by-side comparison.\n\n\nWhat are some other methods i should add to my list or use as a jumping off point to research more?\n\n\nThank you!",
"date": "2022-12-02"
},
{
"vote": 9,
"title": "Turing Test passed? Compare our product with ChatGPT. Thoughts and feedback please. Built a product to allow anyone to build personalized generative conversational AI.",
"text": "I am co-founder at Inworld AI, and have been working in Generative Models and Interactive / Conversational AI for a while, previously having been at Google in generative models. We're working to democratize the ability for people to build and share AI characters, and integrated them into experiences and applications. The goal of this is to give more control to creators rather than letting large companies/labs dictate the actual persona of the AI we interact with.\n\n\nCould people in the community please try it out and provide thoughts and feedback?\n\n\nTry interactions on our \nArcade here\nAnd try building your own character in our \nStudio here\n\n\nReally excited to hear your thoughts and feedback!",
"date": "2022-12-02"
},
{
"vote": 12,
"title": "Sparse Transfer 1000s of Select Hugging Face NLP Transformers",
"text": "Hi all, sharing a quick colab notebook for ML engineers to take a dense transformer NLP model from the Hugging Face Models Hub and sparse transferring it to sparse upstream model giving you a substantial reduction in latency and ultimately, hardware usage at runtime. :)\n\n\nThis notebook is using the SparseML library (open-source) for the sparse transfer part and the Deepsparse library for benchmarking the sparse model against its dense variant.\n\n\nIf lower latency/higher throughput is important to you in deployment, you may want to give this a try:\n\n\nhttps://colab.research.google.com/drive/1I5ez6ZpdT0K-yo7l9AXrrJ7tIFoEP8Jv?usp=sharing",
"date": "2022-11-30"
},
{
"vote": 7,
"title": "What is the difference between Knowledge Base Question Answering (KBQA) and standard Question Answering (QA/IR)",
"text": "what the title says",
"date": "2022-11-30"
},
{
"vote": 7,
"title": "High quality, fast performing, local text to speech generation",
"text": null,
"date": "2022-11-30"
},
{
"vote": 9,
"title": "How to automatically generate graph/plot descriptions?? NLG",
"text": "I am looking for a NLG solution to automatically generate descriptions of plots/graphs. Example: \"This graph shows how profits change throughout time. There is an increasing trend until 2022 and then profits stay constant. Profit reaches a climax at this date...\" without having to prepare anything.\n\n\nAnother idea is to generate these descriptions directly from the data and not from the plot, so a solution for this would also help.",
"date": "2022-11-28"
},
{
"vote": 3,
"title": "How to evaluate SciSpaCy's entity linking",
"text": "Hey all,\n\n\nPosting here for the first time, hope someone here might be able to help me out.\n\n\nI'm using \nSciSpaCy's Entity Linker\n with a custom knowledge base. As I'm updating some components of my application (e.g. the underlying language model, sentence tokenization pipeline, the knowledge base itself, etc), I'm noticing that (1) the number of entities that the application picks up changes and (2) the linked concepts themselves change (not the detected entities but the concepts that are linked to these entities). With this in mind, I'd like to be able to evaluate my entity-linking application to understand whether the recently introduced changes help or make things worse.\n\n\nUnfortunately, I cannot seem to find any resources for that. I was hoping to find either an evaluation library of some sort (assuming we are not just interested in a confusion matrix) or a \"gold standard\" dataset with entities in various forms (e.g. abbreviated, inflected, etc) and the expected linked concept.\n\n\nI'm afraid I'm a novice in this field which is why I'm reaching out here, hoping that anyone might be able to point me to a set of useful resources or share some tips with me.\n\n\nMany thanks in advance!",
"date": "2022-11-27"
},
{
"vote": 2,
"title": "Predicting whether sequence of actions corresponds to language description",
"text": "Hi all. I'm trying to predict whether a sequence of actions taken by an RL agent (e.g. [3, 3, 3, 3, 3, 3, 3, 3, 2, 3 3) corresponds to a given natural language description (\"go right and jump over the thing.\"). I have a dataset of (trajectory, lang. description) pairs. How would you approach this? Would you suggest converting the trajectory to language commands - like replacing the 3's with RIGHT, etc., and fine-tuning a transformer model on these inputs?\n\n\nThe output I'm looking for is how related/unrelated the trajectory is to the description, from 0 to 1.",
"date": "2022-11-27"
},
{
"vote": 5,
"title": "What is the meaing of \"formalization of natural lanuguage\"?",
"text": "What is the meaing of \"formalization of natural lanuguage\"?, i couldn't find the definition of formalization in this context.",
"date": "2022-11-27"
},
{
"vote": 5,
"title": "How to input a mixture of text and numerical data in a binary classification task using bert simpletransformers library?",
"text": "I am doing a binary classification task, where there are four columns in the input data: 3 int type and one text: something like: Customer_rating (int), Customer_credit_score(int), Likes_on_review(int), Review_text ---> Classify the review as positive or negative(0,1) I am trying to use \nsimpletransformers library\n, I tried simply tried appending the numerical values separated by \" [SEP] \"\n string between them, so each input looks like\n\n\n&quot;1 [SEP] 0 [SEP] 1 [SEP] Hello, nice to meet you&quot;  \n\n\n\nBut this in turn reduces the accuracy, than just using \"Hello, nice to meet you\" as the only input. May be I am not googling the right terms, can't find a solution to this seemingly common scenario.",
"date": "2022-11-26"
},
{
"vote": 8,
"title": "TF-IDF for classification.",
"text": "Trying to figure out how to use TF-IDF to classify documents. let say TYPE1, TYPE2, TYPE3\n\n\nSo I have training set where my docs already classified. I can calculate idf(W) (idf of each word) in my TYPE1, TYPE2, TYPE3 sets. (i do remove stop words and do stemming).\n\n\nBut now what? When i get a new document, what do i do now?\n\n\nSimply check if the TOP N words with highest idf(w) from TYPE1 present in that document then i classify it as TYPE1 (e.t.c. for TYPE2 and TYPE3)?",
"date": "2022-11-26"
},
{
"vote": 2,
"title": "AI-powered video-based chatbot: The result of merging a conversational algorithm based on GPT, a lipsync engine, a voice cloning technology and a proprietary \"personality cloning\" technique",
"text": "Our company, Pheon, designed the technology that allows us to digitally \"clone\" people and create their \nDigital Twins\n. Based on a set of videos, a short bio and a detailed questionnaire we can create a realistic video-based AI-powered chatbot that will look, sound and, most importantly, communicate just like its human \"prototype\".\n\n\nThe technology consists of three major components: a conversational algorithm, a lipsync and a voice modulation system. A conversational model is based on an NLP pipeline that we created and has GPT in its core.\n\n\nOne of our engineers volunteered to be \nour first model\n, but as the AI training dataset grew bigger, we managed to minimize the effort for creating new chatbots. Now, all we need to create a \"digital twin\" for someone is their consent.",
"date": "2022-11-25"
},
{
"vote": 3,
"title": "An example about how to evaluate pretrained sentiment huggingface model (ex. Bert)",
"text": "I am looking for an example how to evaluate pretrained hugging face transformer with your own dataset, when I am looking for that, there always notebooks or example about how to train your model, but i cannot find any about how to benchmark or evaluate pre trained hugging face model where I can get confusing matrix and f1 scores on each sentiment with my dataset, for example on bert-base-multilingual-uncased-sentiment.",
"date": "2022-11-24"
},
{
"vote": 1,
"title": "An Easy-to-use and powerful NLP library with Awesome model zoo, supporting wide-range of NLP tasks from research to industrial applications, including Text Classification, Neural Search, Question Answering, Information Extraction, Document Intelligence, Sentiment Analysis and Diffusion AIGC etc.",
"text": "[removed]",
"date": "2022-11-23"
},
{
"vote": 2,
"title": "implement tensorflow bi-directional code to pytorch",
"text": "def encoder():\n  enc = keras.Sequential()\n  enc.add(Input(shape=(maxlen,), name=&#039;Encoder-Input&#039;))\n  enc.add(Embedding(num_words, embed_dim,input_length = maxlen, name=&#039;Body-Word-Embedding&#039;, mask_zero=False))\n  enc.add(Bidirectional(LSTM(128, activation=&#039;relu&#039;, name=&#039;Encoder-Last-LSTM&#039;)))\n\n  return enc\n\n\n\nHow can I get the same model in pytorch? I know \nLSTM\n in pytorch has \nbidirectional\n parameter but the way it handles input seems different.",
"date": "2022-11-23"
},
{
"vote": 19,
"title": "Semantic Search with SQLite",
"text": null,
"date": "2022-11-22"
},
{
"vote": 1,
"title": "Text auto-encoder using Transformer",
"text": "Is there any resource or way to build text-autoencoder?\n\n\nHere is a simple example:\n\n\nhttps://alvinntnu.github.io/python-notes/nlp/word-embeddings-autoencoder.html\n\n\n&#x200B;\n\n\nI want to build similar model using transformer or pre-trained bert.",
"date": "2022-11-22"
},
{
"vote": 7,
"title": "Open source dataset for NLP",
"text": "Hi , I am looking for free open source datasets that are out there being used to train large language models.\n\n\nOne such example is Pile dataset \nhttps://arxiv.org/abs/2101.00027",
"date": "2022-11-22"
},
{
"vote": 5,
"title": "Finetuning Bert with SQuAD 2.0",
"text": "I am trying to finetune a Bert model using SQuAD 2.0. dataset. The goal of the trained model is to predict a start token and end token of an answer from the given context. So, while training the model I was providing the start token and end token of the answer. But, in this dataset, there are many questions with no answer, meaning there will be no start token and end token for these questions. \n\n\n\n\nWhat should I provide as a start token and end token for these questions when no answer is present in the given context? \n\n\nAlso, while predicting I do the argmax on every token in the context to find the answer start token and answer end token. So, the questions where the correct prediction should be 'no answer', how the model will decide that? Doing argmax will always return me two tokens for answer start token and answer end token.",
"date": "2022-11-21"
},
{
"vote": 15,
"title": "How do various content-generating services work?",
"text": "Right now sites like \nhttps://www.jasper.ai/\n offer text generation for emails, ads, social media posts and etc. I wonder, do they simply tune a separate gpt-3-like model for each of these tasks? Or there is a new approach to solving this?",
"date": "2022-11-20"
},
{
"vote": 2,
"title": "What sort of approach could I take to identify words that are unusually occurent in a specific area?",
"text": "I want to build a small project that performs some natural language analysis to find which skills are associated with certain jobs, and some further analysis to find out which of those skills are associated with the highest salaries.\n\n\nI see the first step of this to be: Identify from the job requirements words that are much more common in job postings related to your query compared to all job postings.\n\n\nSo all job postings my have common words like: the, a, it, ... and maybe more specifc words like ... learner, relocate etc.\n\n\nBut for 'software developer' there will be an exceptionally high rate of ... javascript, c++, SQL.\n\n\nI can imagine trying to come up with a rudimentary system myself to do this and it failing horribly. I was wondering if there were more rigorous approaches, or even libraries (ideally javascript/node libraries) for achieving this?\n\n\nOr should I take a different approach altogether?",
"date": "2022-11-19"
},
{
"vote": 4,
"title": "Using cmudict in Python?",
"text": "Cmudict is a .dict file and I can't find anything online about this extension. How to I get python to read through the file \nf.read\n() style? Or how can I convert this to a .txt file? Every G2P code on github uses cmudict so it must be possible I guess\n\n\nIf anyone knows this would be greatly appreciated; I haven't been able to find any information using google",
"date": "2022-11-18"
},
{
"vote": 17,
"title": "[Hiring] ML Engineers and Python Backend Developers",
"text": "Hey everyone,\n\n\nMy company \nPrivate AI\n is currently looking for \nremote\n ML engineers and Python backend developers. I'm posting them both together here as there's no solid line between the two roles for us.\n\n\nIn a nutshell, we build high performance transformer models to detect personal information. It's similar to NER. The ML engineers are responsible for R&D and model development, whilst our backend team is responsible for optimizing and deploying the models.\n\n\nYou can read the full \nPython JD here\n, here are some highlights working for us:\n\n\n\n\nWeâ€™re growing rapidly, even in the current climate: Revenue is up 5X since last year. Our customer base includes several large enterprises.\n\n\nNothing is on fire: Our product is successfully in production with multiple customers.\n\n\nGreat work-life balance: Whilst weâ€™re a startup and overtime will be required sometimes, we heavily value work life balance and usually stick to regular hours. This is not a 60 hour per week job.\n\n\nData is a separate team.\n\n\nThere seems to be a big trend in industry to hire lots of ML people and them put them in roles that have very little to do with ML. That's not us.\n\n\nA backend developer for us does a lot of what an ML Engineer would do at other companies, such as model optimization.\n\n\n\n\nThe roles are remote. Points if you're based in Canada, especially for the ML Engineer roles. Otherwise anywhere is fine, but we prefer to stay away from US, UK, FR and DE.\n\n\nIn both roles, \none thing we really value is good coding skills\n. You should have at least a few years writing production code under your belt. The backend position in particular is a great fit for someone looking to transition into ML, or who might not have experience with Transformer models.\n\n\nFor the ML Engineer role, we'd like to see a couple of years working with transformer models in production. This role is a great fit if you'd like to do something more than just use pre-built HuggingFace models and pipelines.\n\n\nI'm not a recruiter\n, feel free to DM me any questions :)",
"date": "2022-11-18"
},
{
"vote": 10,
"title": "Gridspace speech technology lecture series (for MIT IAP 2023)",
"text": null,
"date": "2022-11-18"
},
{
"vote": 1,
"title": "How Artificial Intelligence (Ai) sees and interprets the joke/meme \"What do you call a pig that does karate? A pork chop\"",
"text": null,
"date": "2022-11-17"
},
{
"vote": 1,
"title": "Spellcheck and Levenshtein Distance",
"text": "[removed]",
"date": "2022-11-16"
},
{
"vote": 0,
"title": "Dancing mathematician",
"text": null,
"date": "2022-11-16"
},
{
"vote": 8,
"title": "When using the Whisper model ( the transcription and translation model released by openAI). Do you need to do some audio processing to optimize the results or does it do that on it's own?",
"text": null,
"date": "2022-11-16"
},
{
"vote": 3,
"title": "Parsing query intent",
"text": "Hey, was wondering if anyone had insight/resources on how different apps like fantasical are able to be semantically filtered. Ie, you can write a query like â€œWhat events do i have in the \nnext month\nâ€? And filter results by time period. Or these search engines that are able to filter by â€œshow me \narticles\n about X?â€ and only filter on article type data? (And combinations thereof). Are these just keyword base filter (are there any existing libraries? can this be done POS based or otherwise?) or are these doing an actual semantic match on certain parts of the query (n grams or something?) and on extracted meta? lang model chaining? canâ€™t seem to find anything about how this works :( \n\n\nThanks so much for any help!",
"date": "2022-11-16"
},
{
"vote": 1,
"title": "Using Earth Mover's Distance for multi-dimensional vectors with unequal length",
"text": "Edit\n: Here is the code I am using to generate the vector of the sentence:\n\n\nsentence= &quot;Hello this is a test sentence.&quot; \ntokens = tokenizer(sentence, return_tensors=&#039;pt&#039;, padding=True, truncation=True, max_length=512) output= model(tokens[&#039;input_ids&#039;], attention_mask=tokens[&#039;attention_mask&#039;]) \ntoken_vectors = output.hidden_states[-1].detach() \nreturn token_vectors[0]\n\n\n\nHi everyone.\n\n\nI am working on a project which involves calculating sentence similarity. Context vectors for each token in a sentence are generated using Hugging Face's BERT, and these context vectors are appended together to generate the sentence vector (vector of vectors). Upon researching various metrics, I came across Earth Mover's Distance (or Wasserstein Distance). I did not opt for Cosine Similarity since my vectors do not have the same length (since sentences might not be of the same length) and my vectors are also multi-dimensional (since each token is represented by a context vector).\n\n\nI found \nScipy's\n implementation, but this is only for 1D distributions. I also found a \nStack Overflow question\n which seemed promising until I realized that despite catering for multi-dimensional vectors, it requires vectors to be of equal length.\n\n\nDoes anyone have any suggestions on how I may implement this? Or perhaps, other techniques for possible implementation are also appreciated.\n\n\nThanks  in advance!",
"date": "2022-11-15"
},
{
"vote": 4,
"title": "Have a problem maybe you can help me with",
"text": "Letâ€™s say I have a large corpus comprised of sentences about trees and that in some sentences the word â€œtreeâ€ appears multiple times. Such as: \n\n\nâ€œThe apple tree is the most beautiful treeâ€ \n\n\nhow would I automate a process that lets me remove the second appearance of â€œtreeâ€? \n\n\nWhat Iâ€™m after is a way to target particular words that appear en masse and delete those instances where a sentence has the targeted word more than once. \n\n\nThank you!",
"date": "2022-11-15"
},
{
"vote": 1,
"title": "Game of Tongues: How Duolingo Built A $700 Million Business With Its Addictive Language-Learning App",
"text": null,
"date": "2022-11-15"
},
{
"vote": 1,
"title": "I have pdf files in which I need to extract the text, but the pdf is a conversation between two people in which there will be the person's name and then a colon (followed by what they say). How do I extract the text so it's just continuous text of what they say and not with the name of the people?",
"text": "For example the pdf will look like this:\n\n\nJennifer:         I am at the house right now\n\n\nKevin:             Okay I will be there soon\n\n\nJennifer:        Save me some food. \n\n\nKevin:             Okay I will \n\n\n&#x200B;\n\n\nI just want to just extract the text and have it like this\n:\n\n\n I am at the house right now Okay I will be there soon Save me some food Okay I will \n\n\n&#x200B;\n\n\nAlso each pdf file has a header that consists of a straight horizontal line with a name of an organization at the top and page number. The header is at the very top of every page and at the very bottom of every page there is something similar. How do Ignore those parts and just extract the text like I explained?",
"date": "2022-11-15"
},
{
"vote": 0,
"title": "Release 0.8.3 Â· capitalone/DataProfiler",
"text": null,
"date": "2022-11-14"
},
{
"vote": 1,
"title": "Multi class problem with highly skewed data.",
"text": "I have a multi class classification problem, where data is distributed to classes as follows:\n\n\ntarget_1 : 400 samples\n\n\ntargets_2 : 150 samples\n\n\ntargets _3: 15 samples\n\n\ntatrgets_4 : 3 samples\n\n\ntatrgets_5 : 1 sample\n\n\nIs it ok to balance classes to 400 samples using data augmentation (I'm using contextual bert embedding to augment low resources classes) and then proceed the training?\n\n\nAlready test it with pre-trained bert and got 91% accuracy. which seems fine but i'm not sure if this approach is correct.",
"date": "2022-11-14"
},
{
"vote": 54,
"title": "NLP Demystified 15: Transformers From Scratch + Pre-Training and Transfer Learning",
"text": "The Transformer architecture underpins several state-of-the-art models today, and by the end of this module, you'll have a clear understanding of how it works.\n\n\nWe'll explore every layer of the Transformer, then build one from scratch. We'll then learn how to leverage pre-trained models from Hugging Face and OpenAI for our own projects.\n\n\nWe'll cover:\n\n\n\n\nSubword tokenization with byte-pair encoding (BPE).\n\n\nSelf-Attention and how it addresses the drawbacks of recurrence.\n\n\nHow Multi-Head Self-Attention works.\n\n\nEvery step of the encoder and decoder.\n\n\nWhy and how to use pre-trained models in our own work.\n\n\nHow BERT works.\n\n\nHow GPT learning to \"predict the next token\" leads to all sorts of capabilities.\n\n\n\n\nIn the two-part demo, we'll\n\n\n\n\nBuild a working Transformer from scratch.\n\n\nFine-tune a pre-trained transformer for question answering, and explore all the cool things GPT-3 can do.\n\n\n\n\nÂ \n\n\nYouTube link: \nhttps://www.youtube.com/watch?v=acxqoltilME\n\n\nColab link: \nhttps://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_transformers_and_pretraining.ipynb\n\n\nBy the end of this module, you'll have a solid understanding of the Transformer architecture and know how to use pre-trained Transformers to build all sorts of useful applications.\n\n\nThis is the last module for NLP Demystified. If you found this course useful, let me know! I'd love to hear your thoughts and feedback.",
"date": "2022-11-14"
},
{
"vote": 3,
"title": "OpenAI whisper loss function and word levels timestamps",
"text": "I am wondering which loss function is used here during the training. I know it is a multitask training process they did, but I am interested in discovering how does the algorithm detects words. I have red the paper but found anything about that there.\n\n\nAs an example, Wav2Vec2 used CTC loss. I can leverage on logits distribution to have precise enough word level timestamps.\nWith language models applied to wav2vec2 things get trickier, but still had the word offsets from beam search on which I could rely to approximate word level timestamps (beginning and end). \n\n\nWith whisper, instead, it is even trickier. The logits and the tokens to me seem to be at word level, with partial word approximation. This raises a lot of questions on my side, hence my post here.",
"date": "2022-11-14"
},
{
"vote": 3,
"title": "Coreference Resolution Dataset",
"text": "I am currently running experiments on coreference resolution for scientific documents using the SciERC (2018) dataset and was wondering if there were more recent datasets for this particular task with scientific documents.",
"date": "2022-11-12"
},
{
"vote": 19,
"title": "Getting started with semantic search",
"text": null,
"date": "2022-11-12"
},
{
"vote": 4,
"title": "Text classification with BERT in Julia (about model parameters)",
"text": null,
"date": "2022-11-12"
},
{
"vote": 6,
"title": "Average Emotion Scores According to NLP-based AI of News Headlines vs. Twitter Posts vs. Blog Posts",
"text": null,
"date": "2022-11-12"
},
{
"vote": 25,
"title": "Best open source annotation tool for NLP?",
"text": "I am working for a startup as a Data Scientist and I have to setup the annotation pipeline from scratch.\nI have to annotate for NLP tasks like multi-label text classification and NER.\nI have tried Docano and Argilla.io.\nArgilla is really good for both these NLP tasks only con is I am not able to run it on a server for multiple annotators to work on and collaborate.\nCan anyone recommend the best tool they have found useful?\nIf not open sourced any economical paid product out there? (Prodi.gy is 500usd one time, something cheaper or limited period subscription would also be helpful since it will be difficult to convince then to directly invest 500usd at once)\nPlease share your experience if you have any best practices for setting up the annotation pipeline :)",
"date": "2022-11-10"
},
{
"vote": 2,
"title": "Alternatives to padding",
"text": "I am working on a project which requires me to consolidate the representations of several vectors of different shapes. Is there any alternative method to padding that can be used for this?",
"date": "2022-11-10"
},
{
"vote": 3,
"title": "Audio to Text Data Processing Pipeline",
"text": "Hi All,\n\n\nI'm working on a side project that involves transcription and speaker identification for audio files (podcasts, presentations, etc.), and I'm wondering if the community has any advice for Google Cloud Platform architecture.\n\n\nA few things to note:\n\n\n&#x200B;\n\n\n\n\nI will likely \nNOT\n be using Google's Speach-to-Text, since I have been getting better quality results with solutions like Whisper and Assembly AI. Therefore, I will need to build Python code as part of the solution to process the audio files and pass them to Whisper/Assembly AI\n\n\nIt would be nice to set up a trigger that starts the flow whenever a new audio file is placed in a bucket\n\n\nWe will be processing, potentially, up to a few hundred hours of audio per month (and likely more in the future)\n\n\n\n\nOne solution I was thinking of was creating a Cloud Function that was triggered when an audio file was placed in a storage bucket. The Cloud Function would then process the file and update a database with the transcription and speaker identification.\n\n\nIf anyone has experience with or suggestions for how to go about this, please let me know!",
"date": "2022-11-10"
},
{
"vote": 6,
"title": "[Tutorial] How to Train LayoutLM on a Custom Dataset for Document Extraction with Hugging Face",
"text": "Wrote this guide to help developers/data scientists learn how to fine-tune LayoutLM on a custom dataset for PDF doc extraction tasks with transformers. Hope you find it helpful!\n\n\nhttps://www.butlerlabs.ai/blog-posts/tutorial-train-layoutlm-on-a-custom-dataset-with-hugging-face\n?",
"date": "2022-11-09"
},
{
"vote": 1,
"title": "I am trying to read in text from a pdf file. The pdf file is a dialogue between \"speaker 1\" and \"speaker2\". I don't want the speaker 1 or speaker 2 part, I just want read in the text so that it text is just continuous instead of separating the text from different speakers.",
"text": "How would I go about creating a function that does this?",
"date": "2022-11-09"
},
{
"vote": 4,
"title": "any use case of ontology+NLP?",
"text": "Hello, I had a course on NLP and 1n introduction to knowledge representation (introduced to RDF, but not to SPAEQL). I learned that deep learned ontologies are hot topic. Do you know good practical materials to reproduce this task? Thanks",
"date": "2022-11-09"
},
{
"vote": 9,
"title": "How does Spacy's multiprocessing work?",
"text": "I'm working on a project that uses spacy's coreference library neuralcoref, which is really slow. I haven't been able to figure out how to put it on GPUs yet, so I've been trying to use spacy's multiprocessing functionality by creating a pipe with n_process >1. This doesn't seem to speed things up at all though. Why might that be? And what can I do to speed things up?",
"date": "2022-11-09"
},
{
"vote": 7,
"title": "How do I transform many pdfs into text?",
"text": "Hello,\n\n\ncurrently I am working on a research project and I have a directory full of pdf files.\n\n\nIn a next step, I want to:\n\n\ncrop out specific pages of each document\n\n\nremove tables, pictures, and formatting\n\n\nand save the plain text in a text file.\n\n\n&#x200B;\n\n\nIs there a package to do this automatically for each document?\n\n\n&#x200B;\n\n\nThank you very much in advance!",
"date": "2022-11-08"
},
{
"vote": 2,
"title": "Duo Lingo &amp; other ideas",
"text": "Hi, I am a HS junior looking for unpaid internships/projects for next summer. I am interested in languages (Latin, Greek, currently learning Mandarin) as well as CS. Someone suggested I reach out to Duo Lingo to see if I could get any role (unpaid of course) but I could not find an email contact on the page. Would you have any ideas? Also would appreciate any other avenues for next summer related to these interests.  I am open to all ideas. THank you so much",
"date": "2022-11-07"
},
{
"vote": 2,
"title": "How to split the text into main points based on given main points?",
"text": "I am working on a problem where I have two texts T1, T2. T1 contains some important points that I have entered. How can I make sure that T2 has those points? I am aware of the algorithms like cosine, jaccard, BERT for semantic similarity but the problem is that they apply to the whole text whereas I want point-wise similarity i.e. T2 must contain the T1 points although the order and words used may differ a bit.\n\n\nBy points I meant bullet points covering discrete concepts and I basically want to check how many discrete concept in my T1's points are covered in T2 where a point from T1 could also be spread across multiple sentences. \n\n\n&#x200B;\n\n\nExample:\n\n\nSo T1 could have the following two points:\n\n\n- The Queen reigned from 1943 to 2022.\n\n\n- The Queen was the second longest reigning monarch.\n\n\n&#x200B;\n\n\nNow T2 could either be:\n\n\nThe Queen was the second longest reigning monarch with her reign spanning 1943 to 2022.\n\n\nor\n\n\n- The Queen reigned from 1943 to 2022.\n\n\n- She was Britain's second longest monarch.\n\n\nIn both these cases, T2 should be considered to contain both points in T1.",
"date": "2022-11-06"
},
{
"vote": 14,
"title": "WhyML - Wav2vec2 A Framework for Self-Supervised Learning of Speech Representations",
"text": "Hi guys,\n\n\nI have made a video on YouTube \nhere\n where I go through the Wav2Vec2 paper and explain each section. This is a new series on my channel that I started. I hope you enjoy it!\n\n\nAs always, any kind of feedback is more than welcomed! :)",
"date": "2022-11-06"
},
{
"vote": 1,
"title": "I like models",
"text": "[removed]",
"date": "2022-11-06"
},
{
"vote": 0,
"title": "How to pronounce certain Chinese words correctly.",
"text": null,
"date": "2022-11-06"
},
{
"vote": 9,
"title": "Guidance needed: Extracting diseases and symptoms from medical text",
"text": "I am thinking about learning to extract diseases and symptoms from medical texts (books, sites) and to map each disease with their symptoms. How should I go about building the necessary knowledge and skillset to do the same? Kindly comment and/or point me to any source that can help me.\n\n\nI do not have much experience in the field of NLP (I have only done some n-gram + TF-IDF + cosine similarity based fuzzy match projects).",
"date": "2022-11-05"
},
{
"vote": 1,
"title": "Extracting diseases and symptoms from medical text",
"text": "[deleted]",
"date": "2022-11-05"
},
{
"vote": 26,
"title": "Topic modeling with semantic graphs: a different approach",
"text": "Dimensionality reduction with UMAP combined with HDBSCAN is a popular topic modeling method found in a number of libraries. txtai takes a different approach with a semantic graph.\n\n\nWhen enabled, txtai builds a semantic graph at index time as it's vectorizing data. These vector embeddings are then used to create relationships in the graph. Finally, community detection algorithms build topic clusters.\n\n\nThis approach has the advantage of only having to vectorize data once. It also has the advantage of better topic precision given there isn't a dimensionality reduction operation (UMAP).\n\n\nRead more here: \nhttps://neuml.hashnode.dev/introducing-the-semantic-graph",
"date": "2022-11-05"
},
{
"vote": 27,
"title": "Large language models (LLMs) often make mistakes that are difficult to correct | Check out this research that study the problem of quickly editing these models",
"text": null,
"date": "2022-11-04"
},
{
"vote": 5,
"title": "NLP approach similar approaches to Natural Language Inference?",
"text": "My problem setting is very analogous to what NLI problems assume.\n\n\nI have a claim/gold standard argument ( similar to the hypothesis).\n\n\nI have a bunch of statements that discuss the argument.  (similar to the premise) \n\n\nAnd they either support it, refute it, or use it in a neutral manner (similar to entail, contradict, and neutral). \n\n\nI tried evaluating this problem under NLI settings and tried a few methods (Roberta fine-tuned on MNLI, BART fine-tuned on MNLI). But they don't seem to work particularly well. Note: I used inference on the pre-trained models and was not able to fine-tune the models further because of a smaller dataset, and lack of human resources, which likely explains the lack of good performance. \n\n\nTo add more context, the arguments are a bunch of scientific facts, and statements are extracted from social media platforms.\n\n\nI am wondering if I am missing the point by strictly formulating the problem as a NLI problem or are there better approaches to tackle it. \n\n\nAny pointers would be greatly appreciated. Thanks.",
"date": "2022-11-04"
},
{
"vote": 3,
"title": "When it comes to topic modeling, named entity recognition, and entity extraction. Is Spacy the go to library or is Bert used or both?",
"text": "I will be working on a project in which we will be given many documents. And we want to eventually build a semantic search. I am new to NLP. I have been reading up on it for the past several days and watching tutorials. Spacy seems to do score similarity between documents and what not so I figured I will use that, but read that google and many people use BERT for topic modeling, so I am not sure which to use or if I use them in tandum with each other.",
"date": "2022-11-04"
},
{
"vote": 0,
"title": "Language in WEB 3",
"text": "what do you people think the future of language in web 3.0",
"date": "2022-11-03"
},
{
"vote": 10,
"title": "Small bert with 4 layers giving me better performance than bert base.",
"text": "I was testing a text classification problem with small version of bert (L-4, H-128, A-2). I was a little bit surprised because it has a result of 90% accuracy although when I tested with bert base, the result does not exceed 70%. I don't know how can i interpret this!!",
"date": "2022-11-02"
},
{
"vote": 5,
"title": "Bidirectional conditional text generation (generating text around another text)",
"text": "I have a paired dataset of source text A and output text B.\n\n\nFor each row of the dataset, A may occur in the front, middle, or end of B.\n\n\nI want to create a conditional text generation model that generates context in the form of text B in both directions, given a key phrase in the form of text A.\n\n\ntext_b = generated_context_front + text_a + generated_context_end\n\n\n\nStrategies that I can think of are masking the context of text A like \"[mask] text A [mask]\" and filling them in to generate text B. Or, maybe I can try text generation models like GPT, but isn't GPT only supposed to look in one direction, not both?\n\n\nAny ideas on what I can try?",
"date": "2022-11-01"
},
{
"vote": 5,
"title": "PEER : A Collaborative Language Model for generating text from Meta AI",
"text": "We generate textual content through  a collaborative writing process.  We start with an initial draft, ask for suggestions, and repeatedly make changes and then generate content.  Agnostic of this process, todayâ€™s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. \n\n\nTo address these shortcomings, META AI created PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. This model incorporates user inputs to generate text. \n\n\nI believe this approach needs to be followed to  augment AI with human inputs to improve the results. May be this approach can be extended to other modalities like images, audio and video as well\n\n\nI have made a video on PEER. Do checkout\n\n\nhttps://youtu.be/xf5U6CdnnkA",
"date": "2022-11-01"
},
{
"vote": 0,
"title": "Hi! I am new to this subreddit. I would to ask someone a specific question about NLP.",
"text": "Hi, would someone please DM me. \n\n\n&#x200B;\n\n\nThank you.",
"date": "2022-10-31"
},
{
"vote": 1,
"title": "Freely available word-frequency lists (verbs, English)?",
"text": "I'm looking for a comprehensive list of English verbs, sorted by how frequently they occur (in a representative corpus). Any ideas where to get such a list? Thanks",
"date": "2022-10-31"
},
{
"vote": 16,
"title": "Very fast graph-based keyword extraction",
"text": null,
"date": "2022-10-31"
},
{
"vote": 1,
"title": "Parsing Specific Information From Wiki Dumps",
"text": "I'm working on an NLP project and I need to parse Wiki Dumps for it. I want the parsing to be done in a way that after parsing we have folders divided by domains, for example:\n\n\n&gt; Cats\n\n\n- all the wiki dumps articles that have anything to do with cats\n\n\n&gt; Dogs\n\n\n- all the wiki dumps articles that have anything to do with dogs\n\n\nand so on.\n\n\n&#x200B;\n\n\nI've thought of doing it by catching words from Wiikipedia infoboxes,  but it seems very roundabout.\n\n\nIf anyone has any smarter ways to go about it, please let me know. Thanks.",
"date": "2022-10-31"
},
{
"vote": 1,
"title": "Looking for Text Summarization Model Capable of Summarizing an Entire Book",
"text": "[removed]",
"date": "2022-10-30"
},
{
"vote": 1,
"title": "When it comes to the inputs for deep learning (whether it be RNNs or Transformers). Are the inputs in the form of a bag words (count vectorizer, TF-IDFF, etc ).",
"text": "I am confused about how inputs are fed to deep learning models in the context of natural language processing.",
"date": "2022-10-28"
},
{
"vote": 7,
"title": "Hands-on class with - How to build Machine Learning collaboratively?",
"text": "Hi Everyone,\nWe are organizing a Hands-on class with HuggingFace  How to build Machine Learning collaboratively? (an online demo  workshop @ 5:00 PM Irish time) in collaboration with the HuggingFace. Our speaker for the  demo is none other than Manuel Romero.\nLink: \nhttps://nuigalway-ie.zoom.us/j/94149846053\nManuel  is a HuggingFace fellow and an artificial intelligence engineer  (Natural Language Generation/ Natural Language Processing) and software  architect at NarrativeAI with years of experience in the field, he  specializes in machine learning (ML). Manuel is considered one of the  most prominent professionals in Spain in the field of NLG and the most  contributor to Hugging Face Hub with over 300 models shared in the Hub.\n\n\nIn  this talk, you're going to learn how to create Machine Learning apps  and demos using Streamlit and Gradio, Python libraries for this purpose.  Additionally, you'll see how to share them with the rest of the Open  Source ecosystem. Learning to create graphic interfaces for models is  extremely useful for sharing with other people interesting them.\n\n\nWhat is required to follow it?\n\n\n\n\nBasic knowledge of Python\n\n\nConceptual knowledge of ML\n\n\nA google Collab account\n\n\nHugging Face Hub account",
"date": "2022-10-28"
},
{
"vote": 1,
"title": "Optimal way to split long sentences for tokenization?",
"text": "Transformers have a maximum number of tokens they accept, making it difficult to deal with long sentences. There are use cases where we could split up the long sentences into smaller sentences, each of which fits in the model. I would like to split a sentence such that each chunk fits exactly in the max_length accepted by the tokenizer (except the for the last chunk, which can have fewer tokens). I have not found a clean and efficient way of achieving this, nor can I find one on Google. Does anyone have a function to achieve this?\n\n\nTo clarify, my aim is to be able to take a sentence as input, get a list of tokenized chunks that I can feed to the model to get a list of outputs from the model. This should happen in such a way that no parts of the original sentence are lost.This means I do not necessarily need to split the sentence itself before tokenisation; the splitting could happen after tokenisation, but any special tokens should be taken into account.",
"date": "2022-10-28"
},
{
"vote": 1,
"title": "Weekly newsletter summarizing current research",
"text": "[removed]",
"date": "2022-10-27"
},
{
"vote": 19,
"title": "CROWDLAB: open-source tools for data labeled by multiple annotators",
"text": "Hi Redditors! Many of us in NLP use multiple annotations to get higher quality labels for our text â€” yet AFAIK there is no open-source python package for \ndata labeled by multiple annotators\n â€” so we \nbuilt one\n, \nbenchmarked it\n, and released \nthe CROWDLAB paper\n.\n\n\nAfter many long nights, I'm psyched to share the new easy-to-use and effective CROWDLAB algorithm that can use \nany classifier\n to estimate:\n\n\n1 - AÂ \nconsensus label\nÂ for each example that aggregates the individual annotations.\n\n\n\n\nmore accurate than aggregation via majority-vote and common crowd-sourcing algorithms\n\n\n\n\n2 - AÂ \nquality score for each consensus label\nÂ which measures the confidence that the consensus is correct.\n\n\n\n\nuses well-calibrated estimates that account for the: number of annotations for each example and their agreement, prediction-confidence from a trained classifier, and trustworthiness of each annotator vs. the classifier\n\n\n\n\n3 - AÂ \nquality score for each annotator\nÂ which estimates the overall correctness of their labels.\n\n\nSurprise!\n All 3 tasks are estimated in one line of open-source code via \ncleanlab.multiannotator.get_label_quality_multiannotator\n .\n\n\nfrom cleanlab.multiannotator import get_label_quality_multiannotator  \n\nget_label_quality_multiannotator(multiannotator_labels, pred_probs)  \n# multiannotator_labels: matrix with rows = examples, columns = annotator labels, NA = missing label \n# pred_probs: predicted class probabilities from any trained classifier\n\n\n\nExtensive benchmarks on real-world multi-annotator data show that CROWDLAB produces significantly better estimates for all three tasks thanÂ algorithms from crowdsourcing like: majority-vote, GLAD, Dawid-Skene, etc.\n\n\nUsing simple weighted ensembles rather than complex generative models makes CROWDLAB results easy to understand, efficient, and reproducible. An added benefit â€” CROWDLAB also works well for datasets that include examples with a single annotation (useful for folks who have a tight data labeling budget ðŸ˜‰).\n\n\n\n\nBlog post: \nhttps://cleanlab.ai/blog/multiannotator/\n\n\nPaper: \nhttps://cleanlab.github.io/multiannotator-benchmarks/paper.pdf\n\n\nTutorial: \nhttps://docs.cleanlab.ai/stable/tutorials/multiannotator.html\n\n\nBenchmarks: \nhttps://github.com/cleanlab/multiannotator-benchmarks\n\n\nCode: \nhttps://github.com/cleanlab/cleanlab\n\n\n\n\nHave fun using CROWDLAB!",
"date": "2022-10-27"
},
{
"vote": 6,
"title": "Synthetic data generation methods",
"text": "I'm looking for resources on various methods of creating synthetic datasets since my training data is limited.\n\n\nIn my case, I have a small data set of 400 mundane everyday behaviors, e.g.,\n\n\n\n\nLaughed out loud at something I thought of.\n\n\nRead a book.\n\n\nComplimented someone.\n\n\nSlept past noon.\n\n\n...\n\n\n\n\nI'm looking to increase this list to contain ~ 5k - 10k records. My previous attempt was to fine-tune GPT-2 to this brief list and use unconditional generation (or prompt it by a verb) to increase the list. Unfortunately, fine-tuning GPT-2 on the 400 item data set only outputs gibberish, so I suppose that I need to find some other ways to synthesize more data. Any ideas? Thank you",
"date": "2022-10-26"
},
{
"vote": 4,
"title": "Online free conferences",
"text": "Hello.\nI'm a student who has to attend a conference on language technology/computational linguistics this year. I would have liked to find an online, free conference but can only seem to find very expensive one. Any help on the matter would be appreciated.",
"date": "2022-10-25"
},
{
"vote": 9,
"title": "Relation Extraction",
"text": "Hi All, I am a master's student in Computer Science and have been reading some papers on Relation Extraction. Now  I wanted to do it on some dataset but I don't know how to start. I feel I understand the theory very well but when it comes to implementation, I am stuck. I don't know how to start with RE. Any help will be much appreciated.",
"date": "2022-10-25"
},
{
"vote": 1,
"title": "If anyone's trying to learn new data science skills!",
"text": "[removed]",
"date": "2022-10-25"
},
{
"vote": 0,
"title": "Phd in NLP",
"text": "people , help me choose which country to go for phd in nlp US ( unsafe bc of mass shootings) , UK , ETH zurich",
"date": "2022-10-25"
},
{
"vote": 1,
"title": "AI Voice Commands API",
"text": "Having computers able to understand what we want from them through verbal commands opens up many possible applications to make our lives easier. Unfortunately, it's often hard to properly leverage the latest methods to accomplish that task. That's where Brillibot comes in. Brillibot is a simple-to-use voice command API that takes only a few minutes to set up with your own commands.\n\n\nhttps://youtu.be/MFT8BFLArVA",
"date": "2022-10-24"
},
{
"vote": 3,
"title": "Terminology for multiclass, multilabel, 'multi level(?)' problem",
"text": "Hello everyone,\n\n\nI work in healthcare and am trying to see if I can use NLP for a classification task on complex sentences. To explain, I have different labels, and each label has multiple levels. I am not sure on the correct terminology however. I have label X, and X always exists as one of 4 'levels':\n\n\n\n\n'absent' or 0\n\n\n'few', or 1\n\n\n'many', or 2\n\n\n'everywhere', or 3\n\n\n\n\nExample sentences then look like: 'I have no X', or 'I have a little bit of X', or 'there is X everywhere'. However, I also have labels Y and Z, which also have multiple levels. To complicate things further, one sentence can often contain information about multiple labels. As an example:\n\n\n\n\n'X and Y are both absent in this man'. classification would be X:0, and Y:0\n\n\n'He has no X but a lot of Z'. classification would be X:0 , and Z:2\n\n\n'There is Z everywhere, but very little Y'. classification would be Z:3 and Y:1\n\n\n\n\nSentences in my corpus can also be about something completely different in which I am not interested:\n\n\n\n\n'Q was quite small'.\n\n\n\n\nDoes anyone know the correct terminology for a problem like this? I have tried doing research on similar problems, I was thinking a regression based solution might be needed for the different levels. However, since I'm not sure on the exact name, it is hard. Also, if anyone has any suggestions for approaches, that would also be very helpful! I do have a few thousand high quality training sentences.",
"date": "2022-10-24"
},
{
"vote": 11,
"title": "Multi class classification problem(categories and sub-categories)",
"text": "I have a multi class classification problem where I have to first classify the texts with categories and then classify for each category, the subcategories. I have 15 categories, and each category has 9~10 subcategories. How can I approach this problem? Should I train a model (I will be using BERT pre-trained) for example to detect the categories, and then train 15 models (??), each one is specialized on a category to detect its sub-categories?  Is there a better approach?",
"date": "2022-10-24"
},
{
"vote": 1,
"title": "Chai App Unlimited Messages For IOS &amp; Any Device",
"text": "[removed]",
"date": "2022-10-22"
},
{
"vote": 0,
"title": "The Best Generative Model",
"text": "So, I want to make a NLP project in which I fine tune a language model in order it to produce some reviews (I will use my own dataset of film review from IMBD). I also want generation to be controllable (chosing sentiment). Which pre-trained models can be beneficial to try/use for my task? I myself know some models but I also want other opinions cause at these point all the information is kinda chaotic in my mind...",
"date": "2022-10-21"
},
{
"vote": 20,
"title": "Remote Data Scientist â€“ NLP &amp; Recommendations",
"text": "Hey everyone,\n\n\nWe're looking for an experienced data scientist to join our small data team, and I wanted to open it up here instead of casting into the abyss of Linkedin. High-level stats: we've been in business for over 20 years â€“ we're in the top-5 of our segment, we're 100% fully remote, and we keep EST hours. Our team is focused on using ML (NLP and deep learning, generally) for efforts like extracting signal from unstructured text (e.g., slot filling, classification), enhancing our recommendation/matching pipeline, and modeling our segment of the market.\n\n\nI'm someone who's worked in some FAANG-types, some startups, and in academia, and there are a few qualities that I appreciate in particular about this role:\n\n\n\n\nthe technical challenges we work on are like the fundamental NLU research tasks that I worked on in grad school, except without the p-value hacking or SOA-bake-off games. These are topics a lot of us are interested in, but, when we find a company working on them, we've usually joined company once the interesting decisions have already been made â€“ that's not the case here. I'm able to explore what is, for me, the substantial intersection between my interests and the company's technical needs\n\n\nthe data team has the freedom and independence to do our work properly and to generate results we believe in. We do maintain a good delivery cadence and we apply \"all the adages\" (e.g., don't let perfect be the enemy of the good), but (1) respect and trust are extremely strong between teams and (2) deliverables are kept small and are usually POCs or MVPs that we want to evaluate experimentally\n\n\nNo blood in the water â€“ no one is competitive or cutthroat; we're all \"pulling in the same direction\" and it feels that way. Nothing is territorial\n\n\nWe have a lot of data and the resources to analyze it pragmatically\n\n\nCompetitive compensation\n\n\n\n\nTo succeed here, I think the list above should sound appealing â€“ and, of course, that's not everyone. Some people are happiest working onsite, some personalities enjoy a collegial but competitive atmosphere, etc., but for a certain set of sensibilities, this role offers, what has been for me, a unique opportunity.\n\n\nAs far as technical background, real-world experience with these will be helpful:\n\n\n\n\nTransformer-based NLP models like BERT for tasks like text and token classification as well as entity extraction\n\n\nPython (we tend to share work in Jupyter notebooks)\n\n\nML libraries like some of these â€“ NumPy, pandas, SciPy, scikit-learn, SpaCy, fast.ai, XGBoost, PyMC3\n\n\nPyTorch or TensorFlow (experience with Huggingface in particular is relevant)\n\n\nTasks like classification, clustering, efficient matching, outlier/anomaly detection, churn analysis, LTV analysis\n\n\n\n\nAlso relevant:\n\n\n\n\nInterpretability (e.g., Shapley values, similarity measures)\n\n\nModel optimization (GPU/CPU, inference/training, distillation)\n\n\nSQL, Bash\n\n\nUS Citizen, able to work EST hours\n\n\n\n\nFeel free to DM me with any comments or questions.",
"date": "2022-10-20"
},
{
"vote": 6,
"title": "The Geoffrey Hinton NLP Fellowship is now accepting applications! (By Univ.AI)",
"text": null,
"date": "2022-10-20"
},
{
"vote": 2,
"title": "[D] GPT-3 is a DREAM for citation-farmers - Threat Model Tuesday #1",
"text": null,
"date": "2022-10-18"
},
{
"vote": 1,
"title": "[D] GPT-3 is a dream for citation-farmers - Threat Model Tuesday #1",
"text": "[deleted]",
"date": "2022-10-18"
},
{
"vote": 13,
"title": "txtai 5.1 released - new translation models, OpenAI Whisper transcription and ARM Docker images",
"text": null,
"date": "2022-10-18"
},
{
"vote": 1,
"title": "Using Marqo to set up a multilingual legal search database!",
"text": "[removed]",
"date": "2022-10-18"
},
{
"vote": 11,
"title": "NLP Analysis: Binary classification of translations of text",
"text": "There's some text that I'd like to run an experiment on, but I only have access to the English translation of the source text (Ancient Greek). Basically, in this source text, there is disputed authorship that I'd like to investigate.\n\n\nI though of fine-tuning a Huggingface binary classifier with a set for each translation composed of: \n\n\n\n\ntexts that are definitely from the author\n\n\ntexts that are definitely not from the author (from the same source)\n\n\n\n\nI had previously trialed this with just one translation, but I thought that the results could be attributed to the translator's word choices. If I add additional translations, will the binary classifier's prediction for disputed texts across available English translations tell us anything meaningful? Could an composite score tell us anything meaningful?",
"date": "2022-10-17"
},
{
"vote": 22,
"title": "Interested in linguistics and programming, so NLP feels like the right path to work towards. How strong in math do I need to be for this field?",
"text": "I love my linguistics and programming classes, but I struggle to get anywhere quickly with math. How much of an issue will this be for me later down the road?\n\n\nEdit: for a little more context, I did most of a linguistics degree in ~2014, took some time off school, and returned this year to study CS. I'm working my way through precalc II right now while I take the introductory CS courses. \n\n\nI'm pretty sure it's Trig that I'm struggling with and not math in general, but I'm nervous about the amount of work it takes me to understand math when CS and Ling are both so much more joyful and easy to work on.",
"date": "2022-10-16"
},
{
"vote": 4,
"title": "ANN library that allows queries on a subset of the whole tree?",
"text": "Any suggestions on an Approximate Nearest Neighbor library that allows me to find the nearest neighbor of a query vector, from among a subset of the entire vector tree?\n\n\nA toy example would be: if my tree consists of vectors v1, v2, v3 and v4\n\n\nand I want to find the ANNs of a query vector q1, but only among v1 & v2. \n\n\nMost of the ANN libraries allow you to find ANN from the entire tree (or at least I can't seem to figure out how to get them to work with a subset of the tree)\n\n\nAnother option would be if there is a quick way to rank these vectors (v1 & v2) by similarity to q1. \n\n\nIn my actual use case I would have to rank about a 1000 vectors, which using the traditional cosine similarity calculation in python is a bit slow.",
"date": "2022-10-16"
},
{
"vote": 5,
"title": "The Path towards Building Multi-Stakeholder Search and Recommendation Systems: Part-I",
"text": "Most recommendation systems today are multi-sided, with multiple stakeholders. Consequently, the systems need to optimize for catering to various stakeholders (ex: consider uber eats, where you have the eaters, delivery partners & restaurant partners - each with a  different set of expectations from the platform.)  - Find out how these systems are designed, optimized and explore the inner workings and learn how some parts of these systems are built in practice.\n\n\nIn a series of long articles -  we want to share our learnings on this topic. Towards that end, here is our first blog on the subject:\n\n\nThe Foundation: A Notes on Recsys, LTR, Ranking Evaluation metrics & Multi-Objective Ranking in practice.\n\n\nIn this First Part, we actually begin by explaining the Problem statement,  setting up background on common patterns of building recommendation systems in the industry today, methods of developing ranking models  (LTR), and popular metrics to evaluate ranking models & then introduce various approaches to multiple objective optimizations applied to recommendation systems, and dive a bit into some examples from Etsy,  Linkedin & Expedia to understand how this is solved in practice.\n\n\nIn the upcoming posts, we will expand on this subject in more detail and also look at sample implementation using the popular H&M  recommendations dataset.\n\n\nCheck this out, and let us know if you find something missing here or would like to be covered or maybe suggest improvements.",
"date": "2022-10-14"
},
{
"vote": 10,
"title": "Topic Modeling with BM25 and Graphs",
"text": null,
"date": "2022-10-13"
},
{
"vote": 7,
"title": "NLP book recommendations for non-technical folk..?",
"text": "Hi all! I've worked on the business side of AI companies for a few years now, and recently moved into the NLP field. I'm looking for a good book to learn a lot of the basic principles etc, but probably won't be delving in to writing any python code etc myself..! Any recommendations for something that's relatively accessible? Thanks!",
"date": "2022-10-13"
},
{
"vote": 8,
"title": "How the Guardian approaches quote extraction with NLP",
"text": null,
"date": "2022-10-13"
},
{
"vote": 3,
"title": "Cross-Lingual Transfer Learning (XNLI)",
"text": "I was reading this paper on XNLI.  \n\n\nAnd I wanted to understand what TRANSLATE-TRAIN entail.  \n\n\nI will write down what I understood.  \n\n\nTRANSLATE-TRAIN: In this, we train N models. N stands for 15 languages. So we train 15 separate models for each language. How do we test this model? Should we run each of these 15 models per language and jot down the average accuracy under each language? For eg: We train 15 language models, then test each of these 15 models on the English test set and then calculate the average accuracy. Does this sound right?  I have been struggling with this baseline for so long. \n\n\nI have been struggling with this baseline for so long. Any help and views are appreciated.\n\n\nPaper",
"date": "2022-10-13"
},
{
"vote": 1,
"title": "Trying to install FastText on Jupyter notebook (I have Windows)",
"text": "[deleted]",
"date": "2022-10-13"
},
{
"vote": 1,
"title": "multilingual abstractive summarisation using XLM-R",
"text": "hi, has anyone did multilingual summarisation task using XLM-R before? I'm not sure if it supports  EncoderDecoder framework, which imo is necessary for summarisation task, if anyone is aware of it please let me know. I would like to fine tune XLM-R with non-english dataset. Any resources are appreciated!",
"date": "2022-10-12"
},
{
"vote": 1,
"title": "Deep learning model for aspect prediction",
"text": "I have about 100 reviews and their corresponding extracted aspects/entities. Is there any machine learning or deep learning models (maybe transformer based) that I can train using these reviews and aspects data? I want to build a model that can predict the aspects/entities of the reviews.",
"date": "2022-10-11"
},
{
"vote": 9,
"title": "A spaCy trick involving Sense2Vec to detect (abbreviated) video games",
"text": null,
"date": "2022-10-11"
},
{
"vote": 2,
"title": "Bert - word embeddings from a text",
"text": "I would like to take the word embeddings of a text and visualize them at the same plot (for understanding reasons). The question is how i should pass the text into the pretrained BERT model? At first, i separated the text on sentences and passed each one separetely, but im not sure if this had the right results.",
"date": "2022-10-11"
},
{
"vote": 1,
"title": "Annotation tools - what are your pain points?",
"text": "Hi all,\nAs part of an educational exercise, I'm trying to define the requirements for an annotation tool for NLP projects.   \n\n\nIf any of you is willing to share, I would love to hear what you are working with currently, and what is missing in existing solutions.   \n\n\nThanks!",
"date": "2022-10-11"
},
{
"vote": 2,
"title": "Please help me with abstracted text summarisation and generation from weighted keywords",
"text": "Suppose I have a list of weighted keywords/phrases, such as \"solar panel\", \"rooftop\", etc. The weights are in [0,1] with higher weights indicating a stronger preference for specific keywords, so \"solar panel\" may have a weighting of 0.3 and \"rooftop\" may have a weighting of 0.2, for example. The sum of keyword weights is 1.\n\n\nFor each keyword/phrase, I additionally have a number of contextual sentences which are also weighted and carry a positive, negative, or neutral sentiment/connotation. For example, one contextual sentence related to the \"solar panel\" phrase might be \"good for the environment\" which is labelled with a positive sentiment and  carries a weight of 0.2. The sum of weights for each keyword's contextual sentences is 1, so the sum of weights for all contextual sentences across all keywords is N, where N is the number of individual keywords.\n\n\nFinally, I also have weighted linkages in [0,1] between keywords/phrases which, again, sum to 1. For example, the directed linkage from \"solar panel\" to \"rooftop\" may have a weight of 0.2 while the directed linkage from \"rooftop\" to \"solar panel\" may have a weight of 0.4.\n\n\nI would like to use these weighted keywords, phrases, contextual sentiment-labelled sentences and linkages to create a summary in natural language. I realise that I'm working in reverse from the typical text summarisation objective, but I believe that the richness of my data should make the task a little easier.\n\n\nHow should I approach it? Should I first use a model to summarise the text contained within each of the contextual sentences before attempting to extract more basic keywords that can be used to generate summary text? How should I process the data? Is it worth pursuing a two-step approach, where a basic model summarises the keywords and contextual sentences in basic language before a secondary model transforms it to richer, more natural language?\n\n\nI would be very grateful for any guidance or recommendations.\n\n\nEdit: I'm very new to NLP, so I apologise for my lack of terminology and mathematical formalism.",
"date": "2022-10-11"
},
{
"vote": 2,
"title": "I am attempting to use the whisper model however it keeps giving me a FileNotFoundError",
"text": "First I imported whisper and did model = whisper.load_model(\"base\") . I get no errors\n\n\nBut when I do:   audio = whisper.load_audio(\"C:/Users/john/Documents/sampleaudio.wav\")  It says FileNotFoundError: [WinError 2] The system cannot find file specified. \n\n\nHowever when I do: print(os.path.isfile(\"C:/Users/john/Documents/sampleaudio.wav\") ) I get back a \"True\". So the file is definitely there, but whisper for some reason is giving me a file not found. \n\n\nI do think that it might have something to do with an ffmpeg installation. In the github repository of whisper it says that installing ffmpeg is a prerequisite for the whisper model. I haven't installed that so maybe that is why it is giving me an error ? However on google collab I didn't install ffpeg but whisper worked fine for me. But the way in which I uploaded the audio was specific to google collab  (from google.colab import files uploaded = files.upload() )\n\n\nAny help would be greatly appreciated!",
"date": "2022-10-10"
},
{
"vote": 5,
"title": "Weak Supervision for Natural Language Processing workshop summary",
"text": "I had the pleasure of running a workshop on Weak Supervision at Crunch Conference in Budapest on 5/10/2022. Let me share some ideas from the workshop here:\n\n\nWith simple and efficient out-of-the-box machine learning APIs finetuning and deploying machine learning models has never been easier.\nFor many companies the larger challenge is understanding the goal posts of machine learning projects and the lack of labelled data.\nWeak supervision can help:  \n\n\n\n\nlabelling data more efficiently   \n\n\nfinetuning your models on noisy labelled data.\n\n\n\n\nThe workshop used \nskweak\n a \nspacy\n based weak supervision library to demonstrate how to use labelling functions to generate noisy labelled data.\nHere's an example \nskweak\n labelling functions:\n\n\nfrom skweak.base import SpanAggregator\n\nclass MoneyDetector(SpanAggregator):\n    def __init__(self):\n        super(MoneyDetector, self).__init__(&quot;money_detector&quot;)\n\n    def find_spans(self, doc):\n        for tok in doc[1:]:\n            if tok.text[0].isdigit() and tok.nbor(-1).is_currency:\n                yield tok.i-1, tok.i+1, &quot;MONEY&quot;\n\nmoney_detector = MoneyDetector()\n\n\n\nThis labelling function extracts any digits that are preceded by a currency.\n\n\nskweak\n allows you to combine multiple labelling functions using \nspacy\n attributes or other methods.\n\n\nUsing labelling functions has a number of advantages:\n\n\n\n\nðŸ’ª larger coverage, a single labelling function can cover many samples\n\n\nðŸ¤“ involving experts, domain expert annotation is expensive, domain expert labelling functions are more economical due to coverage\n\n\nðŸŒ¬ï¸ adopting to changing domains, labelling functions and data assets can be adapted to changing domains\n\n\n\n\nI am in the process of developing a cohort based Natural Language Processing course and running a survey to find the best topic if you'd be happy to take this survey or want to learn more about my course please message me.",
"date": "2022-10-10"
},
{
"vote": 1,
"title": "What is the best method to memorize vocabulary fast?",
"text": "[removed]",
"date": "2022-10-09"
},
{
"vote": 1,
"title": "Generate sentences from list of words?",
"text": "[deleted]",
"date": "2022-10-09"
},
{
"vote": 2,
"title": "OCR software to translate a japanese pdf(images)?",
"text": "I need a free or paid(it has to be really good) OCR that can scan an entire pdf of images--theres no actual text, and translate them for me, and finish it so the text is ontop of the words, like how google translate and other mobile apps work, but I need it done on PC so i can just put in a pdf and it'll batch scan/translate, because there are hundreds of pages and it'll be a gigantic hassle to do each page individually especially when i think it'll be inaccurate. Maybe there's some translation services on fiverr or someplace that you can point me to if you think that's a better option and its not too costly. It's this tutorial book that has no english or any other translations out there anywhere. I'd really appreciate it.",
"date": "2022-10-09"
},
{
"vote": 2,
"title": "Understanding Gradient*Input",
"text": "If you know the answer even for only one of the following, kindly request you to share.\n\n\nI just started to learn \nfeature attribution\n and I read that \nGradient*Input\n is the starting point for many gradient-based attribution techniques. However I have hard time understanding few aspects of it.\n\n\n\n\nIs Gradient*Input something we compute for the whole dataset? Does it give a number for how important each feature is?\n\n\nI asked question (1) because Input is also involved in Gradient*Input, so it kinda looks like something we compute for each and every input in our dataset\n\n\nIf yes for question(2), how to go from this attribution calculated for every input data point to feature attribution of the whole model?\n\n\nI can understand why gradient is a signal for how important a variable is. But why are we multiplying input value also? For instance, high gradient implies that for even negligible increase in the input, the output is going to grow a lot. Why should we let input value affect the gradient by multiplying? Coz the input may actually be 0 essentially killing high gradient.\n\n\nCan we look at IntegratedGradients as generalized version of Gradient*Input?",
"date": "2022-10-08"
},
{
"vote": 2,
"title": "How to register for AACL-IJCNLP 2022",
"text": "The AACL-IJCNLP2022 website has this enigmatic sentence:\n\n\n&gt; At least one author of each accepted paper must register for AACL-IJCNLP 2022 by the early registration deadline.\n\n\nBut there doesn't appear to be any way of registering for the conference. Is there some other site you have to go to for registration that everyone knows about but somehow I missed the memo?",
"date": "2022-10-08"
},
{
"vote": 3,
"title": "Does anyone know any one word translation dictionary?",
"text": "Does anyone know an offline translation tool that doesn't translate sentences to sentences and isn't even using ML at all, but provides a list of translations of one word or one term? Something like Google's translator does when you input a single word. It gives you a list of translations and their frequencies.\n\n\nI am essentially asking for an offline dictionary in form of a database or even just a very tidied e-book...\n\n\nIt could be considered ML if I can choose an arbitrary pair of languages, which is exactly what I need, but even if it is hardcoded for just some pairs of languages it is still very good.",
"date": "2022-10-08"
},
{
"vote": 10,
"title": "How can I process data with mapping between high-level activities and low-level actions?",
"text": "I am not an export on NLP. I heard about knowledge graph and ontology sometimes, but I don't really understand what they mean or if they are relevant here.\n\n\nBasically, what I want to do is to generate some visualization or learning something useful from such text data (numbers of activities and actions are limited):\n\n\n{ 'cooking' : ['open_fridge', 'take_meat', 'heat_stove', 'cut_onion', 'put_mean_into_pan']\n\n\n'reading' : ['walk_to_couch', 'turn_on_light', 'grab_book']\n\n\n'resting' : ['take_off_cloth', 'turn_on_light', 'take_shower', 'lay_on_bed']\n\n\n'cooking' : ['grab_milk', 'cut_apple', 'mix_food', 'heat_stove']\n\n\n......\n\n\n}\n\n\nI am imagining that we can try something like these:\n\n\n(1) Generate some clustering visualization to group low-level actions (i.e \"open_fridge\", \"take_meat\") around high-level activities (i.e. \"cooking\")\n\n\n(2) Learn some graph related pattern for activities: i.e. during cooking, we usually open fridge before taking meat (there exists particular order for doing things). And it is unlikely \"taking shower\" is part of \"cooking\"\n\n\nI am not an expert on that, and I believe there should exist some other or better ways to process/learn from such data. Could someone provide some guidance on this and suggest some relevant papers or software tools for me to study? Thanks in advance.\n\n\nBascally, I hope to obtain something from the above data like these:   If \"open_fridge\" happens, it's 50% chance that the activity is \"cooking\", but if \"take_shower\" happens, it is 2% chance the activity is \"cooking\". If \"cut_onion\" comes after \"open_fridge\", it is 75% chance that the activity is really cooking. So I want to learn such knowledges and hopefully visualize this in a nice way.",
"date": "2022-10-08"
},
{
"vote": 1,
"title": "The science behind the new â€œAlexa, what should I watch?â€ feature",
"text": null,
"date": "2022-10-07"
},
{
"vote": 8,
"title": "Newbie question about chatbot modern techniques",
"text": "Hi!\nI've recently got collected over 3000 most ridicilous phrases of my friend. I've created a telegram chat bot with him, but currently he selects something random and random amount of responses, which have own charm but obviously not enough. I'm not NLP researched or ML engineer, so Im not very familiar with the techniques which can I use to give some more intelligence to the bot.\nBasically I know about GPT-3 but it feels like something not what I need and too advanced.\nAny ideas/proposals? I've used python for the chatbot (thinking to move to C++ though), so libraries or ready solution are very welcom",
"date": "2022-10-07"
},
{
"vote": 1,
"title": "ðŸ’¢ Create a Face Mask Detector in 5 min with OpenCV | Keras | TensorFlow - Python and Deep Learning",
"text": null,
"date": "2022-10-07"
},
{
"vote": 4,
"title": "7 Best Natural Language Processing Courses (2022)",
"text": null,
"date": "2022-10-07"
},
{
"vote": 1,
"title": "Animated explanation of machine learning concepts ðŸ‘‰",
"text": null,
"date": "2022-10-06"
},
{
"vote": 4,
"title": "How to detect Outliers in a set of Sentence Embeddings?",
"text": "I have N sentences, hopefully most of them have the same semantic meaning. \n\n\nI've encoded each sentence and now I have N embedding vectors. Probably most of the vectors are close to each other. \n\n\nI've calculated pairwise cosine distance and now I have NxN distance matrix. \n\n\nBased on this matrix, how do I find the outliers, i.e. sentences with very different meaning?\n\n\n\n\nN is small, let's say 10<N<100",
"date": "2022-10-06"
},
{
"vote": 1,
"title": "Semantic search of Stack Overflow with codequestion",
"text": null,
"date": "2022-10-06"
},
{
"vote": 44,
"title": "spaCy just got a new experimental corefernce tool",
"text": null,
"date": "2022-10-06"
},
{
"vote": 4,
"title": "Any updated information on when Speech and Language Processing 3rd Edition will be published?",
"text": "I know this question was asked some time back but at the time the speculation was for a release around 2019 - since then the \nbook website\n has been updated with a draft as of January 2022 which includes all but two chapters ('Introduction', and 'Computational Semantics and Semantic Parsing') but the same site has the words \n\n\n> When will the whole book be finished?  Don't ask.   \n\n\nDoes anyone have any inside information?",
"date": "2022-10-06"
},
{
"vote": 1,
"title": "ðŸ’¢ How to use OpenImages to Create Datasets for Yolo",
"text": null,
"date": "2022-10-06"
},
{
"vote": 1,
"title": "I am working on a speech2text project. I am working with WAV audio files for the first time. I am trying to understand how they are encoded.",
"text": "When I printed an audio file's shape (WAV audio file) it returned an array: (399821,) . I am trying to understand what this means. Does this have something to do with frequency?",
"date": "2022-10-05"
},
{
"vote": 0,
"title": "Recommend me some corpus building tutorials for topic modeling",
"text": null,
"date": "2022-10-04"
},
{
"vote": 0,
"title": "Bias Variance trade-off explained ðŸ‘‡",
"text": null,
"date": "2022-10-04"
},
{
"vote": 27,
"title": "Researchers at Meta AI Introduce EditEval: An Instruction-Based Benchmark for Text Improvements",
"text": "For various applications, including question answering, textual entailment, and summarization, large pre-trained language models have demonstrated excellent text production capabilities. However, most work using language models has concentrated on producing immutable text in a single pass. Contrasting this is the way humans naturally create texts, which is an iterative process of little phases, each serving a specific function. Often, a change is necessary only after a substantial portion of the text has been written, such as when reorganizing or eliminating contradictions or inconsistencies.\n\n\nThe existing paradigm of producing text passages in a single pass can be very constricting in this regard. Furthermore, todayâ€™s continuous left-to-right generating paradigm is less controlled and inflexible to human collaboration and input. Further, a lack of skilled human mediation in the writing process can negatively impact the final productâ€™s quality. Production tools such as Smart Compose from Google 3 and text predictions from Microsoft 4 can collaborate with humans to compose articles and emails. However, these primarily focused on sentence completion and were not created to improve the previously written text.Â \n\n\nContinue reading\n | \nCheck out the\n \npaper\n,\n \nproject\n \nand\n \ngithub",
"date": "2022-10-04"
},
{
"vote": 9,
"title": "Encoder-decoder BART model",
"text": "Over the last few weeks I tried to implement a encoder-decoder BART model in golang using the library spago v.0.7.0 . The training of the model ultimately failed as too much memory was used. The machine being used had 32gbs of RAM. Although due to lack of documentation in spago Iâ€™m not too sure how I wouldâ€™ve utilised the outputted models. \n\n\nThe question I have: is there an easy way using python to train an encoder-decoder model for a custom language? What I mean by custom is that I donâ€™t want to use a preset for English to French for example. How easy would it be to covert a custom model into a hugging face one ? \n\n\nDoes anyone know how to utilise spago? Or how to make the training much more memory efficient? \n\n\nSorry about all the questions, thanks for your time",
"date": "2022-10-04"
},
{
"vote": 3,
"title": "Defined Term/Alias finder",
"text": "Does anyone know of a package/pre trained model that can find defined terms (â€œLossesâ€ means any loss financial or otherwise suffered in the calandre year.) and named aliases (such and such llc (the client).\n\n\nIâ€™ve had decent enough luck using regex to match the defined terms but the aliases have proved quite tricky.",
"date": "2022-10-04"
},
{
"vote": 2,
"title": "I am looking at open source multilingual ,open source, speech 2 text models. How do I do speech detection for audio that switches between english and spanish?",
"text": "I am mainly looking at OpenAI's whisper model and somewhat wave2vec 2.0 (facebook). The audio files that I am using are conversations that go back and forth between Spanish and English. Are there any general ideas for tweaking or modifying these the code of the models so they can transcribe audio that switches between english and spanish?",
"date": "2022-10-03"
},
{
"vote": 5,
"title": "OCR for small font low resolution text",
"text": "I'm trying to do OCR on images that are low resolution and contain text in a small font. I tried tesseract and had unusable results. I tried preprocessing the input image by increasing contrast and adjusting brightness. The results didn't noticeably improve.\n\n\nThe text is just at the limit of human reading, but I can make out the words so I believe it should be possible to OCR this.\n\n\nDoes anyone have any suggestions? Maybe there is something better than tesseract for this?\n\n\nIf I'm asking in the wrong place please let me know.",
"date": "2022-10-03"
},
{
"vote": 6,
"title": "NLP use cases in Finance",
"text": "[removed]",
"date": "2022-10-03"
},
{
"vote": 14,
"title": "Current research / areas of interest in computational morphology?",
"text": "Iâ€™ve been doing most of my research in the area of creolistics, however their morphology and morphology is general is the most interesting aspects to me. I recently read a paper on computationally modeling reduplication which interested me. What else is going on in that field right now, I have no idea where to begin.",
"date": "2022-10-02"
},
{
"vote": 28,
"title": "New Python text similarity package",
"text": "NLP Python nerds, I just put out a language agnostic, CPU-friendly text similarity package. \n(repo here)\n\n\npip install simphile\n\n\nChoose your favorite method:\n\n\nfrom simphile import jaccard_similarity, euclidian_similarity, compression_similarity\n\ntext_a = &quot;I love dogs&quot;\ntext_b = &quot;I love cats&quot;\n\nprint(f&quot;Jaccard Similarity: {jaccard_similarity(text_a, text_b)}&quot;)\nprint(f&quot;Euclidian Similarity: {euclidian_similarity(text_a, text_b)}&quot;)\nprint(f&quot;Compression Similarity: {compression_similarity(text_a, text_b)}&quot;)\n\n\n\nYou all are probably deeply familiar with each of these techniques (perhaps with the exception of Compression Similarity):\n\n\n\n\nCompression Similairty\n â€“ leverages the pattern recognition of compression algorithms\n\n\nEuclidian Similarity\n â€“ Treats text like points in multi-dimensional space and calculates their closeness\n\n\nJaccard Similairy\n â€“ Texts are more similar the more their words overlap\n\n\n\n\nWould love to hear what you think!\n\n\nMore docs, examples, unit tests, etc at the repo",
"date": "2022-09-30"
},
{
"vote": 4,
"title": "Installing FastText for Python, Windows 10",
"text": "Hey!  I am completely new to this field, I only have some basic skills in  Python, but I am attempting to use FastText to implement analogies (of the kind of \"king - man + woman = queen\"). I am having problems with installing FastText; in particular, I am trying to do it in Visual  Studio Code. After running\npip install numpy\npip install scipy\npip install pybind11\npip install wheel\nI ran \npip install fasttext\n, and I got this message:\n\n\nCollecting  fasttextNote: you may need to restart the kernel to use updated  packages.    Using cached fasttext-0.9.2.tar.gz (68 kB)   Preparing  metadata (setup.py): started   Preparing metadata (setup.py): finished  with status 'done' Requirement already satisfied: pybind11>=2.2 in  c:\\users\\sdt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages  (from fasttext) (2.10.0) Requirement already satisfied:  setuptools>=0.7.0 in  c:\\users\\sdt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages  (from fasttext) (63.2.0) Requirement already satisfied: numpy in  c:\\users\\sdt\\appdata\\local\\programs\\python\\python310\\lib\\site-packages  (from fasttext) (1.23.3) Building wheels for collected packages:  fasttext   Building wheel for fasttext (setup.py): started   Building  wheel for fasttext (setup.py): finished with status 'error'   Running  setup.py clean for fasttext Failed to build fasttext Installing  collected packages: fasttext   Running setup.py install for fasttext:  started   Running setup.py install for fasttext: finished with status  'error'\n\n\nOutput  exceeds the size limit. Open the full output data in a text editor   error: subprocess-exited-with-error    Ã— python setup.py bdist_wheel did  not run successfully.   â”‚ exit code: 1   â•°â”€> [20 lines of output]         c:\\Users\\sdt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\dist.py:771:  UserWarning: Usage of dash-separated 'description-file' will not be  supported in future versions. Please use the underscore name  'description_file' instead         warnings.warn(       running  bdist_wheel       running build       running build_py       creating  build       creating build\\lib.win-amd64-cpython-310       creating  build\\lib.win-amd64-cpython-310\\fasttext       copying  python\\fasttext_module\\fasttext\\FastText.py ->  build\\lib.win-amd64-cpython-310\\fasttext       copying  python\\fasttext_module\\fasttext\\__init__.py ->  build\\lib.win-amd64-cpython-310\\fasttext       creating  build\\lib.win-amd64-cpython-310\\fasttext\\util       copying  python\\fasttext_module\\fasttext\\util\\util.py ->  build\\lib.win-amd64-cpython-310\\fasttext\\util       copying  python\\fasttext_module\\fasttext\\util\\__init__.py ->  build\\lib.win-amd64-cpython-310\\fasttext\\util       creating  build\\lib.win-amd64-cpython-310\\fasttext\\tests       copying  python\\fasttext_module\\fasttext\\tests\\test_configurations.py ->  build\\lib.win-amd64-cpython-310\\fasttext\\tests       copying  python\\fasttext_module\\fasttext\\tests\\test_script.py ->  build\\lib.win-amd64-cpython-310\\fasttext\\tests       copying  python\\fasttext_module\\fasttext\\tests\\__init__.py ->  build\\lib.win-amd64-cpython-310\\fasttext\\tests       running build_ext        building 'fasttext_pybind' extension       error: Microsoft Visual  C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build  Tools\":\n \nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/\n...â•°â”€>  fasttext  note: This is an issue with the package mentioned above, not  pip. hint: See above for output from the failure.\n\n\nCan usinganyone help me understand what to do, and what exactly is the problem?  Do you have any tips in general on installing FastText, would you recommend to use anything different from Visual Studio Code? Thank you!",
"date": "2022-09-30"
},
{
"vote": 0,
"title": "From english native, is there have english grammar mistake?",
"text": "\"i just have pico4 experience, that so good against pico3,mostly in pixel update, but the vst function dizzy in my way.\"",
"date": "2022-09-30"
},
{
"vote": 16,
"title": "Deepmind Introduces â€˜Sparrow,â€™ An Artificial Intelligence-Powered Chatbot Developed To Build Safer Machine Learning Systems",
"text": "Technological advancements strive to develop AI models that communicate more efficiently, accurately, and safely. Large language models (LLMs) have achieved outstanding success in recent years on various tasks, including question answering, summarizing, and discussion. Given that it allows for flexible and dynamic communication, dialogue is a task that particularly fascinates researchers. However, dialogue agents powered by LLMs frequently present false or made-up material, discriminatory language, or promote risky behavior. Researchers may be able to develop dialogue agents that are safer by learning from user comments. New techniques for training dialogue agents that show promise for a safer system can be investigated using reinforcement learning based on feedback from research participants.\n\n\nIn their most recent publication, researchers from DeepMind introduce Sparrow, a practical dialogue agent that lowers the likelihood of dangerous and improper responses. The purpose of Sparrow is to teach dialogue agents how to be more beneficial, accurate, and safe. When itâ€™s necessary to look for information to support its arguments, this agent can converse with the user, respond to questions, and conduct Google searches to help evidence. Sparrow increases our understanding of how to educate agents to be safer and more productive, ultimately contributing to developing safer and more useful artificial general intelligence (AGI).\n\n\nContinue reading\n| \nPaper\n| \nReference Article",
"date": "2022-09-29"
},
{
"vote": 4,
"title": "Abstractivr Summarization - What's the best model/platform?",
"text": "Hi folks,\n\n\nI am looking to do an abstractive summarization for Financial News Articles.\n\n\nHave been through the sub searching for previous posts on the subject and saw BART and Pegasus (both on HUGGINGFACE) getting recommended..\n\n\nI also have paid access to OPEN AI for abstractive summarization and it uses GPT-3 based models (text-davinci-002).\n\n\nWanted to check the sub's opinion on \n\n\n\n\nwhat should be the best among the 3 models above\n\n\nif there's any better way to go about abstractive summarization\n\n\nIs abstractive summarization good enough to give out some very good summaries, or should I stick to extractive summarization?\n\n\n\n\nThanks",
"date": "2022-09-28"
},
{
"vote": 2,
"title": "[Repost] Text Mining Project on eating disorders &amp; social networks: help us!",
"text": "[removed]",
"date": "2022-09-28"
},
{
"vote": 2,
"title": "Word2Vec (CBOW and Skip-Gram)",
"text": "To my knowledge there are 2 different ways of implementing word2vec, one is CBOW and the other is skip-gram. I understand their respective architectures and the intuition behind the model to a good extent. However I have the following 2 burning questions\n\n\n\n\nConsider \nCBOW\n with \n4 context words\n, why the input layer has \n4 full-vocabulary length one-hot vectors\n to represent these 4 words and take average of them? Why can't it be just \n1 vocabulary length vector with 4 ones\n (in otherwords \n4-hot vector\n)?\n\n\n\n\nCBOW\n takes inputs as context words and predict a single target word which is a \nmulticlass single label problem\n and it makes sense to use \nsoftmax\n in the output. But why do they use \nsoftmax\n in the output for a \nskip-gram\n model which is technically a \nmulticlass multilabel problem\n? \nSigmoid\n sounds like a better deal since it has the potential to make \nmany neurons approach 1 independent of other neurons",
"date": "2022-09-28"
},
{
"vote": 5,
"title": "Fine-tune stable diffusion on corpora",
"text": "Hi everyone. I hope this fits in this sub. \n\n\nIâ€™m curious as to whether there is any way to fine-tune a stable diffusion (text2img) model on, say, a larger (1000+) corpus of (img, text) pairs.\n\n\nI have seen posts on \ntextual inversion\n which enable one to fine-tune the underlying embeddings. However, this method seems to work well with 3 - 5 new examples and doesnâ€™t scale well. \n\n\nSo, is there a way to efficiently fine-tune on more data? Compute cost is not an issue. \n\n\nThanks.",
"date": "2022-09-28"
},
{
"vote": 5,
"title": "Storing word / document vectors in RDBMS",
"text": "I'm curious to know if anyone has experience storing word or document embeddings in SQL DBs? \n\n\nWhenever I've worked with trained word / document embeddings, I've stored the models as blobs on disk.  Just curious about practice and experience elsewhere.\n\n\nI've recently stumbled upon smaller projects, like FREDDY (\nhttps://github.com/guenthermi/postgres-word2vec\n), a Postgres extension that looks interesting. The ability to write ad-hoc similarity queries in SQL seems like it might be valuable in some circumstances. I'm not sure about performance or storage efficacy.\n\n\nI can't really imagine much of a transactional use case; but perhaps there are analytical stores where this makes some sense?",
"date": "2022-09-27"
},
{
"vote": 1,
"title": "Speech-to-Speech: Use your own voice to control an AI voice with Resemble AI",
"text": "Just released a new way to create synthetic media using AI Voices. Speech-to-Speech by Resemble AI will allow you to control your AI voice with any audio file/mic input you provide it with. Here's a quick video showing how it works:\n\n\nhttps://youtu.be/cXtgdsWw1xI\n\n\nhttps://www.resemble.ai/speech-to-speech/",
"date": "2022-09-27"
},
{
"vote": 10,
"title": "txtai 5.0 released - build semantic search applications and workflows",
"text": null,
"date": "2022-09-27"
},
{
"vote": 2,
"title": "Question regarding merging a model with long term memory from previous messages",
"text": "Could someone please point me where I could learn how to implement a model with long term memory and the ability to not go off topic? I was thinking of making a database that stores every input and do a round check targeting specific words before the output. This might seem to generic but that's the only solution I could come up\n\n\n&#x200B;\n\n\nExample of what I mean by this is:\n\n\nuser: \nMy favourite color is red, what is yours?\n >!<--- Now this is stored in a database!<\n\n\nmodel: \nRed is my favourite color too\n\n\n* after chatting for a while the user will test the model memory *\n\n\nuser: \nwhat is my favourite color?\n\n\nmodel: \nyour favourite color is red\n >!<-- This is the desired output, or at least one of it's forms!<\n\n\n&#x200B;\n\n\nOf course it is not limited to the user input but as well as what facts the model might say.",
"date": "2022-09-27"
},
{
"vote": 4,
"title": "Language Modeling for Sequence Labeling of Long Text with Specialized Corpus (X-Post r/MachineLearning)",
"text": null,
"date": "2022-09-26"
},
{
"vote": 0,
"title": "This sub is interesting and I see lot of spark in lot of people.",
"text": "[deleted]",
"date": "2022-09-26"
},
{
"vote": 7,
"title": "Best Natural Language Processing Books for Beginners to read in 2022 -",
"text": null,
"date": "2022-09-26"
},
{
"vote": 9,
"title": "Mistakes you made",
"text": "Hello, I'm going to start learning NLP. Before starting, I wanted to know about the mistakes that you people made along the way that slowed down or halted your progress. I wish to avoid them in my learning plan.\nYour experience is very valuable",
"date": "2022-09-26"
},
{
"vote": 1,
"title": "I am trying to find a speech to text model. I am confused between the difference between using API and using an open source model to do this.",
"text": "I am still confused as to what API means, but I have heard it used in several different contexts. At a meeting someone said it might be better to use an API as opposed to an open source model. I have also heard open source models called APIs. I am confused.",
"date": "2022-09-25"
},
{
"vote": 0,
"title": "The Best Machine Learning Courses on Udemy (2022)",
"text": null,
"date": "2022-09-25"
},
{
"vote": 2,
"title": "Please help me understanding Perplexity and Coherence in LDA models",
"text": "Hello! Self taught programmer here!\n\n\nI'm using a unsupervised LDA topic finder algorithm that identifies N_topics in a corpus of 246 PDF documents averaging at 10-12 pages each.\n\n\nI used GENSIM in Python with the following parameters:\n\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                id2word=id2word,\n                                num_topics=num_topics,\n                                random_state=100,\n                                update_every=1,\n                                chunksize=100,\n                                passes=10,\n                                minimum_probability=0.01,\n                                alpha=&quot;symmetric&quot;)\n\n\n\nComing to the evaluation of the model, my (limited) understanding is that the human check of the results is the best option. However, I came across the two metrics of Perplexity and Coherence.I tried a few tests changing N_topics and I get the following results:\n\n\n\n\n\n\n\n\nN_topics\n\n\nPerplexity\n\n\nCoherence\n\n\n\n\n\n\n\n\n2\n\n\n-7.36538968171728\n\n\n-0.9669535933638094\n\n\n\n\n\n\n3\n\n\n-7.336756853859304\n\n\n-1.0998795804362185\n\n\n\n\n\n\n4\n\n\n-7.322581092883721\n\n\n-1.3384321812486712\n\n\n\n\n\n\n5\n\n\n-7.342551524382316\n\n\n-1.443651719648194\n\n\n\n\n\n\n6\n\n\n-7.363654156222966\n\n\n-1.4542193508988934\n\n\n\n\n\n\n20\n\n\n-9.715173353737418\n\n\n-7.081862849861636\n\n\n\n\n\n\n50\n\n\n-13.43033594196777\n\n\n-7.266167864167421\n\n\n\n\n\n\n100\n\n\n-19.712061640831593\n\n\n-7.120279967346224\n\n\n\n\n\n\nOn the one hand, I think that N_topics = 4 gives me good results because the 4 topics reflect my expectations. However, as a rule of thumb, the lower the Perplexity, the better. \n\n\nMy questions are: \n\n\n\n\nLet alone the mathematics, what are Perplexity and Coherence in a nutshell? \n\n\nAre these scores in line with the average results of similar models (i.e., does my model \"work well\"?) \n\n\nHow should I select my N_topics considering that N > 6 would be tough to justify when looking at the topics?\n\n\n\n\n Thank you in advance!",
"date": "2022-09-25"
},
{
"vote": 4,
"title": "Question maybe you can help me with",
"text": "Hi. I'm wondering if anyone can point me in a direction or recommend any research that concerns clustering words around an established topic or word or whatever you want to call it. The idea is basically a kind of inverted topic modeling where you begin with a particular topic and then use ML to cluster those words that co-occur around that topic with more or less frequency. Thank you!",
"date": "2022-09-25"
},
{
"vote": 3,
"title": "Complete Video Tutorial for OpenAI's Whisper Model for Windows Users - Uses Python - How to do speech-to-text transcription (speech recognition - NLP) for free with better quality than Google's Premium API",
"text": null,
"date": "2022-09-25"
},
{
"vote": 9,
"title": "I'm looking to improve the quality of my pronunciation and clear enunciation, and I wondered if there are speech recognition scores to tune the quality of my spoken English or German.",
"text": null,
"date": "2022-09-25"
},
{
"vote": 7,
"title": "OpenAI Whisper: SOTA Speech To Text With Microphone Demo",
"text": "OpenAI has released a Speech To Text model that nears human performance.  This video goes over the basics of the model, as well as how to run it with a microphone.\n\n\nhttps://youtu.be/nwPaRSlDSaY",
"date": "2022-09-23"
},
{
"vote": 4,
"title": "How can I get word-level timestamps in OpenAI's Whisper ASR?",
"text": "I use OpenAI's \nWhisper\n python lib for speech recognition. How can I get word-level timestamps?",
"date": "2022-09-23"
},
{
"vote": 3,
"title": "Use WordNet to collect homonyms",
"text": "Does anyone know whether it is possible to retrieve homonyms of a given word using WordNet (specifically its \nNLTK\n \"interface\")?\n\n\nFor example, let's say I'm interested in the word \ncoach\n, which can either refer to a \nbus\n or a \ntrainer\n. Is there a way, using WordNet, to get a list of homonyms with their respective senses? Something along these lines:\n\n\ncoach: bus, van...\ncoach: trainer, mentor...",
"date": "2022-09-23"
},
{
"vote": 1,
"title": "How to fine-tune GP",
"text": "[removed]",
"date": "2022-09-23"
},
{
"vote": 10,
"title": "Build audio transcription vector search engine with OpenAI Whisper",
"text": null,
"date": "2022-09-23"
},
{
"vote": 17,
"title": "Stuck in the industry experience paradox",
"text": "Hey there!\n\n\nI'm about to graduate with my master's degree in NLP and I am starting to freak out, as I  thought landing an entry-level job in the field would be way easier. I've managed to get a couple job interviews but it's all ending too similarly: \"You're great, but we need someone with more experience...\". How can I be the candidate with more experience, when no one will give me the chance to create that experience in the first place?\n\n\nThe last position I applied for LITERALLY described tasks I am passionate for AND good at. However, after the interview, they claimed I didn't \"have enough experience\" and went on to describe things that weren't even mentioned in the job description...\n\n\nMy question is how can I prove to them that even if I don't know something right now, I can just... (suspense crescendo) LEARN IT? (applause) I know this sounds cliche, but I am indeed a fast learner and very dedicated to new projects. I just won't rest until I understand things and make them work. In the last project I was part of I quickly taught myself SQL and Apache Maven in order to make it work.\n\n\nI am working on personal projects, of course, trying to build a better portfolio, but things take time... I thought having a degree in the field and showing that I do have some knowledge would land me at least an internship...\n\n\nIs there anything besides Python, R, Java, Pandas, Seaborn, Spacy, NLTK, TensorFlow, PyTorch, Hugging Face, SQL and Django that you think makes someone's resume particularly attractive? Even though I didn't plan to be a software engineer, I am now working on learning React and Express.js just so I can get hired. Would you say that's the right call? I'm starting to feel like I really don't have what it takes and should probably just give up.\n\n\nThanks in advance!",
"date": "2022-09-23"
},
{
"vote": 1,
"title": "In this link: (https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md) Facebook has made many speech to text models open source and you can load their pytorch models. But how do I see the way in which each model is coded?",
"text": "I was able to upload a pytorch speech to text model of theirs, but how do I see the programing and actual code underlying the model, so I can train it on my data by manipulating the parameters and what not?",
"date": "2022-09-23"
},
{
"vote": 15,
"title": "[P] My co-founder and I quit our engineering jobs at AWS to build â€œTensor Searchâ€. Here is why.",
"text": null,
"date": "2022-09-22"
},
{
"vote": 8,
"title": "Amazon launches $500,000 Alexa Prize TaskBot Challenge 2",
"text": null,
"date": "2022-09-22"
},
{
"vote": 3,
"title": "How do I record and transcribe every word that comes out of the mouth of Nick Fuentes?",
"text": "That may be a click-bait headline. Sorry if so. But it is a serious question.\n\n\nI just started a graduate program that has a capstone project at the end of it. Mine involves measuring political behavior of viewers of (pick any online content creator you want who is 90% or more political and/or news related). I used Fuentes in the headline, as that's a content creator I'd be interested in studying. His words that is, as they relate to moving some one or some group to do something, anything. Or, just studying the evolution of rhetoric over time.\n\n\nThe problem with this part is, nobody is transcribing and storing any online content like his. We have every word Tucker Carlson has ever spoken on-air, and they are relatively easy to get to. LexisNexis is a for-profit service provider for these types of transcripts.\n\n\nMy question is, any suggestion in getting a computer or server setup that will listen in real-time to some online streaming show, and transcribe and store the words?\n\n\nI've tested this with the most basic of things, and it worked. I just fired off a Fuentes video on my iPhone, set the phone beside my laptop that has Otter on it, and let it run for an hour or so, and it did the trick. But that's not a practical solution.\n\n\nAny suggestions?\n\n\nThe biggest goal for this hair brained idea of mine is to setup an open source network of servers (maybe academic managed and grant funded) that transcribes every (mostly) political news/info content creator's show, and makes those data freely available to the public as a means of researching whatever anyone wants to ask.\n\n\nAnd if there's a better sub to post this question in, please tell me and I'll move it there.\n\n\nThanks!",
"date": "2022-09-22"
},
{
"vote": 8,
"title": "Question about Text classifying/Zero shot classification",
"text": "I'm trying to classify titles of articles into topics from an existing list of 10 topics. I'm using Facebook/bart-large-mnli. \n\n\nHow can I tell if the labels I'm using are good enough to get accurate results? For example, one of the topics is \"Sport\" and I'm getting different results for \"Sport\" and \"Sports\". Also, one of my topics is \"Media & Celebs\" which gives totally different results from using just \"Media\" or just \"Celebrities\"\n\n\nCan anyone shed some light about how this works and if there's a way to figure out how to fine tune the topics I'm using to get the most accurate results?\n\n\nThanks",
"date": "2022-09-22"
},
{
"vote": 16,
"title": "OpenAI's Whisper released",
"text": "OpenAI just released it's new language model Whisper (open-source)\n\n\n \nopenai/whisper (github.com)",
"date": "2022-09-21"
},
{
"vote": 9,
"title": "Starting project request.",
"text": "I've been reading/learning NLP for the past year. I went through the Harvard course CS50 and CS50AI and further read a couple of books. But I'm stuck in the \"reading\" cycle of just expanding on topics. I want to start with doing some projects. I did the projects in the CS50 course but mos of the setup is done for you and just solve some specific problems. I want to know if there are some guided/semi-guided projects in for NLP that I can do? It seems to be very intimidating to do a project all on my own or I just don't know where to start.",
"date": "2022-09-21"
},
{
"vote": 4,
"title": "If you are curious about building fast NLP prototypes, come join me for co:lab friday meetup this Friday at 12:00 pm ET ðŸ¤—",
"text": null,
"date": "2022-09-21"
},
{
"vote": 13,
"title": "(Volunteer) research assistant positions in NLP?",
"text": "I am looking for research assistantship positions in the NLP space, since I'm interested in the field and would like to get more exposure. I've been in the industry for ~3 years as a SWE in FAANG, leading some projects in the infra space (C++/Python). However, I do have some experience in applied ML from prior internships and did my undergrad in pure math and EECS. I add this background primarily because I believe not being in university may add friction in the process.\n\n\nWhat I'm looking for is:\n\n\n\n\nMeaningfully contributing to fundamental NLP problems that will benefit the wider NLP community (either via publications at well-received conferences or large open-source contributions)\n\n\nSpending 20 hours per week (ideally remote, and even unpaid is fine)\n\n\nWorking with experts in the field (academics with specialization in NLP, professors in the ML/NLP space, etc.)\n\n\n\n\nAny pointers on how I could go about pursuing such opportunities? Thanks in advance!",
"date": "2022-09-21"
},
{
"vote": 17,
"title": "Character.ai opened their beta for their chatbots (founded by Noam Shazeer who invented ML Transformers)",
"text": "ðŸ‘‡ Everyone is sleeping on this startup ðŸ‘‡\n\n\n@character_ai\n has < 1,000 followers and itâ€™s founded by the OG: Noam Shazeer who helped invent ML Transformers and design Googleâ€™s LaMDA system. They are providing the best chatbots trained open to the public.\n\n\nhttp://www.character.ai\n is a platform to â€œcreate and talk to advanced AIâ€.\n\n\nI used it to:\n\n\n\n\nChat with chatbots\n\n\nCreate my own chatbot\n\n\nCreate a room with chatbots talking to each other\n\n\n\n\nI wrote a thread about how I used Character and some predictions for the future of the company.\n\n\nRead it here: \nhttps://twitter.com/davidtsong/status/1572282224741085184",
"date": "2022-09-21"
},
{
"vote": 12,
"title": "Original applications of BERT...",
"text": "Do you know exotic uses of big language models like BERT (or similar)?\n\n\nFor example I found \nMusicBert\n which is very cool! He relies on midi notes and can predict a masked note...\n\n\nDo you know others examples? T",
"date": "2022-09-20"
},
{
"vote": 5,
"title": "Summarization of one sentence.",
"text": "Hello, I am not familiar with NLP, and when I was reading about about Bert for summarization, it seemed that it summarizes long text, are there ways to summarize one sentence and get like 3 words? or are there techniques other than summarization that can do better? thanks.",
"date": "2022-09-20"
},
{
"vote": 12,
"title": "Spelling correction with transformer models",
"text": "What is the current best practice for using transformer models for spelling correction? I suppose one would frame it as a translation task? Any introduction or tutorial that you can recommend?",
"date": "2022-09-19"
},
{
"vote": 19,
"title": "Interpretation of topic modeling results between LDA and BERTopic",
"text": "Iâ€™ve been utilizing BERTopic to output some results of my own corpus, but I am unsure whether the results (the output of the Python code) can be interpreted the same way as those produced by the traditional method which is LDA.\n\n\nI know that the two methods have different mathematical/computational mechanisms in terms of operation, but their outputs pretty much seem to be the same. So I was wondering if it is right to interpret the results of BERTopic the same way as I would interpret the results of LDA implementations.\n\n\nThe reason I am asking this question is that I havenâ€™t been able to find papers or tutorials specifically explaining about BERTopic outputs and how the interpretation of its outputs is similar/different to LDA. Also, papers that utilize BERTopic for research are very scarce, making it difficult to understand the interpretation methodology (and yes of course, I read the BERTopic paper.). There aren't any educational resources on BERTopic I found so far either.\n\n\nCan someone provide some help on this matter? If there is a link I can refer to or study regarding my question (such as educational resources on BERTopic vs LDA in terms of their **output interpretation**), as well as specific humanities/social science research examples that use BERTopic and compare it to LDA, that would be greatly appreciated. Of course, if you provide a direct answer to the question, that is also welcome. \n\n\nFYI, I am working in \ncomputational social science\n and trying to use BERTopic for my research, and I wish to know how to understand the output of the BERTopic model that is shown in Python (if this helps).",
"date": "2022-09-18"
},
{
"vote": 3,
"title": "Deep Dive into how Predicting Future Weights of Neural Network is used to mitigate Data Staleness while Distributed Training",
"text": null,
"date": "2022-09-18"
},
{
"vote": 6,
"title": "BERTopic for longer texts?",
"text": "I've been having great success using BERTopic to model topics from a short-document corpus. As promised, the topics are indeed way more interpretable than those extracted by other topic modeling algorithms, and the interface is easy and just works.\n\n\nHowever, the same cannot be said of a corpus containing long documents (~30.000 characters). Given how fast the whole training phase is, I suspect they're actually being truncated (maybe this is obvious, given the well-known limitation of transformers).\n\n\nSo I'm trying to come up with a strategy to deal with these longer texts, and any advice and/or pointers would be much appreciated. Maybe dividing the documents into 512-ish token chunks, and then averaging probabilities for each topic would work...?\n\n\nThank you!",
"date": "2022-09-18"
},
{
"vote": 3,
"title": "how does Luceneâ€™s implementation of HNSW KNN defer from others?",
"text": null,
"date": "2022-09-17"
},
{
"vote": 10,
"title": "Any easy tool to cherry pick rare words from text for vocabulary building?",
"text": "Seems the most straightforward thing to do would be to list every single word from the text according to its overall English rank, perhaps with some cutoff point to keep the list length down. Somehow, my google fu is failing me, but surely such a simple tool already exists somewhere. This would spare me from having to write down every word I don't recognize as I read a text--I could just take a glance at the list and see all of those words at a glance.",
"date": "2022-09-17"
},
{
"vote": 5,
"title": "Teaching Kids about Language Models",
"text": "Hey everyone! Iâ€™m giving a talk to middle schoolers in a few weeks and Iâ€™m planning on discussing foundation models (e.g. GPT-3, BERT, etc). What are some fun examples/use cases to showcase these models? \n\n\nIâ€™ll probably at least do \nhttps://aidungeon.io\n but looking for others!\n\n\nIâ€™m also looking for simple experiments that we know cause things like hallucinations and other issues. Thanks!",
"date": "2022-09-17"
},
{
"vote": 0,
"title": "Create Word Embeddings from my own Corpus",
"text": "How to Train my own corpus with ElMo, GPT3 embedding techniques . Please help me with the solution code. Please I tried so hard there's no tutorial. Please help urgent for project.",
"date": "2022-09-17"
},
{
"vote": 6,
"title": "Hey people i wanted recommendations of courses in udemy for nlp as i want to work on language recognition and translation also for building an end to end humanoid kind of system so any help is very helpful for me.",
"text": null,
"date": "2022-09-16"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Use Yolov5 with Free GPU on Google Colab",
"text": null,
"date": "2022-09-16"
},
{
"vote": 1,
"title": "What do the `&lt;extra_id&gt;` tokens in T5 generation mean?",
"text": "[deleted]",
"date": "2022-09-16"
},
{
"vote": 11,
"title": "How to train a Spacy model for multi label classification",
"text": null,
"date": "2022-09-15"
},
{
"vote": 2,
"title": "Running gpt-neox-20b on colab",
"text": "Has anyone managed to run EleutherAI's gpt-neox-20b on colab? If so: Is colab pro+ required?",
"date": "2022-09-15"
},
{
"vote": 3,
"title": "How feasible is it to create a model to extract key information from safety documents?",
"text": "Hi guys, so I recently got an offer for an internship during my summer break next year. The job scope is that I have to develop a way to extract key information from safety documents using NLP. Currently have zero experience with NLP or machine learning in general but I have roughly 9 months of preparations before the internship, and I'm currently doing a research project on NLP. So experience wise, I will have some time to familiarise myself with the technology.\n\n\nHowever, the deadline to accept the offer is quite soon. Furthermore, the internship will be minimally guided since the section of the company that wants to hire me has no experience with computer science or machine learning. \n\n\nThe length of the internship is 3 months and I just wanted to ask how feasible the job scope is considering my background. Thanks in advance!",
"date": "2022-09-15"
},
{
"vote": 7,
"title": "Is pytorch or tensorflow better for NLP?",
"text": "I am trying to ease in to NLP. Is pytorch and tensor flow both used in tandem with each other or is one better than the other?",
"date": "2022-09-14"
},
{
"vote": 19,
"title": "What are good resources to learn NLP for someone with no prior experience?",
"text": "I have to learn how to do NLP. I have to get caught up to speed relatively quickly. I want to do topic modeling, sentiment analysis,  text summary/paraphrasing, translation, and transcription.  I Will be programming in Python. Can I please have good recommendations for textbooks, videos series etc?",
"date": "2022-09-14"
},
{
"vote": 19,
"title": "Semantic search made easy with Marqo",
"text": "I am one of the founders of \nmarqo.ai\n. We are building an \nopen source tensor search engine\n. It allows anyone to build semantic search or image search based applications in a couple of lines of code. We are making it incredibly easy to use with a simple and intuitive API and sensible defaults. Although simple to use, we also have quite a few customization options and will be adding a lot more in the near term.  Some of the key features are semantic search, lexical search and pre-filtering. You can start using it today at \nour github\n (or contribute!) or read a bit  more about the technical aspects \nhere\n. A more complete list of features is:\n\n\n\n\nâ€œBatteries includedâ€ tensor search application. Create a tensor search engine in 3 lines of code.\n\n\nDesigned for the cloud and horizontal scalability.\n\n\nPrefiltering with a query DSL (domain specific language).\n\n\nEfficient approximate KNN (k nearest-neighbours) search using HNSW algorithm.\n\n\nAutomatic batching and parallel indexing.\n\n\nIndexing using CPU, GPU or multiple GPUs.\n\n\nSearching using CPU or GPU.\n\n\nONNX support.\n\n\nText-text, Text-image, image-text and image-image search.\n\n\nHybrid search â€” tensor and lexical search.\n\n\nPre-trained text and image models.\n\n\nResults re-ranking.\n\n\nText and image search highlighting.\n\n\nIntegrations with popular libraries like \nCLIP\n, \nHuggingface\n and \nSbert\n (and more to come!).\n\n\n\n\nFeel free to let me know your feedback or suggestions for features.",
"date": "2022-09-14"
},
{
"vote": 1,
"title": "Semantic search",
"text": "[removed]",
"date": "2022-09-14"
},
{
"vote": 1,
"title": "Software for tagging speech with text for ASR dataset creation",
"text": "Hi\n\n\nI am a native Konkani speaker and wanted to create a speech-to-text dataset using existing audio files.\n\n\nCan anyone point me to software for doing this efficiently?",
"date": "2022-09-12"
},
{
"vote": 9,
"title": "Is spaCY and NLTK used in conjunction with each other generally or is it good practice to use one over the other?",
"text": "I am learning the basics of NLP, but I am not sure if I should learn both at this point or just one. Also, where does BERT fit into this?",
"date": "2022-09-11"
},
{
"vote": 20,
"title": "MLPerf submission from Neural Magic: 175X increase in NLP Performance utilizing sparsity",
"text": null,
"date": "2022-09-10"
},
{
"vote": 3,
"title": "How To Increase Recall When Given Imbalanced Dataset For Machine Learning Model?--[Article]",
"text": null,
"date": "2022-09-10"
},
{
"vote": 1,
"title": "Recommended ways of building a chatbot/autoresponder for a recommendation system",
"text": "[removed]",
"date": "2022-09-10"
},
{
"vote": 3,
"title": "Semantic search for my personal books ?",
"text": "Is there a software tool that takes my books, and lets me do a qualty search(preferably semantic search) against them ?",
"date": "2022-09-09"
},
{
"vote": 9,
"title": "Three Ways to Build a Text Classifier with the Cohere API",
"text": "https://txt.cohere.ai/classify-three-options/",
"date": "2022-09-09"
},
{
"vote": 1,
"title": "Chatbot/Autoresponder bot notable implementations",
"text": "[removed]",
"date": "2022-09-09"
},
{
"vote": 7,
"title": "finding words representing similar concepts",
"text": "I would like to cluster phrases which represent similar concepts if not same, for example (king, president, emperor), (man, boy, male) something of sort. Should I go for word2vec training from the corpus I have ( I am only interested in phrases in my corpus) or use pretrained models such as BERT or ROBERTa? Any arguments if one can be better than other?",
"date": "2022-09-07"
},
{
"vote": 5,
"title": "Down the semantic rabbit hole",
"text": null,
"date": "2022-09-07"
},
{
"vote": 1,
"title": "Research community",
"text": "[removed]",
"date": "2022-09-07"
},
{
"vote": 11,
"title": "How to find similarities across long documents",
"text": "Suppose someone has written eleven books, and you want to determine how much of the latest book is restatements of claims made in their previous books. Ideally I would want the output to be  sentence by sentence pairing of the latest book with the previous ones. Can someone point me in the right direction?\n\n\n&#x200B;\n\n\nUpdate (9 Sept 2022)\n: In case anyone is interested, I came across \"Plundering Philosophers: Identifying Sources of the EncyclopÃ©die\" Timothy Allen, Charles Cooney, et al. \nJournal of the Association for History and Computing\n. Vol. 13, no. 1, Spring 2010. \nOpen access link\n.\n\n\nAbstract\n: Denis Diderot and Jean le Rond dâ€™Alembertâ€™s \nEncyclopÃ©die ou Dictionnaire raisonnÃ© des sciences, des arts et des mÃ©tiers\n stands as one of the crowning achievements of the French Enlightenment. This monumental work, containing some 77,000 articles written by no less than 140 contributors, was published in Paris between 1751 and 1772 in seventeen in-folio volumes of text and eleven volumes of engravings. As with all reference works, the authors and editors of the \nEncyclopÃ©die\n made extensive use of a vast array of contemporary reference works and scholarship to complete their massive compendium of enlightened knowledge. The identification of sources material used by the philosophes is a massive undertaking in itself, as the authors rarely acknowledged the works upon which they relied in writing their contributions. This paper describes two different experiments to identify sources of the \nEncyclopÃ©die\n. The first applies the \"Vector Space Model\" (VSM) to identify articles that may have been borrowed from the \nDictionnaire de TrÃ©voux\n (1743) â€“ an intellectual rival of the EncyclopÃ©die compiled by French Jesuits in the first half of the 18th century. We find that the Vector Space Model can be an effective means of identifying \"similar\" passages in documents, in this case, potentially borrowed articles that were then examined by human evaluators. Overall, we conclude that 5.32 percent of all of the articles in the \nEncyclopÃ©die\n that were examined were borrowed from the Jesuit critics of the philosophes. The second experiment, building on the first, applies what we call Pairwise Alignment of Intertextual Relations (PAIR) to detect passages borrowed from another important predecessor of the \nEncyclopÃ©die\n, Louis MorÃ©ri's popular \nGrand dictionnaire historique (1671-1759)\n, which was also a product of Jesuit scholarship. Given the genealogical character of the MorÃ©ri dictionary, which represented an understanding of knowledge radically different than that of the \nencyclopÃ©distes\n, we were nonetheless able to identify more than 400 shared passages between the two works using the PAIR approach. These findings shed new light on the composition process of the EncyclopÃ©die and suggest that the intellectual battle lines between the Jesuits and the philosophes may not have been as firmly established as previously understood. We conclude by outlining improvements to both the VSM and PAIR models, which we expect will make further identification of similar passages more effective.",
"date": "2022-09-06"
},
{
"vote": 0,
"title": "Use NLP to create universe inside computer",
"text": null,
"date": "2022-09-06"
},
{
"vote": 16,
"title": "What should I know if I want to \"learn an ontology\"?",
"text": "Hey everyone, got a really hard question here and need an advice. What do I need to know if I want to \"learn an ontology\" (nlp sense)?\n\n\nSpeaking of my case, the ontology will be used for \"enriching unstructured data with knowledge via mapping onto structured ontologies\".\nMy guess regarding the meaning of this is that each (almost) word in a document will be associated with an entity from an ontology as well as there will be some kind of graph of relationships between the words which is inferred from the ontology as well.\n\n\nSo the problem is two-fold:\n\n\n\n\nlearn an ontology;\n\n\nuse the ontology to perform document analysis.\n\n\n\n\nMy particular interest here is what are the ML/NLP technologies involved here and how they are used (in both learning and using ontologies).\n\n\nLearning an ontology\n\n\nI have done a little research regarding ontologies and it seems this is a really deep topic. Key steps I see:\n\n\n\n\ndecide the field of interest;\n\n\ndefine the vocabulary;\n\n\ndefine classes and their properties;\n\n\ndefine relationships, constraints and taxonomy.\n\n\n\n\nWell that's basically a little wikipedia on a particular subject. And that's a lot of expertise (in the field of interest) and labor. How does one learn an ontology automatically? What are the NLP technologies used for that (NER, tagging, etc.) and how are they used in the context of ontologies?\n\n\nI have a general nlp background (transformers, rnn, semantic search, translation, ner, pos tagging) and don't really see a way to learn the desired ontology structure (classes and relationships) in an automatic way. If there is no such way, how does one accelerate building of ontologies using nlp and ml pipelines?\n\n\nUsing ontology\n\n\nI have a sort of intuition of how one may use ontology to analyse documents (that's the guess above). Yet there is no clear picture regarding how it works. No concrete questions here due to lack of experience, so any comments are appreciated.\n\n\nIt'd be very much appreciated to have any resources, literature or comments shared, thanks!",
"date": "2022-09-05"
},
{
"vote": 1,
"title": "Is it possible to make compelling synthesized speech with fairly low-quality recordings?",
"text": "I'm new to speech synthesis and am still grappling with the different techniques and their pros and cons. I'd like to build a model from the ground up based on a fairly small corpus of about three hours of natural speech, which covers various speakers and styles. If I understand correctly, an HMM-based approach would probably be the best way to move forward, but many of my recordings have been conducted in less than ideal conditions, with occasional background noise and disfluencies. None of these were recorded in a night club, but many have occasional city noises like car horns. Can anything be done with such a corpus?",
"date": "2022-09-05"
},
{
"vote": 7,
"title": "[NLP] Post title generator",
"text": "Hello NLP geniuses,\nI'm looking for a NLP model that can generate titles for user generated posts. It would be a bonus if the model can generate multiple titles with probabilities so that I can choose the best title. I'm currently looking at transformer based models such as GPT2 and GPT3. I look forward to  recommendations.\nThank you!!",
"date": "2022-09-05"
},
{
"vote": 6,
"title": "Research area help",
"text": "Hi guys, I'm 2nd year computer science major, and I recently undertook a 1 year research programme. It's nothing too serious, it's just a programme in my school for undergraduates to have a small taste of a research career. \n\n\nGoing into university, I didn't really have a solid idea of what I wanted to do in my life, but researching, more specifically in the area of AI and machine learning always intrigued me. That's why I chose to take up the research programme that  focuses on machine learning, in NLP.\n\n\nWith that said, I have absolutely zero experience in these fields. This is kind of my \"square 1\". My supervisor gave me some introductory resources on NLP. I have to choose a scope for my research area but I'm currently stumped. \n\n\nWith my currently limited knowledge, some areas that piqued my interest were in sentiment analysis, either in Tagalog (language spoken in the Philippines) or in Singlish (English-based creole language spoken in Singapore). Another area is in summarisation of articles, which I foresee could be of potential use for university students. Lastly, I'm also kind of interested in using NLP to analyse beginner code, in hopes of giving potential improvements.\n\n\nFor those who are experienced in NLP, I would really appreciate some input and guidance on crafting the scope of my research topic. Are the research areas I listed above way out of my league? Are there any beginner friendly research areas in NLP that an inexperienced university student can undertake for a year? Any help would greatly be appreciated, thanks!\n\n\ntldr: Inexperienced second year cs major needs help narrowing (beginner-friendly) research areas for NLP",
"date": "2022-09-04"
},
{
"vote": 2,
"title": "Changing Authors order.",
"text": "Can someone change the order of authors after a paper has been submitted for review in ACL?",
"date": "2022-09-04"
},
{
"vote": 5,
"title": "Transitioning from Web development to NLP",
"text": "Hi guys, I've been doing web development for a few years now and I'm pretty much familiar with Full stack development and I've been doing pretty good as a full stack developer since I came across a project that introduced me to text based generation using spacy and since then I've been obsessed with the technology of natural language and looking to switch careers.   \n\n\nI need some practical advice on the learning path and what are the best frameworks to learn, I've heard some of them like and tried some like spacy and nltk. Would like to know more and things that would help me get a job as an NLP engineer.",
"date": "2022-09-03"
},
{
"vote": 5,
"title": "Everything you need to know about Activation Functions[article] which function is suitable for an NLP project?",
"text": null,
"date": "2022-09-02"
},
{
"vote": 1,
"title": "Everything you need to know about Activation Functions [Article]",
"text": "[deleted]",
"date": "2022-09-02"
},
{
"vote": 4,
"title": "Suggestions for checking if the text contains \"Purpose\"",
"text": "By purpose, I imply phrases like:\n\n\n\n\n\"MS Excel \nallows\n users to create tables\"\n\n\n\" Tableau \nchecks\n for daily profit.\"\n\n\n\"This access \nprovides\n read only access to the daily reports\"\n\n\n\n\nBasically words/phrases like allows, checks , provides\n\n\nTIA",
"date": "2022-09-01"
},
{
"vote": 0,
"title": "Announcement of Fellows selected for Univ.AIâ€™s MLOps Fellowship!!",
"text": "[removed]",
"date": "2022-09-01"
},
{
"vote": 27,
"title": "I'm releasing a library for calculating phone distances and mapping between phonemic alphabets",
"text": "If anyone here uses phonetic representations for their work, this could be relevant for you.\n\n\nI recently worked on a project that required mapping phones from a high-resource language to a low-resource one and found that there were no well-documented tools out there to do so. So I made my own: \nhttps://cdminix.me/phones\n\n\nWhat it does:\n\n\n\n\nvisualises phones based on their PHOIBLE features\n\n\nconverts between different phonetic alphabets\n\n\nmaps phones from one language to their closest match in a different language\n\n\ncalculates distances between phones\n\n\n\n\nI hope this is helpful to some - I'm also happy about any & all feedback if you find some use for the library.\n\n\nEdit: Sorry for the error in the post title, it should say \"phonetic\" and not \"phonemic\"",
"date": "2022-09-01"
},
{
"vote": 1,
"title": "remove a certain word",
"text": "Is there a way to remove a certain word from a list of sentences if that word appears after a list of words? A bit confusing. For example I want to remove the word \"and\" if \"and\" appears after a list of words ([ \"red\", \"blue\", \"green\"]). I know how to remove a word if it appears after one particular word but is there a way to do the same for a list of words? A regular expression? Thanks in advance.",
"date": "2022-08-31"
},
{
"vote": 14,
"title": "Any resources for transformers?",
"text": "Is there any resource where I can learn the basics of transformers or how to build those? I want to build a transformer for my project. Thank you.",
"date": "2022-08-31"
},
{
"vote": 5,
"title": "How to convert a positive sentence to negative or vice versa?",
"text": "Does anyone know how to convert a sentence into its negative form? Any lead would help alot. Thankyou.",
"date": "2022-08-31"
},
{
"vote": 8,
"title": "What do you recommend me to self-learn for a good NLP background? (Not a STEM major)",
"text": null,
"date": "2022-08-30"
},
{
"vote": 3,
"title": "Knowledge graphs",
"text": "Any knowledge graphs resouces links !? And also curious to know its real world applications. Im aspiring to startup narrowing to KG",
"date": "2022-08-30"
},
{
"vote": 7,
"title": "looking for links to model document paragraph and sentence level sentiment hierarchically",
"text": "I am looking for code/paper for modelling the 3 levels of sentiment present in a document in a hierarchical fashion.\n\n\n- using sentences to predict paragraph level and so on.",
"date": "2022-08-30"
},
{
"vote": 43,
"title": "What are your favorite NLP books?",
"text": "Hi, everyone. I'm looking for recommendations of NLP books you found particularly enjoyable or taught you a key skill for your career. Would also be interested to know its level (beginner/intermediate/advanced). What book do you consider as \"must read\"?",
"date": "2022-08-29"
},
{
"vote": 9,
"title": "Looking for a tool that highlights cognates between Germanic languages in a text",
"text": "I was thinking about working on a Python project for this (based of a \nwiktionary.org\n database) but I thought I should ask first before getting to it.\n\n\nIs there a tool that highlights cognates between two Germanic languages or more in a given text? The shared cognates can be from other languages (Latin, French, etc.) and don't have to be strictly Germanic.\n\n\nTake for example these excerpts (English, German, Swedish) :\n\n\n&#x200B;\n\n\n>\"All human beings are born free and equal in dignity and rights. They  are endowed with reason and conscience and should act towards one  another in a spirit of brotherhood.\"\n>\n>\"Alle Menschen sind frei und gleich an WÃ¼rde und Rechten geboren. Sie sind mit Vernunft und Gewissen begabt und sollen einander im Geist der BrÃ¼derlichkeit begegnen.\"\n>\n>\"Alla mÃ¤nniskor Ã¤ro fÃ¶dda fria och lika i vÃ¤rde och rÃ¤ttigheter. De Ã¤ro utrustade med fÃ¶rnuft och samvete och bÃ¶ra handla gentemot varandra i en anda av broderskap. \"\n\n\n&#x200B;\n\n\nIf I select for example \"free\" in the English excerpt, I would like it to show me \"frei\" and \"fria\" as its German and Swedish cognates. Or just highlight every word in the English text that has a lexical similarity with a given (Germanic) language.",
"date": "2022-08-29"
},
{
"vote": 1,
"title": "French model for paraphrasing generation",
"text": "Hello, am trying to find a French model for paraphrasing generation on hugging face but I didnâ€™t find. Do someone know which model can I use ? Examples and tutorial are welcome too",
"date": "2022-08-29"
},
{
"vote": 2,
"title": "Any idea about ALBERT Pre-Training Hardware and Time?",
"text": "Hi,\nI am thinking about pre-training ALBERT on my personal corpus of about 150 Million sentences.\nCan someone from their experience please give me an estimate about the most efficient hardware required and expected training time along with how much it might cost?\nI read somewhere that Nvidia A-100 40GB might be the way to go, is that right? How many instances would I need and what might be the training time?",
"date": "2022-08-28"
},
{
"vote": 7,
"title": "Ideas on how to create \"pseudo\"-dimensions for embeddings",
"text": "While writing this, I realize that I find it hard to describe the problem so pardon the confusing title. My problem looks like this:\n\n\nI'm using sentence embeddings to see if sentences are related to a specific topic. In this toy example, let's assume that the topic is \"\nattitudes towards animals\n\". I have a predefined set of sentences that fall into this category, so I obtain sentence embeddings for this set and average each vector to get a \"vector prototype\" for the topic. Now I use cosine similarity to examine how semantically related each new, unseen sentence is to the topic (e.g., \"\nI like cake\n\" results in a low similarity of .23 and \"\nI like horses\n\" receives .65). So far so good.\n\n\nI would now like to further differentiate if the sentence regards one of two sub-clusters of \"attitude towards animals\", namely \"attitudes towards dogs\" and \"attitude towards horses\". I'd like to scale the cosine similarity along a continuum where -1 is the topic prototype for dogs and 1 is the topic prototype for horses. Meaning that:\n\n\n\"\nI'm afraid of dogs\n\" may get a score of \n-.67\n\n\n\"\nHorses make me happy\n\" may get a score of \n.71\n\n\n\"\nI enjoy working in the garden\n\" may get a low score in any direction, since it's generally unrelated e.g, \n.14\n or \n-.14\n\n\nMy current approach is to obtain embeddings for each sub-topic, average them and calculate the distance to each. If the sentence is closer to the dog-subcluster, the sign of cosine-similarity to the parent cluster \"attitudes towards animals\" is simply reversed. I'm not sure if this is a good idea, so I was wondering if this is a common problem and how to tackle this? Or maybe you guys have better ideas? Thanks",
"date": "2022-08-26"
},
{
"vote": 1,
"title": "Do I need a master's, or could I do just as well with only a BS?",
"text": "[deleted]",
"date": "2022-08-25"
},
{
"vote": 9,
"title": "Which companies use multiple machine translation providers at the same time?",
"text": "Hello everyone,\n\n\nI was wondering which companies can use multiple machine translation solutions at the same time. For example, using a vendor that performs well for each language pair.\n\n\nWe have developed an aggregator of machine translation APIs and I would like to know which companies might be interested in this.\n\n\nBest,",
"date": "2022-08-25"
},
{
"vote": 7,
"title": "OpenAI-powered Mac writing assistant",
"text": "I am the creator of \nElephas\n, a native Mac writing app that brings features of AI to your Mac system. It uses GPT-3 (by OpenAI) behind the scene. It has some major distinctions from other AI writing apps,\n\n\n\n\nIt works on all your Mac applications. Say you are writing an email on Apple Mail, you can let Elephas complete your email. Or, if you are writing on Google Docs, Elephas can complete that paragraph for you.\n\n\nYou use your own OpenAI keys. So you pay for what you use.\n\n\n\n\nIt received good attention on the r/macapps subreddit. As of now, I have more than 100 paid customers.\n\n\nHere are the features grouped by end users,\n\n\nContent Writers\n\n\nðŸ‘‰ Generate blog ideas and outline for any keywords\n\n\nðŸ‘‰ Produce a complete article given an outline\n\n\nðŸ‘‰ Repurpose your content for social media\n\n\nðŸ‘‰ Write headlines that convert\n\n\nðŸ‘‰ Write a Tweet or LinkedIn post\n\n\nBusiness Executives\n\n\nðŸ‘‰ Turn short instructions into ready-to-send professional emails\n\n\nðŸ‘‰ Complete your business proposal\n\n\nðŸ‘‰ Convert your meeting notes into a summary\n\n\nðŸ‘‰ Create a presentation in 30 seconds\n\n\nðŸ‘‰ Rewrite sentences to match a style.\n\n\nTechnical Professionals\n\n\nðŸ‘‰ Write a technical blog\n\n\nðŸ‘‰Write interview questions\n\n\nðŸ‘‰ Write code in any language\n\n\nðŸ‘‰ Fix grammar mistakes in the selected text\n\n\nðŸ‘‰ Understand any code with our \"Explain code\" feature\n\n\nYou can check out the product \nElephas\n\n\nLifetime deal\n\n\n I have added a limited lifetime deal for $79. You can get it here \nhttps://kambanthemaker.gumroad.com/l/zaknv",
"date": "2022-08-25"
},
{
"vote": 12,
"title": "Rubrix release 0.17.0 with support for the spaCy training format",
"text": "Highlights:\n\n\nðŸ³ You can now prepare your dataset for training spaCy pipelines with one line of code.\n\n\nðŸ’¾ You can now read very large datasets in batches!\n\n\nâœ… Bulk validate and review with 50 and 100 records page sizes.\n\n\n&#x200B;\n\n\nRead about this release:\n\n\nhttps://github.com/recognai/rubrix/releases/tag/v0.17.0\n\n\nâ€”\n\n\nNew to Rubrix?\n\n\nRubrix is the open-source framework for data-centric NLP\n\n\nhttps://github.com/recognai/rubrix\n\n\nTo get started, join our super friendly community of NLP practitioners:\n\n\nhttps://join.slack.com/t/rubrixworkspace/shared_invite/zt-whigkyjn-a3IUJLD7gDbTZ0rKlvcJ5g",
"date": "2022-08-25"
},
{
"vote": 11,
"title": "7 spaCy Features To Boost Your NLP Pipelines And Save Time",
"text": null,
"date": "2022-08-25"
},
{
"vote": 5,
"title": "Shakespeare Semantic Search",
"text": null,
"date": "2022-08-25"
},
{
"vote": 4,
"title": "Facebook AI Researchers Open-Source â€˜LLM.int8()â€™ Tool To Perform Inference In Large Language Models (LLMs) With Up To 175B Parameters Without Any Performance Degradation",
"text": null,
"date": "2022-08-24"
},
{
"vote": 15,
"title": "Follow this step by step guide to train entities and extract relations with BERT Transformer and spacy3",
"text": null,
"date": "2022-08-24"
},
{
"vote": 15,
"title": "My cookbook for data creation",
"text": "Disclaimer: crosspost of \nhttps://www.reddit.com/r/MachineLearning/comments/wv5nls/d_my_cookbook_for_data_creation/\n \n\n\nI have been working NLP for 6 years and I  don't think we talk enough about how to create good data. Models and  even deployments are more sexy than data creation, let's change this!    \n\n\nHere's my conceptual framework for data creation:\n\n\n\n\ndefine success in business terms\n\n\nmap the data with stakeholders for buy in\n\n\nrapid prototype from data to deployment\n\n\niterate on dataset creation\n\n\n\n\nTechniques to get more data bang for buck:\n\n\n\n\nweak supervision\n\n\nactive learning\n\n\n\n\nConsiderations around dataset creation:\n\n\n\n\nin-house vs crowdsourcing\n\n\ntool recommendations\n\n\n\n\nMy cookbook for data creation\n \n\n\nDefining success in business terms\n\n\nA good dataset is one that creates business value.\n  Making sure stakeholders, domain experts, and engineers are on the same  page is hard.  I find sharing a mock of the model and data  can really  help move stakeholders understand whatâ€™s possible. Sharing an initial  annotations with stakeholders can give quick feedback around conceptual  errors and reduce the risk of overhyping the project.\n\n\n&#x200B;\n\n\nMap the data with stakeholders for buy in\n\n\nI show the data to stakeholders as a user interview.\n  I am looking to see how usable is the data and strengthen my conceptual  understanding of the business problem. Itâ€™s a successful interview if I  know more about how  experts understanding of the problems. So for  example for Comtura to  model sales processes, whenever I hear the  importance of qualification I would link that with qualification  methodologies.\n\n\nI like having 2  mindmaps one with the expertâ€™s conceptual view and one view around how  can these expert concepts be modelled. For example maybe I will model  sales qualification as a span classification problem where some spans  are related to sales qualification, maybe instead I model it as a  paragraph level document classification problem.  \n\n\nRapid prototype from data to deployment\n\n\nPerfect  is the enemy of done. As a data scientist there are so many   interesting ways a model could be improved. If you think your model is   feasible then thereâ€™s no need for further improvements until it hits   production. Speeding up the data -> model -> business value flow  is essential otherwise how do you know if your data is creating business  value?  \n\n\nIterate on dataset creation\n\n\nIf  you feel you are on the right track based on your prototype you can  start refining your dataset creation process based on expert and user   feedback. I like to have 3 steps in creating a dataset:\n\n\n\n\nannotation\n\n\ndata review\n\n\ndataset creation feedback\n\n\n\n\nAnnotation\n\n\nCreates labelled data\n\n\nData review\n\n\nReview  the created data qualitatively with a supervisor checking output  periodically and quantitatively with inter annotator agreement and model  metrics  \n\n\nDataset creation feedback\n\n\nI like having an annotation booklet. \nThe annotation booklet is a living  document where I keep task defintions, ambigous examples, and past discussions.\n  With periodic feedback sessions where we discuss challenging samples and process improvement possibilities.\n\n\nTechniques to get more data bang for buck\n\n\nWeak supervision\n  is a way to generate weak labels for unlabelled data programatically.  Weak supervision has the most potential to improve  your dataset  creation process. Labelling functions generate weak labels from  unlabelled data. A labelling function can have good signal for 2-3% of  your  data. With such a wide coverage labelling functions can improve  performance a lot. Labelling functions can also be easily explained to  domain experts allowing experts to improve your labelling functions.\n\n\nActive learning\n  is a form of model in the loop annotation where low confidence samples  are annotated to improve model confidence. I have used prodigy in the  past for this. It worked sometimes, sometimes not.\n\n\n&#x200B;\n\n\nConsiderations\n \n\n\nIn-house vs crowdsourcing\n\n\nFor  quick prototype or if quality not so important crowdsourcing works well  as you can scale your tasks to a large pool of annotators. I prefer  in-house annotation though as it allows you to build a relationship with  annotators. You can retain the best performing annotators and iterate  on your annotation process way easier with in-house annotation. In my  experiece the long term  cost / sample also favours in-house annotators  as crowdsourcing requires 2/3x more levels of review or annotation due  to low inter annotator agreeement.\n\n\nTool recommendations\n\n\nOpen  source: I recommend snorkel to get started with weak supervision.  Doccano and Label Studio are great general annotation interfaces\n\n\nPaid: Prodigy is a great annotation tool with active learning support from the creators of spacy",
"date": "2022-08-23"
},
{
"vote": 15,
"title": "Predicting a partial word? input: 'hell', output: [hello, help]",
"text": "I've been looking into T5 and other language models, and constrained generation based on this article: \nhttps://huggingface.co/blog/constrained-beam-search\n\n\nbut can't find anything. Are there any keywords or phrases I should be using? what are some possible techniques to look into? thanks.",
"date": "2022-08-21"
},
{
"vote": 2,
"title": "Specific questions abt basic word embedding",
"text": "Source of this table\n\n\n\n\n\n\n\n\n\n\nman\n\n\nwoman\n\n\nking\n\n\n\n\n\n\n\n\ngender\n\n\n-1\n\n\n1\n\n\n-0.95\n\n\n\n\n\n\nroyal\n\n\n0.01\n\n\n0.02\n\n\n0.95\n\n\n\n\n\n\n\n\nWhy does man get -1 for \ngender\n feature? Shouldn't it be 1 if the word is strongly related to the feature? 0-weakest, ... 1-strongest \n\n\nWhy does king get negative value for Gender?  \n\n\nWhat is the range of word embedding values? -1 to 1? \n\n\nWho is the one who decides the values there (0.95, 0.02, etc)? Annotators?",
"date": "2022-08-21"
},
{
"vote": 3,
"title": "Best method to 1) predict next word 2) predict rest of current word?",
"text": "Trying to do predictive text in python. Are there any open source libraries in python or Pytorch models to do this?\n\n\nI've tried ngrams but got poor results. I also haven't been able to find any good off the shelf libraries.\n\n\nAdvice? what's the SOTA?",
"date": "2022-08-21"
},
{
"vote": 0,
"title": "Question about Conversational Design Institute",
"text": "Conversational Design Institute has been advertising a lot. They offer different certification bundles. I can afford some of them and not others. Would I be able to get a job in language technology if I take these certifications? I had talked to a temp agency about being a Conversation Designer for a health insurance company. I remember that their requirements focused more on Bachelors and Masters degrees.",
"date": "2022-08-20"
},
{
"vote": 1,
"title": "Language learning audiobooks and resources",
"text": "[removed]",
"date": "2022-08-20"
},
{
"vote": 3,
"title": "Donut ðŸ© : OCR-free Document Understanding Transformer (Research Paper Walkthrough)",
"text": null,
"date": "2022-08-20"
},
{
"vote": 7,
"title": "open source equivalent of AWS Textract?",
"text": "Does anyone know any package (preferably python), that can go beyond the regular OCR and extract tables/forms/etc. from files/images?",
"date": "2022-08-19"
},
{
"vote": 1,
"title": "If you guys know, ikenna creates his own app. The name and so on is out, where everyone can sign for the waitlist. The app name will be fluyo: https://fluyoapp.com/?kid=2E4Q1M",
"text": "[removed]",
"date": "2022-08-19"
},
{
"vote": 0,
"title": "How can we pass a list of strings to a fine tuned bert model?",
"text": null,
"date": "2022-08-18"
},
{
"vote": 3,
"title": "Could someone tell me what am I doing wrong here?",
"text": "Hey, so I recently made my first project in NLP . It is an HMM for POS Tagging in Hindi .  I had previously done one of Coursera's assignment for the same for English . Therefore I used a lot of the code from there and modified it for Hindi. But as I observe now the accuracy comes out lower in HMM as compared to the simple Markov Model. Would someone be kind enough to point out places I could make improvements to get better accuracy ? General comments on the code would also be appreciated.\n\n\nEdit: Link- \nhttps://colab.research.google.com/drive/18A1jr27W5lFhmTPVKqmH3mqUutQSA8GP?usp=sharing",
"date": "2022-08-18"
},
{
"vote": 1,
"title": "Advanced Natural Language Processing in Google Sheets",
"text": null,
"date": "2022-08-17"
},
{
"vote": 4,
"title": "txtai 4.6 released - new workflow steps and performance improvements",
"text": null,
"date": "2022-08-17"
},
{
"vote": 9,
"title": "Google AI Open-Sources â€˜Raxâ€™, A Python Library for LTR (Learning to Rank) in the JAX ecosystem",
"text": null,
"date": "2022-08-16"
},
{
"vote": 6,
"title": "The Latest Language Model From Meta AI, â€˜Atlas,â€™ Has Outperformed Previous Models Like â€˜Palmâ€™ And Reached Over 42% Accuracy On Natural Questions Using Only 64 Examples",
"text": null,
"date": "2022-08-16"
},
{
"vote": 9,
"title": "NLP Writing Assistant -- Autocomplete Everywhere",
"text": "Hey y'all, we're a small team of engineers and writers - we created a Chrome Extension that gives suggestions as you type on almost every site across the web.\n\n\nIt can help complete your thoughts, or get new ideas when you're stuck, and has been super helpful for our own writing and communication (especially emails!).\n\n\nWe're still developing it, but just made some big updates and would love to hear your feedback - hope a post like this is allowed!\n\n\nLink if you'd like to try it: \nchrome.google.com/webstore/detail/hyperwrite/kljjoeapehcmaphfcjkmbhkinoaopdnd",
"date": "2022-08-15"
},
{
"vote": 1,
"title": "if you're having a hard time categorising invoices, you should check the article below in which you can learn how to use a multimodal data to fine tune a pre trained BERT model !",
"text": null,
"date": "2022-08-15"
},
{
"vote": 1,
"title": "txtai 4.6 released - new workflow steps and performance improvements",
"text": "[deleted]",
"date": "2022-08-15"
},
{
"vote": 11,
"title": "what topic should I select in nlp for research that doesn't require huge computational training to produce results?",
"text": null,
"date": "2022-08-15"
},
{
"vote": 1,
"title": "ðŸ’¥ Evaluate Rephrased Sentences by Using an NLP Model (Google T5 Transformer)",
"text": null,
"date": "2022-08-15"
},
{
"vote": 11,
"title": "How to normalise 2moro to tomorrow?",
"text": "Is it something related to text clustering?\nE.g. otw to On the way, comp.sci to computer science.",
"date": "2022-08-15"
},
{
"vote": 16,
"title": "Advice on getting a masters at almost-30",
"text": "Hi everyone,\n\n\nI will be turning 30 next year and I am contemplating getting a masters. To sum up my situation & background: I got my bachelor in 2016. My course was really named \"foreign languages and literatures\" but I pretty much steered it completely towards linguistics & language acquisition (I pretty much took all the linguistics classes they had available, including computational linguistics).\n\n\nIn an ideal world I'd have kept on studying linguistics, but then the only career that would've been available to me would've been in academia and I'm from a country where academia is notoriously nepotistic (I'm in Europe, btw).\n\n\nOther than that, back in the day with a degree like mine you could aspire to being a teacher or a translator. Pretty hard to make a decent living as either of those so I decided to not continue my studies and start working. 1 move abroad and 6 years later I now work in XR tech as a software tester (!).\n\n\nI've recently seen targeted ads for some \"language technologies\" masters that are in-between linguistics and programming /AI-related/data analysis-adjacent. Given my job in software required me to get fairly technical, it looks like the perfect opportunity to steer my career back into linguistics. \n\n\nDon't get me wrong - my job is fairly interesting and I enjoy it, but eventually if I want my compensation to grow I can't stay a tester. I'd either have to move upwards and become a manager, or move sideways and learn to program to write automated tests (which pays very well but is dreadfully boring).\n\n\nLinguistics is really what made my eyes twinkle. It's what I would ramble about on a first date. I chose financial stability back in the day because I have to pay my own bills, but if there's a way to have both, why not?\n\n\nI am single and I have no debt. I'd love to take a couple years to work part-time while I get a masters. However, I'm trying to figure out if this makes sense financially in the long-term.   \n\n\nI guess my questions are: what kind of jobs are you doing? do you enjoy them? are there many opportunities in your field? how much is your salary approximately? are there any masters you'd recommend?\n\n\nThanks all <3",
"date": "2022-08-14"
},
{
"vote": 3,
"title": "Tips for writing a research paper",
"text": "Hi everyone,\n\n\nCurrently a math bachelor student, i am doing an internship in natural language processing.\nI realise i am a quite newbie in this field, but using some mathematical theorems i came to concieve a theoratical semi-supervised nlp model for classification and i was wondering whether or not i should write a paper about it since it gave fairly good results.\nMy first question is how can i decide whether or not it's relevant to write a research paper about the model.\nMy dilemma lies in the fact that i am not really sure if it's worth the effort to write a paper about it, especially that i am afraid people might find it quite basic and not worth the time to read it. So what criteria should i consider in doing so?\n\n\nMy second question, if i decide to start the writing, does any of you have tips about how to write it properly?\n\n\nThank you for your time",
"date": "2022-08-14"
},
{
"vote": 3,
"title": "Dense Passage Retrieval for Open-Domain Question Answering (Research Paper Walkthrough)",
"text": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. \n\n\nIn this work, we see that retrieval can be practically implemented using dense representations achieving SOTA results.\nPaper summary: \nhttps://lnkd.in/dzRmaqvX\n\nPaper link: \nhttps://arxiv.org/abs/2004.04906",
"date": "2022-08-14"
},
{
"vote": 1,
"title": "Proper way to check whether a person is linked to the requested class from text",
"text": "Hey, a NLP beginner here! I'm learning nlp right now and I've stumbled upon an interesting problem that I'm trying to get a hold onto it. Small description about the dataset, I've a dataset that holds info about Name, Text and Class so I have a classification problem. Namely, the problem I face and I try to solve (or find the right approach) is the following: I need to check whether a person with name A is linked with the class from the text. For example, let's say the name is Jeff Bezos, the text is about a murder but Jeff Bezos is a witness in that murder and the class is Murder so the output should be False (indicating that Jeff Bezos didn't do the murder). The question I have now is, what approach should I use this for this binary classification (yes, linked with the class / no, isn't linked with the class) and can I have some examples if there are some already trained models? Thanks in advance!",
"date": "2022-08-14"
},
{
"vote": 15,
"title": "[Presentation Feedback] - Sentence Embedding Fusion",
"text": "I recently recorded a 6 minute walk-though of our recent paper \nFusing Sentence Embeddings Into LSTM-based Autoregressive Language Models\n and am thinking which parts I did well and which ones I need to work on. This isn't a RoastMe request but I would still like to get some constructive criticism that would help me tell a better story in a presentation.\n\n\nhttps://www.youtube.com/watch?v=U8-h7pZFHc0\n\n\nI may be stretching my request, but the same applies to the writing in the original paper. My goal is to write in a non-complicated and captivating way but I accept that for now that's not the case.\n\n\nhttps://arxiv.org/abs/2208.02402",
"date": "2022-08-14"
},
{
"vote": 7,
"title": "Can GPT like decoder only models be used for classifcation",
"text": "Hi I am wondering if one can use GPT like decoder only models for classification? I understand that these models are most suited for things auto-complete the sentences predicting the most probable next word, due to the representation computed for a given token in this architecture depending only on the left context. Hower, can/should such a model be used for classification, since it is technically possible? Are there any use cases for it or is it not actually a model intended for such a purpose?",
"date": "2022-08-13"
},
{
"vote": 5,
"title": "Where can I get pre-trained Dependency-Based Word Embeddings?",
"text": "I am looking for pre-trained embeddings from the paper \nDependency-Based Word Embeddings\n. \n\n\nI found a blog \npost\n by one of the authors with links, but they do not work. \n\n\nI need them to reproduce the results from the paper \nA Theme-Rewriting Approach for Generating Algebra Word Problems\n. There is a \nrepo\n with the code for the paper which has links to data needed, but they don't work either.",
"date": "2022-08-13"
},
{
"vote": 3,
"title": "bert for relation extraction",
"text": "i am working with bert for relation extraction from binary classification tsv file, it is the first time to use bert so there is some points i need to understand more?\n\n\n\n\nhow can i get an output like giving it a test data and show the classification results whether it is classified correctly or not?\n\n\nhow bert extract features of the sentences, and is there a method to know what are the features that is chosen?\n\n\ni used once the hidden layers and another time i didn't use i got the accuracy of not using the hidden layer higher than using it, is there an reason for that?",
"date": "2022-08-13"
},
{
"vote": 11,
"title": "Learning spaCy for a final project",
"text": "Hi! How are you all?\nI am Fiamma and I am currently studying my last year of Systems Engineering and going through the last subject, in which we have to develop a group project. In this, we are using spaCy for the first time, so we have a lot of doubts and we are really newbies ðŸ˜…\nThis time, I want to ask you guys if you know some tool that allows me to numerize in spanish, because I see that numerizer doesn't seem very friendly with spanish, the language that the project understands, since we are from Argentina.\nAlso, any advice on the subject will be very welcome.   \n\n\nðŸ¥°",
"date": "2022-08-12"
},
{
"vote": 1,
"title": "ðŸ’¥ How to use Hasty Annotation Tool | @Hasty",
"text": null,
"date": "2022-08-12"
},
{
"vote": 2,
"title": "what are some techniques that can be used to gain insights from reviews ( other than sentiment analysis and the usual statistical methods)",
"text": "Hello everyone, so I have been working on developing some business-related NLP projects lately and I wanted to explore something new other than the traditional statistical methods and sentiment analysis, i did my own research and came across NER and sarcasm detection, but other than that I couldn't find anything else, do you guys have anything ( papers, resources ) to help with that and any suggestions or ideas are deeply appreciated",
"date": "2022-08-11"
},
{
"vote": 3,
"title": "Neuraan: The NLP API specialized in the Spanish language",
"text": "Hello Reddit!\n\n\nWeâ€™re Israel & Mario, cofounders of Neuraan (\nhttps://neuraan.com/en\n). It is difficult for natural language processing developers in LatAm to get their applications to have the same accuracy in Spanish as in English. We create an API that helps them correct those inaccuracies and increase the adoption of their solution.\n\n\nWe have +6 years of experience developing chatbots and voice assistants for large enterprises across LatAm and was very common to end up doing some research and developing algorithms to enhance the accuracy that the commercial solutions like Dialogflow, Watson, Amazon Lex, and Azure Cognitive deliver because they provide a general solution for non-English languages.\n\n\nTo provide an extra layer to improve the performance of conversational applications in Spanish we developed a RESTful API that allows developers and enterprises to provide a few examples of the intents and entities used by their applications to enhance the recognition. We are using a few shot learning strategies developed using various Machine Learning strategies including deep neural networks along with self-developed algorithms.\n\n\nThis solution permits the enhancement of the recognition of goal-oriented applications (create a support ticket, extract data from a user conversation, make a telephonic sale, etc.) instead of standard NLP engines' general Spanish.\n\n\nOur pricing is based on API requests.\n\n\nWe look forward to feedback from the Reddit community!",
"date": "2022-08-11"
},
{
"vote": 9,
"title": "How good is BERT tokenizer?",
"text": "I'm using \nBERT pretrained\n for the 1st time & found smtg weird here. The original word 'demonstrators' has \nsplit into 3 tokens that have different meanings\n.\n\n\noriginal = &quot;Thousands of demonstrators&quot;\ntokenized= [&quot;Thousands&quot;, &quot;of&quot;, &quot;demons&quot;, &quot;##tra&quot;, &quot;##tors&quot;]\n\n\n\n\n\nWill this affect the model performance?\n\n\nWhat's the function of '##' here?",
"date": "2022-08-11"
},
{
"vote": 0,
"title": "Details on GloVe300D embedding header",
"text": "E.g. apple represented by [0.1 , 0 , 0 , 0.8 .... 0]\n\n\ntotal 300 columns there, how to find out the column headers?\n\n\n0.8 could probably mean 'apple' is 80% related to 'fruits'. so fruits is column header.",
"date": "2022-08-11"
},
{
"vote": 4,
"title": "Text Classification and Semantic Exploration of \"Hello, World!\"",
"text": "https://txt.cohere.ai/hello-world-p2/",
"date": "2022-08-11"
},
{
"vote": 2,
"title": "Imbalanced sentence pair classification",
"text": "I am training a multi class sentence pair classifier. The inputs are sent A, sent B (sort of repetition of sent A (by another person)) . The classes signify different types of errors in sent B. \n\n\nHowever the data is highly imbalanced, approximately\n\n\nClass 1 : 850\nClass 2: 125\nClass 3: 90\nClass 4: 160\nClass 5: 45\n\n\nAlso the test data is recorded at different source compared to train data.\n\n\nMy approach was to fine tune a bert model for sentence pair task. Until now I have tried oversampling, undersampling, different losses (weighted cross entropy, focal loss and dice loss). But the precision, recall measures of class 5 is very poor.\n\n\nIs there anything else I can try to improve the performance? Would Pretrained nli model help? I also have around half a million records of unlabelled sent A. Could I try training bert on this data before fine tuning it?",
"date": "2022-08-10"
},
{
"vote": 4,
"title": "Transformers changing embedding size (pytorch)",
"text": "Since embeddings in pytorch acts as lookup table, Is there any difference between these two codes?\n\n\n(model.shared means embedding layer (T5 Transformer))\n\n\nmodel.shared = new_emb\nmodel.lm_head = new_head\n\n\n\nand\n\n\nmodel.shared.weight = new_emb.weight \nmodel.lm_head.weight = new_head.weight\n\n\n\nThe reason that i am asking this question is:\n\n\nWhen i use both, i get different loss values (cross-validation loss)\n\n\nLoss for code piece 1: \nhttps://i.stack.imgur.com/D0sz7.png\n\n\nLoss for code piece 2:\nhttps://i.stack.imgur.com/FvhMW.png",
"date": "2022-08-10"
},
{
"vote": 3,
"title": "NLP in contract management and the importance of labeling in model fine-tuning",
"text": "I came across this article in which I got  to learn more about the use case of NLP in contract management and the importance of labeling in model fine-tuning, thought you may enjoy it too ! \n\n\nhttps://ubiai.tools/blog/article/Contract_management_NLP",
"date": "2022-08-10"
},
{
"vote": 6,
"title": "Where to start/go?",
"text": "Hi guys,\nI was introduced to corpus-based terminology research during my studies when I was doing MA in Translation. I loved it and would love to pursue it further to maximise my native languageâ€™s specialised language discourse.\n\n\nI really don't know what to do or even the relevant information that I need to know  implications etc.\n\n\nCould you please help.",
"date": "2022-08-10"
},
{
"vote": 5,
"title": "Guide to Building a Question-Answering System for PDF Tables",
"text": "Guide to Building a Question-Answering System for PDF Tables\n\n\nWrote up a guide that shows how technologies like Amazon Textract, Sentence transformers, and question-answering models can be combined to allow people to ask natural questions of data that's normally locked away in PDFs and only retrievable with Ctrl-F.",
"date": "2022-08-09"
},
{
"vote": 10,
"title": "Looking for Twitter sentiment analysis datasets for multiple languages around the world.",
"text": null,
"date": "2022-08-09"
},
{
"vote": 2,
"title": "Fake News Detection",
"text": "Planning to work on Fake news detection using machine learning. Can anyone tell me which models perform well for this topic?",
"date": "2022-08-09"
},
{
"vote": 2,
"title": "Looking for sentiment analysis of Wikidata",
"text": "Is anyone aware of the sentiment analysis of the Wikidata entries? I've seen some research that is using  Wikidata for data augmentation but no model or, preferably, a labelled dataset of Wikidata entries, like \"war criminal (Q11075015)\" negative, \"Nobel Prize (Q7191)\" positive)",
"date": "2022-08-09"
},
{
"vote": 2,
"title": "How to extract embeddings from CBOW?",
"text": "I trained a CBOW model on a corpus using 3 words before/after as context. \nNow that the model's trained, how do I actually get the embeddings?\nIn Skip-gram, we have a single input vector so I can pass in the word I want embeddings for into the model and extract the output of the single hidden layer to the embedding vector.\nFor CBOW, there are 6 inputs in my case. They are being projected into a 500dim vector(the output of the hidden layer). This is my word embedding but for which word? Is this the embedding for the word predicted by the model?",
"date": "2022-08-08"
},
{
"vote": 9,
"title": "Best models for sentence similarity with good benefit-cost ratio?",
"text": "For a web project I need to compute the similarity between small text paragraphs (up to 1000 words) or even between a small set of keywords (up to 20 words).\n\n\nI tried huggingface transformers with sentence transformers, model ' all-distilroberta-v1', while the quality of the similarity was very good it was very slow and it uses a lot of memory. It uses 768-dimensional vectors internally to compute the similiarity.\n\n\nFirst question:\n Where can I find smaller transformer models?\n\n\nSecond question:\n Are there other libraries that are better suited for this kind of (rather simple NLP) task?\n\n\nI would like to reduce the computational and memory footprint as this will be integrated into a web app.\n\n\nMany thanks",
"date": "2022-08-08"
},
{
"vote": 14,
"title": "GitHub - public-law/new-dale-chall-readability: A fresh implementation of the New Dale-Chall readability formula. The focus is correctness and modern coding style. Its output is tested against samples from the original publication.",
"text": null,
"date": "2022-08-08"
},
{
"vote": 3,
"title": "Math needed for NLP",
"text": "I apologize if this is the wrong place to ask, but I'm working on getting into the field of NLP and I was hoping to get some advice.\n\n\nI have a BA in Linguistics and German (double-major) from a few years ago, and I'm now working on a BS in CS, with the hope of pursuing a NLP-focused PhD in CS. I don't have much of a math background, so I am working on building that during my BS in CS. However this 2nd degree is on a much shorter time-frame than the first degree, and it would be difficult to fit much more math in other than the courses I already have planned (listed below). I'm wondering if this would look good enough for CS-NLP PhD admissions, or if I need to really go out of my way to add in a bit more. I appreciate any and all advice. Thanks!  \n\n\nStats for Engineers\n\n\nCalc 1-4  (differential calc, integral calc, vector calc, differential equations)\nintro to matrix algebra\nintro to series (prereq for probability)\nintro to probability\nlinear algebra 1",
"date": "2022-08-07"
},
{
"vote": 8,
"title": "Finding words that are more common than usual",
"text": "I have a body of text, and would like to find words in this text that stand out, in the sense of being used more often than what is usually seen in the English language.\n\n\nSo far, I have been playing with the ratio between the frequency of each word in the text and the corresponding frequency in a large corpus of English text. This gives promising results, but these results are not robust: some words appear only once in the text and still end up ranking highly because they are very rare, causing the ratio of the two word frequencies to be high.\n\n\nWhat would be a robust way of finding these words?",
"date": "2022-08-07"
},
{
"vote": 4,
"title": "How to use CoreNLP with a large corpus(14.7 GB)?",
"text": "I have a corpus as a text file which is 14.7 GB in size. I have 16GB of RAM. \nI need all the annotators in the pipeline but looks like that is not enough memory. \n\n\nThe output format I need is conllu. \n\n\nIs there any way to split the file into chunks and combine the individual conllu outputs?\n\n\nOr is there any other way around this?",
"date": "2022-08-06"
},
{
"vote": 1,
"title": "'FlavaModelOutput' object has no attribute 'contrastive_logits_per_image'",
"text": "I have used a code of Flava model from this link:\n\n\nhttps://huggingface.co/docs/transformers/model_doc/flava#transformers.FlavaModel.forward.example\n\n\nBut I am getting the following error:\n\n\n&#039;FlavaModelOutput&#039; object has no attribute &#039;contrastive_logits_per_image&#039;\n\n\nIf someone can help that would be great.",
"date": "2022-08-06"
},
{
"vote": 9,
"title": "Is there a NLP tool that can create questions from a given website/text?",
"text": "I tought about a quiz/index card generator that can extract the text from sites like Wikipedia. Think it could be an interessting way of learning. But I am new to NLP. Is it even possible?",
"date": "2022-08-04"
},
{
"vote": 25,
"title": "What are your main pains working with NLP in non-English languages?",
"text": null,
"date": "2022-08-04"
},
{
"vote": 4,
"title": "Text cleaning with Deep Learning methods",
"text": "For my postgraduate dissertation, I am working on various sentiment analysis multi-class classification problems i.e negative, neutral, and positive for tweets. I have already done some English and German model training for traditional machine learning models and now moving on to neural networks. \n\n\nI am a bit confused about the text cleaning aspect of this. For the traditional machine learning, I cleaned text by \nlower casing, removing links, twitter specific characters (hashtag, usernames, RT), then tokenising it, removing stopwords, and stemming\n. \n\n\nI have seen many tutorials which do not clean the text to that extend. The most they do is remove punctuations, numbers, and special characters, as well as  lower casing. According to these tutorials, deep learning methods do not need as much text cleaning as traditional models. Is this the case?\n\n\nAm I okay to assume that I can just do basic text cleaning, then follow through with tokenising, pad sequencing and word-embedding (thinking Word2Vec), and then just use the models. I understand that this varies by use case, just wanted to get a feel from the overall methodology.\n\n\nFrom what I understand, BERT is a method that requires the least text cleaning.",
"date": "2022-08-04"
},
{
"vote": 2,
"title": "Creating a dashboard using natural language",
"text": null,
"date": "2022-08-03"
},
{
"vote": 5,
"title": "Neuraan: The API to create applications with NLP in the Spanish language",
"text": "Hello Reddit!\n\n\nWeâ€™re Israel & Mario, cofounders of Neuraan (\nhttps://neuraan.com\n). It is difficult for natural language processing developers in LatAm to get their applications to have the same accuracy in Spanish as in English. We create an API that helps them correct those inaccuracies and increase the adoption of their solution.\n\n\nWe have +6 years of experience developing chatbots and voice assistants for large enterprises across LatAm and was very common to end up doing some research and developing algorithms to enhance the accuracy that the commercial solutions like Dialogflow, Watson, Amazon Lex, and Azure Cognitive deliver because they provide a general solution for non-English languages.\n\n\nTo provide an extra layer to improve the performance of conversational applications in Spanish we developed a RESTful API that allows developers and enterprises to provide a few examples of the intents and entities used by their applications to enhance the recognition. We are using a few shot learning strategies developed using various Machine Learning strategies including deep neural networks along with self-developed algorithms.\n\n\nThis solution permits the enhancement of the recognition of goal-oriented applications (create a support ticket, extract data from a user conversation, make a telephonic sale, etc.) instead of standard NLP engines' general Spanish.\n\n\nOur pricing is based on API requests.\n\n\nWe look forward to feedback from the Reddit community!",
"date": "2022-08-03"
},
{
"vote": 1,
"title": "BLOOM â€” BigScience Large Open-science Open-Access Multilingual Language Model",
"text": null,
"date": "2022-08-02"
},
{
"vote": 1,
"title": "chinchilla's wild implications - Data, not size, is the constraint on language model performance",
"text": null,
"date": "2022-08-02"
},
{
"vote": 2,
"title": "sentence transformer vector dimensionality reduction to 1",
"text": "[deleted]",
"date": "2022-08-02"
},
{
"vote": 1,
"title": "Is JavaScript useful for NPL?",
"text": "Hello! \n\n\nI've been studying linguistics for some years now, but I'm only a beginner programmer, and I my goal is to work with NLP. The problem is I, just like many, dodn't know exactly where to start!\n\n\nRecently I found out about Free Code Camp, and I am learning a lot with it. But this course is aimed for aspiring Web Developers (HTML, CSS and JavaScript), which I wouldn't really like to be.\n\n\nAm I in the right path anyway to work with NPL? Or should I focus on Python as others have said?",
"date": "2022-08-01"
},
{
"vote": 1,
"title": "Clustering time series data (doc2vec vs sentence transformers)",
"text": "[deleted]",
"date": "2022-08-01"
},
{
"vote": 6,
"title": "Fine tuning BART on only entailments isn't working as expected",
"text": "Hi all ! This is my first post on here, so please excuse any formatting issues.\n\n\nI'm working on a Few Shot Text Classification project. I have 5 classes to classify phrases into (frustration, confusion, positive feedback, request action, refusal). I have 125 entailments per class (so 625 total phrases in my dataset). Around 280 of these phrases were in the original dataset given to me for my use case (collected from actual call transcripts from a contact center). The remaining phrases I generated using GPT-3. Keep in mind, all of these are entailments.\n\n\nI'm using the zero shot classification pipeline of bart-large-mnli on HF. I'm pretty confident my fine tuning code is on point. I fine tuned bart-large-mnli on these 625 entailments, but somehow during inference, its performance degraded. The unfine-tuned BART was more confident on correct predictions and was more frequently accurate than the fine tuned BART. Something is clearly going horribly wrong for me. I did a little digging around and have a few suspicions, but I'd love to get your opinion as well.\n\n\n\n\nI found this \ncomment\n, which seems to suggest you need to have an equal number of contradictions as well. Is fine tuning only on entailments is hazardous? Do I need to include contradictions as well?\n\n\n\n\nThis comment also seems to suggest using an unbalanced dataset i.e using many training samples for one class and few/no training samples for another class. What is your take on this?\n\n\n\n\nOr is this just a problem of hyperparam tuning? Do I need to play around with num_epochs, weight-decay, etc.?\n\n\n\n\n\n\n&#x200B;\n\n\nI'd really appreciate any insights you have :)",
"date": "2022-08-01"
},
{
"vote": 2,
"title": "Language models with custom vocabularies?",
"text": "Hi. I'm wondering if there are any language models that have their own custom vocabularies. As far as I'm aware, there are numerous language models pre-trained in particular domains (e.g., BioBERT, SciBERT, FashionBERT, etc.) but these mostly use the original BERT model's vocabulary.\n\n\nIn contrast, in the biomedical domain there was a paper titled \nPretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art (Lewis et al., 2020)\n which released a RoBERTa model with a vocabulary specifically created from biomedical text.\n\n\nAre there any other language models that are made like this? Thanks.",
"date": "2022-08-01"
},
{
"vote": 10,
"title": "Text Classification state of the art?",
"text": "Hey everyone! I'm currently doing a project that requires classification of closed caption data from TV shows into 28 different classes. I'm currently using SentenceBERT to create encodings of the text and computing the cosine similarity between the target labels for each sentence. Then outputting the ranked list of the 28 classes based on the relevance scores. Does anyone have any input on better ways to do this or other state of the art models that can help me other than sentenceBERT?",
"date": "2022-08-01"
},
{
"vote": 2,
"title": "German Sentiment dataset",
"text": "I'm trying to build a model based on German datasets. I currently have my own dataset of tweets which I hand-labelled myself but I would like to compare its performance to that of actual datasets. I'm trying to use the SB-10K German Corpus dataset found \nhere\n. \n\n\nThe issue I have with it is that the dataset file only includes the label and the tweet ID. I have tried scraping them with my own script as well as the one provided by the SemEval GitHub \npage\n, but both return that the tweets are unavailable, for all 10000 tweets.\n\n\nThis is a bit of a long shot, but I'm hoping someone has access to the dataset to could try if they can retrieve the tweets via their tweet IDs. Getting good German datasets is crucial for my postgraduate thesis so any help would be much appreciated!",
"date": "2022-07-31"
},
{
"vote": 13,
"title": "When using BERT to do text classification (sentiment analysis) on news articles, should I do it sentence by sentence?",
"text": "I split the article into sentences and loop through them, then I average the sentiments over the number of sentences and classify the text with the highest average sentence sentiment score. (We only need the one most descriptive sentiment)\n\n\nIt is simple, and it seems to work, but should I have done it differently for better results?",
"date": "2022-07-30"
},
{
"vote": 1,
"title": "generalized operator precedence parser",
"text": "I made a language analysis system that works using neural nets and a generalized operator precedence parser. The results are interesting. This video has some highlights: \nhttps://www.youtube.com/watch?v=9SAzzmK13PA",
"date": "2022-07-29"
},
{
"vote": 2,
"title": "University Research Project",
"text": "Hi everyone, I'm a master's student at the University of Bath and I am conducting some research into the field of AI and software development. I've created a survey to get a better understanding of developer communities, how they work, and a few other questions about content. It is fully anonymous and the information collected will be used deleted once the project is over. It shouldn't take more than 5 minutes of your time and I appreciate any help that you guys could give me for this.\n\n\nhttps://form.jotform.com/akat2406/academic-research\n\n\nIf this has been flaired wrong or doesn't meet the subreddit rules please let me know and I will edit/take the post down\n\n\nIf you want to know more about the project feel free to message me and I can explain it in a more detailed manner, thanks again and hope you have a good day.",
"date": "2022-07-29"
},
{
"vote": 5,
"title": "Why is word2vec linear?",
"text": "I was trying to implement a simpler version of word2vec from scratch. While reading up on it, I realised that word2vec implementation does not use any non linearity! Why is it so good at creating embeddings without a non linearity? And isn't non linearity a crucial part of NN? How can a linear function capture complex relationships between words?",
"date": "2022-07-29"
},
{
"vote": 11,
"title": "New Google Smart Glasses Translate Speech Real Time",
"text": null,
"date": "2022-07-27"
},
{
"vote": 5,
"title": "In a rough spot with classifiers",
"text": "Hello everyone! \n\n\nI am a masters student working on my thesis. I have been in the weeds with self-taught NLP analysis for application in an extremely niche model. I have sentiment analysis working great through TextBlob, but the big issue I am facing is classifying whether or not a given sentence is one where an agent is talking about themselves or another. It gets quite nuanced and has run into some issues.\n\n\nHas anyone ever worked with a model like this?",
"date": "2022-07-27"
},
{
"vote": 11,
"title": "Help me understand Doc2Vec similarity scores",
"text": "I am assessing a bunch of fairly large financial disclosure documents (over 100 pages each and thousands of tokenized sentences each) and my hypothesis is that most of them share a high degree of similarity as they are based on similar templates. \n\n\nTo test this (in python) I am implementing gensim Doc2Vec by training a Doc2Vec vector on 1 base document and assessing (cosine) similarity of out-of-sample documents with the base doc. For some documents I get \"incredible\" similarity scores of over 0.9, for others even negative numbers and I am trying hard to get a better understanding of the vectors. \n\n\nFor example the out-of-sample document leading to a similarity score of 0.9 does not share a single identical sentence with the trained document (identical in the sense of comparing sentences in a loop), yet the similarity score is (imo rightfully) high since both documents have been issued by the same law firm and concern a highly similar financial product, thereby underlining the assumption that the providers just use templates. \n\n\nAny empirical input on the validity of Doc2Vec for performing similarity checks between 2 documents.",
"date": "2022-07-26"
},
{
"vote": 6,
"title": "could u suggest quick and practical nlp course/book (non-video)?",
"text": "Hi - could u suggest quick and practical nlp course/book?\nI prefer non-video courses as I can read significantly faster then watch videos.\nis there anything practical, less academic and more real life and structured start to finish to get up to (some) speed with language processing skillset? Thank you!\nI do have some college math and statistics so that's some start I guess.",
"date": "2022-07-26"
},
{
"vote": 1,
"title": "ðŸ’¥ How to use Hasty Annotation Tool",
"text": null,
"date": "2022-07-26"
},
{
"vote": 26,
"title": "NLP Demystified 13: Recurrent Neural Networks and Language Models",
"text": "Hello r/LanguageTechnology:\n\n\nI just published a new module for my \nfree course on NLP\n. In this module, you'll learn all about recurrent neural networks and how to use them to build language models for text generation.  \n\n\nhttps://www.nlpdemystified.org/course/recurrent-neural-networks\n\n\nWe'll cover:\n\n\n\n\nThe problem with bag-of-words approaches and how to address them using recurrence.\n\n\nSimple recurrent neural networks (RNNs) to process text as sequences.\n\n\nWhat language models are, how they're trained, and how to use them for text generation.\n\n\nHow to capture long-range dependencies using Long Short-Term Memory (LSTM) networks.\n\n\n\n\nIn the demo, we'll build a sequence tagger and a language model to generate text.\n\n\nColab notebook\n\n\nIf you find it useful, remember to sign up for course updates (no spam).",
"date": "2022-07-25"
},
{
"vote": 15,
"title": "[D] Running Large Language Models in Production: A look at Cohere's The Inference Framework (TIF)",
"text": null,
"date": "2022-07-25"
},
{
"vote": 1,
"title": "Advice on Approach",
"text": "Hello, r/LanguageTechnology.\n\n\nI'm looking to build a web application to demonstrate a text processing pipeline and I'm looking for advice/ideas on how to approach this project.\n\n\nThe idea is that the user will paste in a large blob of text and click 'submit'. Then the application will transform the text in step 1, display the result, and wait for the user to click 'next.' The application will repeat this basic process for X number of steps (likely less than 10). In the end, the application will offer to download the resulting text file to the user. For this version, the transformations will be relatively simple (i.e., change the text to lower case; text cleaning; spell check; etc.).\n\n\nI know one way to do this is with a Jupyter-style notebook, but my whole idea revolves around this being a web application. In a later version, I won't show every step (I'll just show the end result). But as I'm developing this, I want to be able to see what's happening.\n\n\nI'm wondering if others have built anything like this or if there is a commercial service that does this type of thing (i.e., an off-the-shelf solution, not a custom build-out).\n\n\nAny comments will be most appreciated.",
"date": "2022-07-25"
},
{
"vote": 1,
"title": "Byte Pair Encoding Tokenization in NLP",
"text": null,
"date": "2022-07-25"
},
{
"vote": 7,
"title": "What are all the 300-dimensional vectors used for google word2vec model?",
"text": "Hi\nIm new to nlp and just testing waters now with models, frameworks, techniques and related math.  \n\n\nIm looking at this model: \n\nhttps://code.google.com/archive/p/word2vec/\n\n\nCan someone explain what are all the \"300 embeddings\" in \n\nhttps://code.google.com/archive/p/word2vec/\n, \nmodel .bin can be found here: \nhttps://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n ?\n\n\nHere's the example for the word expected:  \n\n\nexpected 0.06347656 0.265625 0.06982422 -0.008728027 -0.17578125 0.012573242 0.18457031 -0.037841797 0.20898438 0.087890625 -0.064941406 0.125 0.06689\n453 -0.18359375 -0.07080078 0.033935547 0.052978516 -0.14257812 0.12597656 0.08544922 -0.33398438 0.018310547 0.043701172 -0.15917969 -0.06542969 -0.1\n2060547 -0.013122559 0.103027344 0.03564453 0.31835938 0.13378906 0.014038086 0.028320312 -0.16601562 -0.10839844 0.080566406 -0.099121094 0.015136719\n 0.079589844 -0.115234375 0.04272461 -0.37304688 0.23339844 0.09667969 -0.07714844 -0.31054688 0.009887695 -0.0022125244 -0.13476562 -0.17773438 0.083\n984375 -0.24707031 -0.08886719 0.34179688 0.16601562 0.25976562 -0.08496094 0.014587402 0.23242188 0.071777344 -0.359375 -0.15332031 -0.1640625 -0.267\n57812 0.110839844 0.21191406 -0.047851562 0.38671875 0.052001953 -0.0008354187 0.00049972534 -0.0058288574 0.21679688 0.26757812 -0.015991211 -0.08935\n547 0.234375 0.22558594 -0.018066406 -0.059814453 0.10058594 -0.15917969 -0.20898438 0.052490234 -0.12451172 -0.11816406 -0.096191406 0.011474609 0.05\n517578 -0.16699219 0.37695312 -0.17675781 -0.11767578 -0.13085938 -0.0010070801 -0.3359375 -0.025756836 -0.022949219 -0.032958984 -0.02368164 0.159179\n69 -0.060058594 0.031982422 -0.07373047 0.013793945 -0.05908203 -0.037841797 -0.08935547 0.027832031 0.03930664 0.15039062 0.14550781 -0.025634766 -0.\n12695312 -0.068847656 0.049804688 0.28125 -0.048339844 0.076660156 0.23632812 0.050048828 0.095703125 0.09277344 -0.15136719 0.024902344 -0.22070312 -\n0.10546875 -0.035888672 -0.32226562 0.083496094 0.16015625 0.07861328 0.08642578 0.032470703 -0.052490234 -0.107910156 0.047851562 0.08886719 0.057373\n047 0.072753906 -0.09277344 -0.19433594 0.110839844 -0.080078125 0.017333984 -0.12988281 0.08691406 -0.13867188 0.12451172 -0.23144531 -0.030517578 0.\n049316406 0.063964844 0.20019531 0.013183594 0.044433594 -0.16992188 0.18066406 -0.12890625 0.0040283203 0.23144531 0.03930664 0.23242188 0.07324219 0\n.18457031 -0.007873535 0.13867188 0.109375 0.11230469 -0.21386719 -0.13867188 -0.004852295 -0.038085938 -0.095214844 -0.0154418945 -0.10253906 0.14550781 -0.13183594 -0.10644531 -0.026977539 0.010620117 0.049316406 -0.13476562 0.1328125 -0.03857422 -0.11767578 -0.0072631836 0.015197754 0.31835938 0.109375 0.25 0.14941406 0.02746582 -0.24414062 -0.03955078 0.08935547 0.18066406 -0.024536133 -0.032714844 -0.18066406 0.103515625 0.01184082 -0.05908203 0.080078125 0.16113281 0.038085938 0.018676758 0.06591797 0.18457031 -0.06640625 -0.07324219 0.053710938 0.0703125 0.19042969 -0.15039062 -0.115234375 0.049316406 0.019165039 -0.017944336 0.22265625 0.16015625 -0.0069274902 -0.16796875 -0.13476562 0.0019454956 -0.24707031 0.045166016 0.18652344 0.051513672 -0.09033203 0.0036468506 0.14941406 0.11328125 0.044433594 0.106933594 0.055664062 -0.03515625 -0.22949219 0.10107422 0.103027344 -0.037597656 0.037597656 0.122558594 -0.19433594 -0.13574219 -0.03466797 0.017333984 0.020874023 0.38085938 -0.106933594 -0.14746094 0.04638672 0.17578125 0.095703125 0.20800781 0.059570312 0.067871094 -0.055664062 -0.080078125 -0.017822266 0.021118164 0.013549805 -0.06982422 0.12792969 0.13964844 -0.043701172 0.080078125 -0.024291992 0.123046875 0.12695312 0.20898438 -0.01574707 0.11035156 -0.33789062 0.15527344 -0.059814453 0.005493164 0.16503906 -0.033935547 0.16503906 -0.12402344 -0.14648438 -0.15917969 0.10205078 0.15136719 -0.17480469 0.18652344 0.045410156 0.3359375 0.103027344 -0.10498047 -0.12597656 -0.080078125 0.0071411133 0.17285156 0.21582031 -0.01977539 -0.09667969 0.15429688 -0.21679688",
"date": "2022-07-24"
},
{
"vote": 7,
"title": "Create a model to identify tweets referring to the user's location.",
"text": "Hi friends,\n\n\nI'm new to NLP.\n\n\nI'm trying to create a model to flag tweets containing information useful for identifying the user's location.\n\n\nThe model should consume texts on the input, and return a confidence score for the tweet to be relevant to identifying the user's location.\n\n\nFor example:\n \"born and raised here! Yankees forever!\" would have a higher confidence score than \"AWS has been down for a while now\" or \"my favorite cities are Paris, Milan, and London\"\n\n\nThe training data set may be found \nhere\n. It includes over 8M tweets, along with the user's geographical coordinates (latitude and longitude) and basic meta data, such as language and the time of post.\n\n\n&#x200B;\n\n\nHow should I approach this problem? What methods I can use to achieve the results?",
"date": "2022-07-24"
},
{
"vote": 6,
"title": "hey, i am working on an extrinsic plagiarism detector using python.. any help, ressources.. i appreciate it",
"text": null,
"date": "2022-07-23"
},
{
"vote": 5,
"title": "prediction logits using lxmert with hugging face library",
"text": "how can we get the prediction logits in the lxmert model using hugging face library? It's fairly easy to get in visualbert, but I'm not able to get it with the lxmert model. In case of visualbert model, the keys I'm getting are :\n\n\n[&#039;prediction_logits&#039;, &#039;seq_relationship_logits&#039;, &#039;attentions&#039;]\n\n\nand with the help of lxmert mode, the keys are :\n\n\n[&#039;language_output&#039;, &#039;vision_output&#039;, &#039;pooled_output&#039;, &#039;language_attentions&#039;, &#039;vision_attentions&#039;, &#039;cross_encoder_attentions&#039;]\n\n\nEven though there's a mention of prediction logits in the documentation I am not able to get them, if someone can help that would be great.",
"date": "2022-07-23"
},
{
"vote": 1,
"title": "Larger Embedding lower loss",
"text": "[deleted]",
"date": "2022-07-23"
},
{
"vote": 9,
"title": "What is the difference between adding words to a tokenizer and training a tokenizer?",
"text": "The title says it all. I was researching this question but couldn't find something useful. What is the difference between adding words to a tokenizer and training a tokenizer?",
"date": "2022-07-22"
},
{
"vote": 17,
"title": "Which is the best sentence-embedding technique for the following use case?",
"text": "I want to get some advice regarding how I should go about the following problem. Given 5 labels and 5 sentences I want to match each sentence to the most appropriate label. I believe that sentence embedding is the best technique for this, but these labels and sentences will be containing a lot of technical jargon which isn't as common in the pre-trained datasets. I have a collection of textbooks that I would like to use for transfer learning so that I can train my model on these technical phrases. Which sentence-embedding technique would best fit this use case whether its Doc2Vec or SBERT or Universal Sentence Encoder or something else. I am relatively new to the NLP space so any advice would be much appreciated.",
"date": "2022-07-21"
},
{
"vote": 0,
"title": "Recommend me an interesting nlp project",
"text": "Hi I am looking for some cool fun nlp ideas for a final year project i have to do. Whatever fun or crazy ideas you had but didn't have time to develop or maybe something you're currently working on, whatever it is - will be greatly appreciated!\n\n\nThanks.",
"date": "2022-07-20"
},
{
"vote": 7,
"title": "No-Code AI: Integrate the NLP Cloud API Into A Bubble.io App",
"text": "Hello,\n\n\nThanks  to the rise of no-code platforms like Bubble.io and the creation of  brand new cutting-edge AI models based on Transformers, like GPT-3, GPT-J, GPT-NeoX, Bart, and more, it is now possible to create advanced  AI applications without writing a single line of code.\n\n\nI just made an article that shows how to connect the \nNLP Cloud API\n to a \nBubble.io\n  application  in order to perform advanced AI operations like  summarization,  paraphrasing, NER, question answering, blog post  generation, product  description creation, and much more:\n\n\nhttps://nlpcloud.com/no-code-ai-integrate-nlp-cloud-api-into-bubble-io.html\n\n\nI hope that you will find this tuto useful!\n\n\nPlease don't hesitate to comment!\n\n\nJulien",
"date": "2022-07-20"
},
{
"vote": 0,
"title": "[D] Right Embedding for Named Entity Recognition for labelling English words and entities with a specific format extracted from OCR",
"text": "I have a problem that I'm working on currently which includes using Named Entity Recognition for text labelling outputted from OCR. \n\n\nSince the text is extracted from the OCR, it is highly prone to errors, especially when having handwriting and poor-quality scans. \n\n\nMy first concern is that when we have such a problem where sometimes the English words are not detected properly, letter O as 0, random spaces and so on, \nhow can word embeddings handle this?\n\n\nAnother issue I have is that, in addition to labelling some English entities, I want to extract entities like IDs which can follow a variety of formats like let's say: A-123-B15 or UIO-353-4513. And of course, due to the OCR, these can be read with random spaces in between, dashes as dots or spaces, letters as some other numbers (e.g. B as 8) and many other issues. For this kind of problem, \nwhat do you think is the best way to embeddings?\n I think it's better to use character-level embeddings with letters that are hot-encoded. But \nhow to handle this with the previous problem?\n\n\n&#x200B;\n\n\nI really appreciate your answer.\n\n\nThanks a lot.",
"date": "2022-07-20"
},
{
"vote": 11,
"title": "Please help understand example code from Hugging Face for GPT2",
"text": "tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n\n\nmodel = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)\n\n\noutputs = model(**inputs, labels=inputs[&quot;input_ids&quot;])\n\n\nloss = outputs.loss\n\n\nlogits = outputs.logits\n \n\n\n&#x200B;\n\n\nThe code above is from hugging face documentation. Assuming I have an optimizer and do something like:\n\n\nloss.backward()\n\n\noptimizer.step()\n \n\n\nAm I successfully \"finetuning\" the model with one input example? What exactly is happening in the line:\n\n\noutputs = model(**inputs, labels=inputs[&quot;input_ids&quot;])\n \n\n\nWhy is labels the input ids of the inputs? \n\n\nIs it training as follows? Given \"Hello\", next correct output is \"my\". Given my, next correct output is \"dog\", and so on? \n\n\nI want to fine-tune so that given input text sequence XXX, the model outputs one token. How could I fine-tune it that way? (I want to use LMHead, not sequence classification).",
"date": "2022-07-20"
},
{
"vote": 7,
"title": "Building an NLP project from scratch, starting with the project overview",
"text": "So I came across this tutorial on how to build an NLP project and I wanted to share it with you, I'm still new both on Reddit and in natural language processing so I'm trying to find the best tutorials to start with.\n\n\nUBIAI NLP project tutorial 01\n\n\nIf anyone of you guys has a recommendation for a beginner course, please suggest it!",
"date": "2022-07-19"
},
{
"vote": 1,
"title": "How to tag text as a different class? I have data describing in which someone describes their personality traits and want to separate between good and bad traits.",
"text": "The task is simply explainable but I imagine very hard. I have a large (1000+) recordings from a questionnaire in which people are asked to describe some of their good and bad personality traits and also what traits they would like to gain/work on. \n\n\nThe problem is that they are given one answer box for this. Some people start with listing their good, some with their bad ones, some list only their bad, some list only their good. \n\n\nWhen I read them I can separately them very easily, but because there is not much structure I am having trouble with building a simple rule based algorithm.\n\n\nThree examples:\n\n\n```\n\n\nOne thing that I would to improve is my social ability. I find it hard approaching people and starting a conversation with a stranger. Also I am quite lazy sometimes. One thing that I am very proud of is my perseverance as well as my loyalty to my pets.\n\n\n```\n\n\n``` \n\n\nI am not a person with great perserverance, this is something I must work on. Sometimes I can be very lazy and I do not study the subject enough. I have trouble getting started with learning and then I keep pushing it forward, until I get into a time crunch and then I get stressed out and the quality of my work suffers. One thing that I am happy with is my ability to connect with people and I consider myself a very social person.\n\n\n```\n\n\n```\n\n\nI am less satisfied with how much of a perfectionist I am. This can sometimes get in the way during work assignments because they will then take me too long. Besides that, I am a good listener and am empathetic with people.\n\n\n```\n\n\nI have written an algorhitm that basically checks for words such as `improve`, `satisfied`, `change` and then classify what follows either as negative or positive until a word is met that negates this and registers everything as positive from that point on. \n\n\nHowever, things still keep escaping or being misclassified. Negation is part of the problem as well as people who do not use words such as `improve`, `satisfied` but just write down some good and then some bad personality traits such as this:\n\n\n&#x200B;\n\n\n```\n\n\nI work very hard and make a lot of progress when I zoom in on something. I am a family man and love my partner.\n\n\n&#x200B;\n\n\nI think that I sometimes can be too stubborn for my own good. I feel that my perseverance is not what it used to be.\n\n\n```\n\n\nThese things then do not trigger anything on the algorithm (which is currently about 22% of all cases). All in all, I'm hoping for a better approach. I could manually tag a few hundred and then have a model extract the rest but I'm not sure if this is a good idea and where even to begin. Does anyone here have any recommendations?\n\n\nCheers! Appreciate all help because I am stuck in the mud :p :(\n\n\nThis will be part of a pipeline so doing it manually is not an option. One obvious option is to separate the good and bad parts from the questionnaire but for this task it is also not an option.",
"date": "2022-07-18"
},
{
"vote": 13,
"title": "No Language Left Behind Translation Model from Hugging Face Model Hub",
"text": null,
"date": "2022-07-18"
},
{
"vote": 3,
"title": "A lightweight javascript library for manual data masking.",
"text": null,
"date": "2022-07-18"
},
{
"vote": 6,
"title": "Create a Wine Recommender using NLP on AWS",
"text": null,
"date": "2022-07-18"
},
{
"vote": 26,
"title": "BERT-Large: Prune Once for DistilBERT Inference Performance",
"text": "BERT-Large (345 million parameters) is now faster than the much smaller DistilBERT (66 million parameters) all while retaining the accuracy of the much larger BERT-Large model! We made this possible with Intel Labs by applying cutting-edge sparsification and quantization research from their \nPrune Once For All paper\n and utilizing it in the \nDeepSparse engine.\n It makes BERT-Large 12x smaller while delivering 8x latency speedup on commodity CPUs. We open-sourced the research in \nSparseML\n; \nrun through the overview here and give it a try\n!",
"date": "2022-07-16"
},
{
"vote": 12,
"title": "Figurative Language Understanding Shared Task",
"text": "As a part of the Columbia NLP group, we are introducing a new shared task for figurative language understanding around the textual entailment paradigm, where the hypothesis is a sentence containing the figurative language expression (e.g., metaphor, sarcasm, idiom, simile) and the premise is a literal sentence containing the literal meaning.  \n\n\nThere are two important aspects of this task and the associated dataset: 1) the task requires not only to generate the label (entail/contradict) but also to generate a plausible explanation for the prediction; 2) the entail/contradict label and the exploration are related to the meaning of the figurative language expression.  \n\n\nDetails here: [\nhttps://figlang2022sharedtask.github.io/\n]  \n\n\nPlease feel free to participate and reach out to me for details  \n\n\nYou can register here: [\nhttps://codalab.lisn.upsaclay.fr/competitions/5908\n] \n#language\n \n#nlp",
"date": "2022-07-15"
},
{
"vote": 5,
"title": "Why ambient computing needs self-learning",
"text": null,
"date": "2022-07-15"
},
{
"vote": 1,
"title": "ðŸ’¥ Yolo Object Detection Tutorial TensorFlow | Complete Guide for Beginners Part #1",
"text": null,
"date": "2022-07-15"
},
{
"vote": 7,
"title": "Why are we training Segment Embedding in BERT?",
"text": "In BERT we have segment embeddings that are used for \"Segment Embeddings with shape (1, n, 768) which are vector representations to help BERT distinguish between paired input sequences.\"\n\n\nYes, but why. There are just 2 sentences, why are we making it so complicated and using 768-sized vector representation for 0 or 1? And we are adding them to the token and positional embeddings.\n\n\nSo it will be like :\n\n\nSegment Embeddings:\n\n\n[0.321,0.231,...,0.434,0.312,0.123]\n\n\nPosition Embeddings:\n\n\n[0.123,0.6435,...,0.231,0.121,0.321]\n\n\nEven If we sum those embeddings, summation will be like any embedding. How this summation will make the model distinguish between paired input sequences?",
"date": "2022-07-15"
},
{
"vote": 3,
"title": "Running GPT-NeoX-20B With Hugging Face",
"text": "GPT-NeoX-20B is the largest public GPT model, but unless you have a GPU with nearly 48GB+ VRAM it can be difficult to run through Hugging Face.  This video goes over how to run this massive model, with ideally someone having nearly 48GB of total Vram(on 1 or more GPUS) but also allowing one to run the model with a single 3090(though slowly).\n\n\nhttps://www.youtube.com/watch?v=2o1_HVZr8Vs&ab_channel=Blake",
"date": "2022-07-14"
},
{
"vote": 1,
"title": "How do transformers differentiate tokens and use them?",
"text": "\"[SEP] tokens are useful to differentiate the questions from answers through type_ids\" Yes, but how is this helping model to understand that \"I should look paragraph and generate answers from here\"? We don't have if-else inside the model that will say: \"if type_id==1, generate questions from here\"\n\n\nThe same question appears for this example too:\n\n\n[CLS] previous question [Q] current question [SEP] text [EOS]\n\n\nHow model says: \"I should look at the previous question, understand the meaning from here too with the current question, and answer the question.\" We need if-else in here too like:\n\n\nIf there is a previous question: get the meaning from it and use it with the current question\n\n\nOne more example based on this paper:\nhttps://arxiv.org/pdf/2005.01107v1.pdf\n In this paper, we have a dataset like this: \nhttps://huggingface.co/datasets/iarfmoose/question_generator\n\n\nIn the t5 transformer, if we don't have those and tokens, the model will not learn anything. And I have no idea how specifying and we are helping the model to generate questions based on that answer in the context.\n\n\nI hope I am clear with my question.",
"date": "2022-07-14"
},
{
"vote": 4,
"title": "How to build a model for detecting \"intents\" (tags based on input text as Watson assistant) in text",
"text": "Hi, new in this world, I have good experience on programming, but not in IA, and not sure how to look for this kind of structure for a model that based on a list of trained tags return which is the more closely related to a text \n\n\nIs more about learning how is made that implement that, I'm looking for help about where to look for info on how to build it or how I look for that\n\n\nThanks",
"date": "2022-07-14"
},
{
"vote": 5,
"title": "Meta AI Introduces the First Model Capable of Automatically Verifying Hundreds of Thousands of Citations at Once",
"text": "Wikipedia is the most used encyclopaedia of all time for research, background information, and other purposes. It has over 6.5 million articles. The question of how to verify the accuracy of the information always lingers in the background, despite how easily accessible it is. Although Wikipedia is crowdsourced, which mandates that facts be verified, all quotations, contentious assertions, and information about living individuals must include a citation. However, keeping up with the more than 17,000 new articles uploaded each month is challenging for volunteers due to the ever-increasing amount of material available. The discovery that a US teen wrote 27,000 entries in a language they did not speak in 2020 is one incident that shows that online encyclopaedias are not a perfect source of information. Although malicious attempts to edit Wikipedia articles occasionally occur, factual errors are typically the result of a well-meaning person making a mistake.\n\n\nContinue reading\n| \nCheckout the\n \ngithub",
"date": "2022-07-13"
},
{
"vote": 1,
"title": "How special tokens in BERT-Transformers work?",
"text": "I was trying to understand how tokens work and all I understood is that tokens are the representation of the input in a more meaningful way (data preparation for the \"encoder of transformer\" or \"BERT\").\n\n\nBut when i see use of special tokens like this: \nhttps://arxiv.org/pdf/2005.01107v1.pdf\n, i realised that you can actually \"specify\" your purpose while training your data.\n\n\nFor example, in an answer in \nStackOverflow\n it says :\n\n\n\" Just an example, in extractive conversational question-answering it is not unusual to add the question and answer of the previous dialog-turn to your input to provide some context for your model. Those previous dialog turns are separated with special tokens from the current question. Sometimes people use the separator token of the model or introduce new special tokens. The following is an example with a new special token [Q]\"\n\n\n[CLS] previous question [Q] current question [SEP] text [EOS]\n\n\n\nBut it doesnt explain how any NLP model can use and can be trained in these tokens.  \n\n\nHow is it being training such a way that it understands that \" i should be aware of previous question to answer current question\" ?",
"date": "2022-07-12"
},
{
"vote": 37,
"title": "BigScience AI Researchers Open-Source â€˜BLOOMâ€™: An Autoregressive Multilingual Large Language Model Larger Than GPT-3 and OPT-175B",
"text": "BigScience Project introduces BLOOM (BigScience Large Open-science Open-access Multilingual Language Model), the first multilingual Large Language Model (LLM) trained in complete transparency by the largest group of AI academics. Unlike the traditional secrecy of industrial AI research laboratories, the project demonstrates the possibility of training promising AI models published by the larger research community responsibly and openly.\n\n\nâœ… Transformers-based LLM \n\n\nâœ… 176B parameters (larger than GPT-3 and OPT-175B)\n\n\nâœ… Trained on 1.6TB text data, the equivalent of 320 times the complete works of Shakespeare\n\n\nContinue reading\n | \nDownload",
"date": "2022-07-12"
},
{
"vote": 1,
"title": "Converting csv data to fastText readable data",
"text": "[deleted]",
"date": "2022-07-12"
},
{
"vote": 1,
"title": "Run transformers model inference in C/C++ and even assembly",
"text": "[deleted]",
"date": "2022-07-12"
},
{
"vote": 14,
"title": "Meta AI's NLLB (No Language Left Behind) Inference Code on Colab",
"text": null,
"date": "2022-07-11"
},
{
"vote": 14,
"title": "What does it mean to freeze the encoder?",
"text": "Lately I have been reading papers related to question answering and information retrieval. Papers like DPR, REALM, RAG and etc. some of them mention freezing the document encoder and then using it later on at query time.\nAs an example, in this \ncolber-QA\n paper, section 2.4, page 5, they say:\n\"ORQA, REALM, and RAG \nfreeze\n their document encoder (and the indexed vectors) when fine-tuning for OpenQA, which restricts the adaptability of the model to this task and/or to new corpora\"\n\n\nWhat does this freezing the encoder mean here?",
"date": "2022-07-11"
},
{
"vote": 3,
"title": "The Modern Tokenization Stack for NLP: Byte Pair Encoding",
"text": null,
"date": "2022-07-11"
},
{
"vote": 1,
"title": "Data for model training",
"text": "I am looking to train a model and am considering using the sentiment dataset from TweetEval. However, it is not processed and contains tweets like:\n\n\n>u/user #MLBCentral David Wright crushing a dinger on his first swing since April 14th. I didn't think he would pull a Wilmer Flores!  \n\n\nI have my own tweets which I will want to perform an analysis on and they have gone through a data cleaning process. My gut feeling says that I should train the model on cleaned data itself because that's the type of data the model will operate on. However, looking through many tutorials and guides,  they do not train with cleaned data. Is this something I should just experiment with based on the various models I am looking at or should the training/test/validation data always be cleaned.\n\n\nPutting the above tweet through my cleaning function, I get the following:\n\n\n>david wright crushing dinger first swing since april 14th think would pull wilmer flores\n\n\nI might change the function so it will remove the date since it doesn't contain much sentiment.",
"date": "2022-07-11"
},
{
"vote": 2,
"title": "Options for Unsupervised Multi-Topic Classification",
"text": "Hi Everyone. Iâ€™m hoping to get some suggestions on a problem Iâ€™m trying to solve. I have a dataset that contains unlabeled review comments on products from customers, and Iâ€™m trying to extract the topics from it to get more insights. \n\n\nIâ€™m currently using a method by just simply extracting keywords after cleaning, lemmatizing etc. For example, I give a topic of â€˜things brokenâ€™ if there are keywords break, not work etc. It works ok, but you can easily tell some shortages such as false positive and false negative words. \n\n\nI know there are many single-topic topic modeling models available out there, but is there a better way to extract multi-topic topics? For example, if a review says:â€The shipping was very slow. It took a month to be delivered. The noise is also very loud.â€ Iâ€™d like to have topics such as shipping and noise to be extracted for it. Any advice is appreciated. Thanks!",
"date": "2022-07-10"
},
{
"vote": 12,
"title": "Highlights for every NAACL 2022 paper",
"text": "Here is the list of all NAACL 2022 (The North American Chapter of the Association for Computational Linguistics) papers, and a short highlight for each of them.\n\n\nhttps://www.paperdigest.org/2022/07/naacl-2022-highlights\n\n\nNAACL 2022 will take place on July 10 2022.",
"date": "2022-07-10"
},
{
"vote": 21,
"title": "AI Researchers From Korea Introduce â€˜DailyTalkâ€™, A High-Quality Conversational Speech Dataset Designed For Text-To-Speech",
"text": "The most important thing for a Text-to-Speech TTS system is to save and communicate the context of the present discourse. Current TTS models have context representation constraints since they perceive each speech independently of the address. The lack of open-source datasets, including spoken dialogues, is one of the key reasons why most earlier studies focused on single utterances.\n\n\nMany popular TTS datasets exist. However, they contain minimal conversation and comprise reading-style utterances in which speakers record audio by reading books or scripts. Some audio corpora derived from real-world conversations or behaviors are publicly accessible, although they have various drawbacks, such as background noise or uneven recording quality. Some recent research offered context-aware TTS models, although they used an internal dataset that is not available to the public.\n\n\nâœ… Claimed to be the first public dataset for conversational Text-To-Speech\n\n\nâœ… A dataset for text-to-speech which can represent various features and situations including conversation\n\n\nâœ… The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license\n\n\nContinue reading\n | \nCheckout the\n \npaper\n \nand\n \ngithub",
"date": "2022-07-10"
},
{
"vote": 1,
"title": "NLP Python Discord",
"text": "[removed]",
"date": "2022-07-10"
},
{
"vote": 1,
"title": "Language Files: Materials for an Introduction to Language and Linguistics 12th{PDF}",
"text": null,
"date": "2022-07-08"
},
{
"vote": 0,
"title": "10x OpenAI Cost Savings - One Line Change",
"text": null,
"date": "2022-07-08"
},
{
"vote": 4,
"title": "What is the norm / good strategy to find and contact the professors in NAACL that I'm interested in PhD?",
"text": "I'm going to attend NAACL '22. I'm wondering how to find the professors that will attend in NAACL (it's easy to know how give workshop and talks, but not that easy to know who will just attend) and how to contact the ones I'm interested in PhD (e.g. cold email to have virutal/in-person coffee chat) ?",
"date": "2022-07-07"
},
{
"vote": 0,
"title": "A quick guide to Amazonâ€™s 45-plus NAACL papers",
"text": null,
"date": "2022-07-07"
},
{
"vote": 4,
"title": "Q&amp;A with OpenAI",
"text": "Hi all, \ncheck out this app\n using OpenAI to answer questions about PyTorch, TF, HuggingFace, and Streamlit. I wish I could claim some sort of ownership over how impressive some of these answers are, but all I did was plug in the OpenAI API, and send queries and contexts retrieved via Pinecone. If you want to see how I pulled it all together, \ncheck out the video\n!\n\n\nIf you have questions let me know!",
"date": "2022-07-07"
},
{
"vote": 1,
"title": "ðŸ”¥ Create A Object Tracker from Scratch with Opencv In 5 min!",
"text": null,
"date": "2022-07-07"
},
{
"vote": 5,
"title": "Which opensource or other API you recommend suitable for Audio -&gt; Text -&gt; Audio?",
"text": "Purpose is to have a feature built using NLP API which can either take a continuous Audio stream and convert it to respective spoken language Text or use audio clip file and if required convert system generated response from Text to Audio.\n\n\nMajor part will be Audio to Text;  Text to audio may not be so frequently used.\n\n\nLanguages in consideration  English to start with.\n\n\nThanks in Advance.",
"date": "2022-07-07"
},
{
"vote": 1,
"title": "As PRO Act Stalls, US Labor Department Rethinks Status of Independent Contractors",
"text": null,
"date": "2022-07-07"
},
{
"vote": 1,
"title": "Regression with input text?",
"text": "Which model or algorithm to train when we have as input a text, and float number (rate) as a label?\nThank you",
"date": "2022-07-06"
},
{
"vote": 3,
"title": "A Tutorial on Using Using Neural Style PT to Transfer the Style of One Image to Another",
"text": "[deleted]",
"date": "2022-07-06"
},
{
"vote": 23,
"title": "Timeline of major developments in NLP - looking for sources",
"text": "Can anyone recommend a review or paper recapitulating the most important developments in NLP in the 20th and 21st century? Thanks a bunch",
"date": "2022-07-06"
},
{
"vote": 8,
"title": "Bilingual Sentiment Analysis Model",
"text": "I have been working and researching on how to make this. But there are some confusing processes and some are more advanced. \n\n\nEx. Filipino and English or\nEnglish and your own language.\n\n\nIt can classify whether the text is positive or negative.\n\n\nI am asking if you are going to do it,\nWhat approach will you guys do?\n\n\nLike what text process should I go for?\n\n\nLabeled Data sets >\nStop words >\nBag of Words >\nTokenized >\n vectorization >\nTF-IDF >\nClassification algo(SVM,NaiveBayes or BERT)\n\n\nPlease correct me. I am still learning and don't know which one to use or which is compatible. Thank youðŸ˜‡",
"date": "2022-07-06"
},
{
"vote": 19,
"title": "Why is there very limited NLP research involving Reinforcement Learning?",
"text": "I would really like to know if there are some major setbacks in using RL for NLP. I tried searching for papers but did not find many.",
"date": "2022-07-05"
},
{
"vote": 2,
"title": "Best classification strategies for training millions of short text without any pretrained data (from scratch)?",
"text": "[deleted]",
"date": "2022-07-05"
},
{
"vote": 3,
"title": "Looking for a mentor! Recommended courses?",
"text": "Hello. I would like to talk to an NLP Engineer, NLP Data Scientist or Computational Linguist to help mentor me on my career path!!\nI got my B.A. in Linguistics, and I'm trying to figure out how to develop my career... I found out about NLP, and I really want to learn about it! \nHowever, I don't have any background in tech at all. \n\n\nBefore going to graduate school, I figured I had better do some learning by myself to prepare myself. I found these courses.... I would like to ask for advice on what courses to take and in what order to take them.\n\n\nhttps://lnkd.in/gqpf89Jc\n\n\nhttps://lnkd.in/g44jWdn6\n\n\nhttps://lnkd.in/gNFEJ_sz\n\n\nhttps://lnkd.in/gW5XkBQd\n\n\nhttps://lnkd.in/geVGrnCU\n\n\nWhere should I start?\n\n\nI want to learn how to collect, process, analyze, and visualize language data. I also want to dip my feet into AI. \nI am interested in collecting and analyzing linguistic data in order to draw conclusions and make data-driven decisions.\nI am also interested in things like improving dictionaries and language learning apps as well as translation and speech recognition.\nOnce I complete a course/courses, I would like to choose a research topic and take it to the graduate level, or take an internship somewhere.\n\n\nThank you so much for your help. I'm so grateful for any advice.\nPlease let me know if you have any other suggestions. \n\n\nKim",
"date": "2022-07-05"
},
{
"vote": 31,
"title": "Google AI Introduces Minerva: A Natural Language Processing (NLP) Model That Solves Mathematical Questions",
"text": "Large language models are widely adopted in a range of natural language tasks, such as question-answering, common sense reasoning, and summarization. These models, however, have had difficulty with tasks requiring quantitative reasoning, such as resolving issues in mathematics, physics, and engineering.\n\n\nResearchers find quantitative reasoning an intriguing application for language models as they put language models to the test in various ways. The ability to accurately parse a query with normal language and mathematical notation, remember pertinent formulas and constants and produce step-by-step answers requiring numerical computations and symbolic manipulation are necessary for solving mathematical and scientific problems. Therefore, scientists have believed that machine learning models will require significant improvements in model architecture and training methods to solve such reasoning problems.Â \n\n\nContinue reading\n | \nCheckout the\n \npaper",
"date": "2022-07-05"
},
{
"vote": 2,
"title": "How to get comprehension level on the text",
"text": "Hello, I have no experience at all in NLP, but I am currently working on a book evaluation system, the system currently evaluates books depending on 2 things; first, the word count. Secondly, the vocabulary level. But, I want the system to consider comprehension level as well, in order to make my evaluation more accurate.\n\n\nThese are the comprehension levels that I know:\n\n\nhttps://drive.google.com/file/d/1OceH_JQZiAPJSuZulF2-uUEy-t0-7xtp/view?usp=sharing\n\n\nSo, I need help on how to add the comprehensive level to the book evaluation process, if anyone has any tips/ideas please let me know, Thank you.",
"date": "2022-07-05"
},
{
"vote": 9,
"title": "Our Open Source Text Annotator",
"text": "We started an open source text annotator to compete/compliment the likes like doccano and prodigy. For some reasons we had to stop it. I thought I would share it here in case anyone likes to take it from here.\n\n\nhttps://github.com/aminooei/TextMark",
"date": "2022-07-04"
},
{
"vote": 6,
"title": "Inter-rater Reliability for NLP Tagging",
"text": "Does anyone know of any formal processes for conducting IRR for NLP tagging? Our lab deals a lot with data that needs to be tagged, and the tagging schemes are often very subjective. As such, we are wondering if anyone can point us in the direction of an established IRR process for NLP tagging. Thanks!",
"date": "2022-07-04"
},
{
"vote": 1,
"title": "Automate Your Translation Language Translation Skills",
"text": "[removed]",
"date": "2022-07-04"
},
{
"vote": 13,
"title": "Datasets in spanish?",
"text": "Hello,\n\n\nI am using some pre-trained models and translating the result to spanish because I can't find a good conversational spanish dataset to fine-tuning microsoft/DialoGPT-large.\n\n\nCan you give me some ideas about where and how can i get this datasets?\n\n\nThank you in advance",
"date": "2022-07-03"
},
{
"vote": 3,
"title": "Help me choose the best technology for my current home project, please",
"text": "I am working on my home project: a discord bot which should gather information after my english classes and sum up all new content in one table (glossary). Therefore, I've decided to create a smart information extractor. Can you advise me the technology which would solve my task the best?\n\n\nMore information about the task:\n\n\nDiscord chat contains following types of messages:\n\n\n\n\nWords and its definitions/IPA (phonetic spelling)/translations. There could be all of the listed options or any subset of them. In any order, with various delimiters. I need to identify what is what in order to put it in one of the glossary's columns: word, definition, IPA, translation.\n\n\n\n\nEx.: \nto stage a rebellion = to start a group who's gonna rebel\nbounce around â€” exchange\nobviously â€” surely / naturally / clearly\n\n\nresilience - [ri'ziliÉ™n(t)s]\nspring /Å‹/\nwaltz  /wÉ‘Ëls/\n\n\n\n\nGrammar mistakes: sentences with crossed words or in bold. Such sentences should be marked and gathered in another table.\n\n\n\n\nEx.:\nPreviously my bf \nwas robbed\n  HAD BEEN ROBBED  OF â‚¬2000\n\nsit\n at home --> stay\nif she listenED attentivelyâ€¦\n\n\n\n\nGarbage. Various messages from teacher and students that shouldn't be included in the result.\n\n\n\n\nFirstly, I thought about writing a formal grammar, but latter I doubted my choice. Every teacher has his own unique style, so polishing the grammar for each of them seems to be too tedious. So, I am open to your suggestions. Thank you in advance!",
"date": "2022-07-03"
},
{
"vote": 4,
"title": "Are you a high schooler or middle schooler interested in linguistics? Come to the 2nd Annual Linguistics League Summer Tournament! STARTS NEXT WEEK!",
"text": null,
"date": "2022-07-01"
},
{
"vote": 13,
"title": "Generate webpage summary images with DALL-E mini",
"text": null,
"date": "2022-07-01"
},
{
"vote": 0,
"title": "CityFALCON, the Spotify of financial content, raises $2m from a client, TBH, Holt, and Seedrs",
"text": null,
"date": "2022-07-01"
},
{
"vote": 11,
"title": "Paraphrase Generator",
"text": "Does anyone know a good API or pre-trained model that I can use to generate paraphrases? I want to use it in order to train sentence embeddings with contrastive loss. Any help or pointers would be great!",
"date": "2022-06-30"
},
{
"vote": 0,
"title": "Named entitiy tagging",
"text": "Hello , I was wondering if there are any sources you have found helpful where I could learn Named entitiy tagging for free  ?",
"date": "2022-06-29"
},
{
"vote": 2,
"title": "Issues with TWARC",
"text": "Has anyone who has used twarc to extract Twitter conversations found that a majority of tweets are duplicate? Is there a woraround for it? Say I extract 50000 tweets related to a particular topic, hardly 4000-5000 are unique. Also, why isn' t the tool able to extract deep conversations?",
"date": "2022-06-29"
},
{
"vote": 9,
"title": "Is there an agreed-upon hierarchy of which LMs are better for word embeddings?",
"text": "For context: my research is in applied ML, but not NLP. I'm running experiments to test if, in my field, word/document embeddings from transformers are better than word2vec. As of now, I'm using BERT, but it seems \nthat was a poor choice\n. Which brings me to this sub's expertise--is there a class of transformers (T5, RoBERTa, etc.) that are better for embeddings that are used in downstream tasks?",
"date": "2022-06-29"
},
{
"vote": 18,
"title": "Yandex Open-Sources YaLM Model With 100 Billion Parameters",
"text": "Transformers are used for translation and text summarising tasks because they can analyze sequential input data, such as natural language. Transformers use the self-attention process and weights the importance of each component of the input data differently. Large-scale transformer-based language models have gained a lot of popularity recently in the disciplines of computer vision and natural language processing (NLP).\n\n\nThey expand in size and complexity frequently, yet it costs millions of dollars, hires the greatest experts, and takes years to construct these models. Because of this, many companies have been unable to use it, and only significant IT organizations have access to this cutting-edge technology.\n\n\nTo address these problems, \nYandex has developed the largest\n \nYaLM model to date, which uses 100 billion parameters\n. This largest GPT-like neural network for English is currently available for free. The researchers used a pool of 800 A100 graphics cards, 1.7 TB of online materials, books, and countless other sources to train the model over the course of 65 days. They have published the model and relevant materials on GitHub under the Apache 2.0 license, allowing both academic and commercial use.Â \n\n\nContinue reading\n | \nGithub",
"date": "2022-06-28"
},
{
"vote": 3,
"title": "Next run of the co:rise NLP cohort starts July 11th",
"text": "If you'd like a discount use REDDIT50 at check-out",
"date": "2022-06-28"
},
{
"vote": 8,
"title": "Evaluation measures for search and recommendation systems",
"text": "Hi all, I wanted to share a \"deep dive\" article I wrote covering some of the most \npopular evaluation measures (specifically offline metrics) for search and recommendation systems\n, eg retrieval. I was finding it difficult to find a single place that spoke of all of these in an easy-to-read way, so I hope this helps! It covers:\n\n\n\n\nRecall@K, Precision@K\n\n\nMAP@K\n\n\nMRR\n\n\nNDCG@K\n\n\n\n\nLet me know what you think, thanks!",
"date": "2022-06-28"
},
{
"vote": 0,
"title": "why chinese (and japanese) languages are damn brilliant",
"text": "[deleted]",
"date": "2022-06-27"
},
{
"vote": 3,
"title": "Generating Images from Text Prompts with VQGAN-Clip, Python, and TensorFlow [Tutorial]",
"text": "[deleted]",
"date": "2022-06-27"
},
{
"vote": 0,
"title": "Any news on GPT-4?",
"text": "When will GPT-4 be released?",
"date": "2022-06-27"
},
{
"vote": 8,
"title": "Zero-Shot Relation Extraction from Text as a Natural Language Inference Task",
"text": null,
"date": "2022-06-26"
},
{
"vote": 2,
"title": "Noisy Data",
"text": "[deleted]",
"date": "2022-06-25"
},
{
"vote": 14,
"title": "Topic Identification of a Single Scentence/Tweet",
"text": "Hi,\n\n\nI'm relatively new to NLP and have to use it in my Masters dissertation. In my project I'm exploring if NLP can be used to create reports about a user from thier Twtter account so that they can better understand their exposure online. The report would include information, such as their interests and political stances. (All the participants of in the study are aware of the data I'm collecting on them btw).\n\n\nI want to collect tweets from a given user and for each of their tweets identify the topic/subject, then determine their sentiment towards that topic. The aim of this is to calculate the average sentiment score of each topic to determine what the user likes and dislikes the most. However, I canâ€™t seem to find any unsupervised models for finding the topic/subject of a single tweet (which is typically 1-2 sentences long). One thing Iâ€™ve looked at is combining the tweets into a single body of text and then using an LDA model. The issue with this is that combining the tweets means I canâ€™t determine the average sentiment towards each topic. Ideally, I wouldnâ€™t want to use a classifier/pretrained model.\n\n\nDoes anyone know of a suitable model? Also, Iâ€™m open to suggestions on other ways I could get this to work, so please let me know if you can think of anything.\n\n\n&#x200B;\n\n\nUPDATE:\n\n\nEnded up going for a pretrained BERT model, optimised for zero-shot classification.",
"date": "2022-06-25"
},
{
"vote": 5,
"title": "Possible to Auto generate a multiple choice question and 4 answer options?",
"text": "Hello, wondering if it is possible to automatically create a series of multiple-choice questions from data (say wiki data) and offer 4 answers (3 being incorrect, one being correct)...? The challenge is also to make sure that 3 of 4 options have relevance to the question too. If it is possible I'd love some nuggets on how to go about it. Thank you!",
"date": "2022-06-25"
},
{
"vote": 2,
"title": "Project Guidance",
"text": "Hello , I am currently working on a large project where  one of my tasks is  to extract the generic name of a medical product , for example the product could be 'NICOTEX 2MG TABLETS' ,and I have to extract the word NICOTEX from it . I've got around 8000 values in the data set , I was wondering what would be a smart way to extract all this information from the dataset.\n\n\nI am currently thinking of manually annotating some of the values inorder to build a training set using Spacy. Any suggestions would be appreciated.",
"date": "2022-06-25"
},
{
"vote": 6,
"title": "Best approach for predicting special entity based on surrounding words?",
"text": "[deleted]",
"date": "2022-06-24"
},
{
"vote": 1,
"title": "Learning Python from Scratch for Qualitative Data Analysis",
"text": "Hi all! I work at a massive company doing qualitative research where we mine mass amounts of unstructured data (open ended survey responses) to deliver insights to other parts of the business. We are wanting to start utilizing Python for this purpose, but the problem is that neither myself or my coworker who have been tasked with this have any real programming knowledge and don't know anything about Python. I have dabbled with using R for data analysis, but I know it's different. \n\n\nI put together a list of Coursera Python courses that I think are relevant to this topic. My boss is suggesting I learn Python to the point that it becomes my job, so I figured I should probably understand the ins and outs as much as possible. Could anyone possibly look at this plan and see if it makes sense? Is it missing anything? Is it too in depth? Are there better options for any of them? I have no idea what a lot of the course topics are or if they are really relevant.\n\n\nP.S. My coworker and I both have M.S. degrees in a research and stats focused field (I/O Psychology) in case it is helpful in satisfying some of the prerequisites.\n\n\nCourses\n\n\n\n\nPython for Data Science, AI & Development\n\n\nApplied Machine Learning in Python\n\n\nApplied Text Mining in Python\n\n\nNeural Networks and Deep Learning\n\n\nNatural Language Processing with Classification and Vector Spaces\n\n\nNatural Language Processing with Probabilistic Models\n\n\nNatural Language Processing with Sequence Models\n\n\n\n\nI am assuming it will become obvious at some point after I have learned enough when to stop or where I can rely on packages like spaCy and NLTK?",
"date": "2022-06-24"
},
{
"vote": 0,
"title": "Why Googleâ€™s LaMDA AI is conscious: Suspended Google engineer Blake Lemoine speaks out in first podcast interview",
"text": null,
"date": "2022-06-24"
},
{
"vote": 1,
"title": "ðŸ’¥ How to use Pooled Task Feature in UBIAI",
"text": null,
"date": "2022-06-24"
},
{
"vote": 2,
"title": "New direction for NLP?",
"text": "I'm seeing that NLP models calculate word embeddings in relationship to each other, which makes each given model kind of parrot the material on which it was trained.\n\n\nYes, the king - man + woman = queen example is always cool, but wouldn't it be even cooler to type in \"apple, banana\", returning \"fruits\"? Or \"list all known fruits\"...\n\n\nGoogle returns websites that list fruits, but then some are overlapping and incomplete.\n\n\nAnyone any thoughts about the first steps for word groupings models?\n\n\nEdit:\n\n\nAn example: category word (i.e. hypernym): fruits, category set: apple, orange, banana, ...\n\n\nTo a botanist a tomato is a fruit, to nutritionist a tomato is a vegetable, which now calls for the following expansion:\n\n\nuser: botanist, category word: fruits, category set: tomato, apple, orange, banana, ...\n\n\nuser: nutritionist, category word: fruits, category set: apple, orange, banana, ...\n\n\nEdit 2:\n\n\nAnother way to look at sets is:\n\n\n[tomato, [fruit, botanist], [vegetable, nutritionist]]\n\n\n[apple, fruit]\n\n\n[orange, fruit]\n\n\n[banana, fruit]",
"date": "2022-06-24"
},
{
"vote": 3,
"title": "Semantic Search for Ctrl+F",
"text": "Hi Reddit,\n\n\nScout Search is a Find-in-Page replacement that uses semantic search (rather than character matching) to help you find what you're looking for on websites.\n\n\nTry it out and let me know what you think.\n\n\nhttps://chrome.google.com/webstore/detail/scout-search/hgljpodblkjjklailoaefokflfdeffdl",
"date": "2022-06-23"
},
{
"vote": 0,
"title": "Thereâ€™s a better way to get your ML test metrics",
"text": null,
"date": "2022-06-23"
},
{
"vote": 3,
"title": "Evaluating NER HuggingFace models for a domain",
"text": "Hi everyone,\n\n\nI'm comparing off-the-shelf NER systems to one another to see how they perform on literary-historical data (more specifically: a set of books from the 17th century -19th century). I'm not training or improving the models, but trying to use the ones which are available to see how they perform, to decide if they can later be used in historical research and information extraction contexts.\n\n\nI think I understand how to evaluate tools such as spaCy and NLTK, by transforming the output labels into the formats required by e.g. the Python packages nervaluate and seqeval. These both return quantitative metrics (F1, precision, recall,...) necessary to evaluate how the models perform on this data type/domain.\n\n\nI'm not experienced with HuggingFace/transformers and it's quite hard to find sources that have done this before (or is it just me?). I'm wondering if there's an \"elegant\" way to evaluate these models for a domain. Does it make sense to do it the same way as I evaluated spaCy and NLTK, as specified below?\n\n\nAt my disposal: a small gold standard dataset labelled with the \"location\" entity (IOB2).\n\n\nSteps:\n\n\n\n\nalign tokenizations and labels of the gold standard and the model output to create two lists of equal length (using the package pytokenizations).\n\n\nmap labels (e.g.: spaCy's \"LOC\" & \"GPE\" become \"LOCATION\", as in the gold standard data).\n\n\ncalculate metrics using nervaluate/seqeval.\n\n\n\n\nIt seems so convoluted to apply this methodology, but I haven't been able to find a better way. Am I overlooking or not grasping something? Is there an amazing evaluation package or research on evaluation methods of transformers which I don't know about? \n\n\nThank you for your help!",
"date": "2022-06-22"
},
{
"vote": 4,
"title": "Topic Modeling: How best to Qualitatively Code results",
"text": "Hello everyone, \n\n\nSo i'm working on a project where I've run a LDA TFIDF topic model from telegram documents, and have identified 4 categories that achieve a degree of human interpretatability (we opted for this over using additional metrics like coherence score because our goal was to be able to understand the narratives of the topics in a human-understandable way). \n\n\nNow, we're thinking about doing a content analysis of the results within each category, so that we can have a deeper exploration of the results. For developing the coding, we were thinking about building the codes one of two ways:\n\n\n\n\nA random subsample of documents from each category, OR\n\n\nA random subsample of documents from each category \nabove a certain inclusion score threshold, e.g. score>0.8\n.\n\n\n\n\nThe impetus for the latter approach is that it will allow us to define our categories based on more or less exemplars. However, I'm not sure if this is methodologically \"pure\" (although the more I look into these methods, the less pure it all becomes!). \n\n\nIs there anyone out there who has done similar things? What was your rationale?\n\n\nThank you!",
"date": "2022-06-22"
},
{
"vote": 7,
"title": "[Research] Data Labeling Research",
"text": "Hi! \n\n\nI'm doing some market research for a data labeling product and want to ask the actual people in the industry (you) for opinions and what \nactually\n is the reality for the industry. Any/all responses are super helpful, so thank you in advance if you answer/are able to answer my questions. \n\n\n\n\nDoes your company use a data labeling tool? If so, what? If not a specific tool, how do you label your data? \n\n\nWho actually does the labeling? Is it engineers? Outsourced? Someone on Fiverr? \n\n\nAre you aware of data labeling tools that exist on the market? If so can you name a company or two that comes to mind? \n\n\nWhat is the single greatest issue/missing functionality of a current tool you use (if you have one)? Feel free to mention the tool, if that helps add context to the data medium (text, audio, video, image).   \n\n\nI'm currently trying to determine what the most important product features are for text/audio labeling, what would those be for you? (e.g. a specific use case, UI/UX functionality, integrations, automation, etc.)\n\n\nWhat do you think is a fair price for a tool to do data labeling? (specifically text/audio)\n\n\n\n\nEven if you can only answer one or several questions, all responses are extremely helpful! Again, thank you so much for your time and for the help.",
"date": "2022-06-22"
},
{
"vote": 2,
"title": "Where to get started with this project/what approach to use?",
"text": "Hi there,\n\n\nI am interested in doing a project that uses NLP+ML to answer/generate multiple choice questions.\nFor the model training, I have ~200 textual excerpts, and ~10 multiple choice questions (with 4 potential answers) for each textual excerpt.\n\n\nI know the answers to all of the questions, and the first stage of my project is a program that I can feed new textual excerpts + questions to, and then it can pick the right answer from the choices.\n\n\nMy end goal is that I am able to feed the model a textual excerpt, and then it is able to generate a set of questions and answers from the text that look similar to the questions it was trained on.\n\n\nThere are various question types, from asking about the structure of the textual excerpt, interactions between characters, descriptions, etc. Here is an example:\n\n\nA central idea discussed in the passage is that\nA) articulating the reasons for holding an opinion can cause people to decide that they are wrong.\nB) the process of describing an issue in detail can make people more moderate in their views about the issue.\nC) most people are not truly interested in understanding complex ideas.\nD) people are likely to understate their most passionately held positions to avoid offending others.\n\n\nI have a good amount of experience in data-based ML, but am fairly new to NLP and am looking for where to get started with this. Thanks so much!",
"date": "2022-06-21"
},
{
"vote": 1,
"title": "Looking for websites/apps to check English phrase relevancy wrt to location",
"text": "Hello,\nCan anyone help me with a website to learn if a particular English saying or phrase is relevant or popular in a specific country or not?\nFor Eg - Is \"home-cooking\" a commonly used word in the US?",
"date": "2022-06-21"
},
{
"vote": 7,
"title": "Looking for the ability to parse notes, and generate summaries on different objects/topics in the source corpus",
"text": "Hi there. I'm interested in creating a text summariser which can translate historical account-style notes into wiki-style summaries, filtered by different topics. For example, here's an account of a fictional history.\n\n\nFor example:\n\n\n\n\nDoc 1: \"The mayor of the city dispatched a contingent of guards to the outlying village to investigate the rumblings of a potential rebellion against the mayor. During their stay at a local tavern, a guard got into an altercation with the son of the barkeep, Halvard. Halvard's son was badly injured - the town healer says he will be permanently scarred. The guards were chased out of town...\" \n\n\n\n\nDoc 2: \"Halvard was a tall, balding man clad in armour who was the very popular tavern owner in his village. He stood at the iron gates of Fellborne, drunk, with a crowd of villagers behind him, rattling his sword against his shield, and demanding the mayor of the city, Folkr, pay him and his homestead a weregild for the dishonour his family had suffered at the hands of his guards...\"\n\n\n\n\nDoc 3: Folkr offered to facilitate a dual to the death between Halvard and the guard who caused the injury. Halvard fought angrily and drunkenly, and though a tough opponent, was eventually slain by Folkr's guard. The villagers, mournful and resentful, broke out in riot. The mayor ordered the guards to retreat inside the city walls, but in the chaos, many died. The surviving villagers returned to their rebellious villages, and the event eventually went on to be the flashpoint of the Weregild Rebellion, in which the mayor was deposed, much of the city burned, and several thousand died.\"\n\n\n\n\n\n\nI would like the ability to highlight certain detected Named Entities (detecting the named entities and their categories is a separate problem, let's assume we have defined them manually for now) and use them to generate wiki pages from the document corpus. For example, it might, amongst others, detect \"Halvard\", \"Fellborne\", and \"Weregild Rebellion\" as named entities, and create some example wiki pages accordingly:\n\n\n\n\nHalvard: Halvard was a tall, balding popular tavern owner who died in a dual against Folkr's guard. Havlard had demanded a weregild from Folkr for the injury of his son at the hand of the guard. \n\n\n\n\nFellborne: Fellborne is a city with iron gates. It was run by an unpopular mayor. Much of the city was destroyed in the Weregild Rebellion. \n\n\n\n\nWeregild Rebellion: The Weregild Rebellion was sparked against the unpopular mayor Folkr by angry villagers after an altercation between it's guards and Halvard's son, which left him scarred. Folkr was deposed and much of Fellborne was destroyed.\n\n\n\n\n\n\nThe model to summarise knowledge should not assume external world knowledge. It should correctly understand that Fellborne is a city, be able to categorise it such, but not introduce any real world city knowledge into the summary. For example, I should be able to alter the name to \"London\" and it would treat the city exactly the same. It should, however, know what a city is.. so this is quite tricky. \n\n\nHas any work gone into a problem like this before?",
"date": "2022-06-20"
},
{
"vote": 2,
"title": "Seeking Email Dataset",
"text": "Seeking an email dataset to train a couple classification models with, the more the better. My plan is to just make a simple binary classifier, pending on manually labeling (one-vs-all model) a subset of emails. \n\n\nAny suggestions on where I can find a treasure trove of emails would be great! Thanks!",
"date": "2022-06-20"
},
{
"vote": 6,
"title": "In this article, we showcase how to automate your data labeling using transformer models.",
"text": null,
"date": "2022-06-20"
},
{
"vote": 5,
"title": "Movie Genre Prediction with Transformers",
"text": null,
"date": "2022-06-20"
},
{
"vote": 1,
"title": "ðŸ”¥ Create an Image Classifier with OpenCV and Jupyter Lab | TensorFlow image Classifier",
"text": null,
"date": "2022-06-20"
},
{
"vote": 2,
"title": "Fine-tuning BERT for NER with custom label",
"text": "[deleted]",
"date": "2022-06-19"
},
{
"vote": 6,
"title": "NLP Question Answer Models",
"text": "I'm trying to create a model that generates answers based off the question supplied by the user. I thought the question/answer model was going to be it, only to realize that it creates answers by finding the context in an article.   \n\n\nBut now, I'm wondering what model I should look for or if it's even possible to create one that can learn and generalize my answers based on different questions.",
"date": "2022-06-19"
},
{
"vote": 10,
"title": "Researchers Introduce â€˜RANKGENâ€™: A Deep Encoder Model (1.2B Parameters) Which Maps Prefixes And Generations From Any Pretrained English Language Model To A Shared Vector Space",
"text": "Language models (LM) are widely used to assign probabilities to the text. Current language models frequently give a high probability to the output sequences that are repetitious, nonsensical, or unrelated to the prefix given as an input sequence, thus resulting in the model-generated text containing such artifacts. To solve this problem, google researchers have proposed the RANKGEN, a 1.2 billion parameter encoder model. It translates both human-written and model-generated continuations of the prefixes to a common vector space RANKGEN that performs the dot product of the generations with the prefix and provides the rank to determine compatibility between a particular prefix and generations from any external LM.Â \n\n\nThe significant contribution of the research is to utilize large-scale contrastive learning to train RANKGEN, which encourages prefixes to be closer to their gold continuation and to be far away from wrong negatives, as depicted in figure 1. The main goal is to forecast two sequences rather than a single token for prediction. Thus RANKGEN is inspired to intend long-distance associations between the prefix and continuation rather than depending on local context.\n\n\nContinue reading\n | \nCheckout the\n \npaper\n,\n \ngithub",
"date": "2022-06-18"
},
{
"vote": 1,
"title": "Zero-shot Text Classification with Hugging Face ðŸ¤— on Gradient",
"text": null,
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How to generate sentence embeddings using Transformers for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How to generate sentence embeddings using Transformers for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How to generate sentence embeddings using Transformers for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How to generate sentence embeddings using Transformers for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How do I generate sentence embeddings using a Transformer model for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "How do I generate sentence embeddings using a Transformer model for a regression task?",
"text": "[deleted]",
"date": "2022-06-17"
},
{
"vote": 10,
"title": "Text classification with transformers &amp; post-processing with discourse parsing",
"text": "We put together a pipeline for refining predictions of BERT (or any other sequential text classifier) on documents with complex discourse using RST parser. Tested it on stance detection and premise classification tasks. Found helpful, although there aren't many realy long examples in the given dataset.\n\n\nRepo: \nhttps://github.com/tchewik/discourse-aware-classification",
"date": "2022-06-17"
},
{
"vote": 0,
"title": "Can someone help!",
"text": "So I have been working on this project, where I had to find similarity between two data sets and match the columns, but they don't have any semantic meaning (for example finding similarity between \"LPO 1615 City Bus\" and \"LPO 1615 Bus\", where they indicate the same thing and I need to match them). There are around 10,000 rows. So I tried creating my own embeddings but it didn't have much efficiency, is there another method I can try?",
"date": "2022-06-16"
},
{
"vote": 4,
"title": "Can someone help me please",
"text": "Basically i need to build a nlp model where  \n\n\n\n\nA user will provide dataset to me in the form of raw text\n\n\nI need to train the model on that raw text\n\n\nThen i need to divide the dataset into sentences\n\n\nThen the user gives a query then i need to find the most similar sentence to it\n\n\n\n\nI have already tried many pre-trained models like BERT,Infersent etc but im unable to figure out how to fine tune the model on my own dataset \n\n\nAny kind of help is appreciated\nPS : im new to this sub please forgive any kind of mistake",
"date": "2022-06-16"
},
{
"vote": 3,
"title": "How To Run DALL-E Mini On Your Own PC(Windows Version)",
"text": "DALLE-Mini has become really popular, so much so that the public access points are commonly overrun with traffic. Using the video below, one can run DALL-E Mini on their own Windows computer, with or without a GPU! No more waiting on some overrun third-party service!\n\n\nhttps://www.youtube.com/watch?v=OqEuEe-xSKk",
"date": "2022-06-16"
},
{
"vote": 11,
"title": "Chomsky, Finite State Automata and Naturale Language Grammar",
"text": "Hi! Is there anyone who could provide me a clear explanation of the reason why finite-state automata are not adequate for illustrating a natural language grammar? In particular I'm referring to Chomsky, 1957 (\"Three models for the description of language\"). I found it hard to understand the mathematical explanation at the beginning: any advice or recommended bibliography?",
"date": "2022-06-16"
},
{
"vote": 1,
"title": "ðŸ’¥ The Basic Branches of AI Explained for Beginners",
"text": null,
"date": "2022-06-16"
},
{
"vote": 3,
"title": "Salesforce Researchers Open-Source â€˜Taichiâ€™: A Python Library For Few-Shot NL",
"text": "Although FSL is a very active area of study with a wide range of potential applications, data scientists and software engineers have not had easy access to commercially available, user-friendly libraries for speedy exploration.\n\n\nThe well-known Chinese martial art of Tai Chi emphasizes developing â€œsmart strength,â€ such as using joints as levers to generate significant power with little effort.\n\n\nThe Salesforce research team found it very inspiring how this mindset of Tai Chi meshes so well with few-shot learning (FSL) research, where the goal is to train models with good performance with little data. Inspired by this, they created an FSL library, which employs clever techniques to get good performance with minimal effort. They hope it may aid others in their model training in low-data settings.\n\n\nâœ… Tai Chi philosophy applied to machine learning (Result: one can train models even if only a few examples are available)\n\n\nâœ… Beginner-friendly yet powerful (Doesnâ€™t require users to have high degree of knowledge about FSL)\n\n\nâœ… TaiChi 1.0, contains two main FSL methods: DNNC and USLP\n\n\nContinue reading\n | \nCheckout the\n \ngithub\n,\n \npaper 1\n,\n \npaper\n \n2",
"date": "2022-06-16"
},
{
"vote": 2,
"title": "HELP PLEASE",
"text": "Hi colleagues I really need help here, I graduated from faculty of arts English dep. Then I decided to get into Computational linguistics, I started to take courses in ml, Dl, and nlp.\nI applied for many jobs and internships but unfortunately always rejected.\nI need a help how to get an internship\nIs this difficult for me?\nI'm so disappointed please help ðŸ˜”\n\n\nBTW I'm from Egypt",
"date": "2022-06-16"
},
{
"vote": 2,
"title": "Finding/Collecting a dictionary of verbs that describe human motion in english",
"text": "Such as runningï¼Œjumpingï¼Œclimbingï¼Œdrinkingï¼Œetc.  It can also be sentences or expressions that contain such wordsï¼Œlike riding a horse and driving a car.",
"date": "2022-06-15"
},
{
"vote": 1,
"title": "Recommendation for online courses",
"text": "Can anyone recommend me some good courses for nlp tasks, python-programming and data-science in generell?\nI've managed to build a sentiment-analysis prototype for my company using spacy and streamlit, but i feel like i've reinvented the whell way too often. \n\n\nI want to learn some best-practise for programming and nlp task like topic-modelling, sentiment-analysis and word-embeddings",
"date": "2022-06-15"
},
{
"vote": 10,
"title": "Is there a solution to my problem? Finding the most used combination of words in a list of sentences.",
"text": "Hello everyone. I want to find the most used combination of words within a list of sentences. As an example I have these sentences:\n\n\nI have a pretty house.\n\n\nI used to own a pretty house.\n\n\nI was getting a pretty village.\n\n\nAt the end I want to have an output similar to this:\n\n\n  dictionary = {\n    &quot;a pretty&quot;: 3,\n    &quot;I&quot;: 3,\n    &quot;a pretty house&quot;: 2,\n    &quot;a&quot;: 3,\n    &quot;I have&quot;: 1,\n    &quot;I used&quot;: 1,\n    &quot;I was&quot;: 1,\n    &quot;I have a&quot;: 1,\n    &quot;I used to&quot;: 1,\n     etc...\n   }\n\n\n\nMy end goal is to be able to analyse many sentences which have a similar structure and to be able to run the program which then tells me which combination of words of any length was the most common. Is there a name for this problem or does there exist a library or should I just program it with pure python?",
"date": "2022-06-15"
},
{
"vote": 9,
"title": "Topic Modeling with Lyrics",
"text": "Hey I'm kinda a newbie to topic modeling (and ML/NLP in general), so I was wondering if people could help me make sense of this situation...\n\n\nI compiled about 350 song lyrics, and for each one I scraped the comments off of \nsongmeanings.com\n. I tried to run them through LDA topic modeling. I didn't see anything interesting in the results. It basically just thinks the songs are all just \"love love yeah yeah oh oh...\" - I know I know, my uncle's been saying the same thing for years, but obviously songs *do* have topics even if they're shallow ones- love, breakups, parties, inspirational, etc..\n\n\nSo I thought maybe I just have too little data for LDA, so I tried using a Biterm Topic Modeler. After nearly a day of waiting it produced better but still mostly uninteresting results. I posted the LDAvis HTML here: \nhttps://songisms.vercel.app/topics.html\n (this one has the songmeanings data, so it has a lot of words like hate, listen, think, meaning, etc.)\n\n\nI tried with between 5-20 topics and the results weren't different. Is this just not a good use case for topic modeling, or is there a better way to approach it?",
"date": "2022-06-15"
},
{
"vote": 6,
"title": "Meta AI Research Releases A Direct Speech-To-Speech Translation (S2ST) Approach That Enables Faster Inference And Supports Translation Between Unwritten Languages",
"text": "A new Meta AI research introduces a direct speech-to-speech translation (S2ST) approach that enables faster inference and supports translation between unwritten languages. This method works better than earlier methods since it does not rely on text generation as an intermediate step, in contrast to prior methods. The team claims that this is the first direct S2ST system trained using actual, publicly available audio data instead of synthetic audio for numerous language pairs.\n\n\nPresent-day speech-to-speech systems immediately convert source speech into target speech spectrograms, which are multidimensional continuous-value vector representations of the frequency spectrum. However, because translation models must learn numerous distinct facets of the relationship between two languages, it might be challenging to train them using voice spectrograms as the aim.\n\n\nThe researchers used discretized speech units instead of spectrograms, which they derived by clustering self-supervised speech representations. Unlike spectrograms, discrete units can take advantage of existing natural language processing modeling techniques and decouple linguistic content from prosodic speech information. They claim to have made three important improvements using discretized speech units:Â \n\n\n\n\nThe S2ST system performs better than earlier direct S2ST systems\n\n\nIt is the first direct S2ST system to be trained on real S2ST data for many language pairings\n\n\nIt makes use of pretraining with unlabeled speech data.\n\n\n\n\nContinue reading\n | \nCheckout the\n \npaper 1\n,\n \npaper 2\n,\n \npaper 3\n \nand\n \ngithub",
"date": "2022-06-14"
},
{
"vote": 21,
"title": "We Made Autocomplete for Reddit!",
"text": "https://reddit.com/link/vc9xwa/video/44stvuwfpm591/player\n\n\nHey y'all, we created a Chrome Extension (HyperWrite) that gives suggestions as you type, and can help complete your thoughts or give you new ideas when you're stuck.\n\n\nWe're still developing this, but have gotten solid feedback from our integrations with Gmail, Medium, and ~15 other sites. We just added Reddit compatibility and would love to hear your feedback - hope a post like this is allowed!\n\n\nNote: After you download the extension, you will have to turn on your Reddit integration on your dashboard - let us know if you have any issues.\n\n\nLinks: \nhyperwriteai.com\n or \nchrome.google.com/webstore/detail/hyperwrite/kljjoeapehcmaphfcjkmbhkinoaopdnd",
"date": "2022-06-14"
},
{
"vote": 1,
"title": "Does weak-labeled data match up in accuracy to hand-labeled data? This article provides a hands-on exploration of this issue",
"text": null,
"date": "2022-06-14"
},
{
"vote": 3,
"title": "Linguistics courses for a career in NLP research",
"text": "I'm a second-year CS major and I think that in the future I want to do some form of AI research. I find NLP very interesting and may decide to focus on that, so I was wondering how beneficial it would be to take some basic linguistics courses, possibly as far as a linguistics minor (though I don't know if I'd actually be able to fit that in), with that in mind? Are these courses easily applicable to NLP research, or would my time be better spent on more mathy and computational courses?",
"date": "2022-06-14"
},
{
"vote": 3,
"title": "Why use BIO in Ner",
"text": "I am studyin Ner and all models use the BIO tags. I just don't get the point of using it?",
"date": "2022-06-14"
},
{
"vote": 1,
"title": "ðŸ’¥ Create A Object Tracker from Scratch with Opencv In 5 min!",
"text": null,
"date": "2022-06-14"
},
{
"vote": 1,
"title": "AI detecting how well you pronounce American English",
"text": "Hello folks,  \n\n\nFYI we've created a quick and free English pronunciation test: \nhttp://testyourenglishpronounciation.com\n\n\nIt assesses how well you pronounce each sound and give detailed results! =)",
"date": "2022-06-13"
},
{
"vote": 1,
"title": "ðŸ”¥ Cosine Similarity and Cosine Distance",
"text": null,
"date": "2022-06-13"
},
{
"vote": 15,
"title": "New Amazon AI Study Introduces Pyramid-BERT To Reduce Complexity via Successive Core-set based Token Selection",
"text": "âš¡ The researchers replaced previously used heuristics with a coreset based token selection method justified by theoretical results.  \n\n\nâš¡ While comparing Pyramid-BERT to several state-of-the-art techniques for making BERT models more efficient, the research group was able to speed inference up 3- to 3.5-fold while suffering an accuracy drop of only 1.5%, whereas, at the same speeds, the best existing method loses 2.5% of its accuracy.\n\n\nContinue reading\n | \nCheck out the\n \npaper\n \nand\n \npost",
"date": "2022-06-12"
},
{
"vote": 44,
"title": "Spotify Research Open-Sources â€˜Basic Pitchâ€™: A Machine Learning Tool For Converting Audio Into MIDI",
"text": "Basic Pitch offers a number of advantages:\n\n\nðŸ‘‰ Polyphonic + instrument-agnostic:Â Unlike most other note-detection algorithms, Basic Pitch can track multiple notes at a time and across various instruments, including piano, guitar, and ocarina. Many systems limit users to only monophonic output (one note at a time, like a single vocal melody), or are built for only one kind of instrument.\n\n\nðŸ‘‰ Pitch bend detection:Â Instruments, like guitar or the human voice, allow for more expressiveness through pitch bending: vibrato, glissando, bends, slides, etc. However, this valuable information is often lost when turning audio into MIDI. Basic Pitch supports this right out of the box.\n\n\nðŸ‘‰ Speed:Â Basic Pitch is light on resources, and is able to run faster than real time on most modern computersÂ \n\n\nContinue reading\n \n| Check out the\n \npaper\n,\n \ngithub\n,\n \nproject\n \nand\n \npost",
"date": "2022-06-11"
},
{
"vote": 30,
"title": "Lessons learnt after my Masters degree in NLP (Germany)",
"text": "I wrote a blogpost about my experience and what I learnt during my Masters degree in NLP in Munich, Germany.\n\n\nI hope you like it! ðŸ‘‰ \nhttps://anebz.eu/lessons-masters-nlp",
"date": "2022-06-10"
},
{
"vote": 9,
"title": "Blogpost: how to detect sarcasm with NLP",
"text": "I wrote a blogpost about my research during my Masters degree, it's about detecting sarcasm with NLP, the different methods that have been used in research in the previous years, and the current methods.\n\n\nI hope you like it! âž¡ï¸ \nhttps://anebz.eu/sarcasm-detection-nlp",
"date": "2022-06-10"
},
{
"vote": 16,
"title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
"text": null,
"date": "2022-06-10"
},
{
"vote": 8,
"title": "Any suggestions for handling multiple word expression(MWE) in English NLP?",
"text": "Hi. Currently I am working on English new transcript NLP task for my paper. I have done text analysis with Korean before but it is my first time to doing it in English text.\n\n\nSo... I figured that python's NLTK and Spacy and Flariy have quite good performance about pos tagging and named entity recognition. But I am still having trouble parsing multiple word expressions such as United States and Health Care. Since I might conduct semantic analysis after, I am afraid that I can not skip this process. What I want to do is identifying MWE's only for quite commonly used. (So it is okay to do not capture new coinages or slangs)\n\n\nSince MWE is quite important step in English NLP, I think people already have made some cool package to deal with this. But I just haven't found it yet... I have tried Spacy's noun chunking and gensim's phrase detection but none of them gave me a perfect result.\n\n\nI may not need fancy machine learning based language models but only need a MWE dictionary list...But, I've googled a lot in the past two weeks and couldn't find it.\n\n\nSorry for my English and any advice would be appreciated.",
"date": "2022-06-09"
},
{
"vote": 1,
"title": "End to End Question Answer model",
"text": "Hi there,I'm looking for an end-to-end question answering model. Currently, I'm using salient extracting from the questions + user utterance, and then using tf-idf to get the answer with the highest score, but this doesn't work well.. I would like to know if you know anything open-source/service that I can give it a user utterance + a set of questions and answers, and then it returns the answer with the highest score, and it should work quite fast (less than a second).\n\n\nThanks in advance!",
"date": "2022-06-08"
},
{
"vote": 2,
"title": "Lexical logogenesis in English",
"text": null,
"date": "2022-06-08"
},
{
"vote": 1,
"title": "ðŸ˜¥A silly question about BPE?? Does \"@@\" appear in model vacabulary?",
"text": "Hi there! I have a silly question about BPE algorithm.\n\n\nFor example, when word \"actor\" is divided into \"act@@ or\" and put into a model. Will it be represented as \"act or\" or \"act@@ or\"?\n\n\nOr for \"act\" and \"act@@\", will their vector be identical or different as model inputs?",
"date": "2022-06-07"
},
{
"vote": 1,
"title": "ðŸ”¥ How to Train A Wild Fire Detector with Yolov5 on Google Colab",
"text": null,
"date": "2022-06-07"
},
{
"vote": 6,
"title": "Machine Learning Research Paper Selection Survey",
"text": "Hello, I am hoping to get responses for a survey on how an individual in the machine learning community chooses a research paper. The responses will be used for a research project in a user experience design class that I am currently taking. All responses will be anonymously collected.\n\n\nI will aggregate the findings and share them with my user experience design class and any Reddit thread that has allowed me to post the survey to it.\n\n\nThe survey should only take 2-3 minutes to complete and there are only 7 questions.\n\n\nAgain, your responses are anonymous.\n\n\nIf you are interested, please consider responding to the survey here: \nhttps://forms.gle/x68hN3NeVGbWV3DXA",
"date": "2022-06-07"
},
{
"vote": 0,
"title": "Gathering people",
"text": "Hello, \n\n\nI am currently looking for undergraduate/graduate students who are interested in joining eBay hackathon, which is the link below.\n\n\nhttps://eval.ai/challenge/1733/overview\n\n\nPlease look it up, and if you are interested, I want to make a team to be in high performance.\n\n\nLet me briefly introduce myself. Right now, I am a prospective transfer student in Virginia tech majoring in computer science, and I am very interested in natural language processing (NLP). I am an undergraduate student, so might donâ€™t have much knowledge but I am a fast learner, and strongly want to learn natural language\nProcessing. \n\n\nIf you are interested, I want you to give me direct message for more info. Thanks!",
"date": "2022-06-07"
},
{
"vote": 1,
"title": "20 Beginner-Friendly Text datasets to hone your Natural Language Processing skills",
"text": "[removed]",
"date": "2022-06-06"
},
{
"vote": 1,
"title": "Need help with medical event detection project. Can anyone help?",
"text": "[removed]",
"date": "2022-06-05"
},
{
"vote": 2,
"title": "What is the difference between DPR and REALM",
"text": "I went through both papers and I cant find any big difference between them. All i could find was:\n\n\n\n\nREALM uses different pertaining techniques (like prohibiting trivial retrievals, initiation, salient masking and null document).\n\n\nas far as i understood, DPR is faster only because they used FAISS. is that right?\n\n\n\n\nIs there any other difference?",
"date": "2022-06-04"
},
{
"vote": 21,
"title": "Learn how to use Evaluate â€” Hugging Face's newest Python library",
"text": null,
"date": "2022-06-04"
},
{
"vote": 12,
"title": "Amazon AI Researchers Propose A New Model, Called RescoreBERT, That Trains A BERT Rescoring Model With Discriminative Objective Functions And Improves ASR Rescoring",
"text": "ðŸ‘‰ While BERT trained with MLM distillation can improve WER by 3%-6% relative to LSTM, RescoreBERT, trained with a discriminative objective, can improve it by 7%-13% on the same test sets.\n\n\nThe RescoreBERT modelâ€™s key component is a technique called rescoring. The second-pass language model trained from scratch on a small quantity of data can prioritize and accurately rerank the hypotheses of rare words thanks to the rescoring technique. Amazonâ€™s prior work has been integrated to lower the computational expense of computing PLL scores. This is accomplished by feeding the output of the BERT model through a neural network trained to mimic the PLL scores awarded by a more significant â€œteacherâ€ model. Because the distilled model is trained to match the teacherâ€™s predictions of masked inputs, this process is known as MLM (masked language model) distillation. The distilled modelâ€™s output is interpolated with the original score to obtain a final score. This method minimizes latency by condensing PLL scores from a big BERT model to a much smaller BERT model.\n\n\nContinue reading\n | \nCheck out the\n \npaper",
"date": "2022-06-04"
},
{
"vote": 10,
"title": "A Clockwork RNN",
"text": null,
"date": "2022-06-04"
},
{
"vote": 7,
"title": "In this article, we show you how to use Bert transformer with spacy3 to train joint entities and relation extraction classifier",
"text": null,
"date": "2022-06-02"
},
{
"vote": 1,
"title": "ðŸ”¥ How to fine-tune a transformers model for paraphrasing applications?",
"text": null,
"date": "2022-06-02"
},
{
"vote": 0,
"title": "hi, im a student",
"text": "i need something like  \nsynthesia.io\n  but for free, it can be without avatar, just a natural text to speech that i can edit for my school video project",
"date": "2022-06-02"
},
{
"vote": 1,
"title": "NER / POS",
"text": "Is it possible to use standard named entity recognition techniques for part of speech tagging?\n\n\nWhat are those techniques?\n\n\nI am currently learning unsupervised clustering using skip gram embeddings.\n\n\nDoes NER have any standard technique? I assume itâ€™s usually supervised?\n\n\nSo for supervised POS tagging you would start with some human labeled data for parts of speech?\n\n\nThank you",
"date": "2022-06-01"
},
{
"vote": 0,
"title": "Challenge to assess NLP Engineer's skill",
"text": "[deleted]",
"date": "2022-06-01"
},
{
"vote": 5,
"title": "Simplified understanding of this article",
"text": "This is a technique that really interests me:\n\n\nhttps://arxiv.org/pdf/1503.06760.pdf\n\n\nUnsupervised POS clustering.\n\n\nIt appears they use â€œskip gram embeddingsâ€, a form of word embedding which is able to capture syntactic â€œnearnessâ€, as opposed to semantic.\n\n\nCould anyone please help me distill this paper to a simple outline of what they did and how it works? Perhaps some simple Python code to achieve a similar effect? I see they used word2vec.\n\n\nThank you very much",
"date": "2022-06-01"
},
{
"vote": 1,
"title": "ðŸ”¥ How to Setup a Age Detector with Opencv in 5 min!",
"text": null,
"date": "2022-06-01"
},
{
"vote": 12,
"title": "Fine-tuning pre-trained model for Multi-label classification with a huge dataset",
"text": "Hello,\n\n\nI have about 19 million paragraphs labeled with about 7k labels.\n\n\nI want to train a bert model with transformers or simple transformers. I tried some of them with test datasets and want to do the same with my dataset.\n\n\nBut most of the papers/notebooks/samples/guides use multi-hot encoding for the labels and puts it them all in a single pandas dataframe.\n\n\nI tried using sklearn MultiLabelBinarizer for the encoding but the problem i am facing is that if i go that route 19m x 7k x int32 takes 500+ gb of memory, i only have 144, 40gb of which is already taken up by my data that is in a pandas dataframe.\n\n\nI can manually encode the dataset without having to do it in one go or using MultiLabelBinarizer and write to disk. My questions are:\n\n\n\n\nIs there an de-facto accepted way to stream/lazy load this data, any samples for transformers?\n\n\nLike, can i just load my daha in chunks and train the model, save it, then train it further with another chunk of data. Is there a difference between that and just doing it in one go?\n\n\n\n\nMy questions might just be an example of \nXY Problem\n, so if there is a more logical way to tackle this 7k labels problem i am open to suggestions. Some people suggested zero-shot classifiers, but all my labels are like X90-A-120 or G5-J-90, and i am not sure how to \"fine-tune\" a one-shot classifier.",
"date": "2022-05-31"
},
{
"vote": 1,
"title": "ðŸ”¥ How to Using Pinecone to get Semantically Similar QnA from Google's Natural Questions Dataset",
"text": null,
"date": "2022-05-30"
},
{
"vote": 12,
"title": "NLP-centric programming language",
"text": "It seems like programming languages can get developed for very tailored, niche functionalities like Solidity, a language that natively supports blockchain functionalities.\n\n\nHas anyone considered an NLP programming language? It could natively support extremely sophisticated and effective natural language processing - a wide variety of string processing techniques, built-in Regex patterns, and much else.\n\n\nThanks very much",
"date": "2022-05-29"
},
{
"vote": 14,
"title": "Interesting article outlining the current state of GPT-3 use in text generation.",
"text": null,
"date": "2022-05-29"
},
{
"vote": 3,
"title": "To expand or not to expand?",
"text": "Hello all!\n\n\nI am working on a project that will utilizes a naive bayes classifier for text classification. The goal is to classify text reports in the domain of power plant systems into varying outage length. In this process I am experimenting with a bag-of-words model and was not sure how to incorporate abbreviations/acronyms? \n\n\nIn this domain there are many different types of valves, pumps, systems, etc. Would it be best to keep them in their current 2-4 letter abbreviations? I am concerned that by expanding them, I would be capturing too much of a \"microview' of the system components. \n\n\nFor example, 'rpv' typically stands for 'reactor pressure vessel'. By expanding this, the BOW model wont be able to preserve the system of interest and would instead only see 'reactor', 'pressure', 'vessel'. There are many systems in a power plant that share these words and I am not sure if it is worth the trouble expanding the acronyms. \n\n\nThanks for the insight everyone!",
"date": "2022-05-28"
},
{
"vote": 8,
"title": "CoAuthor: A Human-AI Collaborative Writing Dataset For Improving Language Tools",
"text": "Large language models (LMs) provide novel opportunities for interface design. Large language models have undoubtedly advanced to the point where they may be compared to a genuine writer. The models do an excellent job of comprehending the subject matter. Recent LMs (such as GPT-2 and GPT-3) can create a wide range of prose and conversations with unrivaled fluency. These models may be fine-tuned to become more skilled at specific activities, such as email composition or health consultations.\n\n\nLanguage models may greatly assist humans in their writing processes. People have already begun to incorporate these technologies into their workflows, with some publications being created using these tools. Along these lines, Stanford researchers created \nCoAuthor\n: an interface, dataset, and experiment all in one.\n\n\nAccording to the researchers, these technologies work best when supplementing rather than replacing human writing. The objective was not to develop a system that could help users write better and quicker but rather to aid in the writing process and research the successes and failures of such systems. At the same time, users work, CoAuthor logs writing sessions key by key and create an extensive database. As the writer starts typing, he or she can press the â€œtabâ€ key, and the system will provide five GPT-3-generated recommendations. The researchers employed over 60 people to generate over 1,440 stories and articles, each supported by CoAuthor.\n\n\nContinue Reading\n | \nCheck out the\n \npaper\n,\n \nblog\n \nand\n \nproject",
"date": "2022-05-28"
},
{
"vote": 3,
"title": "Skill extraction from resumes",
"text": "This is a fairly explored topic from my GitHub search but I wanted to get subâ€™s opinion. I have a comprehensive list of skills for a match technique but what else can we think of?  Perhaps training a skill NER using a custom training set (need an open source annotation tool).",
"date": "2022-05-26"
},
{
"vote": 11,
"title": "Optimizing for efficiency/memory use with spaCy and dask when preprocessing ~30M medium-large strings",
"text": "I have scraped about 30 million Reddit comments. Now I want to use them to train some classification models. But this volume of data is proving seriously challenging to work with.\n\n\nMy current set up is that the comment strings are stored as a \ndask.Series\n. At first I was using \ndask\n methods to clean the comments in parallel (this step involves multiple passes each using regex), then using \napply(nlp)\n to convert each comment into a \nspacy\n \nDoc\n (this just uses \nspacy\nâ€™s default preprocessing pipeline). But it takes an absolute eternity, before running out of memory and failing.\n\n\nSo something has to give here, but Iâ€™m not sure what. Iâ€™ve considered using \nnumpy\n instead of \ndask\n, but \ndask\n doesnâ€™t seem like the right tool since itâ€™s really optimized for numerical computation, not string operations.\n\n\nSo how on earth can I preprocess this much natural language data??? Let me know if more details are required.\n\n\nEdit: Some additional thoughtsâ€¦\n\n\n\n\nSo far all of this has been done on a standard-issue MacBook Pro, which is very likely just unable to handle this task. Eventually Iâ€™d like to move all my computation to AWS EC2, which seems like the right move. But before that point, Iâ€™d like to make sure my code is as optimized as possible. Iâ€™m not convinced that itâ€™s there yet.\n\n\n\n\nI have also considered using \nspacy\nâ€™s \nnlp.pipe\n to take advantage of multiprocessing, but Iâ€™m not sure yet how best to get that working with data stored as a \ndask.Series\n. Maybe trying to use \ndask\n is actually holding me back after all?",
"date": "2022-05-26"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Using sentence transformer models from Sentence-Transformers and HuggingFace",
"text": null,
"date": "2022-05-26"
},
{
"vote": 1,
"title": "LLM's Zero-Shot Reasoning Prompted by \"Let's think step-to-step.\"",
"text": "[deleted]",
"date": "2022-05-25"
},
{
"vote": 7,
"title": "How to use PyTorch GPU Acceleration on M1 Macs",
"text": "Walkthrough on how to use new MPS layer with PyTorch\n + tests with BERT (via Hugging Face)",
"date": "2022-05-25"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Convert Yolov4 Tiny Weights to NCNN framework (Smart Dashcam Part 3)",
"text": null,
"date": "2022-05-25"
},
{
"vote": 7,
"title": "How can I crowdsource speech/translation data for an endangered Semitic language?",
"text": "Hi everyone!\n\n\nMission:\n  I am trying to crowdsources speech and translation data for an endangered Semitic language: Neo-Aramaic\n\n\nWhat I have done already:\n There are many many dialects of spoken Neo-Aramaic (not including Classical Syriac). Documenting all of them is important. But for the most common dialect specifically (Urmi), I have a ~100 page bilingual English/Assyrian corpus in the desired phonetic Romanized \nalphabet\n. I have also created an ASR model with 12% CER to semi-automate the transcription of future unlabeled speech data.\n\n\nChallenges:\n There is simply not enough data! Most dialects simply have no data at all. The corpus I previously mentioned is not great for ML tasks: the sentences in this corpus are not at all common everyday sentences, and there is not much variety.\n\n\nMy idea:\n I want to create a website for crowdsourcing speech and translation data for all dialects of Neo-Aramaic! This is ambitious but it needs to happen. I have no experience so this could be dumb, but my plan is to have platform like Mozilla's \nCommon Voice\n:\n\n\n\n\nThis platform will show the user an English sentence, like \"He goes to the store\".\n\n\nThe user then reads this sentence in their dialect of Neo-Aramaic into their browser.\n\n\nThe user then approximately transcribes what they are saying in English characters.\n\n\nThe user then marks what city they are from / what dialect they have (depending on how informed they are).\n\n\nSomehow the translation is validated (must be done by speakers) and the transcription is corrected to the proper phonetic Romanized alphabet (can be done by any phonetician).\n\n\n\n\nQuestions for you all:\n\n\n\n\nWill a corpus built this way be meaningful? Like, wouldn't this data be very skewed? I hypothesize the data might work well to train an English to Neo-Aramaic translator but not a Neo-Aramaic - English translator.\n\n\nWould the crowdsourced translations be too difficult to validate? I'm not sure how messy things will get. I wish I could speak to someone with experience on similar projects.\n\n\nIf you all think this can work, where can I get the corpus of typical English sentences?\n\n\n\n\nThanks for reading to the end.",
"date": "2022-05-25"
},
{
"vote": 2,
"title": "In this tutorial, we show how to automate job search using fine-tuned NER model. Checkout the article to learn more!",
"text": null,
"date": "2022-05-24"
},
{
"vote": 3,
"title": "Extracting Twitter Threads",
"text": "Hi,\n\n\nI am working on a comprehensive project which includes extracting Twitter threads(main post with multi-level replies), as the work involves analysing conversations for discourse analysis. Has anyone used some tool to extract such data?\n\n\nThanks for the help.",
"date": "2022-05-24"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Setting up Dataset for Smart Dashcam (Part 1 )",
"text": null,
"date": "2022-05-23"
},
{
"vote": 5,
"title": "What method would be better for binary text (sentence) classification?",
"text": "Naive bayes, svms, CNNâ€™s and etc. I have to choose one method but I donâ€™t have enough time to try all of them so Iâ€™m looking for advice. \n\n\nAlso I used logistic regression on my data but the highest accuracy was 78%, If I could go over 80/85% that would do be great.\n\n\nEdit after I finished the project: I tweaked the dataset a bit and fortunately was able to reach an accuracy of as high as 87%. Of course the accuracy might be biased somehow but itâ€™s enough for my case. Also the comments you guys made gave me so much ideas, Iâ€™m exploring them.  Thanks again.",
"date": "2022-05-22"
},
{
"vote": 2,
"title": "AI book reading tool",
"text": "Greetings Folks,\n\n\nIn the past year, we had released a book reading AI tool to search for content within files using natural search, and we had received constructive feedback from the community.\n\n\nToday we are releasing, a new updated version with a fresh UI overhaul (desktop support).\nhttps://rastero.io/intro\n\n\nGlad to share it with you all.\n\n\nHere's an account to try it out!\n\n\nusername\n: \nreddit\n\n\npassword\n: \nreddit2022\n\n\nhttps://reddit.com/link/uvh9fv/video/ggm2hy3wo2191/player",
"date": "2022-05-22"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-05-22"
},
{
"vote": 0,
"title": "EXPLODE STRING IN A MULTIPLES ROW USING REGEX",
"text": "[removed]",
"date": "2022-05-22"
},
{
"vote": 5,
"title": "What's the license for speech files generated from real speech files and do I owe anything to the original speaker?",
"text": "I have trained a text-to-speech system where the voice imitate someone based on some of their recordings. What's the license for the generated speech files and do I owe anything to the original speaker? I live in the US.",
"date": "2022-05-21"
},
{
"vote": 21,
"title": "Best Natural Language Processing Books for Beginners to read in 2022",
"text": null,
"date": "2022-05-21"
},
{
"vote": 15,
"title": "This article showcases a tutorial on fine-tuning layoutLM v2 for invoice recognition starting from annotation to training and inference.",
"text": null,
"date": "2022-05-20"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Use Yolov4 with Live Stream through Webcam google Collab",
"text": null,
"date": "2022-05-20"
},
{
"vote": 10,
"title": "Convert first person to third person - is there a way to do that automatically?",
"text": "He was found in a parking lot with drug paraphernalia.\n\n\nIs there some way to copy and paste it somewhere where it will convert it to say. \n\n\n'You were found in a parking lot with drug paraphernalia.' or  'I was found in a parking lot with drug paraphernalia' \n\n\nwithout having to comb through find and replace he to I then manually change the verb form.",
"date": "2022-05-19"
},
{
"vote": 3,
"title": "Is there any quantitative way to compare results of different sentence embeddings?",
"text": "Hello everybody.\n\n\nI'll preface this by saying I'm quite the amateur within the field. Basically what I've done is created embeddings of unlabelled sentences, reduced the embeddings dimensions, and plotted them on a scatter plot. I'm only really aiming to visualize, so there is no clustering involved. Essentially I am now wondering if there is a quantitative way to compare the scatter plots I've created? \n\n\nThanks in advance.",
"date": "2022-05-19"
},
{
"vote": 1,
"title": "This article showcases how to annotate semi- structured texts whether its pdfs or scanned images using UBIAIâ€™s annotation tool",
"text": null,
"date": "2022-05-18"
},
{
"vote": 6,
"title": "Understanding human psychology with word vectors",
"text": "Since the 1890s, psychologists have thought that a good empirical way to understand personality would be to look to the structure of common personality adjectives. A famous defense of this method goes:\n\n\n>\nâ€œOur common stock of words embodies all the distinctions men have found worth drawing, and the connections they have found worth marking, in the lifetime of many generations: These surely are likely to be more numerous, more sound, since they have stood up to the long test of survival of the fittest, and more subtle, at least in all ordinary and reasonable practical matters, than any that you or I are likely to think up in our armchair of an afternoonâ€”the most favorite alternative method.â€\n JL, Austen \nA plea for excuses\n\n\nBy the 1930s an engineer-psychologist had invented Latent Semantic Analysis (yes, \nthat\n LSA), and built word vectors for personality words. (Read more about that \nhere\n.) The results of this type of experiment came to be known as the Big Five, which is the standard model of personality in psychology (mentioned in ~400,000 articles on google scholar). \n\n\nGiven the linguistic basis of this popular model, it seems that modern NLP can improve it. I have \npublished\n on this and write livelier \naccounts\n on my blog. It's quite interdisciplinary and an area where people with some NLP skills can make a big impact. Hoping to get more people into it!",
"date": "2022-05-18"
},
{
"vote": 48,
"title": "I just shipped part one of Natural Language Processing Demystified; a free, accessible course on NLP.",
"text": "Hi r/LanguageTechnology:  \n\n\nI just published part one of NLP Demystified in full. The course is intended to help anyone who knows Python and a bit of math go from the very basics all the way to today's mainstream models and frameworks.  \n\n\nI strive to balance theory and practice and so every module consists of detailed explanations and slides along with a Colab notebook (in most modules) putting the theory into practice.  \n\n\nIn part one, we cover text preprocessing, how to turn text into numbers, and multiple ways to classify and search text using \"classical\" approaches. And along the way, we'll pick up useful bits on how to use tools such as spaCy and scikit-learn.  \n\n\nNo registration required: \nhttps://www.nlpdemystified.org/\n \n\n\nRemember to sign up for updates!",
"date": "2022-05-18"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-05-18"
},
{
"vote": 5,
"title": "How to add additional module to BERT architecture, then load the original weight and use it",
"text": "I aim to \n\n\n\n\nAdd an additional module to BERT architecture (huggingfaceâ€™s transformers)\n\n\nLoad the BERTâ€™s weight to the BERT model with new architecture\n\n\nThen use BERT directly or continue train BERT\n\n\n\n\nIâ€™m very confused how to do it. Since we usually use \nfrom_pretrained\n directly to load the model (both weight and architecture) from huggingface.\n\n\nIn more detail, Iâ€™m working on both \nprajjwal1/bert-tiny\n and \nbert-base-uncased\n.",
"date": "2022-05-18"
},
{
"vote": 4,
"title": "How good is my summary? Part 2",
"text": "Hello in my previous post I asked how I can evaluate my summary compared to another text or abstract.\n\n\nI found some results using python fuzzy and rouge libraries.\n\n\nHowever I was also recommended to use Sbert. Which I am curious about it due the sentence level semantics which might allow me to avoid contradictions.\n\n\nThe issue of sbert is that I was only able to find sentence comparisons which is good to build a new summary, but not really good to compare to a already made one. Basically I was looking how to do an overall similarly of text and not just sentence by sentence. Is it wise to do say an overall average with all the scores of the sentences pairs? Or is there a better way using sbert?\n\n\nI say sbert but I am continuing to search for even more methods. It is just sbert seems to answer the contradictions part, but not give an overall score.",
"date": "2022-05-17"
},
{
"vote": 8,
"title": "Can the BLEU score be used to choose the best human translator among a pool of samples?",
"text": "Hi everyone!\n\n\nA translation agency is using the BLEU score to select the best human translators among a large pool of candidates. They had only two sample translations against which to compare each test entry. The subject matter is a highly complex legal document 500 words in length.\n\n\nDo you think this is a valid approach? How reliable would the scores be? What are the problems with it?\n\n\nThanks a lot!",
"date": "2022-05-17"
},
{
"vote": 8,
"title": "Best Resources to Learn Natural Language Processing(Books, YouTube...)",
"text": null,
"date": "2022-05-17"
},
{
"vote": 14,
"title": "What are some cool error analysis tricks you've seen?",
"text": "I have one. On a text classification problem we would dump prediction errors to a spreadsheet, look at them, and try to categorize the types of errors. Sometimes this is easy, like parsing errors from OCR confusing the model on a particular word. Other times it's harder because we had to understand what the model was missing.\n\n\nA colleague suggested we run LDA to get general topics across the error samples to try and discern. Seemed like a really cute idea but never got to implementing it. Another thing that worked well was looking at the \"closest\" sample in the training set for comparison, where we could compute distance easily using feature embeddings since we used a neural model (well, Fasttext with some aggregations and a softmax).\n\n\nWhat's some cool stuff you've seen?",
"date": "2022-05-16"
},
{
"vote": 4,
"title": "How to calculate a mean vector out of a batch of vectors in OpenSearch / Elasticsearch?",
"text": "I have 1 billion documents.\n\n\nEach document has a field with a vector named \nembedding\n.\n\n\nEach embedding has 768 dimensions.\n\n\nI want to find the mean vector out of this batch of documents.\n\n\nA mean vector is, for example:\n\n\nAssume I have 3 documents.\n\n\n1 embedding for each document -> 3 embeddings: [1, 2] [3, 4] [5, 6]\n\n\nThe mean vector of this bucket of documents will be [(1+3+5)/3, (2+4+6)/3] -> [3, 4]\n\n\nWhat's the most time-efficient way I can calculate it?",
"date": "2022-05-16"
},
{
"vote": 5,
"title": "Cambridge AI Researchers Propose â€˜MAGICâ€™: A Training-Free Framework That Plugs Visual Controls Into The Generation Of A Language Model",
"text": "The release of Generative Pretrained Transformer (GPT-2) has fetched huge attention towards generative language models (LMs), which are pre-trained on massive amounts of unstructured text and have generated efficient results on a variety of NLP applications. LMs can produce texts constantly utilizing a textual promptâ€™s next-token prediction decoding approach. Models such as CLIP and ALIGN, pre-trained image-text joint embedding approaches, have revived multimodal illustration learning of text and images. Accordingly, it is challenging to integrate the benefits of pre-trained LMs and image-text embedding models to generate visually grounded text. The traditional approaches are generally limited by the object detectors trained with a fixed set of labels. Currently, the ZeroCap approach is utilized for image captioning, which is an unsupervised technique that integrates frozen CLIP and GPT-2. ZeroCap uses gradient update and optimization over the context cache. This approach slows down inference and makes it challenging to utilize in real-world scenarios. This research resolves this challenge by proposing a novel method called i\nMA\nge-\nG\nuided text generat\nI\non with \nC\nLIP (MAGIC) for text decoding. MAGIC uses explicit â€œcontrol knobsâ€ to choose desired outputs, following the direction of both the GPT-2 and CLIP models. Also, it does not require any additional parameters for training. The approach involves a new term called magic sore from boosting the predicted result for demonstrating information close to a given image. The simulation results depict that such a framework allows for zero-shot image captioning and visually grounded story creation using a simple plug-and-play method. This research tests two widely used benchmarks: MS-COCO and Flickr30k. The proposed approach outperforms all unsupervised and weakly supervised baselines, achieving state-of-the-art (SOTA) results across several evaluation measures. Also, the proposed method does not require a gradient update, and hence the inference speed is approximately 27 times faster than earlier zero-shot image captioning SOTA. This approach is also evaluated on visually grounded story generation. On both human and machine evaluations, the proposed method produces high-quality, efficient stories compared to robust baseline methods.\n\n\nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/pdf/2205.02655v1.pdf\n\n\nGithub: \nhttps://github.com/yxuansu/magic",
"date": "2022-05-16"
},
{
"vote": 1,
"title": "Using BLEU and Rouge scores in training?",
"text": "Is it possible to use BLEU and Rouge scores in the training of a deep text generation model, i.e. to maximize the scores in a loss function, albeit this is \"cheating\"? If so, has anyone / any paper done it? Thanks for any pointer.",
"date": "2022-05-16"
},
{
"vote": 4,
"title": "NLP, CL, LCT what's the difference and how can I get in?",
"text": "Hi. I'm a TEFL student (Teaching English as a Foreign language) and I'm interested in getting into one of the mentioned fields. LCT (language communication and technologies) popped up when I was looking into to the erasmus mundus scholarship. Looked very similar to the other two but i can't tell the difference.\n\n\nI'm literally willing to do anything to get into one of these cuz i don't think i can get anywhere with teaching.\n\n\nSo\n\n\n\n\nWhat courses would be necessary for me to take in order to be a qualified candidate?\n\n\n\n\nI think I can be considered as an active student who has done many non academic activities such as being a director in chief of a magazine, printing a poetry book, recording an album etc... Will these help me? What else can I do?\n\n\n\n\nI'm aiming for Europe and possibly the Netherlands. To go there and stay forever hopefully. How can i find a professor to fund me? What are my chances? How can i increase them?\n\n\n\n\n\n\nThank you in advance.",
"date": "2022-05-15"
},
{
"vote": 2,
"title": "How do you get around underrepresented categories for text categorization/NER data?",
"text": "For a lot of clients, I see issues with some categories of data being severely underrepresented in the training data.\n\n\nFor  instance, a category could just have 10 examples in a total dataset  size of 10,000. In such, cases do you downsample, generate synthetic  examples etc?\n\n\nWhat would be the ideal solution?",
"date": "2022-05-15"
},
{
"vote": 0,
"title": "what is the best information retrieval model for text data?",
"text": "[deleted]",
"date": "2022-05-15"
},
{
"vote": 0,
"title": "Ad-free Javascript-free Malaysian news aggregator",
"text": null,
"date": "2022-05-15"
},
{
"vote": 6,
"title": "any good suggestions of pretrained chatbot models? that can be used for recommendations of products and such.",
"text": "I want to build a product recommendation chatbot, and I was hoping that I could use an existing pretrained model or a library that specializes in this,\n\n\nis there anything available like this?\n\n\nThank you",
"date": "2022-05-14"
},
{
"vote": 4,
"title": "M.Sc. Computational Linguistics in Germany / Saarbrucken/Stuttgart",
"text": "Hi,\n\n\nI was admitted to the \nM.Sc\n. \"Language Science and Technology\" program at the UniversitÃ¤t des Saarlandes and am applying to the \nM.Sc\n in CL in Stuttgart as well. I have a B.A. in Speech and Hearing Science/Speech Therapy in the U.S. with very limited programming background (I know some basic Python and that's all atm).\n\n\nFrom what I have read, the uni in Saarbrucken has a very good CS reputation. I am a bit intimidated because I have little technical skill, but they did accept my application which gives me some relief.\n\n\nCan anyone provide me some extra support on the following?:\n\n\n&#x200B;\n\n\n\n\nAre there other unis I should be applying to in Germany? I have dual U.S. and German citizenship\n\n\nWhat courses or skills should I begin learning before the program start in Oct 22?\n\n\nWhat are the best German unis for the topic? I want to make sure I make the correct decision. I know I really need support in all CS and technical skills.\n\n\n\n\nTHANKS!~",
"date": "2022-05-13"
},
{
"vote": 1,
"title": "Top 3 Python for Data Science training programs",
"text": "[removed]",
"date": "2022-05-12"
},
{
"vote": 2,
"title": "Conference ratings",
"text": "What is meant by Core A, A*, A**, B conferences and journals??",
"date": "2022-05-12"
},
{
"vote": 6,
"title": "Best Natural Language Processing Courses you might know in 2022 (Latest)",
"text": null,
"date": "2022-05-12"
},
{
"vote": 2,
"title": "Udacity Natural Language Processing Nanodegree Review- Is It Worth It?",
"text": null,
"date": "2022-05-12"
},
{
"vote": 16,
"title": "[Kaggle Dataset] Elon Musk Tweets (2010 - 2022)",
"text": "Hey! Though I'd share a dataset that I had created a while back here, hopefully you'll get some benefit from it. Cheers.\n\n\nhttps://www.kaggle.com/datasets/ayhmrba/elon-musk-tweets-2010-2021",
"date": "2022-05-12"
},
{
"vote": 2,
"title": "This article and tutorial explains the prediction of named entity recognition ( NER) model using local interpretable model-agnostic explanations ( LIME) Check more articles and tutorials at : ubiai.tools",
"text": null,
"date": "2022-05-11"
},
{
"vote": 12,
"title": "Need advice on learning to apply techniques from research papers.",
"text": "[deleted]",
"date": "2022-05-11"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Use Yolov5 with Free GPU on Google Colab",
"text": null,
"date": "2022-05-10"
},
{
"vote": 6,
"title": "Whatâ€™s the best free text-to-speech module/api out?",
"text": "Yo, I am making a virtual assistant currently with Python and pyttsx3 because itâ€™s free and works offline. Do you recommend any others that perhaps sound less robotic and glitchy?",
"date": "2022-05-10"
},
{
"vote": 5,
"title": "How do I check if two CoNNL scores are significantly different?",
"text": "I have an evaluation batch of 6 items. I ran this on both my system output and a human annotator's results against the gold labels.\n\n\nThe human average CoNLL scores are\n\n\n1\n\n\n0.8475281455\n\n\n0.5003718249\n\n\n0.7957433734\n\n\n0.4920068314\n\n\n0.7339975187\n\n\nand the system average CoNLL scores are\n\n\n0.8526315789\n\n\n0.7730431201\n\n\n0.7143441494\n\n\n0.6639100561\n\n\n0.7212088202\n\n\n0.8165547658\n\n\nThe system mean is marginally higher at 0.7569487484 vs 0.7282746157, however the variances are 0.004943160553 and 0.04008060433 respectively. The latter variance makes me guess that I don't have enough data to show statistical significance at p=0.05, but if I were to run a statistical test, what should I use? The data points don't seem to follow a normal distribution, so I guess a t-test is out.\n\n\nedit: going through the wizard here: \nhttps://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\n I was recommended the Mann-Whitney U Test, which is the non-parametric option. Although now I have a dilemma: whether to select one tailed or two tailed test. My guess would be one tailed, as I would hypothesise that my system performs better than the human. The outcome was as I guess, not statistically significant though.\n\n\n*CoNLL",
"date": "2022-05-09"
},
{
"vote": 1,
"title": "Javascript interview Question In 2022 with detailed explanation",
"text": null,
"date": "2022-05-09"
},
{
"vote": 0,
"title": "The ML pipeline &amp; Types of roles in ML",
"text": null,
"date": "2022-05-08"
},
{
"vote": 1,
"title": "Twitter users divided into communities based on language style",
"text": null,
"date": "2022-05-08"
},
{
"vote": 7,
"title": "Prioritising Keywords using GPT-J",
"text": null,
"date": "2022-05-07"
},
{
"vote": 31,
"title": "Meta AI Introduces Open Pre-trained Transformers (OPT): A Suite Of Decoder-Only Pre-Trained Transformers Ranging From 125M To 175B Parameters",
"text": "Recent advances in AI research have necessitated a massive amount of computing power. While industrial labs have begun to quantify these modelsâ€™ carbon footprints, most do not include the computational costs involved with the R&D phases of research, which in certain circumstances can be significant.\n\n\nOver the last few years, large language models â€” natural language processing (NLP) systems with more than 100 billion parameters â€” have revolutionized NLP and AI research. They demonstrate an incredible new ability to write creative content, perform simple math problems, answer reading comprehension tests, and more after training on a large and varied literature volume. While the public can engage with these models through paid APIs in some situations, full research access is still restricted to a few well-funded laboratories. Researchersâ€™ capacity to understand how and why these vast language models work has been hampered by this restricted access, which has slowed progress on efforts to increase their robustness and reduce known concerns like bias and toxicity.\n\n\nTo allow deeper community engagement in understanding this vital new technology, they published Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets, keeping with Meta AIâ€™s commitment to open research. This is the first time a language technology system of this magnitude has included pretrained models and the programming needed to prepare and use them. They distribute the model under a noncommercial license to focus on research use cases to retain the integrity and prevent misuse. Academic researchers linked with government, civil society, academia, and industry research facilities worldwide will access the model.\n\n\nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/pdf/2205.01068.pdf\n?",
"date": "2022-05-07"
},
{
"vote": 1,
"title": "Named Entity Recognition of companies from unstructured table data",
"text": "Hi, I am experimenting with the tagging of company data in an unstructured txt output.\n\n\nThe txt appears to be rather digits heavy and often only contains two sequences of strings. I want both sequences to be tagged as:\n\n\n- company name (tag: CN)\n\n\n- company location (tag: CL)\n\n\n&#x200B;\n\n\nThe problem that I am facing is that e.g. fine tuning a flair NER model for the two new tags would result in a rather sparse column model - which means in BIO tagging notation e.g.\n\n\n1123123 O\n\n\n123125124 O\n\n\nlong B-CN\n\n\ncompany I-CN\n\n\nname I-CN\n\n\n575755 O\n\n\n1231212 O\n\n\n&#x200B;\n\n\nMy question is, does anyone have experience with this data? Do you suggest to rather go a different approach than NER tagging? Also, I was thinking of populating the column model for fine tuning with \"fake\" text strings, also marked as O's, does that makes sense?\n\n\nAs usual, always happy to hear your feedback and thank you in advance.",
"date": "2022-05-06"
},
{
"vote": 1,
"title": "ðŸ’¥ How to use Google T5-large (a transformers model) for a summarization use case?",
"text": null,
"date": "2022-05-06"
},
{
"vote": 4,
"title": "Best Speech Assessment API in 2022",
"text": "SpeechSuper APIs assess users' audio and give instructive feedback on spoken language activities in language learning.  \n\n\nSee what SpeechSuper APIs offer:  \n\n\nâœ… Phoneme-level pronunciation scores for spoken words & word-level pronunciation scores for spoken sentences and paragraphs, plus completeness, fluency, and speed \n\n\nâœ… Give feedback in real-time, making the user experience more interactive\n\n\n âœ… Detect mispronunciation, syllable stress for spoken words \n\n\nâœ… Spelling and phoneme alignment of words, useful for phonics learning\n\n\n âœ… Detect linking and end-of-sentence tone for spoken sentences  SpeechSuper APIs support English, Mandarin Chinese, German, French, Korean, Japanese, Russian, Spanish, and more languages to come.\n\n\nðŸ‘‰ Try demos here (PC browser only): \nwww.speechsuper.com/demo/english...\n \n\n\nðŸ‘‰ Learn more here: \nwww.speechsuper.com",
"date": "2022-05-06"
},
{
"vote": 17,
"title": "Researchers From Cambridge And Amazon Propose â€˜TRANS-ENCODERâ€™: An Unsupervised Approach of Training Bi- And Cross-Encoders For Sentence-Pair Tasks",
"text": "Several natural language processing and information retrieval tasks, such as textual entailment, paraphrase identification, etc., rely on pairwise sentence comparison. The most frequently accepted approach to sentence comparison is cross-encoding, which involves mapping sentences against each other on a pair-by-pair basis. However, significant amounts of annotated data are necessary to train these cross-encoders, which is a significant drawback due to their time-consuming nature.Â \n\n\nThis stumbling block paved the path for research into how to train entirely unsupervised models for sentence-pair tasks without using data annotation. \nTrans-encoder\n is a completely unsupervised sentence-pair model developed by Amazon Research experts. The corresponding research paper was also presented at this yearâ€™s International Conference on Learning Representations (ICLR), demonstrating how the model could attain 5% greater sentence similarity than previous state-of-the-art models.Â \n\n\nContinue Reading\n\n\nPaper: \nhttps://openreview.net/pdf?id=AmUhwTOHgm",
"date": "2022-05-06"
},
{
"vote": 1,
"title": "Undergrad Stats for Getting into Masters Program",
"text": "Hello everyone, I am currently in my penultimate year of undergrad, so I have to start whittling down my choices for a grad school and start my applications soon. I have really good relationships with 3 of my professors so I feel confident of letters of rec. I also have been heavy into research at my university so I feel very confident on my research experience. The only thing I am worried about is my GPA. I messed around my first two years resulting in a lower (3.2) GPA than I would like. Basically, I was wondering how much that matters as I know stats are less important per se in grad school than undergrad applications. I was wondering for those of y'all who went onto grad school what was your undergraduate experience like? Sorry for the longwinded post, but I would appreciate any replies!",
"date": "2022-05-05"
},
{
"vote": 12,
"title": "Open source logger for spaCy!",
"text": "Hi everyone, we've built a plugin to track and visualise spaCy logs.\n\n\nIt has bult-in support for displaCy visualizations and dashboards to compare multiple runsâ€™ NER/dep-trees side by side.\n\n\nIt's open source. Here's more info about it \nhttps://aimstack.io/spacy\n\n\nWould love your feedback !",
"date": "2022-05-05"
},
{
"vote": 5,
"title": "Semantic search for matching best question-answer pair",
"text": null,
"date": "2022-05-05"
},
{
"vote": 0,
"title": "The science behind ultrasonic motion sensing for Echo",
"text": null,
"date": "2022-05-04"
},
{
"vote": 3,
"title": "Multilingual sentiment analysis in less than a day (or how I learned to stop worrying and love ML)",
"text": null,
"date": "2022-05-04"
},
{
"vote": 6,
"title": "Why do context vectors tend to have dimensions which follow 2^n?",
"text": "Hello. Bit of a newbie here. \n\n\nFor example, with an RNN, the context vector which the encoder outputs tends to have some dimensionality of size like 256, 512, 1024 etc. Why is this? Why not round numbers? I'm guessing these are somehow the most efficient sizes computationally? \n\n\nThanks!",
"date": "2022-05-04"
},
{
"vote": 5,
"title": "Using SPACY 3.2 and custom tagging",
"text": "Hi All,\n\n\nI am using SPACY and have created a custom list of entities to be tagged in text. \n\n\nsen = [&quot;The GenBank/EMBL/DDBJ accession number for the 16S rRNA gene sequence of strain AP-38T is KU201961.&quot;,\n   &quot;The strain produced single, coccoid to rod-shaped, sometime irregular cells forming yellow colonies with a smooth surface after 48h at 28Â°C on tryptone soy agar (TSA; Oxoid). &quot;,\n   &quot;The GenBank/EMBL/DDBJ accession numbers for the 16S rRNA gene and genome sequence of strain 7Y-4T are KT988032 and LQXS00000000, respectively.&quot;,\n   &quot;Observations of the colony morphology, motility, and growth conditions (pH, temperature and salinity ranges) were performed as previously described by Li et al. (2014). &quot;]\nfor doc in nlp.pipe(sen):\n    for ent in doc.ents:\n        print(ent.text, ent.label_)\n\n\n\nAnd the output is :\nThe GenBank/EMBL/DDBJ accession number for the 16S rRNA gene sequence of strain AP-38T is KU201961.\n\n\nGenBank db\n\n\nEMBL db\n\n\nDDBJ db\n\n\n16S rRNA rna\n\n\nAP-38T strain\n\n\nKU201961 ac_no\n\n\nSo, Now I want to extract all the instances of  the tags. And I am unable to figure out a way to write a matcher pattern or any other way to extract the tagged items and that too sentence-wise.\n\n\nAny help would be appreciated.\n\n\nThank you",
"date": "2022-05-04"
},
{
"vote": 15,
"title": "SOTA speech to text framework?",
"text": "Which framework/library would you use for speech to text conversion? I have checked out \nspeechbrain\n, but seems like their latest release were done in last year. Would there be a better choice that has a greater community?",
"date": "2022-05-03"
},
{
"vote": 1,
"title": "First Bay Area Machine Translation Meetup to take place on May 11th",
"text": "[deleted]",
"date": "2022-05-02"
},
{
"vote": 7,
"title": "Bay Area Machine Translation meetup coming next week!",
"text": "The first Bay Area machine translation meetup is happening next week!\n\n\nHear from \nAmazon AI\n, \nSYSTRAN\n, \nUber\n, \nViva Translate\n - researchers, builders, localization team leaders and startup founders\n\n\nhttps://www.meetup.com/machinetranslate/events/285533813/\n \n\n\nIt is organized by the Machine Translate (machinetranslate.org) foundation.",
"date": "2022-05-02"
},
{
"vote": 4,
"title": "Limiting multi-lingual models to a select few languages",
"text": "Multi-lingual models like mBert or XLM-R are designed to cover over 100 languages. My application spans only the top 14 languages. Short of retraining the models from scratch over those targeted languages (which is expensive), can I limit mBert/XLM-R vocab to those 14 languages, distill  and/or fine tune the model, and expect an increase in modeling performance?\n\n\nAny reference, prior art, idea, criticism or suggestion is welcome.\nThank you guys!",
"date": "2022-05-02"
},
{
"vote": 19,
"title": "AI2 Open-Sources â€˜LM-Debuggerâ€™: An Interactive Tool For Inspection And Intervention In Transformer-Based Language Models",
"text": "In natural language processing, a language model is a probabilistic statistical model that calculates the likelihood of a specific sequence of words appearing in a phrase based on the preceding words. As a result, itâ€™s common in predictive text input systems, speech recognition, machine translation, and spelling correction, among other applications. They are a method of converting qualitative text information into quantitative data that machines can interpret.\n\n\nModern NLP models rely on transformer-based language models (LMs). However, a lot more research is to be done under their fundamental prediction development process. Unclear prediction behavior becomes an obstacle for both end-users who donâ€™t comprehend why a model generates certain predictions and developers who want to diagnose or fix model behavior.\n\n\nA new paper published by a group of researchers from Allen Institute for AI, Tel Aviv University, Bar-Ilan University, and the Hebrew University of Jerusalem introduces LM-Debugger, an interactive open-source tool for fine-grained interpretation and intervention in LM predictions. This work will increase the transparency of LMs.\n\n\nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/abs/2204.12130\n\n\nGithub: \nhttps://github.com/mega002/lm-debugger",
"date": "2022-05-02"
},
{
"vote": 5,
"title": "RoBERTa Bytepiece tokenizer - extracting rep positions from sequences.",
"text": "So I'm trying to extract the RoBERTa representations for particular words in a sentence, for example:\n\n\n\"The cat in the \nhat\n went to the pond\"\n\n\nLets say I'm interested in \nhat\n. I noticed that after tokenizing, if I tokenize \"hat\" in isolation, its token ID for hat is different from when I tokenize \"the cat in the hat went to the pond\"...\n\n\nEssentially, I'm trying to do a study on contextualized word-reps, but if the rep for \nhat\n is different when encoded in isolation vs. when in a sentence, I cannot use this token ID to pull the word embedding out (via index) from the sequence of embeddings when the whole series of tokens is fed into RoBERTa...I previously did this with BERT as the token for a word was the same in isolation as it was in a sentence.\n\n\nI've narrowed it down to the bytepiece tokenizer, but I have no idea how to find the word rep for the word of interest in the tokenized sequence so I could then extract the corresponding rep from the sequence of reps for the whole sentence.\n\n\nDoes anybody know how I could achieve this?\n\n\nThanks!",
"date": "2022-05-01"
},
{
"vote": 14,
"title": "PhD or no PhD?",
"text": "Iâ€™m about to publish my first paper on a NLP task. I have to complete my masterâ€™s degree first but soon i have to decide whether or not to do a PhD(i live in Spain but not planning on staying). Is there an actual gap on this area, i mean between having or not a PhD? Ive seen  that every paper is from PhD students.\n\n\nPS: i do enjoy research",
"date": "2022-05-01"
},
{
"vote": 16,
"title": "Why (or how) does a Keras model skip Stemming or Lemmatization steps?",
"text": "This [Keras article / tutorial here][1] does perform text standardization i.e removing HTML elements, punctuation, etc. from the text dataset, however, there is a distinct lack of any stemming or lemmatization before the vectorization step.  \n\n\nI have a bit of experience in deep learning but I am very new to NLP, and I just got to know (from a [different tutorial on Udemy][2], which BTW was using Bag of Words) that using either a Stemmer or a Lemmatizer helps in bringing down the vocabulary size and hence increases performance. I am a bit baffled by the absence of this step in the Keras-way of doing things.  \n\n\nHere is one assumption of mine - is it omitted because a Neural Network model is capable of handling a larger vocabulary size? I cannot think of any other reason(s) as to why that might be the case.\n\n\n  [1]: \nhttps://www.tensorflow.org/tutorials/keras/text_classification\n\n\n  [2]: \nhttps://www.udemy.com/course/machinelearning/learn/lecture/19958908#overview",
"date": "2022-04-30"
},
{
"vote": 7,
"title": "Can anyone point me to an implementation of a Coreference Resolution algorithm following Centering rules?",
"text": "I'm currently working on my master project for a Coreference Resolution system in Icelandic. No work has been done in coreference resolution for Icelandic and so there are no corpora with marked anaphora so my step is to create a baseline rule-based algorithm. My plan is to use Centering theory but I'm having a hard time finding if there are any systems that implement this theory in their solution.",
"date": "2022-04-30"
},
{
"vote": 0,
"title": "NLP startup ideas",
"text": "[removed]",
"date": "2022-04-30"
},
{
"vote": 21,
"title": "Is perplexity a good measure for evaluating language models?",
"text": "I fine-tuned a language model on a dataset for sentence generation and decided to use perplexity to evaluate that model. However, I noticed that visually, the many of the sentences that had lower perplexities made less sense than the ones with high perplexities. Perhaps, i'm misunderstanding this metric but aren't sentences that make more sense supposed to have lower perplexities? I'm a beginner in this area, so if anyone could enlighten me, I'd appreciate it.",
"date": "2022-04-29"
},
{
"vote": 12,
"title": "Blogpost that explains how spaCy models are relatively lightweight because of a Hashing Trick.",
"text": null,
"date": "2022-04-29"
},
{
"vote": 10,
"title": "Hi, I want an advice on how to get a job in nlp",
"text": ". I have a bachelor degree in English literature and linguistics. I took many courses in ml, Dl, and nlp. I have problems with writing codes. any advice?",
"date": "2022-04-29"
},
{
"vote": 6,
"title": "Applying NLP techniques to sustainability",
"text": "Does anyone know of any interesting articles, current research, or areas within sustainability that could use NLP resources such as sentiment analysis, text summarization, text classification, etc. I am interested in this subfield but don't really know where to start looking into it.",
"date": "2022-04-28"
},
{
"vote": 3,
"title": "Advances in trustworthy machine learning at Alexa AI",
"text": null,
"date": "2022-04-28"
},
{
"vote": 7,
"title": "HuSpaCy: Industrial-strength Hungarian NLP",
"text": null,
"date": "2022-04-28"
},
{
"vote": 1,
"title": "High Tech Hackathon For Students !",
"text": "[removed]",
"date": "2022-04-28"
},
{
"vote": 4,
"title": "Any annotation tool for argument mining?",
"text": "Hi, I've been testing several annotation tools for my research at university. Each one has its particular advantages but none of the ones I've tried seem to be created for annotating argument relations. Inception could probably be the most flexible for my needs, yet it requires too many technical requirements for installing and using it as an admin. And most importantly now, it is usable but not optimal for argument mining, which would require being able to see the relations between sometimes across far-away-in-the-text arguments in an organised and intuitive way - both in-text and extracted separately from the original text. Arrows that go horizontally through several lines in a text, although they can at least show a relation across paragraphs, tend to accumulate and are very difficult to follow.\n\n\nI've recently seen Tiara, which has better visualisations for AM,  but you can't see annotations from different annotators and calculate inter-rater agreement, which is basic unless you're just annotating for yourself...\n\n\nDoes anyone know of an annotation app that allows for manually annotating argument units and visualising relations in different texts by more than one annotator?",
"date": "2022-04-27"
},
{
"vote": 3,
"title": "Text Encoder output giving zero values",
"text": "I have been working on an Out of Domain detection model which takes in two strings, one domain string and one out of domain string, and a list of strings that can be taken as an anchor. Based on this I have trained an encoder model that converts these strings into vectors of 100 dimensions. Each input sentence is converted to tokens and then they are converted to word vectors using glove vectors. Post that a CNN model is trained on the vectors. Batch norm is done before pooling and then they are passed through a linear layer to get the final vectors. Below is the code for the model.\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        input_size = 40\n        dim = 100\n        self.batch_size = BATCH_SIZE\n        kernel = 2\n        self.conv_x_in = nn.Conv1d(dim, 300, kernel_size=kernel)\n        self.batch_norm_in = nn.BatchNorm1d(300)\n        self.pool_in = nn.AvgPool1d(MAXLEN - kernel + 1)\n        self.linear_in = nn.Linear(300, 100)\n        \n        self.conv_S_in = nn.Conv1d(dim, 300, kernel_size=kernel)\n        self.batch_norm_S_in = nn.BatchNorm1d(300)\n        self.pool_S_in = nn.AvgPool1d(MAXLEN - kernel + 1)\n        self.linear_S_in = nn.Linear(300, 100)\n        \n    def encoded_x_i_in(self, x_i_in):\n        x_i_in = x_i_in.transpose(1, 2) # make batch x seq x dim =&gt; batch x dim x seq\n        x = F.relu(self.conv_x_in(x_i_in))\n        x = self.batch_norm_in(x)\n        x = self.pool_in(x)\n        x = x.squeeze(2)\n        x = F.relu(self.linear_in(x))\n        return x\n\n    def encode_S_in(self, x_i_in):\n        x_i_in = x_i_in.transpose(1, 2) # make batch x seq x dim =&gt; batch x dim x seq\n        x = F.relu(self.conv_S_in(x_i_in))\n        x = self.batch_norm_S_in(x)\n        x = self.pool_S_in(x)\n        x = x.squeeze(2)\n        x = F.relu(self.linear_S_in(x))\n        return x\n\n    def forward(self, x_i_in, x_j_out, S_in):\n        in_emb = self.encoded_x_i_in(x_i_in)\n        out_emb = self.encoded_x_i_in(x_j_out)\n        overall_emb = [[self.encode_S_in(y) for y in x] for x in S_in]\n        overall_emb = [[torch.sum(y, 0) for y in x] for x in overall_emb]\n        overall_emb = [torch.stack(x) for x in overall_emb]\n        overall_emb = torch.stack(overall_emb)\n        return in_emb, out_emb, overall_emb\n\n\n\nAfter training the outputs are coming as mostly zeros, which makes the resulting vectors almost unusable. Not sure what is the issue here. I can try changing the base vectors from glove to something else but I think the problem stems from something else.\n\n\nAn example of the encoder output for the ``\nin_emb\n` variable.\n\n\ntensor([[0.0000, 0.0413, 0.0174, 0.0000, 0.1767, 0.2171, 0.0037, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0799, 0.0000, 0.0000, 0.3925, 0.0695, 0.2034, 0.0000,\n         0.0422, 0.0000, 0.0000, 0.1015, 0.2971, 0.1513, 0.1093, 0.0368, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0411, 0.0000, 0.0000, 0.1053, 0.0000, 0.0000, 0.0000, 0.0694,\n         0.0000, 0.0000, 0.1459, 0.0000, 0.1779, 0.0464, 0.0333, 0.0000, 0.1581,\n         0.0000, 0.0032, 0.0000, 0.0113, 0.0206, 0.0168, 0.0000, 0.0000, 0.0709,\n         0.0173, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1660, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.1795, 0.2580, 0.0000, 0.0000, 0.2024,\n         0.0416, 0.0000, 0.5906, 0.0000, 0.0350, 0.2130, 0.0000, 0.0000, 0.2970,\n         0.0000, 0.0000, 0.0559, 0.0863, 0.0313, 0.0000, 0.2553, 0.0000, 0.0858,\n         0.0279]])",
"date": "2022-04-25"
},
{
"vote": 17,
"title": "Which huggingface model is the best for sentence as input and a word from that sentence as the output?",
"text": "I am trying to build a pun detector and I would appreciate it if you could help me understand what would the best huggingface model to fine-tune be for this type of task:\n\n\nExample input 1:\n\n\nIf there&#039;s one person you don&#039;t want to interrupt in the middle of a sentence it&#039;s a judge.\n\n\nExample output 1:\n\n\nsentence\n\n\nExample input 2:\n\n\nA good baker will rise to the occasion, it&#039;s the yeast he can do.\n\n\nExample output 2:\n\n\nyeast",
"date": "2022-04-22"
},
{
"vote": 6,
"title": "mGPT model release - multilingual generative transformer for 61 languages",
"text": "Hi everyone. Today we released the mGPT model: multilingual generative pre-trained transformer\n\n\nThe checkpoints are available on Huggingface \nmodel page\n\n\nThe example usage is at the Github repo \nhttps://github.com/ai-forever/mgpt\n \n\n\n\n\nThe model has 1.3 billion parameters\n\n\nThe context length is 512 tokens.\n\n\n\n\nThe model can generate sequences after the input prompt, can be used for fine-tuning or for zero- and few-shot learning:\n\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nmodel_name = &quot;sberbank-ai/mGPT&quot;\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodel.cuda()\nmodel.eval()\n\ntexts = [\n    &quot;My favourite holiday is &quot;,\n    &quot;Ô»Õ´ Õ½Õ«Ö€Õ¥Õ¬Õ« Õ¿Õ¸Õ¶Õ¶ Õ§ &quot;,\n    &quot;ÐœÐ¾Ñ” ÑƒÐ»ÑŽÐ±Ð»ÐµÐ½Ðµ ÑÐ²ÑÑ‚Ð¾ &quot;,\n    &quot;mi fiesta favorita es &quot;,\n    &quot;à¤®à¥‡à¤°à¥€ à¤ªà¤¸à¤‚à¤¦à¥€à¤¦à¤¾ à¤›à¥à¤Ÿà¥à¤Ÿà¥€ à¤¹à¥ˆ&quot;,\n    &quot;æˆ‘æœ€å–œæ¬¢çš„èŠ‚æ—¥æ˜¯&quot;,\n    &quot;Min favorithelg Ã¤r &quot;\n]\ntransformers.set_seed(1337)\nfor text in texts:\n    input_ids = tokenizer.encode(text, return_tensors=&quot;pt&quot;).cuda()\n    out = model.generate(\n        input_ids, \n        min_length=100,\n        max_length=100,\n        eos_token_id=5, \n        pad_token=1,\n        do_sample=True,\n        top_k=0,\n        top_p=0.9,\n        no_repeat_ngram_size=4)\n    generated_text = list(map(tokenizer.decode, out))[0]\n\n```\nMy favourite holiday is ï¿½Thanksgivingï¿½ so, I wanted to share the recipe I made from a recipe I found on the fool, Flockish Street Bakery. The banana bread is delicious and a good way to treat those stained teeth. Everyone loves a chocolate treat, so I thought I would share it with you, hopefully others will like it too. This bread is SO good!! \n---\nÔ»Õ´ Õ½Õ«Ö€Õ¥Õ¬Õ« Õ¿Õ¸Õ¶Õ¶ Õ§ Õ·Õ¡Õ¿ Õ¬Õ¡Õ¾ Õ¥Õ²Õ¥ÕžÕ¬. Õ”Õ«Õ¹ Õ¸Ö‚ ÕºÕ¡Õ¯Õ¡Õ½ Õ°Õ¡Õ²Õ©Õ¡Õ¶Õ¡Õ¯ Õ°Õ¡Ö€Õ½Õ¿Õ¡ÖÖ€Õ«Õ¶\n---\nÐœÐ¾Ñ” ÑƒÐ»ÑŽÐ±Ð»ÐµÐ½Ðµ ÑÐ²ÑÑ‚Ð¾ Ñ” Ð Ñ–Ð·Ð´Ð²Ð¾\n---\nmi fiesta favorita es @marhuval__ La gente queremos fique muy feliz, estoy pensando en celebrarlo el 2 de abril \n---\nà¤®à¥‡à¤°à¥€ à¤ªà¤¸à¤‚à¤¦à¥€à¤¦à¤¾ à¤›à¥à¤Ÿà¥à¤Ÿà¥€ à¤¹à¥ˆ à¤¸à¥€à¤§à¥€ à¤°à¤¾à¤¤, à¤‡à¤‚à¤Ÿà¤°à¤¨à¥‡à¤Ÿ à¤¸à¥‡ à¤œà¥à¤¡à¤¼à¥‡ à¤¬à¤¹à¥à¤¤ à¤¸à¤¾à¤°à¥‡ à¤µà¤¿à¤•à¤²à¥à¤ª à¤¹à¥ˆà¤‚ à¤”à¤° à¤¯à¤¦à¤¿ à¤†à¤ª à¤µà¤¾à¤ªà¤¸ à¤¸à¥€à¤§à¥‡ à¤•à¤¿à¤¸à¥€ à¤˜à¤° à¤¬à¤¸à¥‹à¤‚ à¤®à¥‡à¤‚ à¤˜à¥à¤¸à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤†à¤ªà¤•à¥‹ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ à¤¬à¥ˆà¤ à¤•à¥‡à¤‚ à¤¹\n---\næˆ‘æœ€å–œæ¬¢çš„èŠ‚æ—¥æ˜¯-â€œä¿å«å›½â€æ—¥ï¼â€ æ¾³é—¨è®ºå›\næ¾³é—¨è®ºå£‡&lt;&lt; ä¸Šä¸€ç¯‡ï¼šç‚¹çŸ³æˆé‡‘ï¼æ­¦ç£Š\nä¸‹ä¸€ç¯‡ï¼šä½ è¿˜åœ¨çˆ±å¾—æµ‘èº«å‘æŠ–å—ï¼Ÿä½†å©´å„¿åœ¨å¦ˆå¦ˆèº«ä¸Š~~\n---\nMin favorithelg Ã¤r ute, og din blog er mÃ¸dested for sÃ¥ mange som muligt af dem i Ã¸jeblikket.\n```\n\n\n\nFull language list:  \nAfrikaans, Arabic, Armenian, Azeri, Bashkir, Basque, Belarusian, Bengali, Bulgarian, Burmese, Buryat, Chinese, Chuvash, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Kalmyk, Kazakh, Korean, Kyrgyz, Latvian, Lithuanian, Malay, Malayalam, Marathi, Moldovan, Mongolian, Ossetian, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Swahili,  Swedish, Tadjik, Tamil, Tatar, Telugu, Thai, Turkish, Turkmen, Tuvan, Ukrainian, Urdu, Uzbek, Vietnamese, Yakut and Yoruba.",
"date": "2022-04-21"
},
{
"vote": 2,
"title": "Where did Intent / Slot schema even come from?",
"text": "hi all, i'm writing some internal documentation at my company about labelset design best practices for NLU models. i'd like this documentation to be approachable by new hires / those maybe broadly familiar with classification tasks, but not NLU. it occurred to me that I don't even know where the idea for intent / slot schemata originated. it feels like trying to find a citation for the invention of the wheel. \n\n\n&#x200B;\n\n\nany pointers here? what's the earliest paper we can find that references intent / slot  ontologies or classification?",
"date": "2022-04-21"
},
{
"vote": 7,
"title": "Leetcode for NLP positions?",
"text": "Location: Germany, Baden-WÃ¼rttemberg\n\n\nI recently finished my dual-major MA in English and Computational Linguistics in Croatia, got my Goethe B2 certificate, and built a GitHub portfolio with some NLP/ML projects. I need to tweak my CV a bit, and I plan to start applying for positions in Germany within a week or two.\n\n\nI was wondering if I should focus my prep time for the interviews on Leetcode style problems or actually relevant coding exercises with Spacy/NLTK/TensorFlow etc.\n\n\nWhat are your experiences? Any additional tips are welcome, of course, especially if someone wants to provide feedback on my portfolio, and/or my CV when I'm done.\n\n\nHere's the GitHub link: \nhttps://github.com/SkarletXx\n\n\nNote: if relevant, I already live in Germany.",
"date": "2022-04-21"
},
{
"vote": 38,
"title": "Amazon releases 51-language dataset for language understanding",
"text": null,
"date": "2022-04-21"
},
{
"vote": 2,
"title": "Cross Validation using Simple Transformers for NER",
"text": "While implementing the code below for an IOB dataset with columns of word, labels and sentence_id for NER:- (retrieved from \nhttps://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example\n)\n\n\n14     # prepare cross validation \n15     n=5 \n16     kf = KFold(n_splits=n, random_state=42, shuffle=True) \n17  \n18     results = [] \n19  \n20     for train_index, val_index in kf.split(train_data): \n21             # splitting Dataframe (dataset not included) \n22         train_df = train_data.iloc[train_index] \n23         val_df = train_data.iloc[val_index] \n24         # Defining Model \n25         model = ClassificationModel(&#039;bert&#039;, &#039;bert-base-uncased&#039;) \n26             # train the model \n27         model.train_model(train_df) \n28             # validate the model \n29         result, model_outputs, wrong_predictions = model.eval_model(val_df, f1=f1_score) 30         print(result[&#039;f1&#039;]) \n31             # append model score 32         results.append(result[&#039;f1&#039;]) \n\n\n\nI received this error message:- \"ValueError: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.\"\n\n\nAny help is very much appreciated!",
"date": "2022-04-21"
},
{
"vote": 5,
"title": "Why does it say Import \"transformers\" could not be resolvedPylance\" when I try to import this specific package",
"text": null,
"date": "2022-04-21"
},
{
"vote": 0,
"title": "Feedback for LangSwap (Chance to Earn $25)",
"text": "[removed]",
"date": "2022-04-21"
},
{
"vote": 5,
"title": "NLP techniques related to distinguishing scenes in a story",
"text": "Say I have a story or novel with multiple scenes. Are there any NLP work/techniques that allow me to know when the author switches from one scene to another? Searching things like \"context\", \"scene\" in google does not yield good results.\n\n\nAssume that I do not have a list of characters' names to begin with and there may be nameless NPCs.",
"date": "2022-04-21"
},
{
"vote": 1,
"title": "Unable to get any TF-IDF values when analyzing Twitter data using R",
"text": "I am analyzing (using Tidytext) about 13,000 tweets that I downloaded using the AcademicTwitteR package. As far as I can tell, the problem is that when transforming the data to the Tidytext format, it for some reason assumes I only have 2 documents. As I understand it, each Tweet should be its own document. Can someone tell me what is wrong?\n\n\n tweet_words_total &lt;- tweets_total %&gt;% \n  select(text, id) %&gt;% \n  unnest_tokens(word, text)\n\nmy_stop_words &lt;- stop_words %&gt;% select(-lexicon) %&gt;% \n  bind_rows(data.frame(word = c(&quot;https&quot;, &quot;t.co&quot;, &quot;rt&quot;, &quot;amp&quot;,&quot;der&quot;,&quot;20&quot;)))\ntweet_clean_total&lt;-tweet_words_total%&gt;%\n  filter(!(word %in% stopwords(source = &quot;snowball&quot;)))%&gt;%\n    anti_join(my_stop_words)\n\n\ntotal_corpus&lt;-Corpus(VectorSource(tweet_clean_total))\n\ntotal_tdm&lt;-TermDocumentMatrix(total_corpus,\n                         control = list(weighting = weightTfIdf,\n                                        removePunctuation = T,\n                                        removeNumbers = T,\n                                        stemming = T,\n                                        stripWhitespace = T))\n\n\n\n&gt; total_tdm\n\n\n<<TermDocumentMatrix (terms: 16860, documents: 2)>>\n\n\nNon-/sparse entries: 16860/16860\n\n\nSparsity           : 50%\n\n\nMaximal term length: 36\n\n\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)",
"date": "2022-04-20"
},
{
"vote": 1,
"title": "Hey people im aspiring to work in nlg &amp; publish some research paper in a span of 6 months . Interested ones drop a messenge or DM me",
"text": "[deleted]",
"date": "2022-04-20"
},
{
"vote": 4,
"title": "NLP on Bugzilla issue tracking system",
"text": "Hello guys,\n\n\nI haven't had any experience in artificial intelligence yet, but I think that Natural Language Processing could be helpful here.\n\n\nWe have a issue tracking system which is based on bugzilla. We have about 300,000 entries in this system, where the entries are structured like this:\n\n\n&#x200B;\n\n\n*****************************\n\n\n-- Entered from Customer_XY on 8/4/2021 11:12:39 AM\n\n\nServer:\n\n\nDatabase:\n\n\nVersion:\n\n\nPriority: \n\n\nOwner:\n\n\n&#x200B;\n\n\nObserved Behaviour: Application is blocking since 20 min, Log file entries are as follows\n\n\n...\n\n\n&#x200B;\n\n\n&#x200B;\n\n\n-- From COMPANY/AZR on 9/4/2021 11:00:29 AM\n\n\nLogfile shows that stored procedure xy has been the main cause of the blockings, created new index xy.\n\n\nPerformance should be fine again.\n\n\n&#x200B;\n\n\n&#x200B;\n\n\n=> Maybe again answer from Customer \n\n\n...\n\n\n*****************************\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nSo this issue system also serves us as a knowledge base where many issues have been solved before.\n\n\nIt would now be interesting to have a tool or plug-in that could be integrated into our issue tracking system, which could already give possible solution suggestions based on the customer's entry.\n\n\nThis can be used as help for new \n\n\n&#x200B;\n\n\n&#x200B;\n\n\nI would like to learn more about AI technologies. My main concern now would be this:\n\n\n- What technologies could be helpful here?\n\n\n- The 300,000 entries could serve as training data: All entries written by a \"CUSTOMER\" are problems, while everything starting with COMPANY/xy contains the solutions.\n\n\n&#x200B;\n\n\nI would like to hear what technologies could be used here.\n\n\nThis could help new staff to answer customer problems by having the NPL already suggest what possible solutions to problems could be.\n\n\n&#x200B;\n\n\nThanks a lot",
"date": "2022-04-20"
},
{
"vote": 14,
"title": "Any upcoming biomedical NLP conferences?",
"text": "I recently had a paper rejected from NAACL 2022. I have revised it according to the reviewers recommendations (which weren't all that helpful anyway), and am looking for another suitable conference. The paper's broad topic is knowledge discovery using biomedical text corpora. \n\n\nSo far I have found the International Conference on Biomedical Natural Language Processing 2023 - I can't find much info about this conference, and also their website seems to have a lot of non-NLP submissions under \"selected papers.\" Seems kind of sketch tbh.\n\n\nIt seems like I missed the paper submission deadline for BioNLP 2022 at ACL. \n\n\nDo you guys know of any upcoming biomedical domain-specific NLP conferences that might be coming up and that are still open for submission? \n\n\nAny recommendations would be appreciated. Thanks.",
"date": "2022-04-20"
},
{
"vote": 8,
"title": "Comprehensive Certification for Conversational Design - from UX and copywriting to NLP and AÃ",
"text": "Hello, is there any chance someone could give me some advice for a conversational design certification? I would like a comprehensive course that would cover UX, copywriting and AI. \n\n\nI already work with it but the company I work for fails to help me grow my tool box and I am immigrating to Canada, so a NA certificate would help, since I have lost an opportunity for the lack of formal education on the field. Iâ€™m a differentiated professional with strong sales and customer service background and I do have formal education on data science but only have worked with NLP and honestly, thanks to YouTube, DataCamp and so on. \n\n\nIâ€™m looking into \nconversational design institute\n and since Iâ€™m not making dollars yet, the investment must be very well calculated! \n\n\nThanks in advance",
"date": "2022-04-19"
},
{
"vote": 1,
"title": "Decoder Pre-trained and Encoder Fine-Tuned?",
"text": "Hello I am currently writing my masters thesis and diving into the world of PLMs and NLP models.\n\n\nMy goal is to design an architecture of some kind of PLM model, that asks questions to a human and uses the answer to gain knowledge. Basically it should learn from dialogues with humans.\n\n\nI thought about using an pretrained Transformer model and subsequently use the question-answer pairs as labeld data for fine-tuning. That way a generic model should adapt more and more to the knowledge wihtin a certain domain.\n\n\nAt the moment I am considering whether I should go for an encoder or decoder model. I think they are not too much different but the encoder is better in nlu and the decoder ist better at nlg and few-shot learning, right?\n\n\nBecause I think these models arent so much different I got the following idea: Is it possible to use a unidirectional pre trained decoder model for having the QA dialogue with humans, while fine-tune the model on MLM (bidirecitonal). It is because I think a decoder model would be better in the communication but in the fine-tuning part (to adapt new information in the model, without forgetting the pretrained knowledge and not overfitting new knowledge) MLM seems like the better solution.\n\n\nDo you have any thoughts on this? \n\n\nThank you1",
"date": "2022-04-19"
},
{
"vote": 2,
"title": "Abstractive summarization TLDR Slackbot",
"text": "Weâ€™ve recently launched a new Slackbot that leverages abstractive summarization to TLDR webpages, text, and more. Currently working on adding support for summarizing YouTube videos.\n\n\nSo you share whatever you need summarizating with the Slackbot and you get TLDRs back.\n\n\nCheck it out\n! Currently looking for more feedback and ideas. We also have a \nbeta testing option\n if you want to give it a go.",
"date": "2022-04-19"
},
{
"vote": 4,
"title": "AI thesaurus",
"text": "Has anyone tried making a modern thesaurus trained on a massive corpus of famous writers which is just a neural machine translation engine with the source and target language the same language (a reflexive NMT engine) in order to serve as a modern thesaurus, not only word for word but creative, original, impressive rhetorical phrases and even entire suggested sentence rewordings?\n\n\nI think itâ€™s a good idea and has potential.\n\n\nThanks very much",
"date": "2022-04-19"
},
{
"vote": 3,
"title": "Can machine learning NLP be applied to ancient text/languages?",
"text": "I'm new to machine learning/NLP. Is it possible to apply machine learning to predict constructs/classes/categories in an ancient Hebrew or Sumerian corpus for example?  what is the best Python module or platform to use for this? I took a look at SpaCy and watched a video of Prodigy used on English text, can those be used?\n\n\nDoes it have to be in a dataset format? This is ancient hebrew dataset...word tokens can have up to 6 parts. The morphology breaks it all down:\n\n\nRef,Eng,Heb6,Heb5,Heb4,Heb3,Heb2,Heb1,Highlight,Morphology,Strongs\nGen.1.1-01,in the head,,,,,×¨Öµ××©××™×ª,×‘Ö¼Ö°,p,HR/Ncfsa,H9003=×‘=in/H7225=×¨Öµ××©×Ö´×™×ª=first_Â§1_beginning\nGen.1.1-02,he has cut-out,,,,,,×‘Ö¼Ö¸×¨Ö¸×,,HVqp3ms,H1254a=×‘Ö¼Ö¸×¨Ö¸×=to create\nGen.1.1-03,elohim,,,,,,×Ö±×œÖ¹×”×™×,,HNcmpa,H0430=×Ö±×œÖ¹×”Ö´×™×=God_Â§God@Gen.1.1\nGen.1.1-04,×Öµ×ª-,,,,,,×Öµ×ª,b,HTo,H0853=×Öµ×ª=obj.\nGen.1.1-05,the Heavenly-pair,,,,,×©×Ö¼Ö¸×žÖ·×™Ö´×,×”Ö¸,,HTd/Ncmpa,H9009=×”=the/H8064=×©×Ö¸×žÖ·×™Ö´×=heaven\nGen.1.1-06,×Öµ×ª-and,,,,,×Öµ×ª,×•Ö°,b,HC/To,H9002=×•=and/H0853=×Öµ×ª=obj.\nGen.1.1-07,,,,,×ƒ,×Ö¸×¨Ö¶×¥,×”Ö¸,,HTd/Ncbsa,H9009=×”=the/H0776=×Ö¶Ö«×¨Ö¶×¥=land_Â§3_planet/H9016=×ƒ=verseEnd",
"date": "2022-04-19"
},
{
"vote": 1,
"title": "SparseServer.UI : A UI to test performance of Sparse Transformers",
"text": "You can now load multiple transformers (each model has a unique sparsification recipe) on top of the DeepSparse server behind Streamlit, and it's open-source. This was battle tested on a 16GB of RAM with only 4 core CPU virtual machine. These compute requirements are enough to load up to 19 sparse BERT models in memory and compare their performance on question answering (P.S. they are really fast on just CPUs).\n\n\nðŸ’»code: \nhttps://github.com/neuralmagic/deepsparse/tree/main/examples/sparseserver-ui",
"date": "2022-04-19"
},
{
"vote": 3,
"title": "Using spaCy to find words describing the environment",
"text": "Hey guys, I am working on a program that produces illustrations from pages in story books and I need a way to find words that describe the environment (e.g. mountains, river, ceiling, floor, table, road etc.). Is there a way to do this in spaCy? also any other suggestions of words I can find and use to generate illustrations and how I could find them with spaCy would be appreciated.",
"date": "2022-04-19"
},
{
"vote": 29,
"title": "Why is natural language so hard to process?",
"text": "There are theories that the ambiguity from polysemy and homonymy (many to many matches between terms and meanings/concepts) is a result of optimizing language for communication of ideas between humans.\n\n\nSee for example:\nhttps://medium.com/ontologik/why-ambiguity-is-necessary-and-why-natural-language-is-not-learnable-79f0e719ac78\n\n\nBut frankly I am not at all convinced.  For example, while this explains why one word can have many meanings, how does it explain why a single meaning can be expressed in multiple ways?  And then there is all of the seemingly unnecessary complexity in natural language grammars.\n\n\nFrom my experience, it seems that the real reason is that there are at least two fundamental roles of natural languages. One role is to convey meaning, but another equally important role is to manipulate the thinking of the listener into some state desired by the speaker. The second role appears to have resulted in aspects of natural languages that actually obscure communication of ideas through ambiguity and complexity. This can be useful in poetry or motivational speeches, but when the aim is to transfer knowledge as accurately as possible, e.g. in a classroom or an academic conference, the second role gets in the way. \n\n\nIs anyone here familiar with such a theory and any work that has been done to prove/disprove it?",
"date": "2022-04-19"
},
{
"vote": 4,
"title": "Game Theory and NLP",
"text": "Anyone know of any work using game theory paradigms in NLP? I've been finding some fun ideas with alignment but nothing beyond that. Would appreciate if anyone can suggest some rabbit holes.",
"date": "2022-04-19"
},
{
"vote": 2,
"title": "At what level of text aggregation should I conduct topic modeling?",
"text": "I have a dataset of ~50k documents that are each several hundred sentences long, as well as various metadata about the documents. I have conducted several correlated topic models of these data in R using the stm package.\n\n\nThe problem is that the topics are hard to interpret because even high proportion documents only contain a small amount of text actually devoted to a particular topic. It's also not clear which parts of the document are about one topic vs. another (especially when the topics are related).\n\n\nFor this reason, I'm considering breaking up documents and modeling them at a smaller level of aggregation (sentences; document-folds), then getting document-topic proportions by summing these units within-document. \n\n\n&#x200B;\n\n\nIs this a reasonable strategy? Is there any work on topic modeling at different levels of text aggregation?",
"date": "2022-04-19"
},
{
"vote": 8,
"title": "UKRUWAR22: A Collection of Ukraine-Russia War Related Tweets Dataset",
"text": "It contains 55186 unique tweets in 57 different languages loosely coupled  to the Ukraine-Russia war. The details about the date and time of the tweet, language used, attachments URL, number of likes, retweets and replies, and hashtags used for each tweet are present in the dataset.  The data can be used for sentiment analysis and other NLP-related works.\n\n\nhttps://ieee-dataport.org/documents/ukruwar22-collection-ukraine-russia-war-related-tweets",
"date": "2022-04-18"
},
{
"vote": 21,
"title": "what is the easiest way to deploy a nlp model?",
"text": "I created a model and i have saved .pb file and the variables, from here on, how i deploy my model in a server? lets say i want to send inputs from a front end and get predicted data outputs from the model, how do I establish this?\n\n\nThank you",
"date": "2022-04-17"
},
{
"vote": 1,
"title": "Questions about ACL Rolling Review",
"text": "A few questions about ACL ARR:\n\n\n- If you request to reassign a reviewer, would the editor aim for reassigning all three reviewers or he would go for reassigning only that particular reviewer? Assume you have given a valid reason for reassignment and the editor is convinced.\n\n\n- If you request to reassign a reviewer, can the new reviewer see the previous reviews/scores before submitting his own review? Or he would access to the previous revision after submitting his own review.\n\n\nI already know (have heard) that in many cases reviewers are not available, and it becomes inevitable to get an entirely new set of reviews. I already know this. But my questions are about the case that the reviewer availability is not an issue. Juts trying to find out how things are managed",
"date": "2022-04-16"
},
{
"vote": 7,
"title": "Funds to spend on continued education / training",
"text": "My employer is sponsoring ~$1k of support for any continued training in text analysis / NLP / Python programming that I choose, to be used on any combination of short-courses, books, online subscription services, etc. This money will disappear if unspent. \n\n\nBeyond the free options available, has anyone found success or heard positive things about other specific paid resources? All suggestions are welcome, and perhaps this may benefit others in a similar situation as myself (bonus: resources that teach/use PyTorch over TF are a plus). \n\n\nMany thanks in advance.",
"date": "2022-04-16"
},
{
"vote": 7,
"title": "BERT Toxic text classification problem, detects cvs but not raw texts",
"text": "expected shape=(None, 128), found shape=(None, 3)\n\n\nWhy do i get this error when i pass in a single string to predict?\n\n\nI am trying out a hate speech detection model and it predicts whole datasets but not sentences, it throws this error,\n\n\nany help would be much appreciated! Thank you\n\n\nNotebook link: \nhttps://www.kaggle.com/code/abrh119/test-toxic-comments-bert",
"date": "2022-04-16"
},
{
"vote": 0,
"title": "Why does this notebook uses way too ram?",
"text": "Is there a way to run this notebook without exceeding the memory usage? \nhttps://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing\n any help would be much appreciated! thank you so much",
"date": "2022-04-16"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-04-16"
},
{
"vote": 17,
"title": "Salesforce AI Researchers Introduce Converse: A Task-Oriented Dialogue System That Simplifies Chatbot Building And Handles Complex Tasks",
"text": "One of the long-term ambitions of Artificial Intelligence (AI) has been to create a machine capable of having a meaningful conversation with a person and assisting them in performing tasks. For decades, many academics have dreamed of creating a strong chatbot that can assist humans with very difficult work through natural and fluent chats.\n\n\nRecent developments in this domain have made voice assistants and chatbots popular in our everyday lives. Task-oriented conversation systems, for example, can assist you in booking a table at a restaurant, purchasing airline tickets, cross-checking vaccination appointment status, or tracking the status of your transaction.Â \n\n\nHowever, frequently chatbots get trapped in an infinite loop or misunderstand whatâ€™s being said. This makes people want to connect to a human agent in case of emergencies or urgency, which negates the point of having the chatbot in the first place. Furthermore, building a robust chatbot that can handle several complicated tasks is challenging for many chatbot developers.Â \n\n\nThis article sheds light on a breakthrough achieved by Salesforce AI Research Engineers that helps developers quickly construct smart bots that assist users in performing tasks. The team introduces Converse, a flexible and modular task-oriented conversation system. Using an interactive configuration tool and a little code, bot creators can easily develop jobs in Converse.\n\n\nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/pdf/2203.12187.pdf\n\n\nGithub: \nhttps://github.com/salesforce/Converse",
"date": "2022-04-15"
},
{
"vote": 11,
"title": "Zero-shot and Few-shot Text Classification Methods",
"text": "Few-shot and Zero-shot learning aims for the ML models to make prediction under the scenario of Small number or ZERO examples of the relevant class for these models to learn from. ðŸ”¥ðŸ”¥\n\n\nThis tech report from Cloudera Fast-forward labs talks about various techniques for doing Zero-shot and Few-shot text classification.\n\n\nInterested? Then watch the full report walkthrough: \nhttps://youtu.be/2250AqR1RF8\n\n\nTech Report: \nhttps://few-shot-text-classification.fastforwardlabs.com/",
"date": "2022-04-15"
},
{
"vote": 0,
"title": "purpose of using re.escape()",
"text": null,
"date": "2022-04-14"
},
{
"vote": 1,
"title": "Deep Learning for Natural Language Processing - Applications of Deep Neural Networks to Machine Learning Tasks",
"text": "[removed]",
"date": "2022-04-13"
},
{
"vote": 4,
"title": "Any idea to improve the performance on multi-class classification problem?",
"text": "Hi there!\n\n\nI am recently working on a multi-class text classification problem with BERT. My training set contains 12800 sentences, which are uniformly distributed in 5 classes. The dev set contains 1600 samples. And I am using a pre-trained model \"bert-base-chinese\" from HuggingFace.\n\n\nThe problem is, my dev acc raised from 20% but stuck at 52%. Even if I tried different parameters, including lr, batch_size, maxLength of tokens and dropout rates, the performance didn't get better.\n\n\nCould someone help with this problem, and give some instructions to improve the performance? Or should I turn to another model?",
"date": "2022-04-13"
},
{
"vote": 16,
"title": "Supercharged UI for MLflow",
"text": "Hi guys, we've built a plugin that seamlessly reads MLflow logs and provides a beautiful UI to compare multiple runs with just a few clicks. You can:\n\n\nfilter runs with a super versatile fully pythonic search group and aggregate your metrics / images We are trying make it work seamlessly with MLflow and complement its other awesome features ðŸŽ‰\n\n\nHere is more info about it \nhttps://bit.ly/3xvyYbn\n Would love your feedback!!",
"date": "2022-04-13"
},
{
"vote": 1,
"title": "How to improve your video editing software with language technology AI API?",
"text": null,
"date": "2022-04-13"
},
{
"vote": 1,
"title": "[D] Expert Advice is needed on designing a feedback Loop for a (Textual Classification + NER) task in Production.",
"text": null,
"date": "2022-04-12"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-04-12"
},
{
"vote": 0,
"title": "Entity extractor",
"text": "I am trying to build an entity extractor that can extract the shop number from the full shop name (e.g. McDonalds F1287, result should be 1287). I tried making a custom spacy ner but it does not work at all (ent.text and ent.label are blank). How can I go about doing this? Does this need DNNs? LSTM, CNN? Or transformer models?",
"date": "2022-04-12"
},
{
"vote": 27,
"title": "Curriculum to learn NLP",
"text": "I'm planning on self-studying natural language processing. I'm wondering if someone can create an outline of a curriculum that I can follow. Something like prerequisites before studying NLP, like what math and linguistics concepts I should know. I am already confident in programming and can learn Python easily.",
"date": "2022-04-12"
},
{
"vote": 3,
"title": "Master Program in LT in Gothenburg or Potsdam",
"text": "Hello!\n\n\nI'm looking for a piece of advice where to do my Master's. I'm choosing between Cognitive Systems at Potsdam and LT at Gothenburg. I have a background in linguistics as my major and data science as my minor so I have enough programming background. I'm interested in research facilities and math courses. What should I choose?\n\n\nThank you for your answers",
"date": "2022-04-12"
},
{
"vote": 3,
"title": "Should I go BERT?",
"text": "Should I go BERT if I am just looking for tokenization, POS tagging? I know it is the state of the art way of doing NLP, but given I just need to execute those relatively simple tasks, should I go BERT or something easier?",
"date": "2022-04-11"
},
{
"vote": 14,
"title": "ClinicalBERT model for Medical NLP Tasks",
"text": "Hi,\n\n\nI am using the Clinical BERT model to de id the i2b2 dataset. However, this is the first time I am dealing with all this data science and code, and do not know how to go about doing this. Does anyone here have experience in using these Domain-Specific BERT models? If so, any help would be greatly appreciated!",
"date": "2022-04-11"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-04-11"
},
{
"vote": 2,
"title": "Where can I learn about style transfer in text?",
"text": "I'm interested in improving my writing by making a model to give suggestions on writing style. I've heard that there is work in style transfer for text. Where can I learn this, from the beginner's perspective to implementation? Thanks",
"date": "2022-04-10"
},
{
"vote": 15,
"title": "OpenAI Introduces DALL-E 2: A New AI System That Can Create And Edit Realistic Images And Art From A Description In Natural Language",
"text": "New research by the OpenAI team has released a new version of DALL-E, its text-to-image generation tool. \nDALL-E 2\n is a higher-resolution and lower-latency variant of the original system, generating images based on user-written descriptions. It also has additional features, such as altering an existing image.\n\n\nIn January of 2021, the first DALL-E, a portmanteau of the artist â€œSalvador Dalâ€ and the robot â€œWALL-E,â€ emerged, limited to AIâ€™s capacity to visualize concepts. The researchers aimed to address the difficulties with technical safeguards and a new content policy, lower its computational load and advance the modelâ€™s basic capabilities.\n\n\nInpainting, one of the new DALL-E 2 features, applies DALL-Eâ€™s text-to-image capabilities at a finer level. Users can begin by selecting a section of an existing photograph and instructing the model to alter it. For example, users can cover a painting on a living room wall with a new picture or put a vase of flowers on a coffee table. The model can also fill (or remove) objects while considering factors such as shadow directions in a room. Variations is another function that works as an image search tool for photographs that donâ€™t exist. Users can start with a single image and then make various modifications based on it.\n\n\nContinue Reading\n\n\nPaper: \nhttps://cdn.openai.com/papers/dall-e-2.pdf\n\n\n&#x200B;\n\n\nhttps://reddit.com/link/tzunwv/video/rywbl253nis81/player",
"date": "2022-04-09"
},
{
"vote": 7,
"title": "looking for a generative models company training in Germany",
"text": "Hi everyone. I'm looking for a company or organization that could provide a few days long training on basic nlp in python and training generative text models with huggingface. Onsite in office in Germany. Any recommendations?",
"date": "2022-04-08"
},
{
"vote": 14,
"title": "Dense Passage Retriever(DPR) Open-QA System",
"text": "Hi, I made a video explaining \nDense Passage Retriever(DPR)\n paper. We specifically explain the End to End QA system suggested in the latter part of the paper which discusses how to build an Open-QA system using dense retrievers. \n\n\nDPR was one of the first papers that discussed building dense retrievers using QA pairs only and didn't require a big pretraining computational setup like ORQA or REALM. It is currently used in a lot of places as a dense retriever. You can find \nHugginface\n and \nHaystack\n implementations also. \n\n\nThis video is part of a series on Open-QA using dense retrievers. We have made 2 videos on DPR. In the latter, we discuss how to build a dense retriever from scratch. Thanks for the support and it would be great if you could give any feedback.\n\n\nhttps://www.youtube.com/watch?v=rvcyyJNjPU0",
"date": "2022-04-08"
},
{
"vote": 2,
"title": "Amazon Researchers Developed a Universal Model Integration Framework That Allows To Customize Production Voice Models in a Quick and Scalable Way",
"text": "Alexa and other voice assistants frequently use a range of speech synthesizers, which varies in terms of expressivity, personality, language, and speaking style. The machine learning models that underpin these applications can have vastly diverse architectures. Integrating them into a single voice service is time-consuming and difficult.\n\n\nA new Amazon research presents a universal model integration framework that enables quick, scalable customizable production voice models.\n\n\nContinue Reading",
"date": "2022-04-06"
},
{
"vote": 1,
"title": "Auto generated text based on many conditions",
"text": "Hi,\n\n\nI am new to language processing. I have to do some research on a specific use cases:\n\n\nThe input values are small product descriptions e.g. and a category e.g. \"beverage\":\n\n\n\n\nCOCA COLA LIGHT 20x0,33L GLAS\n\n\n\n\nOutput:\n\n\n\n\nCoca Cola Light 0,33L\n\n\n\n\nBased on the product category there are many conditions as how the output has to look like (e.g. the input contains the word \"vegan\" it has to be the first word in the output etc.). What are some keywords for me to start researching? I came across models like \" GPT-3\", but from what I understood these are more for natural long text creations and not for outputs based on many conditions?\n\n\n&#x200B;\n\n\nSo what I am looking for is to get some keywords to guide me in the right direction\n\n\nthanks guys",
"date": "2022-04-06"
},
{
"vote": 4,
"title": "Methods for measuring inter-word similarity or relatedness",
"text": "What are common (traditional) methods to estimate the degree of semantic similarity or relatedness between words?\n\n\nI am looking for a reasonable baseline to compare \"cosine similarity on word embeddings\" with.",
"date": "2022-04-06"
},
{
"vote": 1,
"title": "Word representations with positive weights",
"text": "The dense vectors usually contain negative and positive values for features. Is there a representation scheme apart from TFIDF and CountVectoriser to positively weight the features for text classification?",
"date": "2022-04-06"
},
{
"vote": 1,
"title": "Chatbot KPI examples every marketer should track",
"text": "[removed]",
"date": "2022-04-06"
},
{
"vote": 2,
"title": "Classifying sections of a document",
"text": "Hi! I work with clinical documents and am trying to develop a method of section classification. Rules-based methods perform okay, but I'm curious about deep learning methods as well. The only neural approach I've seen is \nhttp://www.oeft.de/su/pdf/specom2018.pdf\n, and it only does binary classification of boundaries. \n\n\n&#x200B;\n\n\nI was thinking of using a longformer and using token classification modeled after NER. But these would be really long \"entities\". Anybody have any experience/recommendations for this type of problem?",
"date": "2022-04-05"
},
{
"vote": 34,
"title": "Google AIâ€™s Latest 540-Billion Parameter Model (Pathways Language Model Called PaLM) Unlocks New Tasks Proportional To Scale",
"text": "In recent years, large neural networks trained for language recognition and creation have shown remarkable outcomes in various tasks. GPT-3 demonstrated that large language models (LLMs) could be utilized for few-shot learning and achieve outstanding results without significant task-specific data or model parameter modification. Recent LLMs, including GLaM, LaMDA, Gopher, and Megatron-Turing NLG, have scaled model size, used sparsely activated modules, and trained on larger datasets from more diverse sources to attain state-of-the-art few-shot performance on numerous tasks.\n\n\nIn a recent \nresearch paper\n, Google researchers introduced \nPathways Language Model (PaLM)\n. PaLM is a 540-billion parameter, dense decoder-only Transformer model learned with the Pathways system that allowed efficient training of a single model across several TPU v4 Pods. PaLM was tested on hundreds of language understanding and generation tasks, and it was discovered that it achieved state-of-the-art few-shot performance across the board, in many cases by a large margin.\n\n\nRead this summary in a little more \ndetail Here\n\n\nPaper: \nhttps://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf\n\n\nGoogle blog: \nhttps://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html",
"date": "2022-04-05"
},
{
"vote": 2,
"title": "Scalable framework lets multiple text-to-speech models coexist",
"text": null,
"date": "2022-04-04"
},
{
"vote": 5,
"title": "Edinburgh NLP taught masters vs cambridge research masters",
"text": "I was wondering if anyone here has done the Edinburgh masters and has any thoughts on it vs a research masters. Would the taught course be more likely to give me stronger practical skills compared to a more self-directed research programme focusing on a narrower part of NLP? I'm coming from a linguistics background + mostly self-taught on the computational side of things, so going through a more rigorous, organised course is kind of appealing. I think the end goal is going into industry after the masters, not entirely sure if I might want to end up doing a phd - if that affects where I should go. thanks in advance :)",
"date": "2022-04-04"
},
{
"vote": 3,
"title": "Questions regarding Empath module",
"text": "Hi,\n\n\nI'm using Empath module (\nlink\n) to analyze topics of texts. Is there any way I can check which specific word/ term is categorized into which general big topic (e.g 'violence', 'negative_emotion',...) in the Module?\n\n\nThanks for your help!",
"date": "2022-04-04"
},
{
"vote": 3,
"title": "Determine text quality",
"text": "I very often work with unlabeled and unstructured reviews data. You wouldnâ€™t believe what â€œadultsâ€ will write when they assume the text wouldnâ€™t ever see the light of day. \n\n\nAnyways, I want to calculate a quality metric in order to identify quality and poor text reviews. My first thought went to perplexity and coherence but if someone has any better ideas feel free to share.",
"date": "2022-04-04"
},
{
"vote": 0,
"title": "can NLP professor teach CV?",
"text": "I'm 1st year in phd  and my interest lies in medicine/bio so i think probably CV fits me\n\n\nbut prof who's specializing in CV doesn't take me\n\n\nwhile prof whose specialty is NLP is willing to take me.\n\n\nCan I go for the latter prof?\n\n\n&#x200B;\n\n\n&#x200B;\n\n\np.s  if my subject is Clustering algorithm, can NLP prof teach me?",
"date": "2022-04-03"
},
{
"vote": 11,
"title": "NLP for music generation",
"text": "[deleted]",
"date": "2022-04-02"
},
{
"vote": 23,
"title": "The Token-Dropping Approach Used By ML Researchers From Google and NYU Reduces BERT Pretraining Time And Cost By 25%",
"text": "The Pretraining of BERT-type large language models, which may scale up to billions of parameters, is essential to achieving best-in-class performance on various natural language processing (NLP) applications. However, the pretraining procedure is costly, and it has become a hurdle for the industrial deployment of big language models.\n\n\nIn a research paper, researchers from Google, New York University, and the University of Maryland recommend a simple but effective â€œtoken droppingâ€ method that drastically reduces the pretraining cost of transformer models like BERT while maintaining downstream fine-tuning performance.\n\n\nToken dropping is a technique for speeding up the pretraining of transformer models like BERT without sacrificing their performance on downstream tasks. Starting with an intermediate layer in the model, they eliminate uninteresting tokens to let the model focus on key tokens more effectively, given its limited computing resources. The modelâ€™s last layer then picks up the dropped tokens, producing full-length sequences. They use the built-in masked language modeling (MLM) loss and its dynamics to detect non-essential tokens with little computing complexity. According to their tests, this straightforward strategy decreases BERTâ€™s pretraining cost by 25% while yielding somewhat higher overall fine-tuning performance on conventional downstream tasks.\n\n\nContinue Reading The Summary\n\n\nPaper: \nhttps://arxiv.org/pdf/2203.13240.pdf\n\n\nGithub: \nhttps://github.com/tensorflow/models/tree/master/official/projects/token\\_dropping",
"date": "2022-04-01"
},
{
"vote": 1,
"title": "How to draw associations between Stanza entities (Named Entity Recognition)",
"text": "[deleted]",
"date": "2022-04-01"
},
{
"vote": 0,
"title": "How Chatbot is beneficial in the Retail Industry?",
"text": "[removed]",
"date": "2022-04-01"
},
{
"vote": 5,
"title": "Pro simplification software",
"text": "Iâ€™m an NLP hobbiest and noob.  Iâ€™m looking for a program that will simplify text to a particular grade/Lexie level.  I found some libraries based off of the Newsela corpus, but it is probably beyond me to get it all running.  Iâ€™ve found a few websites as well but they are not great.  Iâ€™m creating social studies content and my writers are bad at leveling.",
"date": "2022-03-31"
},
{
"vote": 4,
"title": "Training BERT models on distinct text fields, best approach?",
"text": "I am using BERT models for text classification. The application is literature classification, where I am utilizing both the title and abstracts of the papers for classification.\n\n\nI am wondering what is the best way to classify things that have distinct text fields. Currently, I am just using string concatenation to create a long string containing the two fields \ne.g.\n for a paper with the title: \"whatever the title is\" and abstract:\"this is the abstract\" it becomes \"whatever the title is this is the abstract\". But this loses the context of these fields being distinct. \n\n\nI have considered prepending the fields with a label \ne.g.\n \"TITLE: whatever the title is ABSTRACT: this is the abstract\". I feel this may provide more context to the model. But I am wondering if there is a more standard and better way to do this. I am sure a similar problem has been solved for instance with email subject vs. text. Do I just need to train separately on the different fields?",
"date": "2022-03-31"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-31"
},
{
"vote": 1,
"title": "Open Domain Question Answering",
"text": "[removed]",
"date": "2022-03-31"
},
{
"vote": 2,
"title": "Research using GPT-3?",
"text": "Hi, I'm currently working on a project to generate short text using generative models such as GPT3 or BART. I was wondering how research on GPT3 is reported as?  You can't possibly just report how much accuracy your prompts are returning, it doesn't seem like much. Does anyone have any references on GPT3 (or text generation) research is done?",
"date": "2022-03-30"
},
{
"vote": 10,
"title": "Hackernews classified by topic with Huggingface",
"text": null,
"date": "2022-03-30"
},
{
"vote": 4,
"title": "BioScope corpus clinical notes, where to find",
"text": "Hello,\n\n\nI am trying to find a \nsubset of the BioScope Corpus\n, concretely the \nClinical notes.\n This file was hosted in a webpage \nhttp://www.computationalmedicine.org/catalog\n\n\nAccording to BioScope \nwebpage\n:\n\n\nClinical free-texts\n: The radiology report corpus that was used for the\n \nCMC clinical coding challenge\n. The negation/hedge annotated version of the corpus can be obtained (due to licencing issues) by downloading the original 'ICD-9-CM coding' corpus from Cincinatti Children's Hospital site and merge it with our annotation:\n \nreadme\n,\n \nmerger software\n.\n\n\nThe links are dead:\n\n\nhttps://ncc.cchmc.org/prod/pestianlabdata/request.do\n\n\nhttp://www.computationalmedicine.org/catalog\n\n\n&#x200B;\n\n\nIs there any available source for getting this corpora, \n\n\nThanks,",
"date": "2022-03-30"
},
{
"vote": 1,
"title": "Can I do more epochs on a saved HuggingFace model?",
"text": "This can be pretty obvious, so thanks for bearing with me!\n\n\nLet's say I trained a huggingface model for 3 epochs and then I saved the model. Can I reload the model and do more epochs from where I left off?",
"date": "2022-03-30"
},
{
"vote": 5,
"title": "AI Podcast: from NLP to Protein-folding",
"text": null,
"date": "2022-03-29"
},
{
"vote": 14,
"title": "Is it possible to measure the intensity of emotion in text using BERT?",
"text": "Hello there, I'm a newbie and still learning NLP, so correct me if I am wrong! It will be a huge help.\n\n\nI'm trying to measure the intensity of anger and/or fear in e-petition data, then analyze whether the intensity of emotion affects the number of signatures.\n\n\nSince I could not find a quality emotion dictionary in my language, I have to create one myself.\n\n\nInitially, I was going to use word2vec and make a list of words closest to \"anger\" and \"fear\", then do the usual dictionary-based sentiment analysis (tagging the intensity of emotion of each petition based on the frequency of word appearances).\n\n\nThen I found out BERT has become the SOTA nowadays, and now I am wondering whether I should use BERT in my case.\n\n\nFrom what I have read, BERT seems to be used for classifying emotion, not measuring the intensity of emotion. But since I am not an expert in this field, I wanted to make sure and ask smarter people to be sure!\n\n\nTL; DR: Is it possible to measure the intensity of emotion in text using BERT?\n\n\nThank you so much in advance!",
"date": "2022-03-28"
},
{
"vote": 0,
"title": "GloVe Question",
"text": "Hi all! Iâ€™m trying to use visual studio to run GloVe but it canâ€™t run the demo shell scriptâ€¦ What does anyone here use to run GloVe? Or, is there any resources for someone who doesnâ€™t know much about programming to follow along with? Thanks.",
"date": "2022-03-27"
},
{
"vote": 17,
"title": "Few-shot NER",
"text": "https://youtu.be/DnQKOuG-I_0\n\n\nFew-shot NER is the task of making work named entity recognition (NER) systems when a small number of in-domain labeled data is available.\n\n\nIn this video, I discuss in details the inner workings of \"Concise Concepts\" python library that facilitates the FSL NER. ðŸ¤©",
"date": "2022-03-27"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-26"
},
{
"vote": 14,
"title": "Explainability for Semantic Search",
"text": null,
"date": "2022-03-24"
},
{
"vote": 1,
"title": "Researchers From UF Health and NVIDIA Build Worldâ€™s Largest Clinical Language Generator, â€˜SynGatorTronâ€™, To Develop Better AI For Rare Disease Research and Clinical Trials",
"text": "A neural network that generates synthetic clinical data is in high demand, as it is a valuable resource that researchers may use to train other AI models in healthcare. Synthetic data allows developers to create simulated patient data free of sensitive protected health information and manufactured with statistical realism. Team NVIDIA and UF Health, The University of Floridaâ€™s academic health institution, have teamed up to do precisely that.\n\n\nS\nynGatorTron\n is a language model developed by the team that can construct synthetic patient profiles based on the health records it has learned from. This model is ranked #1 among the language generators available in the healthcare field, with the ability to handle 5 billion parameters.\n\n\nContinue Reading",
"date": "2022-03-24"
},
{
"vote": 22,
"title": "Few-shot NER: entity extraction without annotation and training based on GPT",
"text": "Hello all,\n\n\nAfter 1 year working extensively with GPT models (GPT-3, GPT-J, and GPT-NeoX), I think I now have a good view on what these NLP models are capable of. It appears that many traditional NLP tasks can  now be achieved thanks to these large language models thanks to few-shot learning (aka \"prompting\", or \"prompt engineering\").\n\n\nNER is a very good candidate because, thanks to these models, it is possible to extract any type of entity without ever annotating and training a new model. Annotation has always been a challenge that has caused many entity extraction projects to simply fail, because it is a long and tedious process.\n\n\nIn this article, I'm showing how easy it is to perform NER thanks to GPT and few-shot learning, without any annotation process: \nhttps://nlpcloud.io/few-shot-ner-entity-extraction-without-annotation-training-based-on-gpt.html\n\n\nIf you also experimented with entity extraction with GPT models, I would love to hear your thoughts. Are you, like I am, impressed by the results? And do you think it means that annotation is a thing from the past?\n\n\nThanks!",
"date": "2022-03-24"
},
{
"vote": 0,
"title": "Is my laptop good enough?",
"text": "Hello,\n\n\nI am new here in the subreddit, so, Hello!\n\n\nI'll be taking NLP and Python courses in a Translation Technologies MA program.\n\n\nHere are the courses if you would like to take a closer look. I will be attending to Ghent University for my 1st year, so the relevant list of courses is 1.4\n\n\n(\nhttps://studiekiezer.ugent.be/master-of-arts-in-technology-for-translating-and-interpreting-en/programma/2022\n)\n\n\nI have a macbook air 2020 i3 - when I asked them about the required specs for the coursei they said it was fine.\n\n\nHowever, I was thinking of upgrading my laptop lately and I was considering a new apple silicon macbook.\n\n\nI don't know what is the python situation with the new M1 machines.\n\n\nI did some research but I couldn't be sure since I am new to computational linguistics.\n\n\nMy friend told me, he is an AI programmer and he graduated from Kent UK, that I would be using cloud computing platforms and unix-based machines such as macbooks offer a better experience in this regard.\n\n\nMy girlfriend also confirmed this, she did her MA in microbiology and used cloud computing for data science.\n\n\nIn the end, I want a bigger screen and I feel the i3 will slow me down.\n\n\nAre M1's good for this stuff or should I stick to intel.\n\n\nAlso, maybe I can get a second hand 15\" MacBook Pro.\n\n\nIf that is a viable option which generation of CPUs should I stay away from?",
"date": "2022-03-24"
},
{
"vote": 13,
"title": "Open Relation Extraction is difficult",
"text": "Hi everyone,\n\n\nI am currently trying to figure out what kind of relation extraction task / approach would be best for  the following problem:\n\n\nGiven a sentence, I want to extract triples (named entity 1, relation, named entity 2). The named entity extraction step is taken care of, so now I only need the relation. So the question becomes: What should the relation look like / What relations am I looking for?\n\n\n\"Classical\" supervised Relation Extraction is mostly focused on n classes (Born_In, Located_In etc.) and because of that too restricted, IMO. At the same time 96 different relations (DocRed Dataset) would capture a lot of information as well.A supervised approach will also fail to find new/unseen relations. Zero Shot Adaptability could prove handy here and I have seen models performing reasonably good but the model would not be able to just generate a new relation if it encounters something unseen.\n\n\nOn the other side of the spectrum, we have Open Information Extraction (OpenIE) or Open Relation Extraction (OpenRE) which focuses on extracting \"any\" relation and these relations would probably be represented by the verbs in the sentence.I like the idea of that because all the information would be captured (high recall) but there would probably be a lot of noise. The problem here is that I want to transfer the triples into a knowledge graph and so I would need a mapping for each extracted Relation to its corresponding knowledge graph relation. I read about Canonicalization but I am not too familiar with the concept.\n\n\nI am leaning towards OpenIE / OpenRE, but I am having difficulties finding relevant papers and the SOTA models.  OpenIE6 and  M2OIE seem to be the newest / best ones. But the BenchIE dataset reports them to be worse than ClausIE, a model that extracts VP-mediated facts from 2013 (\nsource\n)  and the Github implementation is quite bad tbh.Has anyone worked with OpenIE models before? What are your experiences?\n\n\nIs anyone aware of any other approaches that are worth taking a look at? I am open to any remotely connected idea or approach to the problem because I am kind of losing my mind over finding something good.\n\n\nOtherwise, I will spend the next year implementing my own OpenIE/OpenRE system. (/s hopefully)  \n\n\nEDIT: I will probably use AllenNLP for the OpenIE task. It seems way faster than CoreNLPs Model.\n\n\n&#x200B;",
"date": "2022-03-23"
},
{
"vote": 3,
"title": "What kind of technology should I use to get pronunciation, pos on top of tokenization?",
"text": "New to ML, please bear with me if I am missing something obvious. I am writing a convertor that parses Cantonese/Chinese sentence, turning them from a simple string to objects that describe their POS and pronunciation correctly.\n\n\nI am using a hashmap based bi-directional maximal segment algorithm right now, it can segment correctly, but it cannot solve the ambiguity of pos and romanization, which would depends on characters before the one with ambiguity.\n\n\nExample:\n\n\nExample Input: é£Ÿé£¯æž±å¥½é•·\n\nExpected output: [\n{ word: &quot;é£Ÿé£¯æž±&quot;, pos: &quot;noun&quot;, romanization: &quot;sik6faan6toi2&quot; },\n { word: &quot;å¥½&quot;, pos: &quot;adv&quot;, romanization: &quot;hou2&quot; },\n { word: &quot;é•·&quot;,\n // should be &quot;adj&quot;,\n because &quot;å¥½&quot; is used before pos: &quot;adj/verb&quot;,\n // should be &quot;coeng4&quot;, because &quot;å¥½&quot; is used before\n  romanization: &quot;zoeng2/coeng4&quot;\n}]\n\n\n\nIf I want to handle this, can HMM or CRF(if not, what other algorithm should I be using?) handle this? Because based on the example I can found online, it seems like they can only handle label of a single facet(segmentation). Can it handle segmentation, pos and romanization at the same time? What option do I have?",
"date": "2022-03-23"
},
{
"vote": 3,
"title": "Retrieval Augmented Generation (RAG) explained",
"text": "In this video, we explain RAG or Retrieval Augmented Generation. We discuss \nhow to do Open-QA using a generative QA model\n. We discuss in detail both formulations mentioned in the paper, RAG Sequence Model and RAG Token Model. We cover the mathematical formulations and how they are done in code.\n\n\nThe video is part 4 of 8 video series on Open Domain Question Answering. If you are interested in Open-QA or want to know more about it do check out the playlist on \"Open-Domain Question  Answering\" on the channel.\n\n\nI will really appreciate any feedback. Thanks.\n\n\nhttps://www.youtube.com/watch?v=G-AV-kU6qbk",
"date": "2022-03-23"
},
{
"vote": 3,
"title": "Reduce a set of rules",
"text": "Hy guys. I am working on my final project and i got a bit stuck. I manage to generate a set of about 4000 if rules ( e.q. If a > 23 and b <3 then c). I am trying to find a way to reduce the number of rules to get more general ones. Any idea or framework to help would be greatly appreciated. Thanks in advance guys :3.",
"date": "2022-03-23"
},
{
"vote": 1,
"title": "New Partnership Announcement ðŸ¤ðŸ”‰",
"text": "[removed]",
"date": "2022-03-23"
},
{
"vote": 7,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-22"
},
{
"vote": 1,
"title": "Do i always need to finetune hugging face models?",
"text": "I am using models as is to try classification,\n\n\nUnfortunately result for phrase \"i do not like you\" is [0.49945366 0.50054634] meaning that 49.99% if thinks it's negative and 50.01% it thinks it's positive.\n\n\nAll tutorials i saw doo pretraining first on IMDb. Do i need to do that? I thought it's already pretrained and i only need to finetune it.\n\n\ntokenizer = AutoTokenizer.from_pretrained(&#039;distilbert-base-uncased&#039;)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;, num_labels=2)\ntext = &quot;i do not like you&quot;\nencoded_input = tokenizer(text, return_tensors=&#039;tf&#039;)\noutput = model(encoded_input)\nres = tf.nn.softmax(output.logits, axis=1).numpy()\nprint(res)\n\n\n\nEdit: Looks like that model is expected to be finetuned. I used \"\ndistilbert-base-uncased-finetuned-sst-2-english\n\"\n model which is finetuned for sentiment analysis and it produces good results.",
"date": "2022-03-21"
},
{
"vote": 22,
"title": "OpenAI Releases New Version of GPT-3 and Codex That Can Edit or Insert Content Into Existing Text",
"text": "Text processing is a common task in many machine learning applications. These tasks deal with large amounts of text to conduct classification or translation, which necessitates a lot of back-end labor. Itâ€™s challenging to turn text into something that an algorithm can understand.\n\n\nThe OpenAI team has launched new versions of GPT-3 and Codex that can update or insert stuff into it. Instead of only completing the existing text, \nOpenAI API\n can now be used to alter existing content. This includes rewriting a paragraph of text or reforming code, thanks to these additional capabilities. The new work has opened up new possibilities while improving existing ones; for example, insertion is currently being tested in GitHub Copilot, promising early results.\n\n\nContinue Reading Our Summary",
"date": "2022-03-21"
},
{
"vote": 1,
"title": "ðŸ”¥ How to use Hasty Annotation tool Part#1",
"text": null,
"date": "2022-03-21"
},
{
"vote": 11,
"title": "Is it possible to train a model to understand English and then fine tune it for a specific purpose?",
"text": "I want to start this project to learn more about NLP but I want to know if it is also possible to do so. I want to create a recipe generator in my native language (not english) but the dataset that I have for it is really small (max like 40k recipes) so I was thinking of teaching the model how to understand my native language first and then fine tune it using the dataset that I have for the final target purpose that is predicting a recipe and the steps given some recipe ingredients. Thanks in advance if you're reading this.",
"date": "2022-03-20"
},
{
"vote": 13,
"title": "Code + Data for Paper (AAAI 2022): Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers",
"text": "You can find the paper here:\n\n\nhttps://arxiv.org/abs/2201.11870\n\n\nAnd the code and the data here:\n\n\nhttps://github.com/p-karisani/CEPC",
"date": "2022-03-19"
},
{
"vote": 5,
"title": "Paraphraser tool",
"text": "Hi everyone, I need your suggestions on a tool project to paraphrase articles.\n\n\nHere is what I want to do.\n\n\n-Input: text/URL (500-10.000 words per article)\n\n\n-Output: Paraphrased text/article\n\n\nI googled it and  found some proposed solutions which seems ok for my purpose, for example using Parrot Paraphraser:\n\n\nhttps://github.com/PrithivirajDamodaran/Parrot_Paraphraser\n\n\nhttps://huggingface.co/prithivida\n\n\nhttps://www.youtube.com/watch?v=C6gBcL9sAIw\n\n\nOr using an API:\n\n\nhttps://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07\n](\nhttps://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07\n)\n\n\nhttps://rapidapi.com/healthytechguy/api/paraphrasing-tool1/\n\n\n-I do not want an expensive tool coded from scratch\n\n\n-I do not want to use paid tools\n\n\n-I want paraphrased text to be readable (no loss of meaning)\n\n\n-Without grammatical errors\n\n\n-With proper punctuation\n\n\n&#x200B;\n\n\nI guess I need to find  existing trained models for every niche my articles in or train my models from scratch? I am not sure...\n\n\nBut, I am not technical to judge whether those proposed solutions is good for my purpose or not.\n\n\nExpert suggestions would be highly appreciated.\n\n\n&#x200B;\n\n\nThanks in advance.",
"date": "2022-03-19"
},
{
"vote": 0,
"title": "need some help in our collage ðŸ¤ž project its 80% is completed",
"text": "this is an NLP and python based project we are trying to achieve something new this project is almost there but the only file connecting thing is leftoverðŸ˜Œ\n\n\nplease dm me for more information/collaborationðŸ˜Š\n\n\nwe are open to welcoming you to this project\nðŸ¤—\n\n\nnot a paid workðŸ™„",
"date": "2022-03-18"
},
{
"vote": 3,
"title": "Question about unsupervised fasttext learning",
"text": "Hi,\n\n\nI'm not sure if this is the right place but I thought I'd ask. I'm trying to implement fasttext (or something similar) myself to try and go beyond some simple DL models. I think there are really two tasks, the supervised one to classify and the unsupervised to get embeddings like with word2vec.\n\n\nI think the classification one is pretty simple. You get the embeddings of all the words/subwords in a sentence, average them and use that to compare to the class.(like in the old Keras example here: \nhttps://github.com/keras-team/keras/blob/keras-1/examples/imdb_lstm.py\n)\n\n\nI am not entirely sure about the unsupervised learning though. Is it essentially like skipgram? I've been trying to build my own to make sure I get it... is the only difference now that the target vector is actually the sum of the vector for the word and subwords?\n\n\nUsing the Keras code below as a basis (from \nhttps://www.kaggle.com/kesarianubhav/skipgram-word2vec\n) is   the difference that my target embeddings (w) are now multiple ones that I would sum after the \"Dot\" layer (essentially the loss function)? It seems almost too simple!\n\n\ndim_embedddings = 128\n\n# inputs\nw_inputs = Input(shape=(1, ), dtype=&#039;int32&#039;)\nw = Embedding(V, dim_embedddings)(w_inputs)\n\n# context\nc_inputs = Input(shape=(1, ), dtype=&#039;int32&#039;)\nc  = Embedding(V, dim_embedddings)(c_inputs)\no = Dot(axes=2)([w, c])\no = Reshape((1,), input_shape=(1, 1))(o)\no = Activation(&#039;sigmoid&#039;)(o)\n\nSkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\nSkipGram.summary()\nSkipGram.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;adam&#039;)\n\n\n\nAlso, as an aside - if I was limited in memory could I use a common embedding layer?\n\n\nThanks!",
"date": "2022-03-18"
},
{
"vote": 9,
"title": "Topic Modeling: Use SVD &amp; NMF in Python to Find Topics in Text",
"text": null,
"date": "2022-03-17"
},
{
"vote": 8,
"title": "Eigendecomposition appears repeatedly in machine learning, sometimes as the key step of the learning algorithm itself. This video intuitively explains the maths behind one of the most important topics in linear algebra - Eigendecomposition. #MathsforMachineLearning",
"text": null,
"date": "2022-03-16"
},
{
"vote": 3,
"title": "Request for an alternative to 'Annotations' app for OSX",
"text": "Hi, \n\n\nI'm not quite sure if this is the appropriate subreddit for this request, but I am searching for an application compatible with OSX that has the same functionality of \nhttps://www.annotationsapp.com\n, which stopped receiving updates in 2018. It's an app that allows you to highlight and annotate text documents in a simple user interface not unlike the Notes app. \n\n\nI do close readings of short fiction and perform analyses of the stylistic and narratological aspects of the texts that require me to make 100s of annotations. I frequently tag the same lines of text with different labels or annotations.\n\n\nIf anyone could direct me to an alternative application or another subreddit community that can offer advice, I would be eternally grateful.",
"date": "2022-03-15"
},
{
"vote": 14,
"title": "MSc. Program in Natural Language Processing UniversitÃ© de Lorraine, Nancy (France)",
"text": "Did somebody start the 2nd year of the MSc. Program in Natural Language  Processing UniversitÃ© de Lorraine, Nancy (France) without attending the 1st year? Could you tell me what courses you had to take in 1 semester of the 2nd year? Do you have to catch up a lot?",
"date": "2022-03-14"
},
{
"vote": 2,
"title": "Entity Extraction with Large GPT Models",
"text": null,
"date": "2022-03-14"
},
{
"vote": 3,
"title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing (Paper Summary)",
"text": "HealthPrompt is a novel Zero-Shot Learning(ZSL) clinical NLP framework applied the paradigm of prompt-based learning on clinical texts. \n\n\nZero-Shot Learning(ZSL) refers to the use of deep learning models to classify instances from new classes without having seen any training data for those classes.\n\n\nThe authors show that prompts effectively capture the context of clinical texts and perform remarkably well without any training data.\n\n\nPaper summary at \nhttps://youtu.be/1SJm6Zr5yAU\n\nPaper link at \nhttps://arxiv.org/pdf/2203.05061v1.pdf",
"date": "2022-03-14"
},
{
"vote": 6,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-14"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-14"
},
{
"vote": 4,
"title": "[D] Will Attention Based Architecture / Transformers Take Over Artificial Intelligence?",
"text": "A \nwell popularized article\n in Quanta magazine ask the question Â« Will Transformers Take Over Artificial Intelligence? Â».  Since having revolutionized NLP, attention is conquering computer vision and reinforcement learning. I find pretty unfortunate that the  attention mechanism  was totally eclipsed by Transformers which is just a funny name (animation  movie/ toy) for self-attention architecture, although the Google's paper title on Transformers was Â«\nAttention is all you need\nÂ».",
"date": "2022-03-13"
},
{
"vote": 9,
"title": "Sentence classification problem",
"text": "Hello!\n\n\nI'm looking to solve a multi label text classification problem but I don't really know how to formulate it correctly so I can look it up.. Here is my problem :\n\n\nSay I have the document \"\nI want to learn NLP. I can do that by reading NLP books or watching tutorials on the internet. That would help me find a job in NLP\n.\"\n\n\nI want to classify the sentences to 3 labels (for example) \nobjective\n, \nmethod\n and \nresult\n. The result would be :\n\n\nobjective : I want to learn NLP\n\n\nmethod : I can do that by reading NLP books or watching tutorials on the internet.\n\n\nresult : That would help me find a job.\n\n\nAs you would have noticed, it's not a classical classification problem, since the classification here depends on the document structure (unless I'm wrong?)\n\n\nAny idea of the key words to better describe the problem ? or how I might solve it ?\n\n\nMany thanks!",
"date": "2022-03-11"
},
{
"vote": 6,
"title": "Researchers From the University of Hamburg Propose A Machine Learning Model, Called â€˜LipSound2â€™, That Directly Predicts Speech Representations From Raw Pixels",
"text": "The purpose of the paper presented in this article is to reconstruct speech only based on sequences of images of talking people. The generation of speech from silent videos can be used for many applications: for instance, silent visual input methods used in public environments for privacy protection or understanding speech in surveillance videos.\n\n\nThe main challenge in speech reconstruction from visual information is that human speech is produced not only through observable mouth and face movements but also through lips, tongue, and internal organs like vocal cords. Furthermore, it is hard to visually distinguish phonemes like â€˜vâ€™ and â€˜fâ€™ only through mouth and face movements.Â \n\n\nThis paper leverages the natural co-occurrence of audio and video streams to pre-train a video-to-audio speech reconstruction model through self-supervision.\n\n\nContinue Reading my Summary on this Paper\n\n\nPaper: \nhttps://arxiv.org/pdf/2112.04748.pdf",
"date": "2022-03-11"
},
{
"vote": 3,
"title": "[Resource] How to Install Kaldi (Automatically)",
"text": "Hey guys,\n\n\nI was working with \nKaldi\n about a month ago for the first time, and I found the installation process really tricky, so I created an \nautomated Kaldi installation script\n to take care of it in 3 lines of code.\n\n\nI also lay out steps for a \nmanual installation\n if anyone prefers that, but I thought I'd drop this tutorial in here for anyone struggling with Kaldi!",
"date": "2022-03-11"
},
{
"vote": 7,
"title": "Snorkel weak labeling for NER. When a token does not fall under any of the class labels or abstains in NER?",
"text": "I have a program that labels a sequence of words using ontologies. I have labeling functions for class negative, class positive, and class abstain. Should I keep the word unlabeled if it does not fall under any of these class labels and ignore it or should I force-label them under either class negative or abstain? I will be grateful for any hints or help.\n\n\n&#x200B;\n\n\n&#x200B;",
"date": "2022-03-11"
},
{
"vote": 1,
"title": "ðŸ’¥ How to Extract NER (Named Entity Recognition) Using Spacy",
"text": null,
"date": "2022-03-11"
},
{
"vote": 1,
"title": "Measuring the evolution of a given expression across time, in a text collection",
"text": "[removed]",
"date": "2022-03-10"
},
{
"vote": 0,
"title": "One Human Language | Noam Chomsky",
"text": null,
"date": "2022-03-10"
},
{
"vote": 14,
"title": "Comparing accuracy of two sentence similarity algirithm.",
"text": "Hi, we were to implememt sentence semantic similarity matching algorithm. basically what we were supposed to do is to given database of about 300k sentence, we have to find 10 most similar sentence to the query sentence given by the user.\n\n\nThe approach we tried:\nwe created embedding vector from the sentence and created annoy index, to do approximate nearest neighbour  search, so every time user puts query, we create sentence embedding of that query sentence (using same technique we used create embeddings of all question in database) and find its approximate neighbours\n\n\nThe problem:\nsay we used two  methods to create sentence embeddings to see which one gives better result, lets say method A gives good similarity matching but method B is slightly not good as method A ( verified by human inspection) How do i compare which one is better mathematically? like which result have better accuracy.",
"date": "2022-03-10"
},
{
"vote": 1,
"title": "15 Datasets for Word Segmentation on the Hugging Face Hub",
"text": "[deleted]",
"date": "2022-03-09"
},
{
"vote": 1,
"title": "ðŸ”¥ How to Convert Yolov4 Tiny Weights to NCNN framework (Smart Dashcam Part 3)",
"text": null,
"date": "2022-03-09"
},
{
"vote": 1,
"title": "[P] Brat annotation: How to add URL links to each annotations",
"text": "[removed]",
"date": "2022-03-09"
},
{
"vote": 1,
"title": "Brat annotation: How to add URL links to each annotations",
"text": "[removed]",
"date": "2022-03-09"
},
{
"vote": 1,
"title": "Deep Learning for Natural Language Processing",
"text": "[removed]",
"date": "2022-03-08"
},
{
"vote": 1,
"title": "ðŸ”¥ Evaluate Rephrased Sentences by Using an NLP Model (Google T5 Transformer)",
"text": "[removed]",
"date": "2022-03-08"
},
{
"vote": 13,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-07"
},
{
"vote": 9,
"title": "Hey Siri|Hey Google - What can/should we expect from personal assistants next versions?",
"text": "We haven't seen any major breakthroughs on these over the past years, so what kind of advances are \nreasonable\n the next years? What pieces are missing for a more natural/advanced interaction with personal assistants? Can this tech get stuck for another decade?",
"date": "2022-03-07"
},
{
"vote": 3,
"title": "*ACL Findings vs ACL Workshop",
"text": "Hey, I'm doing my PhD in NLP and this is the first time I have attempted an ACL (main conference) submission (I have submitted in EACL though), my paper got rejected and I have worked on a revised version. As this year coincided with the ARR system, allowing revised versions of the same paper to be submitted to different venues, I have the following question:   \n\n\nFor a paper with average reviews (3, 2.5, 2.5) and a metareview of 3, acceptance at the main conference is most probably out of the question.\nHowever, there's a chance that it can be accepted to the NAACL2022 findings or to a relevant ACL2022 workshop.\nIn that case, what would you prefer and why? My personal opinion is that, while the Findings sounds better, you don't even get to attend a poster session. On the other hand, workshops usually attract less polished papers (although I am not sure in the case of ACL), hence having a mixed reputation.",
"date": "2022-03-07"
},
{
"vote": 16,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-07"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-07"
},
{
"vote": 1,
"title": "Research on AI and Language Learning Technologies",
"text": "[removed]",
"date": "2022-03-06"
},
{
"vote": 0,
"title": "Become fluent in any language",
"text": null,
"date": "2022-03-04"
},
{
"vote": 2,
"title": "Reg: Self-studying CS224n",
"text": "Hello everyone! I'm looking for some help. Has anyone gone through Stanford CS224n (Deep Learning for Natural Language Processing). If yes, could you pls recommend if there's a study group or forum that I can consider joining? I really need some peer support + discussions. Thanks in advance!",
"date": "2022-03-04"
},
{
"vote": 15,
"title": "Researchers from Tel Aviv Propose Long-Text NLP Benchmark Called SCROLLS",
"text": "Although lengthier texts contain a significant quantity of natural language in the wild, NLP benchmarks have always primarily focused on short texts, such as sentences and paragraphs. Short text classification has consistently been a driving force behind standard benchmarks like GLUE, WMT, and SQuAD. A considerable amount of natural language is produced in the context of lengthier discourses, such as books, essays, and meeting transcripts, as is widely known. As a result, model structures are required to address the computing constraints associated with processing such long sequences.\n\n\nResearchers from Tel-Aviv University, Meta AI, IBM Research, and Allen Institute for AI (AI2) introduce Standardized CompaRison Over Long Language Sequences (SCROLLS) to tackle this issue. SCROLLS is a collection of summarization, question-answering, and natural language inference tasks that span a variety of topics, including literature, science, commerce, and entertainment.\n\n\nContinue Reading My Full Summary On This Paper\n\n\nPaper: \nhttps://arxiv.org/pdf/2201.03533.pdf",
"date": "2022-03-04"
},
{
"vote": 1,
"title": "Learn a New Language with Preply",
"text": null,
"date": "2022-03-04"
},
{
"vote": 5,
"title": "Detecting random character sequences in tabular strings",
"text": "Hey everyone,\n\n\nI started a project with regard to analyzing \nopen-text answers\n of surveys. I mainly deal with single words, half-sentences and single sentences, however sometimes people just face-roll over their keyboards (xP) and input an arbitrary character sequence (e.g. jdfaskl, adskfls, etc.). I do know the language, so there is not the problem that these character sequences could have a meaning in other languages.\n\n\nAmong other problems, I have to detect these sequences and flag them accordingly. While I have some approaches in mind on how to solve them (checking words against dictionaries, detecting recurring character sequences, checking for vowels, etc.), I still struggle to find a good solution for that. Especially because sometimes people use common abbreviations (like MS for Microsoft) and I really would like to first detect random character sequences and in a second step check against common abbreviations.\n\n\nI do have some experience in this field, but this is my first big project and I would really like to explore options on how to solve this problem thoroughly. \n\n\nAny hint to how this could be solved is appreciated ;)",
"date": "2022-03-03"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-03"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-03"
},
{
"vote": 1,
"title": "Vanilla Transformer",
"text": "Hello, do you guys have any resources for training a vanilla tramsformer model for machine translation? \n\n\nI tried the Tensorflow tutorial(en-pt) , but it seems like I cant get it to work when using a custom dataset from text files. I get confuse with the l\n\"setup input pipeline stage\n\"\n\n\ncan someone explain why it has a tokenizers.pt and tokenizers.en and why cant i use a simple text vectorization layer and such?\n\n\n if any of u guys can help itll be very much appreciated, much thanks!",
"date": "2022-03-03"
},
{
"vote": 4,
"title": "Document-Term Matrix in NLP: Count and TF-IDF Scores Explained",
"text": null,
"date": "2022-03-03"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-03"
},
{
"vote": 2,
"title": "What language is this?",
"text": "[deleted]",
"date": "2022-03-03"
},
{
"vote": 7,
"title": "Semantic treebank of English Holy Bible?",
"text": "Hi.  Does anyone know of a version of an English language bible (preferrably Douay-Rheims, but beggars can't be choosers) that has been parsed into a semantic treebank?  I've seen syntactic treebanks of ancient languages (Latin, Greek, etc.) using dependency grammars, but I haven't come across an English bible in a semantic treebank.\n\n\nAny idea where to look?",
"date": "2022-03-02"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-02"
},
{
"vote": 2,
"title": "[Mozilla + Coqui] Speech Technology Hackathon",
"text": null,
"date": "2022-03-02"
},
{
"vote": 8,
"title": "I am considering a master in Language Technology - advice?",
"text": "https://www.gu.se/en/study-gothenburg/master-in-language-technology-one-year-or-two-years-h2mlt\n\n\n&#x200B;\n\n\nI am looking at this one specifically. I have a bachelor in languages but I have been working in software for 5 year. This would require me to pause my career for 1-2 years, so I wanna make sure it's the right move. Thoughts?",
"date": "2022-03-01"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-03-01"
},
{
"vote": 3,
"title": "Dropout pre-training vs. fine-tuning",
"text": "Is it okay to use a higher dropout during fine-tuning than was used during pre-training a transformer? Are there any best practices around this or any related literature?",
"date": "2022-03-01"
},
{
"vote": 7,
"title": "Resources for sentiment analysis",
"text": "What are the resources you have used to learn sentiment analysis?\nIf you could suggest any resources on building a sentiment dictionary that would be great. \n\n\nThank you for your kind replies.",
"date": "2022-02-28"
},
{
"vote": 18,
"title": "Hey folks, here is another cool Natural language research from Meta AI where they introduce Project CAIRaoke and using this project they built an end-to-end neural network-based model that can power much more personal and contextual conversations",
"text": "The need of the hour is a better conversational AI, not just AI assistants who canâ€™t do more than what has been fed. AI assistants are underwhelming irrespective of whether we interact with them via text or voice. They are easily stumped by a bit of complexity added to the conversation. Imagine how it would be to converse with AI assistants the same way we do regularly, to our people, most naturally and colloquially.Â \n\n\nResearchers from Meta AI come to save the day with their project CAIRaoke. The team has created an end-to-end brain model capable of considerably more intimate and contextual dialogues than current systems. The researchers have already used the model that evolved from this effort. Portal is what they call the product, and the purpose is to connect it to augmented and immersive virtual devices. This integration would benefit the community because it would allow for more comprehensive, multi-modal interactions with AI helpers.\n\n\nYou can \ncontinue reading my summary here\n or even check out the \nMeta AI blog page",
"date": "2022-02-28"
},
{
"vote": 1,
"title": "âœ¨ How to Setup a Google Colab account and use its GPU or TPU for free?",
"text": null,
"date": "2022-02-28"
},
{
"vote": 7,
"title": "Google USE versus BERT",
"text": "Hey folks - first post here! Iâ€™ve been reading a lot about different techniques to build chatbots, and Iâ€™m struggling to understand how something like a Google Universal Sentence Encoder is related to BERT? I know USE has a transformer based architecture option and basically pretrained embeddings, but BERT seems lower level than that? When would I use each? Is USE simpler than BERT?",
"date": "2022-02-27"
},
{
"vote": 7,
"title": "Can you help solve a mystery? \"Sync and corrections by n17t01\"",
"text": "I apologise if this doesn't fit this subreddit, but I can't think where else too look.\n\n\nI was using the DeepL translator app on Android today and translated the German word \"genauso\" to English, which should translate to \"just the same\". instead I got the response \"Sync and corrections by n17t01\". \n\n\nOdd. then I searched for that phrase on duckduckgo & Google and it turns up on web pages all over the Internet in nonsensical ways. there must be an explanation. any suggestions?\n\n\nthanks.",
"date": "2022-02-27"
},
{
"vote": 1,
"title": "Topic Modelling",
"text": "[removed]",
"date": "2022-02-26"
},
{
"vote": 18,
"title": "Using Unsupported Huggingface Models in SpaCy",
"text": "Hey,\n\n\nI've been tinkering a lot with SpaCy as of late, and I was wondering whether it is possible to use any of the models published on the Huggingface website apart from those that are already SpaCy compatible. More specifically, I for example would like to use the Dutch BERT model, BERTje ( \nGroNLP/bert-base-dutch-cased Â· Hugging Face\n).\n\n\n&#x200B;\n\n\nI've searched a lot for info about this, but to no avail. Would appreciate it if anyone could tell me whether this is possible without too much hassle (and without any required retraining of the model) and perhaps point me in the right direction.\n\n\n&#x200B;\n\n\nCheers,",
"date": "2022-02-25"
},
{
"vote": 7,
"title": "[Research] Looking for volunteers to evaluate multilingual dialogue models (chatbots)!",
"text": "[cross-post]\nHi! Are you frustrated that most chatbots are only in English? Would you want your native language to be part of a conversational AI system as well? I certainly do! Thatâ€™s why Iâ€™m doing my masterâ€™s thesis on multilingual open-domain dialogue systems. \n\n\nIf you also feel the same and want to contribute to research on multilingual, intelligent chatbots, then youâ€™re the perfect fit for my thesis project!  \n\n\nIâ€™m currently looking for people who can speak one of the following languages fluently:   \n\n\n\n\nArabic\n\n\nBengali\n\n\nFinnish\n\n\nJapanese\n\n\nKorean\n\n\n\n\nThe task is to \n\n\n\n\npost-edit some data in your language (after I used machine translation to translate it from English to the languages) or\n\n\nevaluate the modelsâ€™ responses in your language.\n\n\n\n\nThere are 30 short dialogue exchanges in the post editing task. Your task is to check the post machine translated text and correct the grammatical and/or fluency issues if needed. \n\n\nAs for the system evaluation, you will chat with the chatbots and rate them on a scale of 5 about your view of their fluency, engagingness, naturalness, etc. \n\n\nIâ€™m looking for 1 volunteer in each language for the post editing task and 2 volunteers in each language for the evaluation task. The tasks are pretty small so they won't take too much of your time. And it'd be fun to chat with the bots (I think) :) The post editing task preferably starts asap and the evaluation task will kick-start by the end of March! If youâ€™re interested, please fill out \nthis form\n :)  \n\n\nIâ€™ll of course credit you in the paper unless you would like to stay anonymous. This is a project Iâ€™m really passionate about, and I hope itâ€™ll encourage more research on multilingual dialogue systems in the community! \n\n\nPlease feel free to let me know if youâ€™d like to know more! I really, really appreciate it. You can dm me or just leave a comment here. \n\n\nTo sign up, please kindly fill out \nthis google form\n. \n\n\nThank you so much for reading this post patiently :) Hope you have a great day!",
"date": "2022-02-24"
},
{
"vote": 1,
"title": "Quick Demo GPT-3 VS GPT-J",
"text": null,
"date": "2022-02-24"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-02-24"
},
{
"vote": 3,
"title": "Best algorithm for grammar checking",
"text": "From a computational linguistics perspective, what is the currently highest performing algorithm for either identifying grammar mistakes in a language or also that plus suggesting corrections?\n\n\nThank you",
"date": "2022-02-23"
},
{
"vote": 10,
"title": "What state-of-the-art NLP systems utilize bayesian modeling?",
"text": "Additionally, for which tasks is bayesian modeling more useful?",
"date": "2022-02-23"
},
{
"vote": 0,
"title": "ðŸŽˆ Build a custom Q&amp;A app with Streamlit and Pinecone to revolutionize your search systems! Join this week's webinar to learn how. ðŸ‘‡",
"text": null,
"date": "2022-02-23"
},
{
"vote": 4,
"title": "Any pretrained models that can detect categories?",
"text": "I'm looking at Google's NLP and they can detect categories based on text like:\n\n\n/Games/Computer & Video Games/Sandbox Games\n\n\n/Games/Computer & Video Games/Shooter Games\n\n\n/Games/Computer & Video Games/Simulation Games\n\n\n/Games/Computer & Video Games/Sports Games\n\n\n/Games/Computer & Video Games/Strategy Games\n\n\n/Games/Computer & Video Games/Video Game Emulation\n\n\nDo any free models or libraries exist that I can play around with?\n\n\nGoogle NLP is too expensive for me.\n\n\nThank you.",
"date": "2022-02-23"
},
{
"vote": 37,
"title": "[N] Who Is Behind QAnon? Linguistic Detectives Find Fingerprints using statistics and machine learning",
"text": "According to the \nNew-York Times,\n using machine learning, stylometry, and statistics on Q texts, two separate teams of NLP researchers from \nFrance\n and \nSwiss\n have identified the same two men as likely authors of messages that fueled the QAnon movement. First the initiator, Paul Furber, a South African software developer and then Ron Watkins took over, who operated 8chan website where the Q messages began appearing in 2018 and is now running election for Republican in Arizona.",
"date": "2022-02-23"
},
{
"vote": 5,
"title": "CoNLL-U token annotations",
"text": "I would like to access the annotations of individual tokens in a CoNLL-U file.\n\n\nThe code in the file I am running (I followed the documentation found in \ngithub\n):\n\n\nimport conllu\n\nf = open(&#039;en_gum-ud-test.conllu&#039;, &#039;r&#039;, encoding=&#039;utf-8&#039;)\nannotations = f.read()\n\nsentences = conllu.parse(annotations)\n\nsentence = sentences[0]\nprint(sentence) # prints TokenList of first sentence in file\n# TokenList&lt;The, prevalence, of, discrimination, across, racial, groups, in, contemporary, America, :&gt;\n\ntoken = sentence[0] \nprint(token) # prints the first token of the TokenList above, The\n\n\n\nInstead of the actual token (The) I would like to get the ordered dictionary:\n\n\n{\n    &#039;id&#039;: 1,\n    &#039;form&#039;: The,\n    &#039;lemma&#039;: the,\n    ...\n}\n\n\n\nI get this only if I run the code line by line in the console and without the print statements, so by running just what's inside the print statements.\n\n\nHow can I get the ordered dictionary by running the file where the code is?",
"date": "2022-02-22"
},
{
"vote": 24,
"title": "Introduction to Sentence-BERT (SBERT)",
"text": null,
"date": "2022-02-21"
},
{
"vote": 1,
"title": "How do CAT tools write segments to an output file",
"text": "When you import text to a CAT tool, it gets converted to something like .xlf, where all the recognised sentences are marked up.\n\n\nThen I assume when using the tool the program has loaded in text into a list from that markup file and is somehow writing each segment back to the output file in the XML.\n\n\nBut how does that work? You could overwrite the file when you save the project in the program. But most CAT tools immediately save each segment. How do they connect the segment to its location in the XML to auto-write there every time you translate another segment?\n\n\nThank you very much",
"date": "2022-02-21"
},
{
"vote": 15,
"title": "I trained the GPT-2 model on the tao te ching. It made some interesting samples.",
"text": null,
"date": "2022-02-19"
},
{
"vote": 12,
"title": "Why does Zero-Shot-Classification not work in this simple use-case?",
"text": "I'm trying to classify \nadjectives\n to see if they apply to \nhumans\n or \nthings\n. \n\n\nI tried several pretrained models for this task on huggingface, such as \ntypeform/distilbert-base-uncased-mnli\n (and many of \nthese\n), but the classification is very unreliable when it comes to single adjectives. I tried all possible types of candidate labels, even aggregated scores for clusters, but cannot seem to solve this reliably. I have about 6 000 adjectives that should be classified. So far I haven't bothered to calculate validity, but intuitively I'd say \"slightly better than chance\".\n\n\nHowever, the examples provided on the model cards seem to work really well. Any ideas on why this doesn't work and how to fix it?",
"date": "2022-02-18"
},
{
"vote": 1,
"title": "Top 10 Language Detection APIs",
"text": null,
"date": "2022-02-18"
},
{
"vote": 8,
"title": "Dodiom - Telegram bot to collect idiom corpus",
"text": "Hi everyone,\n\n\nI've developed a Telegram bot to help you learn English idioms as a multiplayer competitive game while collecting MWE (multi-word expressions) corpus.\n\n\nBot link: \nhttps://t.me/dodiom_en_bot\n\n\nYou can see the results for Turkish version of the bot in this \njournal article\n. You can also find the source code and collected corpus on \nGithub\n. Right now we are collecting corpus for English language and our aim is to compare it to existing human annotated corpora. I'd be very grateful if you try it.\n\n\nAll feedback is welcome and encouraged. I hope this post does not break subreddit rules.",
"date": "2022-02-18"
},
{
"vote": 3,
"title": "Warning: Invalid line when computing TER metric",
"text": "Hi guys, i'm trying to compute the ter metric using this repository:  \nhttps://github.com/jhclark/tercom\n\n\nI have two txt files for the ref. and hypothesis, with this two i have already compute the Bleu (\nhttps://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl\n), Meteor (\nhttps://www.cs.cmu.edu/~alavie/METEOR/README.html#about\n) and Rouge (\nhttps://github.com/pltrdy/rouge.git\n).\n\n\nFor compute the ter metric i use this code :\n\n\n!java -jar tercom.7.25.jar -h /content/pred.txt -r /content/tgt-val.txt\n\n\n\nBut it gives me this error:\n\n\n Warning: Invalid line for every line of the txt file and the final results are:\n\n\nTotal TER: NaN (0.0/0.0)\n\nNumber of calls to beam search: 0 \n\nNumber of segments scored: 0 \n\nNumber of shifts tried: 0\n\n\n\nHow can i solve this problem?\n\n\nThanks all",
"date": "2022-02-17"
},
{
"vote": 1,
"title": "Evaluate Rephrased Sentences by Using an NLP Model (Google T5 Transformer)",
"text": null,
"date": "2022-02-17"
},
{
"vote": 21,
"title": "Deploying GPT-NeoX 20B: lessons learned and a focus on Deepspeed",
"text": "Hello all,\n\n\nDeploying  and using GPT-NeoX 20B reliably in production has been quite a  challenge. You basically have 2 choices: have it run on a single huge  GPU, or on multiple smaller GPUs. Here are a couple of lessons I learned  during this interesting journey:\n\n\nhttps://nlpcloud.io/deploying-gpt-neox-20-production-focus-deepspeed.html\n\n\nIf  some of you used a different strategy, I'd love to hear about it. Also,  if you have an idea about how to perform batch inference on GPT-NeoX  20B, I'm interested in hearing your thoughts.\n\n\nThanks to EleutherAI for their amazing work. Can't wait to see what's next!",
"date": "2022-02-17"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-02-16"
},
{
"vote": 3,
"title": "Applications of DeBERTa for Sentiment Analysis",
"text": "I was wondering if anyone had any cool projects that applied the DeBERTa model for a sentiment analysis task; I'm starting a new project in the field, and I'm just trying to get a good scope of the terrain. Thanks!",
"date": "2022-02-16"
},
{
"vote": 2,
"title": "Similarity of answers across groups to open-ended survey questions",
"text": "Say I have some data in which vegetarians, vegans and omnivores all responded to the same set of open ended questions (presumably about eating habits) and I wanted to determine the strength / difference in their answers.\n\n\nOf course I could simply look at relative word frequencies, topic models etc. But if i wanted to determine the similarity between the answers, would vectorizing each  group's answers and comparing them with cosine similarity make sense? \n\n\nConceptually, I think this is interesting as I would like to see, with some quantitative value, how 'similar' vegans are to vegetarians to omnivores? Does this approach make sense?",
"date": "2022-02-16"
},
{
"vote": 6,
"title": "data format to fine tune gpt-2 for code generation",
"text": "I'm following this \nhttps://github.com/nshepperd/gpt-2\n repo to fine tune the gpt-2 355M model, i've collected (comment,code) pairs from github into a text file where data have the following format :\n\n\n#comment \ncode\n&lt;|endoftext|&gt; \n\n\n\nis this the correct format for fine tuning the gpt-2 model?",
"date": "2022-02-16"
},
{
"vote": 1,
"title": "Join the \"To Learn\" Discord Server!",
"text": null,
"date": "2022-02-16"
},
{
"vote": 2,
"title": "Dialects in Q&amp;A models",
"text": "Hey everyone, I have been trying to implement a Q&A model for a low resource language and I've read up a few papers for the same. I want to know if different dialects across a country might affect the model. And how I can avoid this. Since I'm going to be crowdsourcing my data for this! \n\n\nI thought it would be an issue with GottBERT but they haven't mentioned anything with regard to dialects. \n\n\nAny insights would be appreciated!",
"date": "2022-02-16"
},
{
"vote": 0,
"title": "8 Completely FREE Courses to Learn AI (Artificial Intelligence)",
"text": null,
"date": "2022-02-16"
},
{
"vote": 6,
"title": "precision recall calculation for key phrase extraction",
"text": "suppose I have 5 documents and I ranked keyphrases extracted. How do I calculate precision, recall @ k? The number of keyphrases vary across each document. How does it work? \n\n\n\n\nIs precision@k is average precision at k? 1/k(precision@1+....precision@k) or its just precision@k?\n\n\nIf number of keyphrases extracted vary. For example: doc 1 has 2 keyphrases and doc 2 has 3 keyphrases? How do I calculate precision@3? is it 1/3((doc1 p@1+ doc2@ p@1)/2+.......+ (doc 1 p@3 (0)+ doc2 p@3)/2)? Please bear somehow unclear example.",
"date": "2022-02-15"
},
{
"vote": 9,
"title": "XLMRoberta validation loss increases with mcc score.",
"text": "I am trying to fine tune a bit complex problem with XLMRoberta model and using the parameters as prescribed in the paper, lr of 2e-5 , etc. However, the validation loss (crossentropy) keeps on increasing   with epochs along with other matrices like f1 score , mcc score! Although everyone suggests to use just 2-4 epochs of training but when i try to use early stopping (on mcc score), I noticed that the accuracy keeps on improving with epochs. Is it a valid scenario ? Because as per Machine Learning logics it shouldnt be the case. And what is expected if i fine tune BERT model for large epochs  ?",
"date": "2022-02-15"
},
{
"vote": 0,
"title": "Named Entity Recognition - Label-Label Transitions",
"text": "In NER, what are label-label transitions? And what do the weights refer to?\n\n\nAm I right in thinking that this is the relationship strength between the labels (e.g. I-LOC and B-LOC)?\n\n\n&#x200B;\n\n\nThanks",
"date": "2022-02-15"
},
{
"vote": 1,
"title": "can someone point me to research of a minimal language word set that can be used to describe most other words?",
"text": "Has anyone done research like this?\nFor example  \"to run\" can be described as move fast.\nmove is a base word used in many other word definitions like:\ndrive \"to move in fast object with wheels\"\nI've been trying to find this research, but can't find anything good, can anyone help point me to some good research?",
"date": "2022-02-15"
},
{
"vote": 13,
"title": "Averaging sentence embeddings to create multi-sentence embeddings?",
"text": "Hey everyone. \n\n\nSo, I'm getting started on a project where I'm trying to extract information from short texts with an unsupervised approach. The plan is for me to cluster these texts. I know of Topic Modeling but I'm looking into other options too.\n\n\nWhile the texts are short, they can contain muliiple sentences. I'm basically wondering if it would be considered fine to use a sentence transformer like Sentence-BERT to create sentence embeddings, and for each text with multiple sentences, average the embeddings to create a single one? \n\n\nFeel free to suggest other ideas to create a single embedding for something that contains multiple sentences.",
"date": "2022-02-15"
},
{
"vote": 0,
"title": "Best Free Machine Learning Courses On Linkedin",
"text": null,
"date": "2022-02-15"
},
{
"vote": 7,
"title": "System for Semantic Role Labeling",
"text": "Anyone know of a decent SRL parser?  I'd prefer one that produces output based on the Propbank v3 frameset, not the v2 used in the original OntoNotes release but I'm not sure I'm going to get to be picky.  If you know of a decent one, let me know.  \"Papers with Code\" has some links but they require training.  I was hoping to find something in a little more of a released state.  It doesn't look like Spacy or Stanford's NLP suites offer this functionality.",
"date": "2022-02-15"
},
{
"vote": 21,
"title": "Are there any datasets/models that address the connotation of a word?",
"text": "For example, the word \"blood\", with the same meaning, can connote:\n\n\n\n\nFamily: \"blood\" is thicker than water\n\n\nViolence: there was \"blood\" in the streets\n\n\nPassion: he got my \"blood\" racing\n\n\nBiological substance: \"blood\" pressure\n\n\n\n\nThe languages models I know have a single meaning for a word, and attention models don't address meaning, but rather usage \nand prediction.\n\n\nBefore I go down this research path, I'd like to know if there is already literature surrounding this problem.",
"date": "2022-02-14"
},
{
"vote": 1,
"title": "Knowledge Base for Portuguese",
"text": "Hi,\n\n\nI want to do Entity Linking in text written in Portuguese and just find out that Spacy has an \nEntityLinker\n component in its pipeline. I only need a Knowledge Base database. \n\n\nDo you about Knowledge Base repositories? I'm looking for one more targeted to Portuguese, in particular European (most of the entities I will find in the texts are from Portugal).",
"date": "2022-02-14"
},
{
"vote": 12,
"title": "Noob question: how hard is it to pair quotes and names from the text of a news article?",
"text": "I was reading an article on a political website that was describing how a particular politician had said something that more or less contradicted something they had said a year or two ago.  I thought, wouldn't it be nice to have a public website where you could look quotes that appeared in news articles to see a persons' public statements over time, but also with the context of the time.  As a web developer, building a site to scrape articles and a database to hold the person, quote, and original source article is something I understand, but to automate the process of pairing a quote in the article to the speaker, and do it \nreliably\n, is more difficult (at least to me).  Any thoughts on how hard that would be with NLP or a similar approach?  Thanks in advance.",
"date": "2022-02-13"
},
{
"vote": 43,
"title": "3Blue1Brown Solving Wordle using information theory - YouTube",
"text": null,
"date": "2022-02-12"
},
{
"vote": 20,
"title": "Deepmindâ€™s Latest Machine Learning Research Automatically Find Inputs That Elicit Harmful Text From Language Models",
"text": "Large generative language models (LMs) such as GPT-3 and Gopher have proven the ability to generate high-quality text. However, these models run the danger of producing destructive text. Therefore, they are challenging to deploy due to their potential to hurt people in ways that are nearly impossible to forecast.Â \n\n\nSo many different inputs can lead to a model producing harmful text. As a result, itâ€™s challenging to identify all scenarios in which a model fails before itâ€™s used in the actual world. \nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/pdf/2202.03286.pdf",
"date": "2022-02-11"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-02-11"
},
{
"vote": 4,
"title": "NER - How to determine covariance of entities in named entity recognition?",
"text": "How would I go about deriving the correlation or covariance between entities within text? Relation extraction seems to be more focussed on providing more context to the entities, whereas I am looking to calculate the correlation of entities.\n\n\nThe entities I am looking to use in building the model are likely to have significant overlap, which I would be interested in identifying. For example, would I look for overlap in identified text for different entities, or look at word distance between co-occurring entities?\n\n\nFor example, I have a sample sentence of \"\nCommunication [entity1]\n skills are key for \ninterpersonal [entity2]\n development, \ngroupwork [entity3]\n and \npresentations [entity1]\n.\" I would like to determine the strength of entity1, entity2 and entity3 but uncertain of how best to approach this problem. Would I look to e.g. calculate average word distance between entities, or something else?\n\n\nThe datasets I am looking to run this on will be large (multiple webpages of text, rather than sentences) and so I don't think collecting counts would be appropriate and requires some consideration of how close entities appear.\n\n\nThanks!",
"date": "2022-02-11"
},
{
"vote": 3,
"title": "Attempting to rewrite BERT codebase into RoBERTA, running into shape issue.",
"text": null,
"date": "2022-02-11"
},
{
"vote": 0,
"title": "Real World Example of Machine Learning on Rails",
"text": "[deleted]",
"date": "2022-02-10"
},
{
"vote": 1,
"title": "The computational cost of Text Classification with Universal Embeddings?",
"text": "Hi. I am not experienced with AI but I am trying to use a simple model for one of my personal projects.\n\n\nThe question is: What would be the computational complexity ( just an estimation of it) of a Text Classification with Universal Embeddings for like 1000 data inputs? For example, I have 1000 sentences, and some of them may mean the same thing. So I want to create a list of groups of sentences with the same meaning. Therefore it would look like this -> Input: 1000 sentences. Output: Group 1: 500 sentences mean this thing ( and showing the sentences), group 2: 300 sentences mean this thing, and group 3: 200 mean another thing ( whatever the sentences might be). I am interested in how computational expensive would that be? Would it run on a phone? Implying the input is 1000 sentences.\n\n\nProbably using this: \nhttps://www.npmjs.com/package/@tensorflow-models/universal-sentence-encoder\n but I am not sure if that's the best way. Also, does this model make requests to google to do the heavy lifting? \n\n\nAlso, please don't hesitate to correct me if I got it all wrong... Most likely I got it all wrong.\n\n\nThanks.",
"date": "2022-02-10"
},
{
"vote": 1,
"title": "How to Use transformer models from a local machine and from Hugging Face",
"text": null,
"date": "2022-02-10"
},
{
"vote": 16,
"title": "[P] What we learned by accelerating by 5X Hugging Face generative language models",
"text": null,
"date": "2022-02-10"
},
{
"vote": 2,
"title": "9 Best Courses to Learn to TensorFlow",
"text": null,
"date": "2022-02-10"
},
{
"vote": 15,
"title": "Are there models like \"punctuation_en_bert\" from Nvidia for other languages, in particular German?",
"text": "Hi guys, I want to perform some analysis on transcripts that miss punctuation. The lack of it distorts the results, so I found that there is \"punctuation_en_bert\" from Nvidia that inserts punctuation back. It does a great great job for English. I need something like this for German as well. I wonder if that exists though. Can you point me in the right direction ?",
"date": "2022-02-09"
},
{
"vote": 1,
"title": "What Keeps You Going ? | Noam Chomsky",
"text": null,
"date": "2022-02-09"
},
{
"vote": 5,
"title": "How does an AI Recommendation engine work?",
"text": null,
"date": "2022-02-09"
},
{
"vote": 2,
"title": "Master's thesis topic? - Policy/Government related",
"text": "Hi everyone,\nI'm struggling to frame the topic of my master's thesis, so I'm calling reddit to the rescue! \nIdeally, I'd like to work on something related to policy/government/democracy. I have a few government surveys (50k+ answers) with open ended questions, which were poorly analyzed. I would like to do something a bit more interesting than a simple clustering of the answers, any idea?\nOf course, any other suggestions are welcome!\n\n\nThanks!",
"date": "2022-02-08"
},
{
"vote": 1,
"title": "I would like to put ancient Greek texts through a neural network, in order to individuate multiple authors within them. Where should I get started?",
"text": "Disclaimer: I know absolutely nothing about NLP.\n\n\nI study ancient Greek texts and would love to analyse parts of a single text in order to test hypotheses on its multiple authors. I suspect the text, which is said to have been written by a single author, was actually written by multiple authors. \n\n\nIs there some online resource you would recommend, or some tips you might have as for testing such a hypothesis through NLP?",
"date": "2022-02-08"
},
{
"vote": 5,
"title": "How to create a broad/representative sample from millions of records?",
"text": "Hey all. \n\n\nI have millions and millions of small text documents. Within the project, Iâ€™m looking to generate a relatively small sample to use as a training dataset.  \n\n\nI havenâ€™t been able to find anything really about thisâ€¦ which makes me think Iâ€™m just searching the wrong terms. The one \nstackoverflow comment\n I found goes unansweredâ€¦ \n\n\nI do have some ideas if I have to push on blindly. Maybe do some doc2vec and then work on creating a uniform subset from the vectors? Havenâ€™t given this much thought yet, as you can tell. \n\n\nTldr is Iâ€™m looking for blogs/papers/packages to sample a large text dataset, resulting in a (relatively) small sample to use.",
"date": "2022-02-07"
},
{
"vote": 10,
"title": "BERT: Understanding the Null response threshold in QA systems",
"text": null,
"date": "2022-02-06"
},
{
"vote": 12,
"title": "OpenAI Team Introduces â€˜InstructGPTâ€™ Model Developed With Reinforcement Learning From Human Feedback (RLHF) To Make Models Safer, Helpful, And Aligned",
"text": "A system can theoretically learn anything from a set of data. In practice, however, it is little more than a model dependent on a few cases. Although pretrained language models such as Open AIâ€™s GPT-3 have excelled at a wide range of natural language processing (NLP) tasks, there are times when unintended outputs, or those not following the userâ€™s instructions, are generated. Not only that, but their outcomes have been observed to be prejudiced, untruthful, or poisonous, potentially having harmful societal consequences.\n\n\nOpenAI researchers have made substantial progress in better aligning big language models with usersâ€™ goals using reinforcement learning from human feedback (RLHF) methodologies. The team proposed \nInstructGPT\n models that have been demonstrated to produce more accurate and less harmful results in tests. \n\n\nContinue Reading\n\n\nOpen AI Blog**:** \nhttps://openai.com/blog/instruction-following/\n\n\nPaper: \nhttps://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf",
"date": "2022-02-05"
},
{
"vote": 1,
"title": "Locality-sensitive hashing in Reformer model",
"text": null,
"date": "2022-02-05"
},
{
"vote": 5,
"title": "Using Physics to Teach Computers to Speak! Unity3D + GPT Project Overview",
"text": null,
"date": "2022-02-04"
},
{
"vote": 3,
"title": "How to obbtain probability for entire sequence (Huggingface transformers)",
"text": "I want to encode certain, predetermined sentences, such as e.g.\n\n\ns1 = &quot;He is a good-hearted person.&quot; \ns2 = &quot;He is a blockheaded person.&quot; \n\n\n\nand compare the \noverall probabilities\n for each of these sequences, to see which is more likely.\n\n\nHow do I obtain such probabilities using any of the huggingface pretrained transformers?",
"date": "2022-02-04"
},
{
"vote": 0,
"title": "Add Pandas Dataframe to Google Sheet.",
"text": null,
"date": "2022-02-04"
},
{
"vote": 1,
"title": "How to obbtain probability for entire sequence (Huggingface transformers)",
"text": "[removed]",
"date": "2022-02-04"
},
{
"vote": 3,
"title": "How to represent sequential triples in an ontology?",
"text": "I have the sentence \"The leaf was green before it was brown.\" and I would like to represent it using an ontology.\n\n\nObviously, we have two triples:\n\n\n :leaf :was &quot;green&quot;, &quot;brown&quot; .\n\n\n\nHowever, how do we represent that \n:leaf :was &quot;green&quot;\n comes before \n:leaf :was &quot;brown&quot;\n?\n\n\nI'm thinking there has to be an event like:\n\n\n_:a a :event ; :changes :leaf ; :traitChanged :color ; :from &quot;green&quot; ; :to &quot;brown&quot; .\n\n\n\nWhat are people's thoughts on this?\n\n\nI tried comparing to Stanford CoreNLP annotation, but it just returns this:\n\n\n1.0\tleaf\twas green\tbefore brown\n1.0\tleaf\twas\tgreen\n1.0\tleaf\twas green\tbrown\n1.0\tit\twas\tbrown\n1.0\tit\twas\tbefore brown",
"date": "2022-02-03"
},
{
"vote": 1,
"title": "Need advise for domain specific QA system.",
"text": "I am trying to build a question answering system on instruction manuals (kind of one that comes with electrical appliances) . I am still at a very initial phase and I have some doubts.\n\n\n\n\nThe manuals are in PDF format. They are highly unstructured with tables, images and text in no specific format (different appliance manufacturers have different formats). I have tried various techniques and ways to extract text and split the pdf into smaller sub-documents (like page-wise split, paragraph-wise split, section-wise split, etc.) but none of them seem to be \"the solution\" as each approach has its own drawbacks. Are there any better ways to extract text with any suggestions for pre-processing of data and building a document store?\n\n\nI have tried various pre-trained models on small paragraphs of text. They give near accurate answers to most of the short answer type questions but struggle if the queries are more technical. Moreover the models aren't fit for LFQA. I've been looking into Extractive QA but I am a little lost with so much of information. What adds to the problem is the lack of domain specific dataset that could've helped in fine-tuning the model.\n\n\n\n\nCan you please suggest any resources, code-implementations or projects I could refer to that might help. Any sort of suggestion or thought will be immensely appreciated. Thank you for taking time to read this.",
"date": "2022-02-03"
},
{
"vote": 17,
"title": "[Project] Refining the Natural language processing course - Feedback v2 and thank you",
"text": "Iâ€™m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting March 14th. \nhttps://corise.com/course/natural-language-processing\n.\n\n\nThis is the second run of the class and we learned a lot from the feedback of the reddit community from the first run in November. Some of the changes we're making from the previous iteration:\n\n\n1/ More focus on transformers and less on RNN/LSTM as hugging face is becoming the defacto for any content.\n\n\n2/ Pytorch lightning has some really easy to use interfaces so better organizing the boiler plate code.\n\n\n3/ OpenAI has opened the GPT-3 API so a deeper dive into current possibilities.\n\n\nWould love to continue getting feedback and build this to be a great resource. The plan is to open the content after we refine it to a degree we're happy with. You can join the course (capped at about 30 students) at the link above. If youâ€™re open to giving feedback on the class on how we can do better, happy to give a discount.",
"date": "2022-02-03"
},
{
"vote": 1,
"title": "Local text generation (InferKit alternative)",
"text": "This Radiohead post\n is a good example of what \nInferKit's Text Generation\n can do with the \ndemo page\n.\n\n\nDoes anyone here know of any free software programs or frameworks that can do this sort of text generation locally with a free model?",
"date": "2022-02-03"
},
{
"vote": 2,
"title": "11 Best Natural Language Processing Online Courses",
"text": null,
"date": "2022-02-03"
},
{
"vote": 1,
"title": "â©¢To LearnðŸŒ ",
"text": null,
"date": "2022-02-02"
},
{
"vote": 11,
"title": "Really need some guidance with my dissertation - information extraction on political text",
"text": "I am aiming to do information extraction on Hansard, a record of what is said in UK Parliament. I have 3 months and I am not even sure where to start. I did almost nothing last semester between a really shitty unit sucking up my time and depression. \n\n\nMy supervisor does not seem very in to the field and has other focuses.\n\n\nI have tried applying Stanford's OpenIE to some of the text. It outputs trash. I had intended to aim to make improvements on it, eg improved NER, coreference resolution, filtering input based on sentiment analysis, then maybe some kind of filtration on the output to reduce false positives. A major issue is that it is very poor at NER, outputting relations with fragmented boundaries. I don't know if attacking these problems is feasible in 3 months.\n\n\nI have 3 months. I am losing my fucking mind. Any pointers are appreciated.\n\n\nedit\n\n\nmaybe just scrapping all of that and instead doing some sentiment analysis to investigate MP's views on other MP's and parties over time would be more feasible. I would have a much smaller named entity domain.",
"date": "2022-02-02"
},
{
"vote": 1,
"title": "How to use Google T5-large (a transformers model) for a summarization use case?",
"text": null,
"date": "2022-02-02"
},
{
"vote": 4,
"title": "Is it a good idea to combine encoders and decoders from different models?",
"text": "I was looking at a problem where I only need the decoder of a specific architecture. So would it be a good idea to train my encoder and use a pre-trained decoder from that model?",
"date": "2022-02-02"
},
{
"vote": 1,
"title": "NLP dataset for /r/Ottawa's reactions to the 2022 Freedom Convoy - ongoing discussion of a hot issue.",
"text": "[deleted]",
"date": "2022-02-01"
},
{
"vote": 1,
"title": "Stand-alone sentence segmenter",
"text": "Does anyone know a good standalone sentence segmentation tool / method that isnâ€™t part of a broader NLP module like NLTK or Spacy?\n\n\nIdeally, just a single standalone function. I could pip install it or maybe even just copy and paste in some code into a file.\n\n\nThanks very much",
"date": "2022-02-01"
},
{
"vote": 2,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-02-01"
},
{
"vote": 3,
"title": "Treebanks with PTB style bracketing",
"text": "Hi everyone!\n\n\nAre there any corpora or treebanks that are labeled with Penn Treebank-style constituency trees (other than the PTB itself)?\n\n\nI'm investigating the usage of a certain syntactic construction in English. I'm using the PTB constituency trees, but it would be nice to have as much data as possible. I found the Georgetown University Multilayer Corpus (GUM), and I'm wondering if there are others.\n\n\nThanks in advance!",
"date": "2022-02-01"
},
{
"vote": 2,
"title": "[HIRING] NLP Developer | Japan Remote",
"text": null,
"date": "2022-01-31"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-31"
},
{
"vote": 11,
"title": "Where to find an up-to-date list of the top-k most common words in English",
"text": "I need to identify the ~ 10 000 most common words in the English language as of some reliable / well-established corpus. I intended to use this repo \nhttps://github.com/first20hours/google-10000-english\n\n\nbut I realized that it is somewhat outdated. Any ideas where I could find a comprehensive, up-to-date list of the most common words in English? Thanks :)",
"date": "2022-01-31"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-30"
},
{
"vote": 5,
"title": "Experienced software dev, beginner to NLP. Seeking beginner learning resources, with a specific leaning towards NLP of Chinese language.",
"text": "[deleted]",
"date": "2022-01-28"
},
{
"vote": 1,
"title": "Introduction to Neural Search - Neural network powered search",
"text": null,
"date": "2022-01-28"
},
{
"vote": 2,
"title": "Has anyone ever used spacy with a fit predict structure",
"text": "Spacy is an industry standard, and I have been using it ever since I've been in the field. One thing I have always wanted is for spacy to have fit and predict methods, much like sklearn. I understand spacy has its own forms, like the evaluate method. Of course, it probably won't be hard to build a fit predict method based wrapper around spacy, but \nI am wondering if anyone has ever come across any such wrapper?\n \n\n\nBenefit of such a wrapper would be that when building retraining pipelines with models from various libraries, most of which use fit and predict, being able to call fit and predict on a spacy model would simplify things.",
"date": "2022-01-27"
},
{
"vote": 2,
"title": "Benchmarking NLP Datasets",
"text": "Hello Everyone,\n\n\nI am a newbie in NLP research. My question is - How should we benchmark a new Language dataset/corpus (ex. dialogue dataset, q/a dataset) when there is no publicly available dataset for that particular language? Also what are the possible directions to perform evaluation on the newly prepared dataset. Need suggestions, please.",
"date": "2022-01-27"
},
{
"vote": 16,
"title": "OpenAI Releases Three Embedding Model Families To Optimize Text Search, Code Search and Text Similarity",
"text": "In the last few decades, neural networks have been used for a wide range of tasks, including image segmentation, natural language processing, and time-series forecasting.Â \n\n\nOne promising use of deep neural networks is embedding, a method for representing discrete variables as continuous vectors. An embedding is a low-dimensional space into which high-dimensional vectors can be translated, making it easy for computers to understand the relationships between those concepts. Numerically similar embeddings are also semantically identical. Word embeddings for machine translation and entity embeddings for categorical data are two applications of this approach.Â \nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/abs/2201.10005\n\n\nDocumentation: \nhttps://beta.openai.com/docs/guides/embeddings",
"date": "2022-01-26"
},
{
"vote": 1,
"title": "Multilingual image similarity search with SQL",
"text": "[deleted]",
"date": "2022-01-26"
},
{
"vote": 1,
"title": "Dependency parser from scratch in Python",
"text": "Iâ€™d like to challenge myself to write my own dependency parser for natural language (English) in Python.\n\n\nIâ€™m picturing taking in the sentence one word at a time and somehow working backwards to figure out the tree structure of the sentence.\n\n\nOf course, it should start with tokenization. Perhaps part-of-speech tagging is a necessary next step, to attempt to group certain parts of speech that never dislocate from their complements, like â€œtheâ€, or other words with predictable behavior, like â€œandâ€ and conjunctions and so on?\n\n\nLike the game â€œMindmasterâ€, one can work backward layer upon layer to deduct what the original structure of the sentence is.\n\n\nHowever, maybe I need effective segmentation for this to work? Iâ€™m wondering how periods will affect this procedure. I could ignore them and hope the dependency parse can still work. For example, â€œandâ€ never appears at the end of a sentence.\n\n\nAnother idea is to try to design a neural network myself rather than using a library like Spacy. I just need to know what architecture is optimal and practice training it a bit.\n\n\nAnyone have any recommendations on this?\n\n\nThanks very much",
"date": "2022-01-26"
},
{
"vote": 3,
"title": "Import part of Spacy library",
"text": "Does anybody know if you can just import part of the Spacy library you need?\n\n\nI find â€œimport spacyâ€ to be the slowest part of using spacy.\n\n\nWhat takes so long to load? All the pipeline scripts or something? Because loading the language model with spacy.load() and constructing the doc object with doc(text) are faster than the initial import.\n\n\nIâ€™d like to just load the pipelines I need or something from the module, like â€œfrom spacy import â€¦â€\n\n\nDoes anyone know of this is possible?\n\n\nThanks very much",
"date": "2022-01-26"
},
{
"vote": 3,
"title": "Question regarding the denominators in Kneser-Ney Smoothing",
"text": "Hi everyone!\n\n\nI am currently studying smoothing techniques, specifically Kneser-Ney smoothing. I understand that it helps to handle the case where the next word hasn't appeared in the given context previously. For eg, the corpus could have non zero trigram counts of 'This is a', but no occurrence of the 4-gram 'This is a car'.\n\n\nThe count C(This is a) is captured in the denominator of the lambda term as well, and this lambda term is multiplied with the recursion term. My question is, what if that particular count is actually zero? I hope the following the mini example can make my question clearer - \n\n\nCorpus:\n\n\n<s> <s> You are my friend </s> </s>\n\n\n<s> <s> They are my enemies </s> </s>\n\n\n<s> <s> I have friends and enemies </s> </s>\n\n\nSay we would like to find the probability of the trigram 'are you friend', or P(friend | are you). As per the formulation given in page 9 of the document:\n\n\nhttps://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf\n\n\nThe denominator of two terms consists of the count of the bigram 'are you'. But from the corpus this is zero, and at the same time, each of the individual words 'are' and 'you' aren't UNK, as their unigram counts are 1 and 2 respectively. So how does the recursion proceed now, since we cannot divide by zero?  \n\n\nThank you!",
"date": "2022-01-26"
},
{
"vote": 11,
"title": "How to create a question answering model that can trigger specific actions?",
"text": "I want to create a model that can trigger specific actions based on the input given to the model. For example, if the user asks where is the nearest petrol pump the model will trigger the google maps API and calculate the distance.",
"date": "2022-01-25"
},
{
"vote": 20,
"title": "I made a tutorial on how to do Speech Recognition with Kaldi!",
"text": "Hey everyone,\n\n\nKaldi is a really powerful toolkit for ASR and related NLP tasks, but I've found that the learning curve is a bit steep.\n\n\nHere's\n \na tutorial I made that takes you through installation and transcription using pre-trained models\n, but the cool part is that you can decide how advanced you want it to be!\n\n\nIncluded are Python scripts to automate the entire process, so you can generate transcriptions in just a few lines of code, but I also dive into the code itself to explain what's going on under the hood!\n\n\nI'd love to hear any thoughts and feedback, or future topics you want to see covered!",
"date": "2022-01-24"
},
{
"vote": 4,
"title": "First foray into NLP at work, need feedback on the workflow.",
"text": "Hi everyone, could use some input/thoughts on an NLP workflow I am helping to design based on a pretty unique(?) situation .\n\n\nEssentially this is a binary classification problem; we have around 1000 documents and because of the sheer messiness of the text, a team has provided us with the key phrases taken from these documents that denote whether the document should be a 'Yes'. As opposed to labeling the entire document itself, if that makes sense. \n\n\nBased on this, I was thinking that instead of using the entire set of documents as a corpus (standard approach), that we would take the collection of key phrases as a corpus instead. Then vectorize it using the standard TF-IDF approach, then use that as the input to the classifier.\n\n\nCouple questions on this:\n\n\n\n\nFirstly, does the construction of the corpus in this manner even make sense, since we're not using the entirety of the document?\n\n\n\n\nIs there a way to incorporate BERT embeddings into this? My supervisor is very keen on the cutting edge stuff and wants to incorporate BERT, but best I can tell there's not really a way to do this based on the workflows I have seen. The standard TF-IDF approaches seem different from the BERT workflow, which typically involves fine tuning on my corpus and then making predictions from there.\n\n\n\n\n\n\nThanks!",
"date": "2022-01-23"
},
{
"vote": 2,
"title": "Authorship Attribution dataset for short texts.",
"text": "Hi, I'm looking for an Authorship Attribution dataset with small-medium texts (Mostly social media excerpts if possible). Looked everywhere but couldn't find any, found a great dataset for large texts (Blogs) but none for small texts. Would like to search if one exists to hopefully not have to scrap it myself.",
"date": "2022-01-22"
},
{
"vote": 6,
"title": "[P] Finding and correcting text classification label errors with cleanlab and Rubrix | https://rubrix.readthedocs.io/en/master/tutorials/find_label_errors.html",
"text": null,
"date": "2022-01-22"
},
{
"vote": 0,
"title": "Starting to learn NLP, need some directions. (I'm using catalyst NLP, inspired by spacy)",
"text": "I'm currently in the process of learning NLP. I am using catalyst on c#. \n\n\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\n\n\nHere is a summary of what I would like to achieve. \n\n\nI would like to extract certain information on a sentence. Lets say i have the following texts:\n\n\n\"Sally ate an orange this morning. \"\nOr \n\"Sally is hiding behind the cabinet and she is eating an orange. \" \n\n\nHow do i use the nlp to extract what sally ate?",
"date": "2022-01-22"
},
{
"vote": 22,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-21"
},
{
"vote": 2,
"title": "Detecting the Presence of an Object in a Sentence",
"text": "Is there any way to differentiate between the absence or inclusion of an object in a sentence? For example: there is not a cat in my yard, the cat disappeared from my yard, a cat does not exist in my yard, etc. versus there is a cat in my yard, the cat appeared in my yard, a cat exists in my yard.\n\n\nAny help would be greatly appreciated!",
"date": "2022-01-20"
},
{
"vote": 6,
"title": "Seeking string \"Readability\" metric.",
"text": "Hello fellow enthusiasts, \n\n\nI have a corpus of 150k documents, and their respective OCR outputs. \n\n\nI'd like to assign a Readability score to each document, is there a metric out there for something like that? \n\n\nIn retrospect to my OCR extraction, which took almost a month of runtime to run, I \ncould\n have extracted an OCR-accuracy score along with my strings. I'd like to find an alternative solution instead of re-running it. Knowledge for next time, anyways...\n\n\nI'm open to all thoughts and considerations.",
"date": "2022-01-19"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-19"
},
{
"vote": 5,
"title": "Top 10 Machine Translation APIs",
"text": null,
"date": "2022-01-19"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-19"
},
{
"vote": 1,
"title": "Pyarabic: a python package for the Arabic language ( brief description with basic simplified in english)",
"text": null,
"date": "2022-01-18"
},
{
"vote": 6,
"title": "Fine-tuning reader models for Question-Answering",
"text": "Hi all, I put together some material on \nfine-tuning reader models for open-domain question-answering\n (ODQA). ODQA is an increasingly popular approach to building more human/natural language information retrieval tools. Allowing users to store massive amounts of text data, and then search using natural language questions, it is one of the technologies that powers Google search. Reader models are the final step in an ODQA pipeline, allowing us to extract very specific answers to questions.\n\n\nLet me know what you think, I hope it's useful, thanks!",
"date": "2022-01-18"
},
{
"vote": 1,
"title": "Unicode error in vectorizing text",
"text": "I am trying to vectorize 20-news group data\n using tensorflow TextVectorization layer\n but in TextVectorization layer\n if I limit the vocab size to some number say 10000 then it works fine. However if I preprocess the data or do not set the vocab size to some number then I get UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfe in position 2257: invalid start byte\n error.\n\n\nMy question is have I done something wrong in preprocessing? Because if I set the vocab size to 10000 and do the same preprocessing then this wont work. Also, do I need to set vocab size in 'TextVectorization` but the docs says it can have unlimited size?\n\n\n&#x200B;\n\n\nHere is what I did :\n\n\ni. Get the list of files:\n\n\n train_dir_list = []\n for i in os.listdir(train_dir):\n     f = os.path.join(train_dir, i)\n     for j in os.listdir(f):\n       train_dir_list.append(os.path.join(f, j)) \n\n\n\nii. Create tensorflow data\n\n\ntrain_data = tf.data.TextLineDataset(train_dir_list) \n\n\n\niii. Preprocess data\n\n\ndef preprocess(text):\n   lower = tf.strings.lower(text)\n   # remove emails\n   email_removed = tf.strings.regex_replace(      lower, &quot;\\S*@\\S*\\s?&quot;, &quot;&quot;   )\n   # remove numbers\n   number_removed = tf.strings.regex_replace(       email_removed, &quot;[0-9]&quot;, &#039; &#039;   )\n   # remove punctuations\n   punctuation_removed =  tf.strings.regex_replace(       number_removed, &#039;[%s]&#039; % re.escape(string.punctuation), &#039; &#039;   )\n   # remove multiple blank spaces\n   multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed),      separator=&quot; &quot;)\n   return multiple_space_removed \n\n\n\niv. Create vectorizer: In here if I remove the standardize=preprocess\n and keep the vocab_size and sequence_length\n it works fine. But if I use standardize=preprocess\n either with same vocab_size and sequence_length\n or do not use standardize=preprocess\n but keep the vocab_size and sequence_length\n empty or as default then it gives the UnicodeDecodeError:\n\n\nvocab_size = 10000 sequence_length = 300 \n\n\n\nThis works fine:\n\n\nvectorize_layer = layers.TextVectorization(\n     # removed preprocess\n     max_tokens=vocab_size,\n     output_mode=&#039;int&#039;,\n     output_sequence_length=sequence_length ) \n\n\n\nThis will throw error:\n\n\nvectorize_layer = layers.TextVectorization(\n     standardize=preprocess,\n     max_tokens=vocab_size,\n     output_mode=&#039;int&#039;,\n     output_sequence_length=sequence_length ) \n\n\n\nThis will also give error :\n\n\nvectorize_layer = layers.TextVectorization(#used default parameter) \n\n\n\nv. calling adapt works fine\n\n\nvectorize_layer.adapt(train_data.batch(1024)) \n\n\n\nvi. This is were the error is thrown\n\n\nvectorize_layer.get_vocabulary() \n\n\n\nAlso, on looking up the vocab size when the error is produced, the vocab size is only around 30-40",
"date": "2022-01-18"
},
{
"vote": 1,
"title": "Speak Spanish From Day One",
"text": "[removed]",
"date": "2022-01-18"
},
{
"vote": 16,
"title": "What LSTM Baseline To Use?",
"text": "Suppose you are writing a paper with some new transformer-variant that does well on classification tasks. You want to have a LSTM baseline. How would you go about choosing that LSTM architecture? What about training hyperparameters? Is there a standard, should it be grounded with respect to another paper, does it not matter as long as one explains what the architecture is?\n\n\n&#x200B;\n\n\nThanks!",
"date": "2022-01-17"
},
{
"vote": 21,
"title": "I'm building a neural search plugin for Elastic/Opensearch",
"text": "Hey Everyone,\n\n\nI'm building a plugin for end-to-end neural search (eg SBERT, Hugging Face, Doc2Vec etc) in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.\n\n\nDespite the Elasticsearch website claiming in many places that you can do \"machine learning\" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases (putting to one side some specific cases like anomaly detection), you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first retrieve documents using an algorithm like tf-idf before putting them through neural search.\n\n\nI want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:\n\n\n\n\nOpensearch/ES have added a great deal in terms of functionality to allow for vector search (eg approximate knn algo), but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a \"good enough\" solution.\n\n\nIf the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.\n\n\nI'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.\n\n\n\n\nI think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.\n\n\nLet me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form \nhttps://forms.gle/acmGTK1gPkPZbVJm8",
"date": "2022-01-16"
},
{
"vote": 8,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-15"
},
{
"vote": 0,
"title": "what are the topic modling example?",
"text": "[removed]",
"date": "2022-01-15"
},
{
"vote": 9,
"title": "I'm conducting research in NLP with data pulled from multiple sources, primarily Reddit, Twitter, and Facebook. The data contains different categories which are mentioned in the description below. Is anyone familiar with the ethics or the problems that came up using data like this?",
"text": "The different categories include:\n\n\n\n\nPosts that were deleted by the user themselves.\n\n\nPosts that were banned by the Community moderators.\n\n\nPosts that were banned by the Platform moderators.\n\n\nPages or communities containing posts that were banned by the Platform moderators.\n\n\n\n\nI'm fairly uncertain about whether all the data that was pulled contains reasons for the ban they faced. In the case of deleted posts, there's no such label available.\n\n\nAny idea how to go about this? Any link to cited paperwork that has faced and dealt with similar problems would be great. Links or mentions of authors who might have faced this issue also help as I can try reaching out to them. I'm having some trouble finding sources.\n\n\nEven similar datasets links would be great as I can do a comparison study on this.\n\n\nThanks. :D",
"date": "2022-01-15"
},
{
"vote": 2,
"title": "Farsi &gt; English Translation Model",
"text": "Iâ€™m just wondering if anybody knows of any good Farsi (Persian) > English translation models? Iâ€™ve tried a few of the multilingual ones from Huggingface but the quality isnâ€™t the best",
"date": "2022-01-14"
},
{
"vote": 8,
"title": "Help needed. How to predict profession from short bio ?",
"text": "Hi NLP community,\n\n\nProbably much of a newbie here and need some guidance. I am doing a personal project that aims to predict a person's industry from their short biography.\n\n\n&#x200B;\n\n\nFor example:\n\n\n\" I am a retired engineer and company manager. I do not have a financial background or offer financial advice. blah blah \" => \nPrediction:\n ENGINEERING\n\n\nand\n\n\n\" Damon makes his living as a gap trader, an earnings trader, and an interday trader. In his free time, he writes for ABC, where he focuses on seasonal investing, market timing, and earnings analyses. \" => \nPrediction:\n FINANCE\n\n\n&#x200B;\n\n\nI wanted to ask what approach should i do to make such predictions ? And what kind of public dataset would be useful to train a ML model for such task ?\n\n\n&#x200B;\n\n\nThank you so much !",
"date": "2022-01-14"
},
{
"vote": 2,
"title": "NLP Bias &amp; (un)Fairness Recognizer App with Spacy | Day 2 of #8daysofstreamlit",
"text": null,
"date": "2022-01-13"
},
{
"vote": 0,
"title": "I want to find account where people comment for follow for follow mean follow each other on instagram.when I type follow for follow on instagram many such Insta account pop up in result but what i should type on instagram so I can also reach people of other languages like Russian,Portuguese etc",
"text": "[removed]",
"date": "2022-01-13"
},
{
"vote": 6,
"title": "Pretrained models for multi-label classification (transformer based)",
"text": "Hi,\n\n\nI am looking at multi-lable classification for Twitter-like data. Does anybody know if any open source project (or Huggingface model hub) has any pre-trained models ready to use?\n\n\nI am looking at classification taxonomy such as \nIAB v2 categories\n or Wikipedia categories.  \n\n\nThanks!",
"date": "2022-01-13"
},
{
"vote": 4,
"title": "Alternatives to Google Cloud Translate",
"text": "A lot of cloud computing services have a lot of vendors in the field but is there any other API for machine translation like Google Translate? DeepL or any others?\n\n\nThanks very much",
"date": "2022-01-12"
},
{
"vote": 1,
"title": "Masters program in linguistics/cognitive science?",
"text": "[deleted]",
"date": "2022-01-12"
},
{
"vote": 3,
"title": "Choosing a LT program",
"text": "Hi everyone! I am a BA graduate with background in linguistics and self-taught programmer. I have been looking at two programs in language technology Msc. in Speech and Language Processing in Edinburgh (\nhttps://www.ed.ac.uk/ppls/linguistics-and-english-language/prospective/postgraduate/msc/speech-language-processing\n) and Human Language Technology (\nhttps://linguistics.arizona.edu/master-science-human-language-technology-hlt\n) in the States and I was wondering I anyone has any advice in which would be a better fit? I am hoping to start working in the industry after graduating (open to do a PhD but maybe more in the future) but I am worried about which one would help me be more prepared. The one in Edinburgh is a one-year program while the one in AZ is a two-year program. Would appreciate any help with your viewpoints on this :)",
"date": "2022-01-12"
},
{
"vote": 1,
"title": "Twitter topics",
"text": "Does anyone know how Twitter generates their â€œTopicsâ€?\n\n\nIt seems like they could be machine generated (but human reviewed)? It would be a lot of labor simply brainstorming a huge ontology of trending concepts in the Twittersphere.\n\n\nThey must have some algorithms for analysing and clustering tweet topics.\n\n\nAnd possibly even for automatically suggesting the name of the cluster (the topic / label).\n\n\nAnyone have any guesses how they do it?\n\n\nThanks very much",
"date": "2022-01-11"
},
{
"vote": 3,
"title": "UC Sandiego Researchers Propose A Controllable Voice Cloning Method That Allows Fine-Grained Control Over Various Style Aspects Of The Synthesized Speech For An Unseen Speaker",
"text": "Text-to-Speech (TTS) synthesis is achieved using current voice cloning methods for a new voice. They do not, however, manipulate the expressiveness of synthesized sounds. The task of learning to synthesize the speech of an unseen speaker with the least amount of training is known as voice cloning.\n\n\nUC San Diego researchers propose a Controllable voice cloning method that offers fine-grained control over many style features of synthetic speech for an unseen speaker. The voice synthesis model is explicitly conditioned on a speaker encoding, pitch contour, and latent style tokens during training. \nContinue Reading\n\n\nPaper: \nhttps://arxiv.org/pdf/2102.00151.pdf",
"date": "2022-01-10"
},
{
"vote": 9,
"title": "Is there a way to detect torn up words?",
"text": "Example: Qual ity -> quality\n\n\nI'm using pytesseract to transcribe pdfs, and unfortunately one of the issues is PDF often splits up words at the end of column in two parts . I'm trying to figure out a way to detect when words don't make sense separately but make a normal word combined (using python)",
"date": "2022-01-10"
},
{
"vote": 3,
"title": "Why is Fine tuning a text model so influential on the results?",
"text": "Newbie to this field, but nonetheless BERT was trained on 3.3 billion+ words, when I do a masked learning task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional 1 million words (only ~0.02% more words), suddenly the same task is significantly more accurate.\n\n\nI understand that fine tuning is training the model on my specific task, so of course results will improve, but it is almost as if the model weights the fine-tuned additional words over itself. \n\n\nWhy can such a small number of additional words improve the model so drastically?",
"date": "2022-01-09"
},
{
"vote": 2,
"title": "Best tools for Multi-GPU Model training?",
"text": null,
"date": "2022-01-09"
},
{
"vote": 30,
"title": "Are there any languages in the world which break some NLP fundamental assumptions?",
"text": "So, for context, I've only just started learning NLP, and I've just encountered the word2vec algorithm for the first time. This algorithm calculates the probability of a word appearing at a position in a sentence as a function of what it's surrounding words are, weighted by the distance from that central word, learned from a large corpus of language. So for instance, if you fed it an incomplete sentence: \"the cat jumped over the ... \", it would assign high probabilities to words like \"table\", \"mat\", \"bed\", and assign low probabilities to words like \"blue\", \"boil\", \"running\". \n\n\nAre there any human languages in the world for which the assumptions which the algorithm are built on break? For example, any languages for which the context of a word is \ninversely\n proportional to it's semantic meaning, rather than proportional as this algorithm assumes? \n\n\nAre there any other interesting concepts in NLP which work for some languages, but not others?",
"date": "2022-01-09"
},
{
"vote": 1,
"title": "D.A.I.S.Y Method of learning English",
"text": "[removed]",
"date": "2022-01-09"
},
{
"vote": 11,
"title": "Is there an equivalent concept in NLP to what high-level computer languages (e.g. Python) do to manage user error?",
"text": "Is there an equivalent concept in NLP to what high-level computer languages (e.g. Python) do to manage user error?\n\n\nThat is:\n\n\n\n\nnatural languages may see users doing errors (grammar etc.)\n\n\n\n\nIn computer languages, however:\n\n\n\n\nlow-level languages (e.g. ASM, C) see users doing errors (e.g. in utilizing memory)\n\n\ntherefore people design higher level languages (e.g. Python) that have features that correct these errors\n\n\n\n\nIs there an equivalent concept for natural languages in NLP?",
"date": "2022-01-08"
},
{
"vote": 13,
"title": "Clone your voice and speak a foreign language",
"text": null,
"date": "2022-01-07"
},
{
"vote": 5,
"title": "How can you do efficient text preprocessing?",
"text": "Hello,\n\n\nI am trying to do some basic preprocessing on 2.5GB of text. More specifically, I want to do tokenization, lower casing, remove stop words and top-k words. I need to use spacy because the dataset is in greek and I think other libraries can't support this.\n\n\nHowever, when I try to apply what the spacy documentation or most of the guides/resources mention, it takes forever to complete even half of the techniques that I mentioned above. I stop the execution every time.\n\n\nCould you provide me with some resources that I might have missed, in order to make this procedure run faster?\n\n\nThanks in advance",
"date": "2022-01-07"
},
{
"vote": 7,
"title": "Named Entity Recognition (NER) Ensemble Method",
"text": "[deleted]",
"date": "2022-01-06"
},
{
"vote": 7,
"title": "corpus visualization tools",
"text": "Hello all,\n\n\nWhich are your favorite tools to visualize a corpus?\nDo you prefer a desktop or web solution? And which kind of analyses can you perform with such tools (n-gram, pos, lemmatization ecc)?\n\n\nThanks in advance",
"date": "2022-01-06"
},
{
"vote": 2,
"title": "Looking for a tool to annotate alignments for machine translation.",
"text": "[deleted]",
"date": "2022-01-05"
},
{
"vote": 1,
"title": "A 5 million source code file dataset",
"text": null,
"date": "2022-01-05"
},
{
"vote": 14,
"title": "Advice for stemming historical text",
"text": "So I'm working on some early English text. For example, sometimes \"up\" is spelled \"vp\", or \"himself\" might be \"himselfe\"... or it might not. Is there any advice or good practice for how to handle stemming/lemmata etc.? Has anyone got experience doing word embeddings with this kind of data?",
"date": "2022-01-05"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2022-01-05"
},
{
"vote": 8,
"title": "NLP to Process Academic Citations",
"text": "I have to process undergraduate and postgraduate student essays using spaCy. One of my first step is to remove citations, both narrative and parenthetical ones. And I am using regex to do this. My regex is getting longer and longer and becoming very unwieldy. Moreover, I am assuming students are using APA 7th and not earlier versions or other styles entirely.\n\n\nI am unable to get good results using NER or POS so have to rely on regex.\n\n\nAre there any python NLP packages that will recognise academic citations, both narrative and parenthetical ones? E.g. \"Lee (1990) said ...\", \"... in the study conducted (Lee, 1990)\".",
"date": "2022-01-04"
},
{
"vote": 6,
"title": "Doubt about a point in BERT paper",
"text": "In the \nBERT paper\n it says that during training it mask a fraction of the words and replaces them with random words:\n\n\n> The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.\n\n\nI can't wrap my head about the explaination it gives, can somebody point me somewhere about this part?\n\n\nEDIT: What I don't understand is the justification to do the random word thing.",
"date": "2022-01-03"
},
{
"vote": 6,
"title": "Open Source Chinese Language Thesaurus",
"text": "Are there any open source Chinese language thesauruses? Akin to CEDICT, but with synonyms?\n\n\nI have an application that could really make use of something like that, and without one existing, we'll essentially have to do it by hand, which is fairly laborious.",
"date": "2022-01-03"
},
{
"vote": 10,
"title": "[R] The Illustrated Retrieval Transformer (GPT3 performance at 4% the size)",
"text": null,
"date": "2022-01-03"
},
{
"vote": 5,
"title": "Faster keyword extraction",
"text": "Iâ€™m using KeyBERT to extract 1000 keywords from a file.\n\n\nIt was pretty slow when I did it for only 4 keywords.\n\n\nFor 1000 it ran for almost 15 minutes before I terminated it, I believe it was processing the entire time but itâ€™s just a massive computation.\n\n\nCan anyone advise me on speeding this up? Iâ€™m using a Digital Ocean Droplet.\n\n\nWhat specs do I need to do something like this in hopefully a few seconds? Are we talking 64-core CPU or a certain GPU or something?\n\n\nOr is there any advice on how I can be certain itâ€™s still running, even after like 20 minutes?\n\n\nHow long would you expect an execution like this to take and why? What is it about BERT that is so computation-intensive?\n\n\nThank you",
"date": "2022-01-03"
},
{
"vote": 8,
"title": "NLP tool for simple sentence correction in English (i.e. grammer)?",
"text": "Hi all. A little background: my mother is a Chinese immigrant who is always lacking self-esteem in her ability to speak \"correct\" English. Whenever she sends a text over to someone who is a native English speaker, she always bugs me to correct her sentences so it sounds more \"natural.\" Her English is honestly fine at a conversational level, but could definitely use some editing.\n\n\nI am wondering if there are NLP tools out there that can help my mom with this? Like if someone types a sentence like \"Hi, I almost done\" we can change it to something like \"Hi, I \nam\n almost done\"?\n\n\nThanks in advance.",
"date": "2022-01-03"
},
{
"vote": 9,
"title": "MediaRecorder based smartphone recording vs dedicated app",
"text": "Hi all! I'm proving out an idea for emotion detection using smartphone recordings. Ideally I would like to gather recordings using a web-based application and the MediaRecorder API with smartphones (targeting iOS primarily). Does anyone have experience with doing so? Are the results good enough to work with, or am I better off working on a dedicated app with more control over recording?",
"date": "2022-01-01"
},
{
"vote": 5,
"title": "Next steps for after classification",
"text": "Hello everyone!\n\n\nAfter lots of research and failure, I finally was able to use BERT for classifying text in my dataset.\n\n\nHowever, I feel like a dog that finally caught the car he has been chasing, because I am not sure what to do next.\n\n\nI had a series of questions that I want to pursue but was hoping for a professional opinion.\n\n\nFirst, I want to be able to look at some metrics for seeing how well my model performed. What are good metrics for a multiclass classification task? I know for a fact my classes are imbalanced, so what would be the best way to move forward with this?\n\n\nIn short, what do you ask yourselves once the model is done training and what do you do to evaluate it? How can I improve?\n\n\nI am a nuclear engineer by trade and NLP/DL is still a very new concept and I was hoping to get insight from the masters out there.\n\n\nThanks in advance and happy new year!",
"date": "2022-01-01"
},
{
"vote": 0,
"title": "High-leveled APIs ruined NLP?",
"text": "It seems like things like HuggingFace and Spacy and whatever have done some harm to NLP as a whole.\n\n\nfor instance, I've heard NLP engineers have less pay potential compared to computer vision folk due to most models just being run through their pipelines.\n\n\nalso, it seems difficult to find tutorials post 2018 on topics like NER and such from scratch.\n\n\nEverything is getting abstracted to API's and fewer people are learning things from the ground up.\n\n\nWhat do you think?",
"date": "2021-12-31"
},
{
"vote": 3,
"title": "Teaching transformer \"sentence\" orders",
"text": "Hi there, I'm trying to tackle quite a difficult problem with the help of sentence-transformer-models.\n\n\nIve got a bunch of JSON (alternatively YAML) files from different domains, which contain basically entities as JSON schemas consisting of data fields and descriptions. The entities can be ordered in kind of a hierarchical structure, which is not really strict though and may differ from file to file.\n\n\nI assume that there exist common patterns between those files, precisely how the entities can be ordered in a semantically \"meaningful\" way (a human can understand the structures based on the descriptions). I would like to either\n\n\na) Cluster the schemas to identify similarities between those entities\n\n\nWhat I tried: clustering the descriptions with KMEANS and SentenceTransformers. Problems here:\n\n\n- If I use only the descriptions they get clustered mostly by domain\n\n\n- If I try to cluster the \"raw\" JSON, most models don't find any similarity (tried also CodeBerta etc)\n\n\n=> My idea here would be to fine-tune a model which encodes always two JSON parts as sentence input and I use the description similarity to generate either a classification score or even NLI scores, to train the model on this data, would this be a valid approach or what could be better ideas?\n\n\nb) More of a crazy but interesting idea: If I assume that the \"structure\" can be modeled as a \"sentence\" which consists of \"words\" (embedded entities) than probably some sort of model could learn those \"sentences\".\n\n\n=> How to create \"words\" from sentences? I thought about creating sentence embeddings for all entities, and then building \"entity-sentences\" from the CLS-tokens? How to build a classifier for such \"sentences\"? Are there any good approaches or is there any previous work done?\n\n\n=> Does it make sense to create the model from scratch or would it be helpful to fine-tune an existing model with this approach?\n\n\n=> Would it make sense to look at a completely different sort of ML technology?",
"date": "2021-12-30"
},
{
"vote": 15,
"title": "[P] Ecco - Language model analysis and visualization toolkit",
"text": null,
"date": "2021-12-30"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-29"
},
{
"vote": 8,
"title": "Q: Transformers - Query, Key and Value Vectors in \"Attention is all you need\"",
"text": "Hi everyone!\n\n\nCan someone explain to me how query, key and value vectors are received from the input word embeddings to an encoder or decoder layer? I see how they (the q, k, v vectors or matrices respectively) are used in the multihead attention layer, but I dont understand where they come from. They have to depend on the input word embedding, but how?\n\n\nIn the original transformer paper \n(Attention is all you need)\n I only found those vectors mentioned in chapter 3.2:\n\n\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key\n \n\n\n&#x200B;\n\n\nIf anyone could help me answering this question, it would be great!\n\n\nEDIT:\n\n\nMy thanks to /u/Brudaks, /u/boodleboodle and /u/mehtajineshs for clarification on this by providing explanations and resources.\n\n\nI do understand now, that the vectors depend on the output from previous layers and are received by multiplying previous layer output (or word embeddings, in case of first layer) with randomly initialized matrices, like other weights in an FF network for example are initialized randomly as well.\n\n\nAnd like weights in a feed forward network, the matrices for receiving QKV vectors are learned by backprop.",
"date": "2021-12-29"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-29"
},
{
"vote": 0,
"title": "Researchers At Johns Hopkins Introduce A Machine Learning Model That Can Allow Computers To Understand Human Conversation",
"text": "[deleted]",
"date": "2021-12-29"
},
{
"vote": 3,
"title": "Open Discussion: ways to prevent Voice Synthesis misuse",
"text": null,
"date": "2021-12-28"
},
{
"vote": 1,
"title": "looking for help on approaching problem",
"text": "I currently have a list of sentence fragments that loosely describe listing for sales for houses/apt/mansions etc.  \n\n\nThey might look something like this:\n\n\n[apartment, 4 glazed windows, wood floors and well insulated, with large pool]\n\n\n[large apartment, 4 bedrooms, 1 master bathroom,  carpet everywhere but not in bathrooms]\n\n\n[baby room, 2 bed, half a washroom,  crawlspace attic for storage, garden with swim area]\n\n\nI want to apply labels (keywords) to these fragments to \"standardized\" the language which I can then use to process later.  Knowing to group the following is important:\n\n\n\"large pool\" --> has swimming pool\"\n\n\n\"garden with swim area\" --> \"has swimming pool\"\n\n\nThe \"keywords\" I might want to use for the examples:\n\n\n\n\n[apartment, 4 glazed windows, wood floors and well insulated, with large pool]  ---> [apt, has_floor, has_pool]\n\n\n[large apartment, 4 bedrooms, 1 master bathroom,  carpet everywhere but not in bathrooms]---> [apt, has_floor, has_bedrooms]\n\n\n[baby room, 2 bed, half a washroom,  crawlspace attic for storage, garden with swim area]---> [has_bedrooms, has_attic, has_pool]\n\n\n\n\nI do not need to \"capture\" all the descriptions from the sentence fragments.  And at least, I want to be able to grab the lowest hanging fruit first (right now I have nothing!)\n\n\nI see that I have some  issues:\n\n\n\n\nHow do I break down these \"sentence fragments\"?  So that analysis can be done?\n\n\nHow can I \"group\" text that shows up so that I know what categories I want to create?  Even better, if groupings can be automatically created/suggested\n\n\nEven if I have \"labels\" that I want to assign a set of fragments how do I train a model to actually do this?  (Like if I spent 5 hours (which i have) labeling some very basic categories.... how do I use this?)\n\n\n\n\nOne possible wrinkle I have,  is that I do not care which \"sentence fragment\" correspond to which label.  (when I labeled the dataset, I just said, does this sentence fragment correspond to these labels/keywords) - therefore it is difficult for me to map a \"sentence fragment DIRECTLY to a group with heuristics\" .   In the end, I do not necessarily care (or know) which of the sentence fragments actually correspond to the label, just that this example should have the given labels.\n\n\nI hope my problem description makes sense, and looking for any type of directed help/ approaches.  I have looked at \"tokenization\", \"word count\", \"bag of words\" etc but I am unable to understand it enough to see the full picture of how to use it.\n\n\nAny comments appreciated!\n\n\n[language of choice python]",
"date": "2021-12-28"
},
{
"vote": 10,
"title": "Huggingface for glossary creation",
"text": "Does anybody know of a leading AI model for glossary creation?\n\n\nIâ€™m considering using Spacy for this but so far I found their entity recognition and even their segmentation to be good but not necessarily flawless.\n\n\nI could stick it custom trained models for sure, it honestly might not be that hard.\n\n\nIâ€™m wondering if anybody has gone before me here, though.\n\n\nAn auto-glossary creation tool at minimum should:\n\n\n\n\nRecognise terms, not necessarily entities. Entities appear to be more trivial, like even just years and numbers come up sometimes. Terms are important keywords.\n\n\n\n\nRetrieve context/example sentences from the source documents for each word.. AI is not strictly necessary for this, but it could be leveraged in deciding which sentence containing a term is most â€œrepresentativeâ€. Plus, AI would come in handy for lemma-matching - it should be able to search for any grammatical form of a word in source text, and not match â€œcrudelyâ€ as in maybe a homonym of a word.\n\n\n\n\nIdeally, it should auto-categorize terms (Iâ€™m planning on trying BERT to generate a â€œsimilarity scoreâ€, grouping terms with nearness to each other and then generating a label for that group).\n\n\n\n\n\n\nSo: this is the project Iâ€™m currently working on. Has anybody already done something like this, ready to go?\n\n\nThank you",
"date": "2021-12-28"
},
{
"vote": 12,
"title": "An awesome list about vector similarity search : find open source vector search libraries, service, platform service and research papers",
"text": null,
"date": "2021-12-28"
},
{
"vote": 6,
"title": "I am looking for something like synthesia.io",
"text": "I am searching for a natural text to speech or voice cloning program at least of the quality of \nsynthesia.io\n , Don't need the video part though.  Preferably open source or something cheap.",
"date": "2021-12-28"
},
{
"vote": 5,
"title": "Looking for people to learn Python Coding With",
"text": "Hi there, \n\n\nI've recently graduated with a BA in Linguistics and I'm currently pursuing a career in Computational Linguistics. I plan on applying to an Msc in a CompLing related degree in a year or two, but I'm currently taking some time off to relax and also learn Python Coding and polish my math skills. \n\n\nHowever, learning Python from scratch and also learning it independently has been really difficult as I find myself stuck often with nobody that I could talk to about Python, and also I find myself lacking the motivation to keep going. \n\n\nIt would be really nice and helpful if I had a few people I can go to regarding Python-related things. We could motivate each / help each other out etc. \n\n\nPlease let me know if you're interested!",
"date": "2021-12-27"
},
{
"vote": 19,
"title": "How did you advance your NLP career?",
"text": "Dear all, \n\n\nIf there are any NLP/ML engineers, DS, or researchers out there, I could really use some advice. \n\n\nI am graduating from my MS in Economics with a full-time job lined job as a DS at a well-known fintech company. However, it is driving me crazy to find a clear path forward to pursue a more NLP-involved job down the line. \n\n\nHere is what I currently have that can be classified as NLP \"experiences\": \n\n\n\n\nPast Internships! I have done anything from Product management intern for data products powered by NLP to Management Consultant doing research on the data collection strategies that a client could take to improve their NLP classification outcome \n\n\nResearch! I am writing a paper with researchers from NLP for applying NLP techniques to public policy related documents and is due to publish in the next couple of months \n\n\nCurrent job! The team that I am currently on and hired into (that I have been interning on) uses a lot of NLP for insights discovery. We also plan on launching a large scale NLP product down the line which I will be very involved in given our very lean corporate structure\n\n\n\n\nWhy I think I will have a hard time advancing in the field: \n\n\n\n\nI do not have a CS undergrad or MS in CS \n\n\nMy background in economics dictated that I am good at math but not at linguistics \n\n\nI do not come from a hyper prestigious school like Stanford or MIT but a mid-tier school in the East Coast (US)\n\n\n\n\nI feel everyone in the field is so overqualified for what they are doing (granted people may just be very good imposters)! I have no clue what to do ??? \n\n\nShould I go get an MSCS to compete down the line? How does moving up in NLP careers work? Can any folks shine some light on a very confused young person! \n\n\nI will literally take any suggestions or advice haha. thank u y'all!",
"date": "2021-12-26"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-26"
},
{
"vote": 17,
"title": "creature_feature: Composable N-Gram Combinators that are Ergonomic and Bare-Metal Fast",
"text": null,
"date": "2021-12-26"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-25"
},
{
"vote": 1,
"title": "What are the best source for learning English language:Do these 6 things now",
"text": null,
"date": "2021-12-24"
},
{
"vote": 10,
"title": "How to measure accuracy of a generative chatbot model",
"text": "Hi,\nHow to measure the accuracy of a generative chat bot or any generative model?\nIs it possible?",
"date": "2021-12-24"
},
{
"vote": 3,
"title": "Computational grammar checking",
"text": "Iâ€™ve been interested in this for a long time, hoping to finally crack the code here.\n\n\nIn Swedish an adjective can be determined or not. Like in English the article â€œtheâ€ vs â€œaâ€.\n\n\nThis is a more obscure case, a phrase in a product catalog â€œOn hard floorâ€.\n\n\nNot sure what grammarians would say is going on here. Itâ€™s a general case so I would say itâ€™s non-determined and they dropped the â€œaâ€ just for brevity.\n\n\nOn the other hand it is more comparable to the general case which commonly uses the plural in English: â€œon hard floorsâ€.\n\n\nI would like to have a convenient system to check what is done in Swedish without just leafing through grammar websites and so on.\n\n\nI want to access a most convenient Swedish corpus - not a database requiring a sign up but just an easily downloadable dataset, maybe Kaggle?, or as part of some software package like Spacy.\n\n\nThen I want to execute a formula like â€œshow me matches of sentences of the form â€œpreposition determined adjective nounâ€â€. I can develop it from here but this would be a good start.\n\n\nDoes anyone have a suggestion for an accessible corpus with syntax parsing and searching?\n\n\nThank you very much",
"date": "2021-12-23"
},
{
"vote": 1,
"title": "How to extract custom named entities recognition(NER) using Keras or NLTK",
"text": "[deleted]",
"date": "2021-12-23"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-23"
},
{
"vote": 1,
"title": "How to extract custom named entities using Keras",
"text": "[removed]",
"date": "2021-12-23"
},
{
"vote": 12,
"title": "OpenAI Researchers Find Ways To More Accurately Answer Open-Ended Questions Using A Text-Based Web Browser",
"text": "Long-form question-answering (LFQA), a paragraph-length answer created in response to an open-ended question, is a growing difficulty in NLP. LFQA systems hold the potential to become one of the most important ways for people to learn about the world, yet their performance currently lags behind that of humans. Existing research has tended to concentrate on two key aspects of the task: information retrieval and synthesis.\n\n\nResearchers at OpenAI have recently developed WebGPT. They outsource document retrieval to the Microsoft Bing Web Search API and use unsupervised pre-training to produce high-quality synthesis by fine-tuning GPT-3. Rather than striving to improve these factors, they concentrate on integrating them with more consistent training goals. The team leverages human feedback to directly enhance the quality of answers, allowing them to compete with humans in terms of performance.\n\n\nIn this paper, the team offers two significant contributions. They create a text-based web-browsing environment that can be interacted with by a fine-tuned language model. This enables the use of general approaches like imitation learning and reinforcement learning to improve both retrieval and synthesis in an end-to-end manner. The team also creates replies with references, sections collected by the model when exploring web pages. This is critical because it allows labelers to assess the factual accuracy of answers without having to engage in a time-consuming and subjective independent research procedure.\n\n\nQuick Read: \nhttps://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/\n \n\n\nPaper: \nhttps://arxiv.org/pdf/2112.09332.pdf\n\n\nOpen AI Blog: \nhttps://openai.com/blog/improving-factual-accuracy/",
"date": "2021-12-23"
},
{
"vote": 12,
"title": "Custom Named Entity Recognition (NER) for identifying CVs.",
"text": "I am thinking of creating model for extracting entities in a cv such as \n\n\n\n\nName\n\n\nAddress\n\n\nInstitute\n\n\nDegree\n\n\nSkill\n\n\nCompany\n\n\nSchool\n\n\nDesignation\n\n\nSociety - Ex: sport clubs, school societiesâ€¦ . In spaCy there are a very limited no of entities. What about training a model with these data ?",
"date": "2021-12-22"
},
{
"vote": 7,
"title": "Automatically categorised keyword extraction",
"text": "Standard tools I know for keyword extraction are KeyBERT, PyTextRank, and Spacyâ€™s language object which automatically recognised â€œentitiesâ€.\n\n\nI would like to automatically categorise keywords.\n\n\nI am considering making my own algorithm or adding on a step after the above keyword extraction.\n\n\nI believe it needs to cluster terms in some way - general semantic relatedness like WordNet, or a graph algorithm like textrank, or a similar statistical relationship to its lexical environment, maybe by comparing the BERT-generated vectors for each term and then grouping anything with a similarity above a certain score.\n\n\nThen it needs to guess a category name. Maybe BERT could scan through a list of words (from the text or in general) to see which one scores highest, in terms of relevance?\n\n\nI can think of two ideal scenarios:\n\n\n\n\ntokenize the text\n\n\n\n\nextract the key terms by mathematically noticing how terms cluster together in terms of cooccurrence. Put them in those groups and pull a term from the source text that is â€œrepresentativeâ€, it correlates highest with all of them.\n\n\n\n\n\n\nOr:\n\n\n\n\nget keywords with Spacy (I still donâ€™t know how their method works)\n\n\n\n\ncluster their similarity using a BERT score (as mentioned above)\n\n\n\n\nname each cluster with GPT-3\n\n\n\n\n\n\nCould anyone please let me know what they think of these ideas?\n\n\nThank you very much",
"date": "2021-12-21"
},
{
"vote": 3,
"title": "Spacy for keyword extraction",
"text": "Does anybody know a best Spacy method for pulling out keywords and also context sentences for those keywords, from a text?\n\n\nThank you",
"date": "2021-12-21"
},
{
"vote": 7,
"title": "Looking for a NLP which can rate a text for key features like innovation and disruption",
"text": "I am looking for a NLP that evaluates characterization from a text on a scale of 0-100% and gives me this. I have 1000 records with rating and wanted to ask how complicated it is to build a model for this. I have basic knowledge in Python to start. It is for a paper at my university. \n\n\nIs there any library that is more suitable for this and where is the best place to start. \n\n\nWith best wishes \n\n\nPuzzlehead",
"date": "2021-12-21"
},
{
"vote": 1,
"title": "Google Fast Word Piece Tokenization System | NLP Tokenization",
"text": "[deleted]",
"date": "2021-12-21"
},
{
"vote": 16,
"title": "The Spacy NER model for Spanish is terrible",
"text": "Has anybody tried to use Spacy for NER in Spanish? I downloaded the biggest pipeline, but when implemented on some text it tends to extract full bits of sentences and label them as MISC (miscellaneous). \n\n\nIt does correctly extract people and locations, too, but it seems weird to me that the NER model of one of the 'main' languages would be so bad. Has anybody experienced this?",
"date": "2021-12-20"
},
{
"vote": 15,
"title": "Working in the New York City area; looking for societies or groups dealing with linguistics/ML/NLP",
"text": "Hi,\n\n\nI just became an NLP data scientist and would like to find people in real life.  Would you know where I can find groups to network with NLP data scientists/engineers in real life?\n\n\nThanks,\n\n\nDaniel",
"date": "2021-12-20"
},
{
"vote": 2,
"title": "Best ML to Identify Descriptors of List of Terms?",
"text": "Hello fellow NLP fanatics, \n\n\nI'm back with another inquiry I know some of you geniuses can help answer. \n\n\nIn short, what's the best method, NLP, library, ML to identify the actually descriptors for a list of terms? \n\n\nLet's say that we want to know how people talk about a disease, the simple thing would be to start with bigrams (hate cancer, cancer sucks), but of course nothing is that simple. Take this example: \n\n\n\"It's horrible, diagnosed at 25 I had to deal with cancer...\"  \n\n\nNow we know they refer to the cancer as horrible but it's far removed so bigrams won't work. We could leverage POS and suggest that the first adjective before or after cancer should be observed. However this could also pull in several terms not relevant to the description of the disease. \n\n\nRunning a LDA or topic model might inform more high level discussion categories than specific descriptors. \n\n\nAny suggestions to optimize for this kind of research? \n\n\nMuch appreciated, \n\n\nN",
"date": "2021-12-20"
},
{
"vote": 0,
"title": "Meta AI Introduces A New AI Technology Called â€˜Few-Shot Learner (FSL)â€™ To Tackle Harmful Content",
"text": "For the training of AI models, a massive number of labeled data points or examples are required. Typically, the number of samples needed is tens of thousands to millions. Collection and labeling of these data can take several months. This manual collection and labeling delay the deployment of AI systems that can detect new types of harmful content over different social media platforms. To handle this issue, Meta has deployed a relatively new AI model called â€œFew-Shot Learnerâ€ (FSL) such that harmful contents can be detected even if enough labeled data is not available.Â  Â  Â Â \n\n\nMetaâ€™s new FSL deployment is a step towards developing more generalized AI models that will require very few to almost no labeled data for training. FSL falls under the category of an emerging field in AI called meta-learning, where the aim is â€œlearning to learnâ€ rather than â€œlearning patternsâ€ as done in traditional AI models. The FSL is first trained over generic natural language examples, acting as the training set. Next, the model is trained with new policy texts explaining the harmful target contents and policy-violating content that has been labeled in the past, which acts as a support set. Meta has reported that their FSL outperforms several existing state-of-the-art FSL methods by 12% on an average over various systematic evaluation schemes. For further details, one can consult Metaâ€™sÂ \nresearch paper\n.Â  \n\n\nQuick Read: \nhttps://www.marktechpost.com/2021/12/18/meta-ai-introduces-a-new-ai-technology-called-few-shot-learner-fsl-to-tackle-harmful-content/\n\n\nPaper: \nhttps://arxiv.org/pdf/2104.14690.pdf\n\n\nMeta Blog: \nhttps://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it",
"date": "2021-12-19"
},
{
"vote": 3,
"title": "Newbie Q: why bother using anything but gpt3?",
"text": "Why would u use a model offered from HuggingFace?",
"date": "2021-12-18"
},
{
"vote": 0,
"title": "BERT model is not learning!",
"text": "Iâ€™m working on a project where Iâ€™m using Bert to classify binary stock movements using tweets.\nInput - tweets of last 5 days of a company\nTarget variable - 0 (if closing - opening price <0 ) and 1 (if closing - opening price >0)\n\n\nWhat ever I seem to do the model is not learning (training, validation and test accuracy and MCC) intact the MCC is so random every time.\n\n\nI tried many fine tuning methods - changing learning rates, epoch, dropouts , layer wise learning rate decay, reinitialising last few layers of BERT. But nothing seems to work.\n\n\nAny suggestion as to why this is happening and how to improve it? \n\n\nIâ€™m currently stuck at 50 percent accuracy and my target is 57 percent accuracy.\n Your help is greatly appreciated.",
"date": "2021-12-18"
},
{
"vote": 11,
"title": "Named entity recognition extraction from website",
"text": "Can anyone recommend a most standard technique for extracting all keywords of a specific kind (i.e. of a certain category, like â€œspecies of treesâ€) from a whole website?\n\n\nBonus points if the crawler can identify a good context sentence for that term, as well as judge if a context sentence provides/acts as a definition. Ideally, it would grab a context sentence and a definition for each term.\n\n\nMy first attempt is going to be using Spacy for Named Entity Recognition, maybe their Prodigy software, or maybe GPT-3 for zero-shot classification.\n\n\nDoes anyone know any pre-existing â€œsmartâ€ web crawling libraries which, sort of like Google Search, crawl a website for terms and find a good context sentence for that term?\n\n\nThanks so much to anyone who can send me in the right direction here.\n\n\nThanks very much",
"date": "2021-12-17"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-17"
},
{
"vote": 2,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-17"
},
{
"vote": 4,
"title": "phrase similarity",
"text": "I have a bunch of phrases - not full sentences - for which I want to calculate similarities. (Typically two to six words long, for questions like whether (made up example) \"senior data scientist\" is more similar to \"machine learning engineer\" than to \"project manager\"). I'm looking for an off-the-shelf sort of solution - no one in my company including myself has any NLP experience, and this isn't so important to us that I want to spend weeks developing a whole new skillset for it.\n\n\nMy impression from google is that the way to do this is to turn the phrases into vectors and take the cosine similarity of them. It looks like I could use Sentence Transformers (SBERT) with a pre-trained model to get a vector for each phrase. Or I could get a vector for each individual word from some pre-trained model (which one?) and add them up to make a phrase vector. \n\n\nIs there any better approach that I'm missing? Is the SBERT method the way to go for this problem?",
"date": "2021-12-16"
},
{
"vote": 1,
"title": "GPT-3 language model in writing contest versus college students",
"text": "[deleted]",
"date": "2021-12-16"
},
{
"vote": 1,
"title": "Designing a Framework for Conversational Interfaces using PL design, API Design, and Constraint Programming",
"text": null,
"date": "2021-12-16"
},
{
"vote": 60,
"title": "Free course: NLP for Semantic Search",
"text": "Hi all, the first seven chapters of the course \nNLP for Semantic Search\n that I've been working on have been published today. It's all completely free and covers everything you need to get started with building SotA language models for semantic similarity, from machine translation to question-answering, and more!\n\n\nSemantic search allows us to search language-based data based on the semantics or 'meaning' of a text. It enables machine translation and question-answering, it's how Google understands \"what time is it in NYC?\", and even allows us to search for images using text-based queries.\n\n\nIt is in essence, a way for us to interact with machines in a more human way. NLP fits in as the 'semantic' in semantic search.\n\n\nCurrent chapters are:\n\n\n\n\nDense Vectors\n\n\nSentence Embeddings and Transformers\n\n\nTraining Sentence Transformers with Softmax Loss\n\n\nTraining Sentence Transformers with MNR Loss\n\n\nMultilingual Sentence Transformers\n\n\nQuestion Answering\n\n\nUnsupervised Training for Sentence Transformers\n\n\n\n\nLet me know what you think, I hope you enjoy it!",
"date": "2021-12-14"
},
{
"vote": 0,
"title": "Architecting the Future of Cross-Cloud Data Collaboration https://hubs.li/Q010pnn-0",
"text": "[removed]",
"date": "2021-12-14"
},
{
"vote": 5,
"title": "Has anybody tried to update the Spacy NER model?",
"text": "As the title says, have you ever tried to update the Spacy NER model on your own data (as described \nhere\n)? It seems to me that the NER feature just gets worse after retraining, and I don't understand why.",
"date": "2021-12-14"
},
{
"vote": 6,
"title": "Prefer volume or quality for BERT-based Text classification model",
"text": "Ill train a binary classifier. Yes samples make up about 5 percent of all samples. There are multiple persons doing the labelling. They have a pairwise alpha of 0.65\n\n\nScenario A:\n\n\nLabel each sentence once, and have every 10 sentence for all workers to check reliability. Resulting in 52000 single vote samples, plus 6000 multiple vote samples by all. Together about 3000 positive labels\n\n\nScenario 2\n\n\nTripple label everything, resulting in 20000 samples, where i can majority vote, but only have 1000 positive labels.\n\n\nIn your experience, is the better quality of samples worth the volume?\n\n\nEdit: Added first paragraph, which got lost by copy pasting before.",
"date": "2021-12-13"
},
{
"vote": 0,
"title": "Universal Grammar (2) | Noam Chomsky",
"text": null,
"date": "2021-12-13"
},
{
"vote": 2,
"title": "Are spaces generally used as tokens?",
"text": "I've recently started looking into different language modeling methods and once I got to positional embeddings a whole series of questions sprung up for me.\n\n\nOne of these is: \nDo language models generally use spaces?\n\n\nDuring courses at uni I've heard about using subwords both subwords and character level encodings with many types of language models (rnns to tree-lstms to transformers to seeding input order through probablistic (frequentist) parsers).\n\n\nHowevermuch I might have heard about models, I have heard much less about model \ninputs\n. As such I figured I could ask people who are already more preoccupid with nlp in general: is there any consensus on whether this should be done when working with subwords (either morphemes or something different like bytepair encoding)?",
"date": "2021-12-13"
},
{
"vote": 8,
"title": "Looking for sentiment analysis datasets in the news domain",
"text": "I am searching for multiple multi-lingual datasets for news sentiment analysis.\n\n\nIt could be a headline or the body.\n\n\nEven non-English datasets would be great to look at.",
"date": "2021-12-13"
},
{
"vote": 5,
"title": "Is there a professional role in NLP for people who are good at foreign languages and writing?",
"text": "As the title says, is there such a role, for example someone responsible for collecting, editing, or quality assuring texts, either as input data or generated content?",
"date": "2021-12-12"
},
{
"vote": 4,
"title": "Is there a simple way to split Chinese symbols into words?",
"text": "Seems like \nlist()\n will (in python) split the symbols, but AFAIK the symbols are basically subwords. Is there a way to split the symbols based on word boundaries?",
"date": "2021-12-12"
},
{
"vote": 9,
"title": "Haystack - an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.",
"text": null,
"date": "2021-12-11"
},
{
"vote": 4,
"title": "DataQA: the new Python app to do rules-based text annotation",
"text": null,
"date": "2021-12-11"
},
{
"vote": 23,
"title": "How to get Job in NLP?",
"text": "Hi All, I am currently working in the Embedded field mostly in drivers, Linux kernel, and RTOS. Somehow I got an interest in NLP for the past 6 months. I was going through some courses in Coursera, udemy, and some youtube tutorials. These are really helpful and I did some side projects to test my skills. I want to work in NLP as an actual paying job not only during weekends. This is a completely new field and I really don't know how to get jobs in this field. Please give some tips.",
"date": "2021-12-11"
},
{
"vote": 7,
"title": "How Good is Your Chatbot? An Introduction to Perplexity in NLP",
"text": null,
"date": "2021-12-10"
},
{
"vote": 2,
"title": "A New DeepMind Research Studies Language Modeling At Scale",
"text": "Language is an essential human component because of its role in demonstrating and promoting comprehension â€“ or intellect. It allows people to express ideas, create memories, and foster mutual understanding by allowing them to share their thoughts and concepts.Â \n\n\nThe research and study of more sophisticated language models â€” systems that predict and generate text â€“ has enormous potential for developing advanced AI systems. This includes systems that can securely and efficiently summarise information, provide expert advice, and follow directions using natural language. Research on the possible impacts of language models and the risks they entail are required before they can be developed. This includes working together with experts from many fields to foresee and fix the problems that training algorithms on current datasets can cause.\n\n\nQuick Read: \nhttps://www.marktechpost.com/2021/12/10/a-new-deepmind-research-studies-language-modeling-at-scale/\n\n\nPaper 1: \nhttps://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf\n\n\nPaper 2: \nhttps://arxiv.org/abs/2112.04359\n\n\nPaper 3: \nhttps://arxiv.org/abs/2112.04426",
"date": "2021-12-10"
},
{
"vote": 2,
"title": "Determining subject company (listed stocks) referred to in many short text samples",
"text": "I have a large amount of unlabeled data samples (under 100 words in each sample) where mostly each sample refers to a specific company - which I need to determine; The company can be referred to by stock symbol or name.\n\n\nMy best attempt so far involves parsing out stock symbols using regular expressions, searching a database, and calculating relative Levenshtein distances of the search results vs sample text to make a best guess; this is a bit over 60% accurate - in ideal cases.\n\n\nI have two main issues that I can see:\n\n\n\n\nI am getting some false positives in cases where the symbol actually matches, but it's the wrong company (maybe on a different exchange).\n\n\nIn cases where no stock symbol is specified (just a company name), I am getting no results, as I don't currently handle just company names.\n\n\n\n\nFor issue 1 - The False Positives\n\n\nThe database I am searching against also contains company descriptions or titles for each search result. What would be the best way of comparing the company descriptions of each search result with the sample text to get a more accurate guess? I am thinking some sort of keyword comparison here would help - I know that factoring in the context of the sample text is critical here.\n\n\nFor issue 2 - No Stock Symbol\n\n\nI think the best candidate for this case would be to leverage token classification to find \"ORG\" entities; I have tried this with a few pre-trained models from HuggingFace, but haven't had great results. Can anyone recommend a model that is pre-trained on financial data - or would even just work well for recognizing company names? \nIn addition to this\n, would anyone know of a good dataset or strategy for further training the model for this purpose?\n\n\nIf anyone has an alternate suggestion for issue 2, I would also be open to that. Note that I am relatively new to machine learning, but I do understand the basics of how transformer models work, how to use them, and the different types of classification problems.",
"date": "2021-12-10"
},
{
"vote": 6,
"title": "5 Text Decoding Techniques that every â€œNLP Enthusiastâ€ Must Know",
"text": null,
"date": "2021-12-10"
},
{
"vote": 14,
"title": "Increasing the Accuracy of Textual Data Analysis on a Corpus of 2 Billion Words",
"text": "https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/\n\n\nAt Soroco, we ingest between 200 million and 2 billion words over the course of model training and analysis for a single team of workers using our Scout product. In this blog post, I talk about some tips and tricks that we might use to increase the accuracy of our models, including appropriate processing of text for the purpose of leveraging standard techniques from machine learning. I then demonstrate this by showing how to represent text in a high-dimensional vector space with applications to a toy regression problem.",
"date": "2021-12-10"
},
{
"vote": 28,
"title": "The Toxicity Dataset â€” building the world's largest free dataset of online toxicity",
"text": null,
"date": "2021-12-09"
},
{
"vote": 5,
"title": "Tips about building a chatbot with GPT-3 or GPT-J",
"text": "Hello!\n\n\nI realize I have more and more questions from people trying to leverage GPT-3 or GPT-J for their next chatbot. And usually questions are always about 2 things:\n\n\n\n\nHow to format my requests so the model understands that I am in conversational mode?\n\n\nHow can the model keep an history of my conversation?\n\n\n\n\nI'm answering these 2 points in this quick article: \nhttps://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html\n\n\nI hope it will help!\n\n\nI any question please don't hesitate to ask.",
"date": "2021-12-09"
},
{
"vote": 4,
"title": "Best way to vectorize names of medical conditions/diseases?",
"text": "Let's suppose the aim is to predict, let's say, hospital charges incurred (there are other predictor parameters too). I have thought of the following ways of vectorization so far-\n\n\n\n\nI don't think using word2vec makes a lot of sense because the similarity of words is meaningless here?\n\n\nFind a huge medical corpus online and make a count vectorizer matrix for each row of medical condition. But that would mean the matrix is too sparse.\n\n\nUse only the medical conditions in the dataset as corpus and make a count vectorizer matrix from them?\n\n\nPick only the top few select hundred words and use them as a corpus\n\n\n\n\n&#x200B;\n\n\nIf there's any other way you can think of, do let me know. I accept I don't know much about NLP.",
"date": "2021-12-09"
},
{
"vote": 2,
"title": "open source sentence rephrasing",
"text": "I have numerical data. I can come up with basic sentence (eg you credit score is good). i want to make this response seem natural and  not bot like. ie the response varies everytime, doesnt change the meaning and sounds human. what is the best technology available ?  is NLP cloud's paraphrasing a good fit or are there similar/better services ?",
"date": "2021-12-09"
},
{
"vote": 2,
"title": "A Visual Guide to Prompt Engineering [With GPT language models]",
"text": null,
"date": "2021-12-08"
},
{
"vote": 1,
"title": "Multiple SEP tokens for keyword searches.",
"text": "I am trying to train a siamese-net style BERT based retrieval model that supports both semantic queries and keyword queries for my domain. The keywords can be of different categories (for example, some are related to product specifications, some others to usability features, manufacturers etc.)\n\n\nTo index a document, I want to use SEP tokens to separate the different categories of keywords and the product description. Eg:\n[CLS] <Product description> [SEP] <keywords type_1> [SEP] <Keywords type_2> [SEP]...\n\n\nThe query can be a product description or a keywords or a combination of the two.\n\n\nMy training dataset size is 100K samples. \n\n\nHas anyone used an approach like this? Has it worked?",
"date": "2021-12-07"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-07"
},
{
"vote": 19,
"title": "[Research paper] Hierarchical Topic Modelling Over Time",
"text": "Hello Reddit,\n\n\nI am proud to present you HTMOT for Hierarchical Topic Modelling Over Time. This paper proposes a novel topic model able to extract topic hierarchies while also modelling their temporality. Modelling time provide more precise topics by separating lexically close but temporally distinct topics while modelling hierarchy provides a more detailed view of the content of a document corpus.\n\n\nhttps://arxiv.org/abs/2112.03104\n\n\nThe code is easily accessible on GitHub and a working interface provides the ability to navigate through the resulting topic tree with ease: \nhttps://github.com/JudicaelPoumay/HTMOT",
"date": "2021-12-07"
},
{
"vote": 1,
"title": "Language education architecture",
"text": "Hi all,\n\n\nI'm relatively new to the language domain - designing services that support language education.  What's the best practice for associating metadata to words and sentences?  This would include audio, video, pronunciation, and other words/sentences considered related.\n\n\nI've been reading up on various NLP corpus functionality - which seems lower level (i.e. either just the text, or some structure that is pretty specific)  Even multi-modal corpus doesn't seem to cover everything.  Am I getting the correct sense here?\n\n\nI've seen references to Lexical Resources - which seems like the right direction, but I don't see any dominant libraries for that (I'm a python guy.)\n\n\nIt seems somewhat straight forward to have a persistent lookup, especially if I assign an index key to all the words and sentences that I can then base the metadata on.  But I don't want to reinvent a wheel unnecessarily.",
"date": "2021-12-06"
},
{
"vote": 10,
"title": "Is there an open-source way to replicate entity-level sentiment from Google's Cloud Natural Language API?",
"text": "I'm learning about NLP and was really impressed with Google's Natural Language API (\ndemo\n). It seems that entity-level sentiment analysis is the future of NLP. Has anyone in the community come across open-source libraries that replicate the API for learning purposes? I found an excellent \nrepo\n called ABSA-PyTorch but it seems that all the implementations are classification-based; that is, they return \"positive/negative\" rather than a spectrum between positive and negative. Is there a sub field of Aspect-Based Sentiment Analysis (ABSA) that isn't classification based? I wasn't able to find any keywords despite hours of Google searching.",
"date": "2021-12-06"
},
{
"vote": 2,
"title": "Need help with clustering keywords",
"text": "I have a set of keywords, and can extract similar keywords using word2vec model (with cosine similarity scores) or can calculate similarity scores from BERT model.. I need to cluster the keywords which would be semantically similar. Any help with the type of cluster would be appreciated. Just need a discussion before I try to implement.",
"date": "2021-12-06"
},
{
"vote": 0,
"title": "Megdap - closed captioning services",
"text": "[removed]",
"date": "2021-12-06"
},
{
"vote": 0,
"title": "Megdap - video to text transcription",
"text": "[removed]",
"date": "2021-12-06"
},
{
"vote": 0,
"title": "Megdap - website localization services",
"text": "[removed]",
"date": "2021-12-06"
},
{
"vote": 0,
"title": "Megdap - language translation services",
"text": "[removed]",
"date": "2021-12-06"
},
{
"vote": 5,
"title": "How to use Textblob for semantic analysis?",
"text": "I'm using Textblob to identify if a paragraph text is positive or negative. I'm new to Textblob, for my data I cleaned the data (remove stop word , extend word , punctuations..etc) tokenized the text into sentences then into words then performed lemmatization then applied Textblob to lemmatize data.\n\n\nI read that Textblob do all of these as well as pos tag when calling TextBlob() I was wondering do I need all the steps that I performed before or will calling Textblob be enough?",
"date": "2021-12-05"
},
{
"vote": 5,
"title": "Reproducing WebNLG Challenge 2017 on OpenNMT-py",
"text": "Hi guys, I'm Data Science student and i'm learning to use OpenNMT-py for my master degree thesis. \n\n\nI reproduced the challenge with the old deprecated repository, now I would like to replicate it with the updated repository (as I will need it for a similar task within my thesis).\n\n\nI am now approaching the NLP field, but I am not very clear about some things:\n\n\n\n\nsince it is not a translation task, is it necessary to build a vocabulary like in the machine translation OpenNMT-py tutorial?\n\n\nThe epochs command I noticed has been deprecated, now it works with train_steps, however I am not clear about the \"conversion\", so to speak. With the old repository the number of epochs to train the model with was 13. I tried this by  looking at old problems from these repositories: default train_steps (100000) / deault batch_size (64) * 13 (epochs number of the old repository) = 20313.\n\n\n\n\nis this reasoning correct? Thanks everyone for your attention.",
"date": "2021-12-05"
},
{
"vote": 3,
"title": "What is the difference between Rule-Based &amp; Feature-Based methods in sentiment analysis?",
"text": "I use Textblob to get lable value of texts (positive text or negative) and then used logistics regression for training and prediction , Is this feature methods or rule based method?",
"date": "2021-12-05"
},
{
"vote": 12,
"title": "What are the leading knowledge evaluation models?",
"text": "I'm new to NLP and ML. I've been playing with GPTJ and other stuff provided by huggingface. I'm also playing with compromise and nltk. I have some ideas I want to try with regards to knowledge extraction from multiple sources.\n\n\nOne problem I imagine is, what are the preferred ways to evaluate the truthiness of a statement? I see that T0PP can extract information from within a contained context, but what about from the unbounded context of reality?\n\n\nIf anyone can help me out with clues or ideas that would be awesome! Thanks gang.",
"date": "2021-12-04"
},
{
"vote": 3,
"title": "What is topic modeling and how can it help with sentiment analysis?",
"text": "If I apply it to my data will it change the outcome of my sentiment analysis?",
"date": "2021-12-03"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-03"
},
{
"vote": 4,
"title": "Need ideas for a story generator",
"text": "Hi, I am working on a story generator and rather than fine tuning a pre-trained model, I need more ideas to make it interesting, like maybe how could i make it so it generate the beginning, body and end of the story. Share your thoughts pls thx",
"date": "2021-12-03"
},
{
"vote": 3,
"title": "Sentiment Analysis API vs Custom Text Classification: Which one to choose? - KDnuggets",
"text": null,
"date": "2021-12-03"
},
{
"vote": 6,
"title": "Low rent automated essay scoring",
"text": "I am building an online elementary history course and Iâ€™d like to ask students to write a paragraph on an inquiry question.  E.g. How did the seven years war help cause the revolutionary war?  Unfortunately I donâ€™t have human graders, or a dataset of graded responses or an NLP/ML programmer for that matter.  Iâ€™m thinking I could just count the number of sentences and number of key phrases the student mentions for low rent automated essay scoring.  It might be labor intensive to come up with variation of the keywords.  Does anyone know of open source or commercial solutions like this that work well?  Goal is to give the student enough feedback/scaffolding so that they feel like it is worth writing down their thoughts.",
"date": "2021-12-03"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-12-01"
},
{
"vote": 7,
"title": "How to highlight intonation, word stress in a text?",
"text": "I'm very new to NLP.\n\n\nWhen I give a text file, I want the output to be highlighted with colors to indicate the amount of stress each word should have.\n\n\nUsing spacy it's possible to highlight Parts of speech. I also searched on this sub but couldn't find anything related to word stress.\n\n\nGoogle search provided this result. \nhttps://stackoverflow.com/questions/58251398/how-to-detect-sentence-stress-by-python-nlp-packages-spacy-or-nltk\n\n\nI have the same question too and the answers on SO are focused on speech but not on text.",
"date": "2021-12-01"
},
{
"vote": 3,
"title": "What is the difference between text classification and semantic analysis?",
"text": null,
"date": "2021-12-01"
},
{
"vote": 2,
"title": "NLP with Arabic language",
"text": "Hi, I am new to arabic related nlp and don't have the sense of the actual language. But as par my understanding, there are vowels named 'tashkeel' and 'harakat'. I just don't understand why we would need to remove the vowels ( strip_tashkeel and strip_harakat functions in pyarabic) from Arabic text before processing it further? also I am not getting any good answer regarding it.\n\n\nTIA for your help.",
"date": "2021-11-29"
},
{
"vote": 13,
"title": "Best available pronoun coreference resolution systems?",
"text": "I wanna study singular they and what its status is in current NLP research. Not looking for answers on here, just for pointers.\n\n\n\n\nWhat are the best systems for pronoun coreference resolution?\n\n\n\n\nHave you come across something related to singular they in NLP? For example in pronounc coreference resolution?\n\n\n\n\n\n\nCurious to hear what you know!",
"date": "2021-11-29"
},
{
"vote": 2,
"title": "Looking for NLP cloud-based technologies expert/consultant",
"text": "Hello everyone, \n\n\nMy team is currently looking for experts/consultants in cloud-based NLP/Text Mining technologies.\n\n\nWe are developing a platform that aggregates the best AI engines on the market but some of our prospects want to be supported by specialists in their projects. We are therefore looking for experts for this step of personalized audit (paid) before using our platform. \n\n\nIf you are interested, please send me a message or an email: \ncontact@edenai.co\n! \n\n\nThank you, \n\n\nTaha",
"date": "2021-11-29"
},
{
"vote": 3,
"title": "Separation of train and test sets in vocab creation",
"text": "[deleted]",
"date": "2021-11-28"
},
{
"vote": 7,
"title": "Query Intent Classification in chatbots using distilled transformers",
"text": "Hi\n\n\nI am writing a paper about Query Intent Classification in chatbots and would like to also have a section about distilled Transformers (fx distilBERT) , but have been unsuccessful in finding papers or\n chatbot companies that use such models.\n\n\nAre  distilled transformers simply not used for chatbots? - it seems like a good trade off in terms of use of resources and general performance, like precision.\n\n\nAre there any good papers or company blogs about deploying Distilled Transformer Models in Chabot settings?",
"date": "2021-11-28"
},
{
"vote": 1,
"title": "nlp project",
"text": "Hello everyone,I want to know how to learn nlp. could you please give me some advices ? in fact, these days I learned some basic model like cnn, rnn, lstm, transformer, belt. but I don't know how to imply these knowledge into project, in other words, could you please recommend me some interesting projects for me? thank you so much!",
"date": "2021-11-27"
},
{
"vote": 10,
"title": "What should I visualize for humor detection model to gain some useful insight?",
"text": "I was going through bunch (\n1\n,\n2\n,\n3\n) of humor detection paper. But most papers don't include any visualizations, say some graph related to model being trained. I was thinking to train some language models like BERT, GPT, XLNet. But was guessing what kind of some interesting visualization should I aim for in order to gather the data during training and gain some sort of insight.  \n\n\nOr is it like that these fine-tuning or zero/one/few shot learning based models don't have to train for long and does not involve significant learning \"from scratch\" or they are somewhat black boxes, that's why there is nothing much to visualize?",
"date": "2021-11-26"
},
{
"vote": 1,
"title": "The Reddit Ethereum Dataset - a fresh corpus of all the Ethereum-related posts and comments on Reddit",
"text": null,
"date": "2021-11-25"
},
{
"vote": 1,
"title": "Creating numeric word representation of input sentences resulting in MemoryError",
"text": "I am trying to use \nCountVectorizer\n to obtain word numerical word representation of data which is essentialy list of 160000 English sentences:\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf_train = pd.read_csv(&#039;data/train.csv&#039;)\n\nvectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r&#039;\\b\\w+\\b&#039;, min_df=1)\nX = vectorizer.fit_transform(list(df_train.text))\n\n\n\nThen printing \nX\n:\n\n\n&gt;&gt;&gt; X\n&lt;160000x693699 sparse matrix of type &#039;&lt;class &#039;numpy.int64&#039;&gt;&#039;\nwith 3721191 stored elements in Compressed Sparse Row format&gt;\n\n\n\nBut converting the whole to array to get the numerical word representation of all data gives:\n\n\n&gt;&gt;&gt; X.toarray()\n---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n~\\AppData\\Local\\Temp/ipykernel_11636/854451212.py in &lt;module&gt;\n----&gt; 1 X.toarray()\n\nc:\\users\\crrma\\.virtualenvs\\humor-detection-2-8vpiokuk\\lib\\site-packages\\scipy\\sparse\\compressed.py in toarray(self, order, out)\n   1037         if out is None and order is None:\n   1038             order = self._swap(&#039;cf&#039;)[0]\n-&gt; 1039         out = self._process_toarray_args(order, out)\n   1040         if not (out.flags.c_contiguous or out.flags.f_contiguous):\n   1041             raise ValueError(&#039;Output array must be C or F contiguous&#039;)\n\nc:\\users\\crrma\\.virtualenvs\\humor-detection-2-8vpiokuk\\lib\\site-packages\\scipy\\sparse\\base.py in _process_toarray_args(self, order, out)\n   1200             return out\n   1201         else:\n-&gt; 1202             return np.zeros(self.shape, dtype=self.dtype, order=order)\n   1203 \n   1204 \n\nMemoryError: Unable to allocate 827. GiB for an array with shape (160000, 693699) and data type int64\n\n\n\nFor the example in the linked schikit learn \ndoc page\n, they have used only five sentences. Thus, for them \nX.toarray()\n seem to have returned the array of numerical word representation. But since my dataset contains 160000 sentences, (in error message) it seems that it is resulting in vocabulary of size 693699 (which contains both unique unigrams and bigrams, due to \nngram_range\n parameter passed to \nCountVectorizer\n) and hence facing insufficient memory issue.\n\n\nQ1.\n How can I fix this? I am thinking to simply reject \nX\n and separately transform in mini batches as shown below. Is this correct?\n\n\n&gt;&gt;&gt; X_batch = list(df_train[:10].text)  # do this for 160000 / batch_size batches\n&gt;&gt;&gt; X_batch_encoding = vectorizer.transform(X_batch).toarray()\n&gt;&gt;&gt; X_batch_encoding\narray([[0, 0, 0, ..., 0, 0, 0],\n   [0, 0, 0, ..., 0, 0, 0],\n   [0, 0, 0, ..., 0, 0, 0],\n   ...,\n   [0, 0, 0, ..., 0, 0, 0],\n   [0, 0, 0, ..., 0, 0, 0],\n   [0, 0, 0, ..., 0, 0, 0]], dtype=int64)\n\n&gt;&gt;&gt; X_batch_encoding[0].shape\n(693699,)\n\n\n\nQ2.\n I am thinking to train neural network and decision tree on this encoding for humor detection. But I guess it wont be great idea to have 693699 length vector to represent single sentence. Right? If yes, what should I do instead? Should I opt to use only unigrams while fitting \nCountVectorizer\n (but it will not capture even minimal context of words, unlike bigrams) ?\n\n\nPS: I am creating baseline for humor detection, I am required to use \nCountVectorizer\n.",
"date": "2021-11-25"
},
{
"vote": 1,
"title": "Text summarization",
"text": "Hello,\nIs there any real world text summarization project example ?\nTy",
"date": "2021-11-25"
},
{
"vote": 11,
"title": "How important it is to give sentences to BERT tokenizer rather than the whole text?",
"text": "I'm currently working on document classification, where every document has many sentences within. I intend to use BERT sequence classifier for the task, however as I check out the tokenization results of BERT, I saw that the special token [SEP] is only added at the end of the document, rather than replacing every period in the text - as they are my end of sentence marks. However, I saw that Bert gave \".\" punctuations a specific ID, which means it has some meaning to BERT already.\n\n\nMy question is, should I go ahead and have only [SEP] at the end of the document and hope that the ID corresponding to the punctuation marks can distinguish the sentence-level information, or should I re-do my tokenization while I give the texts sentence by sentence, and then merge the id's into a single vector later? There must be a better way though. I believe knowing where a sentence begins and ends is important for the classification task, so I'm open to suggestions.",
"date": "2021-11-25"
},
{
"vote": 3,
"title": "Neural edit-tree lemmatization for spaCy",
"text": null,
"date": "2021-11-24"
},
{
"vote": 1,
"title": "No data no problem, unsupervised learning and sentence transformers",
"text": "Hi all, I put together \nan article and video covering TSDAE fine-tuning\n for sentence transformer models. Basically, how we can use plain unstructured text data to fine-tune a sentence transformer (not quite \nno\n data, but close!). From the TSDAE paper, you actually only need something like 10-100K sentences to fine-tune a pretrained transformer for producing pretty good sentence embeddings.\n\n\nI was achieving same STSb evaluation with a TSDAE train BERT as I was getting with my own NLI (labeled) dataset trained BERT (using softmax loss). So pretty cool imo - although in reality supervised methods produce better performing models, if you have no labeled data, unsupervised is the way to go.\n\n\nIt was really cool learning about this, planning to do more on unsupervised sentence transformers in the future - let me know what you think!",
"date": "2021-11-24"
},
{
"vote": 3,
"title": "Evaluating quality of synthetically generated questions dataset",
"text": "Hi  all, I have an NLP related question for you! I have synthetically  generated questions in a SQuAD like format (context, question, answer triplets). The data consists of domain specific questions in Dutch as the questions are generated from 1500+ Dutch technical manuals.\n\n\nHow can I evaluate the quality of this dataset and therefore the quality of my question generator?\n\n\nMany, many thanks in advance!",
"date": "2021-11-24"
},
{
"vote": 5,
"title": "Best Way to Identify if a Social Post is Written by a Doctor vs Patient?",
"text": "Hello fellow NLP nerds :) \n\n\nI had a tricky question that I'd love to crowdsource some solutions for.\n\n\nProblem: I'm trying to clean out all the social posts written by doctors vs patients. I've already started to separate based on typical identifiers such as \"as a patient\" vs \"being a doctor\" or \"my patient\" vs \"my doctor\" and \"patient here\" vs \"I treated patient\". The issue is that this process of coming up with ways a patient self identifies compared to a doctor is extremely manual on the upfront. I wanted to check if the community knew of any libraries, previous code or other research that could help speed things up? \n\n\nAny and all ideas, thoughts and suggestions are more than welcomed. \n\n\nAlways all the best, \n\n\nNE",
"date": "2021-11-23"
},
{
"vote": 2,
"title": "Summaries readability improvement",
"text": "I'm doing my research with multi-document summarization for domain-specific texts. We want to show summaries that we generate using our approach (and state-of-the-arts) to domain experts for readability evaluation. Summaries that we generate are pretty good, but hard to read for real people.\n\n\nCould you recommend some python libraries for automatic improvement of readability (capitalization, punctuation, finding orthographic mistakes, etc.)",
"date": "2021-11-23"
},
{
"vote": 24,
"title": "NLP thesis ideas?",
"text": "I am currently doing a postgraduate Computer Science conversion course in the UK and did English Language and Linguistics for my undergrad. I know that I want to combine both fields for my postgrad thesis but I donâ€™t know anything about NLP. I know that there are tons of material out there but I donâ€™t know where to begin. Posting here in the hopes that someone could guide me to some places for NLP or even just give me some ideas for possible avenues to follow for my thesis. Any suggestions would be greatly appreciated",
"date": "2021-11-23"
},
{
"vote": 3,
"title": "Classifying documents in categories using keyword sets, without ML",
"text": "Hi\n\n\nI am trying to classify documents in categories for which I have lists of keywords.\nIdeally the solution should not use machine learning.\n\n\nI was thinking of creating vectors of both the document and the keywords for each category, and consequently calculating cosine similarity in order to see which category has the highest match.\n\n\nHowever, as cosine similarity is aimed at comparing 2 documents rather than 1 document and a list of keywords, I was wondering if this was the ideal solution.\n\n\nAny feedback on either oh this would be highly appreciated. Whether it is optimising the cosine sim, a different approach or proposing ML anyway,... all feedback is welcomed :).\n\n\nThanks in advance.",
"date": "2021-11-23"
},
{
"vote": 1,
"title": "Add-K smoothing",
"text": null,
"date": "2021-11-22"
},
{
"vote": 16,
"title": "[project-showcase] - zeroshot_topics: Label your text data automatically!",
"text": "zeroshot_topics: \nGithub link\n\n\nHand-labelled training sets are expensive and time-consuming to create usually. Some datasets call for domain expertise (eg: medical/finance datasets etc). Given these factors around costs and inflexibility of hand-labelling, it would be nice if there are tools that can help us get started quickly with a minimal labelled dataset - enter weak supervision.\n\n\nBut what if you do not have any labelled data at all? is there a way to still label your data automatically in some way?\n That's where \nzeroshot_topics\n might be useful! to help you to be up and running quickly.\n\n\nzeroshot_topics\n lets you do exactly that! it leverages the power of zero-shot-classifiers, transformers &  knowledge graphs to automatically suggest labels/topics from your text data. all you need to do is point it towards your data.\n\n\n&#x200B;\n\n\nplease check this out and share your feedback.",
"date": "2021-11-22"
},
{
"vote": 1,
"title": "ZeroShotBot - Zero Code, Zero Training chatbot - the future of AI?!",
"text": "[removed]",
"date": "2021-11-22"
},
{
"vote": 1,
"title": "Looking for examples of conversational chatbot companies with recorded demos.",
"text": "I'm doing some research into currently successful chatbot companies (both voice and text) and am looking for chatbot companies that have their chatbots in recorded demos working fully.\n\n\nOne I found is \nBrooke.ai\n which works in the car dealership appointment setting industry as a customer service bot for inbound calls and their demo can be found \nhere\n and also on \nyoutube\n.\n\n\nI tried looking for other companies that have demos as well but couldn't find many that have pre-recorded demos of their ai chatbot product working fully, or are mostly sequence-based chatbot building software such as ManyChat or ChatFuel.\n\n\nCould you guys recommend chatbot companies that have ai with intent recognition software like Google Duplex or Amazon Lex? If they have demo recordings I can listen to or view that would be great. Looking for companies that have been successful in implementing AI in chatbots.",
"date": "2021-11-22"
},
{
"vote": 18,
"title": "[Advice] What are some ways to engage with Academia without a Phd?",
"text": "Hi - a big fan of this subreddit!\n\n\nI am an applied NLP researcher in the industry with a masters. I have a PhD offer but I am in double minds about doing a PhD - because of publishing culture/duration of a phd in the US etc. However, I very much enjoy keeping up with the recently published work and seeing how they can be tweaked and applied to real-world scenarios. \n\n\nWhat are some ways in which I can continue engaging with the academic community without doing a phd? I some how feel that one is not valued as much without a phd (even with equivalent industry experience) so wanted to get opinion from either sides. \n\n\nThanks!",
"date": "2021-11-21"
},
{
"vote": 1,
"title": "GUI app for text processing?",
"text": "Iâ€™m picturing a desktop application where you can highlight some text and say a command like, â€œtokenize these wordsâ€, and the list containing various text elements makes each of the highlighted words a unique element of the list. Or you can highlight a region and say â€œsegment this textâ€, and it does the same but for sentences.\n\n\nIs there any way to do this?",
"date": "2021-11-21"
},
{
"vote": 3,
"title": "Better segmentation than NLTK.sent_tokenize()",
"text": "I am segmenting text in Juno, a Jupyter notebook iOS app. \n\n\nThey donâ€™t support Spacy at the moment.\n\n\nNLTK.sent_tokenize does not segment sentences perfectly, for me.\n\n\nI am thinking my only choice is to write a custom segmentation rule, unless anybody knows a different library with a high quality, AI-intelligent segmenter that can comprehend where the boundaries of sentences are, even if the text is not perfectly formatted.\n\n\nThanks!",
"date": "2021-11-21"
},
{
"vote": 2,
"title": "Which method/model to opt for while identifying semantic similarity?",
"text": "I have a text classification dataset of registered issues. Now within each category of issues there are specific issues that show similar pattern. How can I identify those sub categories within the categories. I dont have any means to manually categories each sub category (rather its impractical). All I understand is that this problem falls under unsupervised learning.\n\n\nI have already performed the text classification using BERT and it works well enough.",
"date": "2021-11-20"
},
{
"vote": 15,
"title": "Job offer advice for new grad interested in NLP",
"text": "Hi everyone.\n\n\nI am a new grad trying to pick my first software engineering job.Â As an undergrad, I had a strong interest in NLP and did research in that area, and I would like to continue working in NLP.Â I am deciding between two job offers.\n\n\nAt the first company, I would be working in NLU/NLG teams for the company's voice assistant technologies. They also publish often, which sounds nice to me since I might consider grad school in the long term. However, they are offering me a systems role, so I willÂ mostly be working on ML infrastructure (C++) without manipulating their models or doing core ML engineering.\n\n\nAt the second company, I am hired as an ML engineer, but I will be working on ranking. The tech stack is mostly Python. The downside is that I wonâ€™t be doing any NLP work.\n\n\nIf I want to have a career in NLP, would accepting the first company be better, even if I am not working directly on the models as an ML engineer? Or would it be most important to have the title of \"ML engineer\" even if I am working on a different problem area?",
"date": "2021-11-20"
},
{
"vote": 3,
"title": "Make a bot based on social media chat data",
"text": "I have very long chats with my friends like one with 4 years of constant msging and then two years of medium level messaging. I also have some groups in which i have heavily participated for the last 6 years. Can i make a chatbot type thing which chats like me? Is there anyone who has already worked on it?",
"date": "2021-11-20"
},
{
"vote": 4,
"title": "I want to spy on myself. My digital and physical stuff. I want to index and map the clutter comprehensively. Topic collection bags? In NLP what do you call this? Anyone done this before? There should be some generic recipe in some Python cookbook out there.",
"text": "Rather than keep rummaging through the clutter I create while working, I want to look for labels. So let's say I have a detailed inventory of my shit, and whatever is on my file system, online accounts, email, etc.\n\n\n\n\nWhat NLP recipes should I follow to build metadata and generate labels for me? \nWhat should I learn?\n\n\nHow do you visualize all of this? \nWhat should I learn?",
"date": "2021-11-19"
},
{
"vote": 2,
"title": "SemEval-2022 Task 09: R2VQ - Competence-based Multimodal Question Answering",
"text": "FIRST CALL FOR PARTICIPATION\n\n\nWe invite you to participate in the SemEval-2022 Task 9: Competence-based Multimodal Question Answering (R2VQ).\n\n\nThe task is being held as part of SemEval-2022, and all participating team will be able to publish their system description paper in the proceedings published by ACL.\n\n\nCodalab (Data download): \nhttps://competitions.codalab.org/competitions/34056\n\n\n&#x200B;\n\n\nMotivation \n\n\n================================================\n\n\nWhen we apply our existing knowledge to new situations, we demonstrate a kind\n\n\nof understanding of how the knowledge (through tasks) is applied. When viewed\n\n\nover a conceptual domain, this constitutes a competence. Competence-based\n\n\nevaluations  can be  seen as a new approach for designing NLP challenges, in\n\n\norder to better characterize the underlying operational knowledge that a\n\n\nsystem has for a  conceptual domain, rather than focusing on individual tasks.\n\n\nIn this shared task, we present a challenge that is reflective of linguistic\n\n\nand cognitive competencies that humans have when speaking and reasoning.\n\n\n&#x200B;\n\n\nTask Overview \n\n\n================================================\n\n\nGiven the intuition that textual and visual information mutually inform each\n\n\nother for semantic reasoning, we formulate the  challenge as a competence-\n\n\nbased question answering (QA) task, designed to involve rich semantic\n\n\nannotation and aligned text-video objects. The task is structured as question\n\n\nanswering pairs, querying how well a system understands the semantics of\n\n\nrecipes.\n\n\nWe adopt the concept of \"question families\" as outlined in the CLEVR dataset\n\n\n(Johnson et al., 2017). While some question families naturally transfer over\n\n\nfrom the VQA domain (e.g., integer comparison, counting), other concepts such\n\n\nas ellipsis and object lifespan must be employed to cover the full extent of\n\n\ncompetency within procedural texts. \n\n\n&#x200B;\n\n\nData Content\n\n\n================================================ \n\n\nWe have built the R2VQ (Recipe Reading and Video Question Answering) dataset, a dataset consisting of a collection of recipes sourced from \nhttps://recipes.fandom.com/wiki/Recipes_Wiki\n and \nfoodista.com\n, and labeled according to three distinct annotation layers: (i) Cooking Role Labeling (CRL), (ii) Semantic Role Labeling (SRL), and (iii) aligned image frames taken from creative commons cooking videos downloaded from YouTube. It consists of 1,000 recipes, with 800 to be used as training, and 100 recipes each for validation and testing. Participating systems will be exposed to the aforementioned multimodal training set, and will be asked to provide answers to unseen queries exploiting (i) visual and textual information jointly, or (ii) textual information only. \n\n\n&#x200B;\n\n\nTask Website and Codalab Submission site: \nhttps://competitions.codalab.org/competitions/34056\n\n\nMailing List: \nsemeval-2022-task9@googlegroups.com\n\n\n&#x200B;\n\n\nImportant Dates\n\n\n================================================ \n\n\nTraining data available: October 15, 2021\n\n\nValidation data available: December 3, 2021\n\n\nEvaluation data ready: December 3, 2021\n\n\nEvaluation start: January 10, 2021\n\n\nEvaluation end: January 31, 2022\n\n\nSystem Description Paper submissions due: February 23, 2022\n\n\nNotification to authors: March 31, 2022\n\n\n&#x200B;\n\n\nOrganization \n\n\n================================================ \n\n\nJames Pustejovsky, Brandeis University, \njamesp@brandeis.edu\n\n\nJingxuan Tu, Brandeis University, \njxtu@brandeis.edu\n\n\nMarco Maru,Â Sapienza University of Rome, \nmaru@di.uniroma1.it\n\n\nSimone Conia,Â Sapienza University of Rome, \nconia@di.uniroma1.it\n\n\nRoberto Navigli,Â Sapienza University of Rome, \nnavigli@diag.uniroma1.it\n\n\nKyeongmin Rim, Brandeis University, \nkrim@brandeis.edu\n\n\nKelley Lynch, Brandeis University, \nkmlynch@brandeis.edu\n\n\nRichard Brutti, Brandeis University,Â \nrichardbrutti@brandeis.edu\n\n\nEben Holderness, Brandeis University, \negh@brandeis.edu",
"date": "2021-11-19"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-11-19"
},
{
"vote": 2,
"title": "Any RASA users out there? Setting variables based on response text chosen",
"text": "I am creating a chatbot but the use case is a little bizarre. More specifically, the chatbot will be playing the role of someone asking questions, and I will be acting as the customer service representative. So for example, I will type â€œHow can I help you?â€, and the chatbot will respond with something like \"What are the meal options on flight ABC?\"I already have a massive list of potential questions the chatbot could ask.\n\n\nSo as you might expect, there will be many text options for when I type â€œHow can I help youâ€â€¦from can I get a flight from A to B on day X to what airports are available in state Y? From what I understand, if there are many text options under something like utter_help_me under responses in domain, a text will be chosen randomly. But I want variables to be set based on what text is randomly chosen. Is there a way to do that? I know this is usually done based on what is typed and not what is returned in a response, so this is the strange part",
"date": "2021-11-19"
},
{
"vote": 1,
"title": "Moment of glory is temporary. It is not forever.",
"text": "[removed]",
"date": "2021-11-17"
},
{
"vote": 4,
"title": "Recommendations for pre-trained word embedding models.",
"text": "Does anyone have any (preferably) free dataset recommendations outside of Googleâ€™s news vector model (too specific/scientific at times) and the Common Crawlâ€™s model (too many typos) for generating similarity scores?\n\n\nGensim, KeyedVectors, usually .bin or .vec files as input I believe.",
"date": "2021-11-17"
},
{
"vote": 9,
"title": "Do you think NLP will be able to comprehend linguistic typology?",
"text": "The idea behind linguistic typology is that there are patterns common to all languages. These patterns repeat themselves at different levels. They are also specific to individual languages.\n\n\nLinguistic classification organizes languages based on structural features, patterns, and linguistic units. It offers a systematic way of grouping languages to discover linguistic properties shared by these languages.\n\n\nSince linguistic classification involves collecting and analyzing data from various sources (fieldwork, literature, language documentation, linguistic atlas, etc.), could something like GPT-3 be able to comprehend it?\n\n\nIâ€™m referring mainly to the translation and localization field.\n\n\nAlgorithms are currently unable to grasp the context and nuances of a text. This means that we still need human translation to interpret cultural references and preserve the style and intention of the original text.\n\n\nHow long do you think it will take for AI to surpass a human translator?\n\n\nMy question is based on \nthis article\n that goes over linguistic typology and why it makes human translators indispensable in localization processes.",
"date": "2021-11-16"
},
{
"vote": 2,
"title": "GPT-J through API + training on custom datasets",
"text": "Anybody checked out Eleuterâ€™s new GPT-J yet?\nI feel like itâ€™s on par with OpenAIâ€™s Curie. It pretty good overall for inference, but I thought it would be cool to fine-tune it on a custom dataset.\n\n\nI personally found it hard to do because of the lack of resources out there, so I ended up putting together this project to simplify custom training of GPT-J and deployment to production after the training. Both can be done through a web interface I built. Also, I added a default pre-trained GPT-J to use through an interface or API too.\nPlease, check it out and give me feedback if you can! \nhttps://www.tensorbox.ai/",
"date": "2021-11-15"
},
{
"vote": 2,
"title": "Why language pars are used the most in the evaluation of machine translation models and why?",
"text": "The title should be \"what language pairs\" instead of \"why language pars\".\n\n\nIn my experience I see English-German and English-Romanian very frequently. Not sure why that is the case.",
"date": "2021-11-15"
},
{
"vote": 9,
"title": "Normalizing Named Entities",
"text": "Machine-Guided Polymer Knowledge Extraction Using Natural Language Processing: The Example of Named Entity Normalization\n\n\nThis paper talks about Supervised Clustering methods for Named Entity Normalization (NEN), a sometimes overlooked but very important area of Information Extraction. We cluster the variations of name with which chemical entities are referred to in literature. We establish the advantage of fastText embeddings over Word2Vec embeddings and show that parameterized cosine distance as well as ensembling of models lead to performance gains for NEN. This is also one of the few works to study normalization for named entities for a niche domain, i.e., polymers. This dataset is one of the biggest out there for normalization and presents unique challenges not present in general English text as the cluster sizes are much larger and cluster size variance is greater than typical synonym clusters.\n\n\nThe code and data for this paper are available \nhere\n. Consider using this data set for bench marking and evaluation purposes if you are working in this area.",
"date": "2021-11-15"
},
{
"vote": 1,
"title": "14 Year Old Builds VC-Backed GPT-3 Powered Search Engine Tool",
"text": "[removed]",
"date": "2021-11-14"
},
{
"vote": 4,
"title": "Question about statistics and algebra for NLP",
"text": "I'm a journalist and freelance translator and I worked in the banking system in my country for many years. A couple of years ago I decided I wanted to get into data science, took a few practical courses and got a job for a consulting company, building simple models for businesses. Nothing too technical.\n\n\nFor about six weeks now I've been getting into NLP to tie my past experiences with my present ones. But I want to dive deeper into the inner workings of NLP to professionalize my profile a little bit.\n\n\nWhat topics do you think I should focus on? I'm particularly interested in learning the basics of statistics and algebra oriented for NLP but I don't know where to start. Thanks in advance!",
"date": "2021-11-14"
},
{
"vote": 17,
"title": "CMU Researchers Develop A Unified Framework For Evaluating Natural Language Generation (NLG)",
"text": "Natural language generation (NLG) is a broad term that encompasses a variety of tasks that generate fluent text from input data and other contextual information. In actuality, the goals of these jobs are frequently very different. Some well-known instances of NLG include compressing a source article into a brief paragraph conveying the most significant information, converting content presented in one language into another, and creating unique responses to drive the discourse.\n\n\nNatural language processing has advanced at a breakneck pace in terms of enhancing and developing new models for various jobs. However, assessing NLG remains difficult: human judgment is considered the gold standard, but it is typically costly and time-consuming to get. Automatic evaluation, on the other hand, is scalable, but itâ€™s also time-consuming and challenging. This problem originates because each work has varied quality requirements, making it difficult to establish what to assess and how to measure it.\n\n\nResearchers from Carnegie Mellon University, Petuum Inc., MBZUAI and UC San Diego recently took a step in this direction by developing \na single framework for NLG evaluation\n that makes it easier to create metrics for various language generation tasks and characteristics.\n\n\nQuick Read\n |  \nPaper\n| \nCode\n | \nCMU Blog",
"date": "2021-11-14"
},
{
"vote": 3,
"title": "NLP switch advice for bio",
"text": "Hi. Could anyone working in NLP shoot me some advice?\n\n\nI'm trying to switch to NLP based work. I'm a biologist/bioinformatician (M.S.) and I've done ML with computer vision in industry. I've even turned down a pretty nice job offer with computer vision, but it had restrictions and not NLP focused so I turned it down. \n\n\nMy goals are to get a jump start on NLP for bioinformatics with protein and gene language models. To that effect, I've been studying pytorch and NLP from scratch. I expect to have a working understand of transformer/BERT based langauge models and a decent example or two to start applying for biology based NLP.\n\n\nHowever, I'm afraid I'm a bit too early for gainful employment strictly working with proteins and genes given the sporadic appearence of job postings.\n\n\nTo summarize, I've got a bio background, I've done ML in industry with computer vision and I am prioritizing a research career using NLP and biology. Today, I \nthink\n I would like a job where I can work with NLP in some context, with enough of a salary I can live comfortably in Spain as a remote worker (am American). I'd like to do this until more opportunity appears.\n\n\nA few questions;\n\n\n\n\nAre there places other than LinkedIn you seek NLP jobs?\n\n\n\n\nWhat skiills can get me a remote NLP job?\n\n\n\n\n\n\n\n\nI've learned the basics of Flask, I would continue figuring this out to serve models on the cloud and make them accessible via REST APIs if it would greatly increase my chances at a paid remote gig.\n\n\nI could do something with huggingface, but I don't know what general project would be good to get non-bio jobs\n-Coding models with JAX?\n\n\n\n\n\n\nWould you recommend a different path on the short-term?\n\n\n\n\n\n\nfocus on finding a computer vision job (And use transformer models to gain more transferable knowledge to future career)\n\n\nfocus on learning the mininmal backend/webdev to get a job to get a paycheck on the short-term?\n\n\n\n\nAny pertinent advice would be appreciated. This is my dead-set goal, but I don't really want to get side-tracked or accept an offer that would require me to establish myself somewhere physically, or do a PhD.\n\n\nThanks",
"date": "2021-11-13"
},
{
"vote": 3,
"title": "Are there anyone studying CS224n from Stanford?",
"text": "Hello guys, if there're anyone else who's studying this course and want to collaborate on homeworks and in general toss me a message!",
"date": "2021-11-13"
},
{
"vote": 0,
"title": "Fine-tuning and API for GPT-J",
"text": "[removed]",
"date": "2021-11-13"
},
{
"vote": 25,
"title": "Could you give examples of types of NLP projects you worked on at work in real business scenarios?",
"text": "I get the impression that Kaggle competitions aren't reflective of real-world applications of data science in NLP, and common NLP examples like chatbots, search engines, and grammar checking are not necessarily the majority of real-world projects either? Am I wrong? Or are real-world business applications of NLP really quite different and unique compared to the examples I just mentioned?\n\n\nCould some of you in the field give me examples of what real-world business projects look like? I want to get a feel of what working in NLP as a data scientist would be like.\n\n\nSide question, is there normally not enough work to go around to just focus on NLP alone as a career, and do you have to do computer vision or other subfields of data science in a typical work setting?",
"date": "2021-11-13"
},
{
"vote": 6,
"title": "Spacy vs NLTK for Spanish Language Statistical Tasks",
"text": "Hey all,\n\n\nI have some experience using both NLTK and Spacy for different NLP tasks. I find myself wanting to gravitate towards Spacy becuase of their community and documentation, but I can't help feeling that for my specific use case NLTK may be the better route.\n\n\nMy idea is to scrape the entirety of a Spanish news site and analyze the content of all their news articles. I want to answer questions such as:\n\n\n\n\nWhat are the top 100 most frequent words used among all their articles.\n\n\nWhat is the lexical diversity across the entire site (and perhaps per article so that I can try to predict which articles are easier to read for non native learners)\n\n\nWhat are the most common n-grams across the entire site to help learners know what vocabulary to study.\n\n\n\n\nBetween NLTK and Spacy, which framework is better for completing tasks such as the above? My guess is both can do it, but I wonder if one is better suited for it than another. \n\n\nThanks!",
"date": "2021-11-12"
},
{
"vote": 7,
"title": "Create semantic search applications with machine-learning workflows",
"text": "&#x200B;\n\n\nhttps://reddit.com/link/qs9ajk/video/slkwnfkqh5z71/player\n\n\nCreate semantic search applications with machine-learning workflows. The demo above shows how various NLP pipelines can be connected together to build a semantic search application.\n\n\ntxtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. txtai has support for processing both unstructured and structured data. Structured or tabular data is grouped into rows and columns. This can be a spreadsheet, an API call that returns JSON or XML or even list of key-value pairs.\n\n\nSome example workflows:\n\n\n\n\nSummarize news articles\n\n\nSummarize and translate research papers\n\n\nLoad and index data via a CSV\n\n\nSchedule a recurring job to query an API and index results for semantic search\n\n\n\n\nReferences:\n\n\nLive Demo\nGitHub\nArticle\nNotebook\nNotebook",
"date": "2021-11-12"
},
{
"vote": 3,
"title": "Speech recognition hackathon (ends Nov. 17)",
"text": null,
"date": "2021-11-11"
},
{
"vote": 1,
"title": "ZeroShotBot - Zero Code, Zero Training chatbot - the future of AI?!",
"text": null,
"date": "2021-11-11"
},
{
"vote": 2,
"title": "Explicit content detector python",
"text": "Hello !\n\n\nI want to build a project thats aimed to detect explicit content in texts, it's just going to flag if the text has explicit content. I already made something that detects explicit words in a text, but I want to do something more complex. As you know, you don't have to use bad words to make explicit sentences, I thought about creating a list of possible explicit sentences, but that would be infinite.\n\n\nWhat are my chances here ? Do I have any other options ?\n\n\nThanks in advance.",
"date": "2021-11-10"
},
{
"vote": 3,
"title": "Experience with Context-Based Sentiment Analysis?",
"text": "Sentiment analysis is a pretty standard problem. It's generally done with short input texts and is not seen as a super difficult problem. However I haven't thought about is adding context to the taskâ€”say like trying to predict the sentiment of one comment given the above comments on a Facebook post. I imagine that adding context could be as simple as concatenating the entire context (e.g. other comments and the original post) to the input given the capabilities of Transformer models. But I've never actually tried to solve a problem like this. Does anyone have experience or insights to share for a problem like this?",
"date": "2021-11-10"
},
{
"vote": 28,
"title": "An Introduction to Language Models in NLP (Part 1: Intuition)",
"text": null,
"date": "2021-11-10"
},
{
"vote": 1,
"title": "Same Document, two OCRs, super classifier?",
"text": "I have 150k documents, 10 pages each document, and two outputs, the Original OCR and my Tessearact OCR. \n\n\nI've built classifiers with the Tesseract output, but am seeking ways to strengthen my model. Model in question: \nhttps://scikit-learn.org/stable/modules/naive_bayes.html\n along with `CountVectorizer(ngram_range(2,2))` \n\n\nIt dawned on me that it might be possible to somehow sandwich the Original OCR with the Tessearact OCR. Would such a thing be possible? Or even useful? \n\n\nI plan to try it out, but what do you say internet? \n\n\n(open to all thoughts and considerations)",
"date": "2021-11-10"
},
{
"vote": 0,
"title": "Please help",
"text": "Hi i am a complete beginner dumbo to NLP and want to try learning topic modeling. Is it okay to use LDA on just 16 documents. They are business reports and I would like to extract topics to assess the trends. \n\n\nOmg please help !!",
"date": "2021-11-09"
},
{
"vote": 1,
"title": "Language model built on LSTM?",
"text": "Hey everyone! Could I get an example (or multiple, if possible) of a language model which as been built on LSTM rather than Transformers? \n\n\nThank you\n\n\nEdit: preferably one that can be used with the Hugging Face API",
"date": "2021-11-09"
},
{
"vote": 1,
"title": "Language model built on RNN?",
"text": "[deleted]",
"date": "2021-11-09"
},
{
"vote": 12,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-11-09"
},
{
"vote": 12,
"title": "NLPAug: what proportion of augmented sentences do you usually add to the dataset?",
"text": "Hi,\n\n\nWe are working on an NLP problem that is near to hate speech detection.\n\n\nWe use a BERT neural network that has 2 outputs:\n\n\n- Rating the sentence\n\n\n- Classifying it among 16 types of speech classes\n\n\nWe have 12K sentences tagged in a dataset.\n\n\nSince the dataset is relatively tiny, we are working on augmenting it with \nNLPAug\n. We use 2 strategies. Synonymisation and back translation.\n\n\nI was wondering if you have experience with that, what is the usual ratio of augmented sentences in your dataset? 1/3, 1/2, 3/4...\n\n\nThanks",
"date": "2021-11-09"
},
{
"vote": 1,
"title": "Is it possible to do an Aspect Based Sentiment Analysis using XLNet?",
"text": "Hi everyone,\n\n\nI am doing an Aspect Based Sentiment Analysis using BERT Model, however, I noticed that the state of art XLNet model over performed the BERT model in most of NLP applications. I couldn't see any implementation for Aspect Based Sentiment Analysis on Internet , so I am curious if it is possible to do it?",
"date": "2021-11-08"
},
{
"vote": 17,
"title": "About to apply for a Master's degree in Computational Linguistics; in want of information from current or former students (especially from Saarland, Tubingen and Stuttgart)",
"text": "Hi everyone,\n\n\nI'm about to complete my bachelor's degree in English studies (I'm in third year, Western Europe), and I have to apply for a Master's degree this year. Alongside my studies, it's now been four years since I've started working as a translator, specialized in localization, and I've had the opportunity to work regularly with famous video games companies and translate a variety of content.\n\n\nI first had in mind to apply for a translation Master's degree, but as I already have had a peek at the translation industry by working, I'd like to broaden my skills so as to get better opportunities in the future as well as career development prospects, since I don't see myself having the same job during all my life.\n\n\nOne of the classes that I appreciate the most where I study, aside from translation, is linguistics. Moreover, I've always had a genuine interest in computing, and even though I'm only doing web development stuff (HTML/CSS/JS), I'm willing to learn other languages and develop my skills in this field.\n\n\nNow, with those two variables in the equation, I think computational linguistics could be a great opportunity for me, as it mixes two of my biggest interests and is still a relevant field with regard to the translation industry.\n\n\nOne of my biggest flaws is maths: it's been more than five years now that I've stopped doing maths, because I didn't need it during my studies. I've seen that some universities in Western Europe accepted students coming from a linguistics background and offered optional courses for such students. From what I've seen, these universities are generally located in Germany, namely Saarland, Tubingen and Stuttgart.\n\n\nAs far as I'm concerned, Germany would be the best choice as, even though I do not speak German, the country is contiguous to where I live and has extremely low fees compared to other universities, such as the University of Edinburgh, or University of Washington in Seattle. Now, here are some specific questions I'd like to ask to current or former students of these German universities:\n\n\nâ€” as someone who has few programming experience but is willing to learn, which university would be the best choice?\n\n\nâ€” how much math knowledge is required? Just enough for programming or more?\n\n\nâ€” how many hours of classes are there on average per week, and does the general schedule allows one to have a job alongside one's studies? To take my own example, where I am, I have about 20 hours of classes per week, about 10 hours of work at home for the university, and 10 to 15 hours of real work (translation).\n\n\nObviously, I'd also love to hear the answers of people not coming from these universities â€” I've taken those as examples because I've heard of them the most on the Internet, but feel free to talk about your own path, it may give me ideas!\n\n\nThank you much for reading!",
"date": "2021-11-07"
},
{
"vote": 2,
"title": "Global Attention Vs Local Attention",
"text": null,
"date": "2021-11-07"
},
{
"vote": 1,
"title": "Quoting in pandas",
"text": "Can anyone please explain what is quoting=n, when reading a pandas data frame\nI got this solution on stack overflow when trying to solve eof error but I don't understand why",
"date": "2021-11-06"
},
{
"vote": 3,
"title": "Identify Scenarios/Topics from dataset",
"text": "Hi Guys!\n\n\nI have the following use case: I have a Dataset containing roughly 100 sentences which are describing certain components of a  multi component system.\n\n\nI am interested to identify which sentence is describing which component in this system. I know that I can use a Topic modeling algortihm like LDA to find topics for each sentence in the dataset.\n\n\nThe problem is, from what I know LDA does not regard context. The difficulty for my specific case, is that there are certain sentences in my dataset that only have semantic value when the context is known. \n\n\nI think its better to showcase an example to illustrate what I mean lol \n\n\nLets assume hypothetically that my Dataset contains 100 sentences describing the various components of a Computer, Like CPU, GPU, Motherboard, ect.\n\n\nand these two sentences are part of the Dataset:\n\n\n\n\nThe GPU is manufactured by ASUS\n\n\nit has 12GB Memory\n\n\n\n\nSo we can see that the first sentence is talking about the component GPU and the algorithm should identify this sentence as GPU Topic, the second sentence is obviously also talking about the GPU if we look at the context (not a problem for us humans), but if we look at the sentence on its own, it would be impossible to say that the algorithm should also classifiy this as GPU topic. So the algorithm should somehow understand that this sentence in its own row in the dataset belongs together with the sentence is the row before inside the dataset and classify it into the same topic GPU.\n\n\n&#x200B;\n\n\nSo my question is, what is the best way to solve this issue, apart from manually letting a human look over the dataset and join rows together ?",
"date": "2021-11-05"
},
{
"vote": 13,
"title": "Using NLP way to identify controversial topics?",
"text": "Hi all,\n\n\nIâ€™m a psychology researcher, and am interested in the prospect of using nlp and topic modelling to find potential controversial topics in online forums (such as here on Reddit). Would there be any particular techniques in nlp (sentiment analysis etc) that could be used to do this?\n\n\nThank you in advance.",
"date": "2021-11-05"
},
{
"vote": 1,
"title": "Study of the assumptions and practices of NLG evaluation",
"text": "[removed]",
"date": "2021-11-04"
},
{
"vote": 1,
"title": "Confidence scores for NER",
"text": "[removed]",
"date": "2021-11-04"
},
{
"vote": 2,
"title": "Context and Resources to apply NLP to source code",
"text": "Hello I am new here and I am a 3rd year data science major looking to work on a personal project regarding applying NLP to identify/classify vulnerabilities in source code (c++, c). Given that I am new to this game, I would be much obliged if more experienced folk could refer me to some resources using code as texts for NLP. I am having trouble finding resources for this myself aside from the odd research paper w/o code :( .",
"date": "2021-11-04"
},
{
"vote": 28,
"title": "Multilingual sentence vectors for 50+ languages",
"text": "Hey everyone, I wrote a pretty long article covering \nmultilingual sentence transformers\n, including how to build our own. It's super interesting imo and I focused on something called 'multilingual knowledge distillation' by Nils Reimers and Iryna Gurevych, which has been used to build sentence transformers that work with 50+ languages - which is incredible to me!\n\n\nIt's really useful for low-resource languages as it just requires translation pairs (like English-to-<insert language here>) and doesn't need that much data either.\n\n\nAnyway, I hope it's useful - let me know what you think, thanks!",
"date": "2021-11-04"
},
{
"vote": 2,
"title": "Have any one used sentence Bert embedding for sentiment analysis ?",
"text": "Not sure if it is feasible to use sentence embedding to do few shot sentiment analysis?",
"date": "2021-11-04"
},
{
"vote": 2,
"title": "Speech Emotion Recognition",
"text": "What are some of the state of the art speech emotion recognition architectures/alghorithms?",
"date": "2021-11-04"
},
{
"vote": 2,
"title": "What method to use for Out-of-distrubution detection?",
"text": "I have a stream of log data from users. There are some comments from users that I would like to classify as distinct from others (only 1 class). This seems to me like an OOD problem where 99% percent of the data could be whatever (i.e normal language) and 1% of them belong to a certain class. Has anyone worked on a similar problem or has any good ideas/papers that I should try implementing?",
"date": "2021-11-04"
},
{
"vote": 3,
"title": "Why one of the features is dominating all rest of the features in my trained SVM?",
"text": "I have been given a task to train the SVM model on \nconll2003 dataset\n for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.)\n\n\nInitially, I built a feature \nfirst_letter_caps\n which returned \n1\n if the first letter of the token was capital else \n0\n. This resulted in the first token of every sentence always getting identified as a named entity. So, I changed it to do that only for non-sentence-start-token and always return \n0\n for sentence-start-token (that is, for the first word in the sentence). This resulted in the first token of every sentence always getting identified as a NON-named entity. So I realized that somehow I have to \"turn off\" this feature for sentence-start-token and not return a fixed value. So, I made this feature return logical OR of other features (explained in next paragraph) for sentence-start-token, thinking that this will have the effect of turning off this feature and this turned out to be true. And this was quite successful. It stopped \"always\" identifying sentence-start-token as either named entity or non-named-entity.\n\n\nBut now, I have a few issues. But let me explain other features first. To avoid \"The\" in \"The Federal Bank\" getting identified as a named entity, I built feature \nis_stopword\n which returns \n1\n when the token is stopword else return \n0\n. Also, I have a third feature \ncontains_number\n which returns \n1\n if the token contained number in it, else returns \n0\n.\n\n\nI have trained \nsklearn.svm.SVC\n with linear kernel. But it never identified tokens containing numbers as named entities. And if stop words had first letter capital (like in \"The\"), it will classify it as named entity. After outputting \nSVC.coef_\n I realized that the issue is that it assigns positive coefficient only to feature \nfirst_letter_caps\n and negative or zero coefficient to all other features. When I plotted feature comparison, I realized that it is using only \nfirst_letter_caps\n feature for decision making:\n\n\n \nhttps://i.stack.imgur.com/HVzvrm.png\n\n\nSomehow the feature \nfirst_letter_caps\n is dominating all other features and SVM decision boundary. How do I fix this? What I am missing?",
"date": "2021-11-03"
},
{
"vote": 13,
"title": "Can open-domain QA models handle yes-no questions?",
"text": "My understanding of open-domain QA is that it receives a question and must retrieve the evidence passage and the appropriate answer within that passage.\n\n\nCan such models handle yes-no questions? I'm just curious because \"yes\" and \"no\" aren't really things you find in, for example, Wikipedia passages.",
"date": "2021-11-03"
},
{
"vote": 6,
"title": "Wav2CLIP: Connecting Text, Images, and Audio",
"text": null,
"date": "2021-11-03"
},
{
"vote": 1,
"title": "Tool for normalizing abbreviations?",
"text": "Hello all, \n\n\nI need to process a text and I'm looking for a Python tool able to transform abbreviations into their standard forms - for example, from \"I'm\" to \"I am\". I could do it by using regex, but I need to save time. \n\n\nDoes anyone know if there exist something like this, or at least a list of abbreviations that could be of use? Thank you in advance!",
"date": "2021-11-02"
},
{
"vote": 2,
"title": "Good stopwords list for sentiment analysis",
"text": "Does anyone know of a good stop words list for sentiment analysis pre processing? \n\n\n&#x200B;\n\n\nI'm trying to avoid removing words like 'can't', 'won't', 'no' etc.",
"date": "2021-11-02"
},
{
"vote": 4,
"title": "Top 10 Named Entity Recognition (NER) API",
"text": null,
"date": "2021-11-02"
},
{
"vote": 2,
"title": "Any movie dataset with movie summaries?",
"text": "Do you know of a dataset that contains movie summaries?\n\n\nDo you know if researchers are legally allowed to download IMDB movie summaries for research purposes?",
"date": "2021-11-02"
},
{
"vote": 6,
"title": "Time Complexity of Transformers (and RNNs and ConvNets)",
"text": "I was watching the guest lecture by the authors of the original Transformers for Stanford's CS224n course on NLP [\nLink-YouTube\n] in which they talk about how Transformers perform much faster than the traditional RNN and ConvNet models \nif the sequence length is orders of magnitude smaller than the model dimension which is usually the case\n. They also had this slide on the time complexities of different models [\nLink-Image\n]. My question is that shouldn't the compute time be independent of sequence length for ConvNets and Transformers since they can be parallelized (while training). And even while testing, can you explain from where did the length^(2) term come for the Transformers? Thanks!",
"date": "2021-11-01"
},
{
"vote": 10,
"title": "Bert embedding NLP",
"text": "We are working on an NLP project using a Universal Dependencies Tamil Tree Bank. The following is the preprocessed data frame where the column Form is to be word embedded using BERT. Since the column is already tokenized only word embedding is left and all examples we came across are taking raw text data and tokenizing using Bert.\n\n\nSo I just wanted to know whether a way to word embed the column using Bert was possible.\n\n\nI have attached a snippet of the preprocessed data in the chat.",
"date": "2021-10-31"
},
{
"vote": 1,
"title": "This week in Console we spoke with FranÃ§ois of Clevy about their open-source, easy-to-use, chatbot programming language and framework called CSML. We learned a lot about what makes CSML tick, *and* we learned that FranÃ§ois is a classically trained Cellist!",
"text": null,
"date": "2021-10-31"
},
{
"vote": 3,
"title": "How can I use features POS tags and chunk ids to train model when the input test sentence wont have them",
"text": "I have been given a task to train the SVM model on \nconll2003 dataset\n for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.) Conll2003 dataset contains part of speech tags and chunk IDs for each token. We used them to train the SVM model. We can also find models' performance against test and validation datasets as both of them also contain part of speech tags and chunk IDs for each token. But what if someone simply inputs some random sentence (without pos tags and chunk IDs) for predicting (as it is out of test dataset)? How should we handle this? Should we altogether avoid these features while training? Or \"somehow\" generate these features for input sentence before feeding it to the model for prediction? If yes, then how this generation is usually done? Also what is the standard approach?",
"date": "2021-10-31"
},
{
"vote": 0,
"title": "Is the phrase \"Department of Defence\" doublespeak if you live in a country where the military is more aggressive towards foreign countries than defensive?",
"text": "[removed]",
"date": "2021-10-31"
},
{
"vote": 3,
"title": "[P] â€œAbstractified Multi-instance Learning (AMIL) for Biomedical Relation Extractionâ€",
"text": null,
"date": "2021-10-30"
},
{
"vote": 1,
"title": "Suggestions on how to classify paragraphs in fiction books to a set of genres",
"text": "Hello everyone! I am new to NLP, and am working on a project where we have to classify fiction books either at paragraph or chapter level into a set of genres (we're keeping a set of 5 main labels like 'romance', 'suspense', 'adventure', 'tragedy', 'comedy') and sub-labels within each main label. \n\n\nWe are using books available from Project Gutenberg, and have some paragraph/chapter breaks ready. However, there are no genre annotations, so based on my background study, I have the following ideas/conclusions:\n\n\nThis seems like a task between text classification and sentiment analysis. I found that text classification seemed to rely a lot on some special seed/key words which may not be the best approach when trying to understand context in a fiction book. Hence I am leaning towards sentiment analysis methods which take into account context, but we do lack labeled data here. \n\n\n\n\nFor an unsupervised technique, I am thinking to start with LDA, and then try to manually match the outputted topics to our main set of genres. I fear this approach would lack capturing context from the text.\n\n\n\n\nFor an unsupervised technique, I have found a paper 'Contextualized Weak Supervision for Text Classification'. I have to try and see how this will fare.\n\n\n\n\nI will try to annotate some books in order to try some supervised methods, but want to keep this as a backup option since it would be a monumental task.\n\n\n\n\n\n\nDo you think I am headed in the right direction? I would appreciate any and all suggestions! Thank you so much.",
"date": "2021-10-30"
},
{
"vote": 2,
"title": "Building a Grammar Model",
"text": "I'm learning an inflected language, and I would like to build a grammar model to check self with.\n\n\nI have a corpus of sentences with grammatical tags (POS, case, conjugation, etc.). I'm specifically looking for something that will check if nouns and verbs are correctly cased/conjugated.\n\n\nIs there an automated tool that could build syntax trees from the corpus, and then check my sentences against them?",
"date": "2021-10-30"
},
{
"vote": 8,
"title": "The Obscenity List - Free Dataset of Profanities",
"text": null,
"date": "2021-10-30"
},
{
"vote": 2,
"title": "How can we assign sentiment score to preprocessed words?",
"text": "I'm currently implementing a domain based sentiment dictionary. And couldn't find a way to assign sentiment score to the preprocessed words. If anyone could give an advice that would be great.\n\n\nThank you for your kind replies.",
"date": "2021-10-29"
},
{
"vote": 1,
"title": "wink on Twitter",
"text": "[removed]",
"date": "2021-10-29"
},
{
"vote": 16,
"title": "Apple AI Researchers Propose â€˜Plan-then-Generateâ€™ (PlanGen) Framework To Improve The Controllability Of Neural Data-To-Text Models",
"text": "In recent years, developments in neural networks have led to the advance of data-to-text generation. However, their inability to control structure can be limiting when applied to real-world applications requiring more specific formatting.\n\n\nResearchers from Apple and the University of Cambridge propose a novel \nPlan-then-Generate (PlanGen)\n framework to improve the controllability of neural data-to-text models. PlanGen consists of two components: a content planner and a sequence generator. The content planner starts by first predicting the most likely plan that their output will follow. Thereafter, the sequence generator generates results using the data and content plan as input.\n\n\nQuick Read\n | \nPaper\n | \nGithub\n | \nDataset",
"date": "2021-10-28"
},
{
"vote": 8,
"title": "improving seq2seq model",
"text": "i'm using an encoder decoder seq2seq model for my chatbot and turns out it's not performing very well. (answering 15 questions right out of 20 questions) are there any ways i can improve the performance or accuracy of it?\n\n\nwhat i can think of right now is the dataset which i have only about 400 questions, but how much data is really enough though ? i read somewhere that increasing the amount of data for those with longer target sequence lengths may help\n\n\nit may also be because of the number of epochs or the word embedding used, will using glove/word2vec be better than keras' embedding layer?\n\n\nwhat else could be affecting the performance of the chatbot?",
"date": "2021-10-28"
},
{
"vote": 2,
"title": "Glove Number of Parameters to Train",
"text": "Hi guys pretty much in the title, but I want to figure out the number of parameters required to train my GloVE model. I have a vocab size of 95k and an embedding dimension of 100. Any help would be really appreciated :)",
"date": "2021-10-28"
},
{
"vote": 5,
"title": "How does dictionary based sentimental analysis work?",
"text": "How can we combine both machine learning approaches with the sentiment dictionaries for predict the severity level in a text? \n\n\nIf anyone simplyfy the general workflow it would be really helpful for me.\n\n\nThank you for your kind replies.",
"date": "2021-10-28"
},
{
"vote": 5,
"title": "NER on non-sentence data",
"text": "I have data being read from pdf's that is english text, more or less, like equipment details such as model numbers, manufacturer names and a ton of technical descriptive info for electrical installations. I am trying to extract specifically model numbers, manufacturers, etc and have attempted to naively do so through an NER model from spaCy. Prior to the NER model, we have some rule based thing, that does not work very well due to the many formats that this data can come in. Is there some better way of doing NER on non-sentence data - note that I do need labels on a word by word basis, not whole piece of text - than using some pretrained english model? I have tried using 'blank' english spacy models which performs even worse. Are there any ideal architectures in tensorflow or some other frameworks that would work better?",
"date": "2021-10-27"
},
{
"vote": 0,
"title": "Video chat in different languages with instant translation",
"text": "[removed]",
"date": "2021-10-26"
},
{
"vote": 0,
"title": "How to run NLP on a PDF file?",
"text": "[removed]",
"date": "2021-10-26"
},
{
"vote": 3,
"title": "What a Cognitive Linguist means by meaning and why it could impact research in #NLProc (an unpretentious unfinished reading list)",
"text": null,
"date": "2021-10-26"
},
{
"vote": 0,
"title": "Looking for partners on a project related to AI and Gender Bias (from a developing country)",
"text": "Hello everyone,\nI'm looking for an NLP researcher from a research project related to Bias and Artificial intelligence by The Feminist AI Research Network. The researcher has to be from a developing country. The term in the document is \"Global South\", which is confusing because it does NOT mean the southern hemisphere. It basically means developing countries.\nMy email is \nhashem.elassad@hotmail.com\n, I can send you the document from there and my LinkedIn is hashem \nhttps://www.linkedin.com/in/hashemelassad/",
"date": "2021-10-26"
},
{
"vote": 24,
"title": "Custom sentence embeddings by fine-tuning transformers",
"text": "Hi all, I put together some videos and articles covering the fine-tuning methods used when creating sentence transformer models, which can be used to create dense vector representations of sentences/paragraphs. It starts with \nfine-tuning on NLI data with softmax loss\n, then the more recent, and effective \nfine-tuning with multiple negatives ranking loss\n.\n\n\nBoth articles and videos look at the PyTorch implementation, then using \nsentence-transformers\n. It's surprisingly easy to fine-tune, and the results (particularly with the latter approach) are really good. I hope you find it useful!\n\n\nLet me know if you have any questions etc :)",
"date": "2021-10-26"
},
{
"vote": 1,
"title": "Recommendation on embedding method",
"text": "Working on a text classification project, Iâ€™ve explored TFIDF and word2vec before for converting text to vectors. Need recommendations on best approach that has worked for you!",
"date": "2021-10-26"
},
{
"vote": 1,
"title": "Issues encoding label column for deep learning",
"text": "Hi, I was wondering if anyone could provide any help? I am carrying out a comparison binary classification of Twitter sentiment model using various models; some sci kit learn ones and a few deep learning / transformer models.\n\n\nMy models run fine for my transformer models and sci kit learn models, however, my LSTM was producing terrible results.\n\n\nWhen using get_dummies() to encode my label column, it was producing a single dimension array of shape (5825, ). When I changed get dummies to produce a two dimensional (5825,2) so the output is more like [0,1] , my model began to run well (with a two neuron output instead of one).\n\n\nIdeally, I'd like to have a single neuron. I've looked online for solutions but can see anyone having a similar issue, could anyone advise at all?",
"date": "2021-10-25"
},
{
"vote": 2,
"title": "How to get a sentiment analysis 'overall score'",
"text": "Hi!\n\n\nI'm currently working on an application that essentially runs sentiment analysis on tweets by users, using Microsoft Azure text analytics.\n\n\nWhenever I send a tweet to the API, the following is returned.\n\n\nsentiment: (positive, negative or neutral)\n\n\nand the confidence scores, so e.g.\n\n\nnegative: 0.03\n\n\nneutral: 0.01\n\n\npositive: 0.96\n\n\nI'm looking to calculate an overall sentiment score, which is essentially an average of all sentiment messages by that user, from 0-100% - 100% being very positive and 0% being negative. \n\n\nWhat I was thinking of is potentially just having a ranking, so e.g. each message will be ranked by:\n\n\nPositive = 1\n\n\nNeutral = 0\n\n\nNegative = -1\n\n\nand then just calculating the average, multiple by 100 and then receive a percentage?\n\n\nAppreciate any advice, Thanks!",
"date": "2021-10-25"
},
{
"vote": 2,
"title": "Using Huggingface Transformers with ML.NET",
"text": null,
"date": "2021-10-25"
},
{
"vote": 13,
"title": "How to extract information from documents with structures",
"text": "So letâ€™s say you have 500 different companies that are your suppliers, and each one of those companies sends you 200 invoices.\n\n\nNow company A always uses its same invoice structure, and company B also uses its same invoice structure, etc. So each company has their different way of designing their invoices. \n\n\nBut all of them have common features: list of products, total price, total VAT, etc. \n\n\nMy objective is to develop on Python (sort of beginner with NLP!) a model that standardises all the information into a structured XML.\n\n\nAny guidance would really be appreciated :)",
"date": "2021-10-24"
},
{
"vote": 11,
"title": "[Python] Best Python NLP library to segment run-on and list-like sentences",
"text": "Hi everyone!\n\n\nI am completely new to NLP and new to Python so I'm feeling a bit overwhelmed by the number of choices at the moment.\n\n\nI need a library that will allow me to take product titles such as these:\n\n\n\n\n50/100pcs Kraft Paper Bag Gift Bags Packaging Biscuit Candy Food Cookie Bread Seen Snacks Baking Takeaway Bags\n\n\nWholesale 2019 New Fashion 3D Mitsubishi Hat Cap Car logo MOTO GP Racing F1 Baseball Cap Hat Adjustable Casual Trucket Hat\n\n\n\n\nand run them through some function that will spit out something like this with added commas:\n\n\n\n\n50/100pcs Kraft Paper Bag, Gift Bags, Packaging, Biscuit, Candy, Food, Cookie, Bread, Seen Snacks, Baking, Takeaway Bags\n\n\nWholesale 2019, New Fashion, 3D Mitsubishi Hat, Cap, Car logo, MOTO GP Racing, F1, Baseball Cap, Hat, Adjustable, Casual, Trucket Hat\n\n\n\n\nSo it's very close to segmenting a paragraph into sentences but not quite.\n\n\nI need something that, ideally, already has a good dictionary and, mandatorily, provides support for both English and Portuguese. The more languages, the better.\n\n\nWhat do you recommend? What specific functions in the recommended libraries should I look into? I have already checked out spacy and it's dictionary was pretty good. Is it the best option? What specific functions would I use for this? Would I need to create one of my own based on grammar?\n\n\nThanks a lot!\n\n\nEDIT:\n Another thing I'd like is a way to detect sections in product titles containing brand and model names. For example:\n\n\nVgate Icar2 Obd2 Scanner ELM327 BT ELM 327 V2.1 Obd 2 Wifi Icar 2\n Auto Diagnostic Tool For Android/Pc/Ios Code Reader\n\n\nThe first part of this sentence, which I bolded for emphasis is basically just the brand and model numbers. Is there a ready-made solution I could use to I automatically detect and segment these, perhaps based on the presence of numbers, abbreviations and unknown words?",
"date": "2021-10-23"
},
{
"vote": 2,
"title": "more questions on text preprocessing for seq2seq models",
"text": "i am doing a chatbot and i have a few more questions on text preprocessing for seq2seq models, hoping some of y'all know an answer to these\n\n\nso i need a large dataset but i'm doing a closed domain one and my current dataset is too small (about 300 questions), how many more questions should i add to make my dataset bigger?\n\n\nsecondly, what should the threshold value (word occurrences) be? if i were to put it as 5, does that mean i have to add more questions for questions with words that did not appear more than 5 times as they will be removed and a wrong response may be given? \n\n\nlastly, questions and answers that are too long or too short have to be removed under preprocessing but most of the questions and answers i have in my dataset are very long. \nshould i shorten them or give a bigger max length value in the codes?",
"date": "2021-10-23"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-22"
},
{
"vote": 2,
"title": "NLP Unsupervised",
"text": "Hi,\nI am working on NLP unsupervised problem.\nProblem statement is to identify the emotions behind each review.\nBut Data is not labelled , I have tried to label it using TextBlob but I am not sure on what should be the Threshold to label the data into Worry,Sad, frustrated,anger etc.\nCan you suggest me any different ways to label it?",
"date": "2021-10-22"
},
{
"vote": 4,
"title": "Updating / Editing vocab.txt for BERT finetuning",
"text": "I am using Huggingface transformers for finetuning a simple classification task.\n\n\n However, I want to update the vocab.txt that comes with standard BERT checkpoint files, with some of the words that are frequent in my training corpus. When I added these words in place of the 'Unusedx' tokens in the 'vocab.txt' , still it was not tokenising the added words.  Can anyone guide me as to with the steps to do it ?",
"date": "2021-10-21"
},
{
"vote": 4,
"title": "T5 text-classification on colab",
"text": "Hi Reddit,\n\n\nI wrote a \nblog\n post and tutorial on how to fine tune a T5 model on colab using free tier resources. Hope someone finds it useful.",
"date": "2021-10-21"
},
{
"vote": 4,
"title": "I need help designing a text-to-pictogram system",
"text": "Hello community.\n\n\nI got my first job at NLP and I need your help. My task is to design an algorithm that receives text as input in the form of medical indications and outputs a series of pictograms that represent the text (text-to-pictogram system).\n\n\nI would appreciate any kind of indications, like what kind of task should I solve. Or some route to follow so as not to waste time.\n\n\nThank you",
"date": "2021-10-21"
},
{
"vote": 4,
"title": "The Idea of â€œPrompt-based Learningâ€ in Natural Language Processing",
"text": null,
"date": "2021-10-21"
},
{
"vote": 1,
"title": "WotNot goes FREEMIUM! ðŸ™†ðŸ™†",
"text": "[removed]",
"date": "2021-10-21"
},
{
"vote": 2,
"title": "Learn Spanish Online: Spanish Pronouns + SPANISH ONLINE TEST",
"text": null,
"date": "2021-10-21"
},
{
"vote": 22,
"title": "Illustrated intro to sentence transformers",
"text": "Another \nillustrated guide\n, this time introducing sentence embeddings with transformers (aka sentence transformers) - an awesome topic I'm excited to write more about, but for now introducing sentence embeddings and transformers, which we can use in cool applications like semantic search or topic modeling.\n\n\nHope you enjoy, feel free to ask me any questions, give feedback etc - thanks all!",
"date": "2021-10-20"
},
{
"vote": 4,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-20"
},
{
"vote": 3,
"title": "[D]Need some perspective on data tagging for NER.",
"text": "Hello reddit peeps. I am using the common BIO tagging method to tag words in a sentence.\nI have structured my data in two lists list a contains the sentence that needs to be tagged listA --> [text] and listB is a list of words contained within the sentence that needs to be tagged listB--->[worda, wordb, wordc,....etc].\nNow i have looked for open source solutions but none seem to quite work, so i wrote my own and it works fine for English language but not for Spanish or other languages. (DM will send the gist link)\nDoes anyone know how to solve this????",
"date": "2021-10-20"
},
{
"vote": 11,
"title": "Categorising into topics and sub-topics",
"text": "Hello Reddit!  \n\n\nI am currently starting on my way into NLP topics and am trying to create the following application.\n1.) I would like to employ python libraries in order to read a document and sort it into one of three PREDETERMINED groups.\n\n\n2.) I would then like to pull out parts of the text and check if they fall into one of the 6 different PREDETERMINED sub-groups\n\n\n3.) I would like to extract the section of the sub-text that falls into each group out of the original text\n\n\nFor example, the document that was uploaded was in the food industry and was talking about vegetables. 1.) Foods 2.)Vegetables 3.) \"these potatoes are good for frying since they have...\"  \n\n\nI was Looking into the LDA topic, but that creates its own groups...  \n\n\nAll ideas and tips are highly appreciated!!!",
"date": "2021-10-19"
},
{
"vote": 2,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-19"
},
{
"vote": 25,
"title": "BigScience's first paper, T0: Multitask Prompted Training Enables Zero-Shot Task Generalization",
"text": "The first modeling paper out of BigScience (\nhttps://bigscience.huggingface.co/\n) is here!\n\n\nT0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks while being 16x smaller!\n\n\nA very big collection of prompts (~2'000 prompts for 170+ datasets) was released (\nhttps://github.com/bigscience-workshop/promptsource\n) along with the model and the paper.\n\n\nThis was an international collaborative effort, with over 40 people across more than 25 organizations.\n\n\nThe group included dedicated researchers and engineers from different universities, companies, and think tanks.\n\n\nModel: \nhttps://huggingface.co/bigscience/T0pp\n\n\nRepo: \nhttps://github.com/bigscience-workshop/promptsource\n\n\nPaper: \nhttps://arxiv.org/abs/2110.08207\n\n\nAdditionally, the T0 models were released in the Hugging Face Model Hub and you can try it out in your browser here: \nhttps://huggingface.co/bigscience/T0pp",
"date": "2021-10-18"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-17"
},
{
"vote": 1,
"title": "Some questions when I read the paper",
"text": "Has anybody read the paper \nMulti-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization\n?\n\n\nI am confused on the input of the decoder. What is the g^0? (the paragraph is the upper of the equation 9) and Why the objective function use the lambda on the L_ext?",
"date": "2021-10-15"
},
{
"vote": 2,
"title": "Japanese search engine",
"text": "I want to build a search engine for Japanese. Japanese is difficult because there are no spaces between words, verbs are conjugated to show tense, negation, and politeness. Japanese is also tricky because it uses multiple character systems: two phonetic systems (one for words from Japanese, hiragana, and one for foreign words, katakana) and a symbolic systems (borrowed from Chinese).\n\n\n&#x200B;\n\n\nWhat would you need to do first before you could create a tf-idf index that is different from English?",
"date": "2021-10-15"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-14"
},
{
"vote": 6,
"title": "We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing",
"text": "Hey there,\n\n\nWe've collected our experiences on teasing out the data readiness of organizations in relation to ML/NLP projects. We describe a method comprised of 15 questions that help stakeholders gauge their data readiness, along with a way to visualize the outcome of applying the method.\n\n\narXiv: \nhttps://arxiv.org/abs/2110.05464\n\n\nAbstract: In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside academia, and that this gap is rooted in poor mutual understanding between academic researchers and their non-academic peers who seek to apply research results to their operations. To foster transfer of research results from academia to non-academic settings, and the corresponding influx of requirements back to academia, we propose a method for improving the communication between researchers and external stakeholders regarding the accessibility, validity, and utility of data based on Data Readiness Levels. While still in its infancy, the method has been iterated on and applied in multiple innovation and research projects carried out with stakeholders in both the private and public sectors. Finally, we invite researchers and practitioners to share their experiences, and thus contributing to a body of work aimed at raising awareness of the importance of data readiness for NLP.\n\n\nAnd the code for the visualizations is here:\n\n\nGitHub: \nhttps://github.com/fredriko/draviz\n\n\nI'll be happy to hear any feedback! :)",
"date": "2021-10-14"
},
{
"vote": 1,
"title": "How the context is stored in context vector in encoder decoder transformer model?",
"text": "I mean I know that transformer for eg BERT can Understand the context of the paragraph but how the BERT model stored the context. \n\n\nI understand that word can be put into vector using on hot encoding or any other approach but storing context into vector I don't get it at all.\n\n\nPlease help.",
"date": "2021-10-14"
},
{
"vote": 6,
"title": "Dense vectors for NLP (and some vision)",
"text": "Hi all, I put together an \narticle and video\n on a few of the coolest (and useful of course) embeddings for NLP, and also text-image with OpenAI's CLIP at the end. Planing on diving into each area in more depth in the future!\n\n\nLet me know what you think, if I'm missing anything or you have any questions!\n\n\nThanks!",
"date": "2021-10-14"
},
{
"vote": 1,
"title": "Ways to reduce memory consumption in Q&amp;A tasks without damage (or at least, not that much) the accuracy?",
"text": "iâ€™m facing this problem: Iâ€™m trying to spend less  memory in my Q&A task using bert. I debugged my steps and saw that  the start_logits and end_logits\n\n\n>start_logits, end_logits = model(**inputs)  \n\n\ncosts more than 11gb of ram. Is there any ways to solve this? I mean,  use less memory to perform this task without harm my model accuracy? If  so, can someone share some of them? And some alternative ways in case  is not possible to do this?",
"date": "2021-10-14"
},
{
"vote": 14,
"title": "Microsoft and NVIDIA AI Introduces MT-NLG: The Largest and Most Powerful Monolithic Transformer Language NLP Model",
"text": "Transformer-based language models have made rapid progress in many natural language processing (NLP) applications, thanks to the availability of large datasets, large computation at scale, and advanced algorithms and software to train these models.\n\n\nThe high-performing language models need many parameters, a lot of data, and a lot of training time to develop a richer, more sophisticated understanding of language. As a result, they generalize well as effective zeroâ€“ or fewâ€“shot learners on various NLP tasks and datasets with high accuracy.\n\n\nHowever, training such models is problematic for two reasons:\n\n\n\n\nThe parameters of these models can no longer be fit into the memory of even the most powerful GPU.\n\n\nSpecial attention is required for optimizing the algorithms, software, and hardware stack as a whole. If proper attention is not provided, the large number of computing operations required can result in unrealistically long training times.\n\n\n\n\nMicrosoft and NVIDIA present the Megatron-Turing Natural Language Generation model (MT-NLG), powered by \nDeepSpeed \nand \nMegatron\n, the largest and robust monolithic transformer language model trained with 530 billion parameters.\n\n\n5 Min-Quick Read\n | \nMicrosoft Blog\n\n\n&#x200B;",
"date": "2021-10-13"
},
{
"vote": 15,
"title": "An illustrated tour of wav2vec 2.0",
"text": "When Transformers started getting popular for NLP, we saw great visualizations to understand better the internals of these models like The Illustrated BERT, GPT...\n\n\nI haven't seen much like that for speech processing, so I wrote this quick post to illustrate the architecture and pre-training process of wav2vec 2.0 (now part of the HuggingFace library).\n\n\nhttps://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html\n\n\nHope this is useful : )",
"date": "2021-10-13"
},
{
"vote": 4,
"title": "Formal Description of Prompting: Systematic Survey of Prompting Methods in NLP (P.1)",
"text": null,
"date": "2021-10-13"
},
{
"vote": 3,
"title": "[Spacy and Yake] 107+ million journal articles, mined: the General Index (4.7 TiB)",
"text": null,
"date": "2021-10-12"
},
{
"vote": 6,
"title": "How do I specify a max character length per sentence for summarization using transformers (or something else!)?",
"text": "Hi there,\n\n\nI am exploring different summarization models for news articles and am struggling to work out how to limit the number of characters per sentence using huggingface pipelines, or if this is even possible/a silly question to begin with!\n\n\nI have the following setup when being passed through the article text and model name of â€˜facebook/bart-large-cnnâ€™, â€˜google/pegasus-cnn_dailymailâ€™ and â€˜sshleifer/distilbart-cnn-6-6â€™:\n\n\nsummarizer = pipeline(â€œsummarizationâ€, model=model_name)\nsummarized = summarizer(article_text, max_length=118, clean_up_tokenization_spaces=True, truncation = True)\n\n\nThe articles range in length from 100 words to 1000 words.\n\n\nI am hoping to cap the number of characters per sentence to 118, a hard cap for my application. When I set max_length to 118 they usually are below this limit but can be, say, 220 characters or sometimes just truncate off at the end.\n\n\nAlternatively, if there are different summarization methods which allow limiting if it is not possible using transformers then would love to hear.\n\n\nWould be wonderful if someone could let me know what Iâ€™m doing wrong!\nThanks a lot",
"date": "2021-10-12"
},
{
"vote": 3,
"title": "How should I engineer features for Named Entity Identification task?",
"text": "I was working on Named Entity Identification (not recognition) task. In this NLP task, given a sentence, the model has to predict whether each word (aka token) is named entity or not. The dataset used was CONLL2003 dataset.\n\n\nInitially, I included a feature \nfirst-letter-capital\n which was \n1\n if a token has its first letter capitalized. The model learned to predict the first word of each sentence as a named entity.\n\n\nSo I removed this feature and added a feature \nfirst-letter-capital-for-non-sentence-start-word\n, which was \n1\n if a word is not the first word of the sentence and has the first letter a capital. This made the model classify the first word of each sentence as a non-named entity.\n\n\nWhen I kept neither, the model predicted no word as a named entity. Why this might have happened? Can someone share their insight?\n\n\nPS:\n\n\n\n\nI am using SVM (and I have to solve this problem with SVM only as that's what the task given to me is).\n\n\nI am not using any word embedding!!! Somehow it was taking a lot of execution time with SVM (may be due to 300 dimensions of embeddings). I simply formed some features by parsing sentences / surrounding tokens of the target token   (I know this simply reduces down this task to possibly non NLP simple classification task)\n\n\nfirst-letter-capital-for-non-sentence-start-word\n required to check if the target token was the first one in the sentence.\n\n\nFeature  \nfirst-letter-capital\n  does not need to consider surrounding tokens\n\n\n\n\n\n\nThere are other features too (like POS tags etc), but they are not much related here as they don't relate with a capitalization of any letter of the tokens",
"date": "2021-10-11"
},
{
"vote": 2,
"title": "Need Help With LDA Topic Modelling",
"text": "Hey There\n\n\nI've been playing with LDA for topic modelling recently and been wondering - how do you assess the results of this model not manually? I looked for ways to do it but didn't find many interesting leads. Also - any rule of thumb for setting the number of topics? and any other useful tips you would give to a newbie in this area?\n\n\nTIA",
"date": "2021-10-11"
},
{
"vote": 1,
"title": "Video Series on How to Create a Virtual Assistant using Python",
"text": null,
"date": "2021-10-11"
},
{
"vote": 1,
"title": "Preparing data for training NER models",
"text": "Training most of the Named Entity Recognition (NER) models for example \nFlair\n usually needs to format data in \nBOI tagging\n scheme as shown below where each sentence is separated by blank line\n\n\nGeorge N B-PER\nWashington N I-PER\nwent V O\nto P O\nWashington N B-LOC\n\nSam N B-PER\nHouston N I-PER\nstayed V O\nhome N O\n\n\n\nBut instead of labeled text data we have entity data separated by newline in text files, so if we process the data in above format it will look something like as below which only contains entity information\n\n\nGeorge N B-PER\nWashington N I-PER\n\nWashington N B-LOC\n\nSam N B-PER\nHouston N I-PER\n\n\n\nIs it ok if processed data looks as above",
"date": "2021-10-11"
},
{
"vote": 9,
"title": "GitHub - winkjs/wink-nlp: Developer friendly Natural Language Processing âœ¨",
"text": "[removed]",
"date": "2021-10-11"
},
{
"vote": 11,
"title": "Zero-Shot Crosslingual Sentence Simplification (NLP Research Paper Walkthrough)",
"text": null,
"date": "2021-10-10"
},
{
"vote": 3,
"title": "When should you train a custom tokenizer/language model?",
"text": "I am trying to better understand when you should train a custom tokenizer and language model for your dataset. My go-to is spaCy and prodigy, but I realize there are limitations. Training a RoBERTa model or something similar with HuggingFace seems like the MLM could give you some advantages over what I would get with spaCy models plus prodigy Active Learning, just given the robustness of the model learning the domain context. My primary cases are NER & text classification. Any suggestions or tips would be greatly appreciated.",
"date": "2021-10-09"
},
{
"vote": 6,
"title": "Training NER models for detecting custom entities",
"text": "Hello everyone, we are working on a task to detect certain \ncustom entities\n in the text, we tried training \nsPacy\n but it's not converging\n\n\nCan anyone suggest some other \nNamed Entity Recognition (NER)\n models which can be trained to detect custom entities",
"date": "2021-10-09"
},
{
"vote": 5,
"title": "Any allennlp users in this sub?",
"text": "I have a whole host of questions that the official allennlp docs are unclear on - too many to post individually here - but no one to answer them.\n\n\nIf there are any allennlp users in this sub who wouldn't mind discussing them with me one-on-one, I would appreciate it tremendously. Apologies for the nebulous post, but thank you in advance!",
"date": "2021-10-08"
},
{
"vote": 1,
"title": "7 Tips to Quick-start Your Work With Text Using Pandas",
"text": "[removed]",
"date": "2021-10-08"
},
{
"vote": 4,
"title": "NLP Conferences with a decent industry track?",
"text": "I just got back from RecSys2021 and was surprised in a good way by the industry presentations. Being mostly a NLP guy - but one who hasn't attended a NLP conference for years, I couldn't stop wondering if any of 'ours' have a similar focus. Are there any good conferences that mix academia with industry?",
"date": "2021-10-08"
},
{
"vote": 3,
"title": "Using CLIP to get sentence/description from image",
"text": "I want to use CLIP to generate a sentence by inputting an image. \n\n\nI've worked with a lot of implementations where the opposite is done. But I'm not very acquainted with modern text generation models. I'm guessing the principle is similar: optimise the latent vector that CLIP gives you and generate text using this latent vector, convert back into CLIP's latent space again and calculate the loss using the CLIP latent of the image. \n\n\nAny suggestions on which model I should use for this? Preferably one that I can run on a 3090.",
"date": "2021-10-08"
},
{
"vote": 2,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-08"
},
{
"vote": 5,
"title": "Removing whitespace between characters",
"text": "Any NLP algorithm that removes extra whitespaces in between characters in a word (not in between words)?\n\n\nExample: \"How m uc h is it?\" should be interpreted as \"How much is it?\" instead of \"Howmuchisit\"\n\n\ncodes:\n\n\ntokens = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(text) if word not in ignore_words] \n\n\n\nappreciate anyone's help!",
"date": "2021-10-08"
},
{
"vote": 1,
"title": "LDA model returns same words in all the topics",
"text": "I'm running an LDA model with 14k unique tokens from 33k documents. The documents are questions and answers from a technical community and are rather short and focused on the same macro topic (SAP cloud Platform).\n\n\nI decided to extract 25 topics as I clustered the tags assigned to the original questions in groups and it seemed logical to divide them in 25 groups.\n\n\nI've run the model with 100 passes and 100 iterations for 7 hours but at the end I am still returned a model in which the topics are defined mostly by the same words and don't show significant differences. What could I do to improve my results?",
"date": "2021-10-08"
},
{
"vote": 3,
"title": "Looking for a table to text codebase",
"text": "Hi, I am trying to implement a table to text summarizer for pharma tables. I am looking for existing codebase which can help me jumpstart the project. Any suggestions? I tried looking for them (papers that use ToTTo, WebNLG etc) but most of them do not have complete code. Thanks!",
"date": "2021-10-07"
},
{
"vote": 15,
"title": "Just finished my first proper NLP project",
"text": "Today I launched my first ever twitter bot  \nAAPLinsight\n that focus on providing sentiment scores on $AAPL. I broke down my approaches in three categories: Apple Products, Company News and Social Media. These sentiment scores come from around 20 different sources in the web. The base model that I used was BERT and I added some additional layers to create a sentiment classiifer that specialises in financial news sentiment. Although it may be quite a simple project, I think it is quite cool and thank you for the subreddit for all the advices!",
"date": "2021-10-07"
},
{
"vote": 5,
"title": "Styleformer performance. Or anything that turns informal to formal.",
"text": "Hi, everyone.\n\n\nI have been playing around with Styleformer today and am wondering about performance. I'm unsure if this is the right place to ask.\n\n\nhttps://github.com/PrithivirajDamodaran/Styleformer\n\n\nI set up a basic Flask server so it would be loaded into ram and each query takes around two seconds on my laptop. What sort of server would be required to make this decently fast? Is it something I'd use DigitalOcean for, or are there better options?\n\n\nSorry if this question is far too basic. It's my first day using Python and this sort of thing. I love the output of Styleformer and would rather use it than an API.\n\n\nCheers.",
"date": "2021-10-07"
},
{
"vote": 1,
"title": "How I learn languages with quotes | beautiful words Italian",
"text": null,
"date": "2021-10-07"
},
{
"vote": 7,
"title": "Is Debatepedia website/dataset non-existent?",
"text": "Hi all,\n\n\nThe other day, I was looking at a paper DDA (Diversity Driven Attention) Model.\n\n\nhttps://arxiv.org/abs/1704.08300\n\n\nThey scraped data from the Debatepedia website for the purpose of Query-Focused Abstractive Text summarization.\n\n\nHowever the links provided (in the bash script for scraping data from Debatepedia) are not accessible. I.e. I cannot access Debatepedia.\n\n\nhttps://github.com/PrekshaNema25/DiverstiyBasedAttentionMechanism\n\n\nDoes anyone know how I can access Debatepedia?\n\n\nThanks.",
"date": "2021-10-06"
},
{
"vote": 1,
"title": "Locate handwriting in mixed text document",
"text": "Hi all!\n\n\nI currently have a project to OCR mixed text documents. Tesseract is fine for machine text but struggles for handwriting.\n\n\nI am looking for a method to only recognise sections with handwriting so it can be shipped off to a Vision API. Does anyone know any low computational methods to do this?\n\n\nOne thought is to use the confidence output from tesseract to filter out bad segments to ship.\n\n\nThanks",
"date": "2021-10-05"
},
{
"vote": 1,
"title": "Hot off the press! Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text",
"text": null,
"date": "2021-10-05"
},
{
"vote": 4,
"title": "Identifying medical information in text?",
"text": "I have access to a large dataset of medical texts - notes from doctors about patients etc. - and was wondering if there is a way to take one such text and automatically create tags for it.\n\n\nLet's say the text describes the condition of a patient with Covid, then the algorithm would look over the text and filter out terms like \"cough\" \"covid-19\" \"high temperature\" etc. \n\n\nI guess what I am looking for is a dataset of medical terms I could use on the texts.\n\n\nIf I want to train an ML model for this, focusing on one disease for the beginning, what would be a good amount of training data? I could tag a bunch of texts myself and just provide this as training data.\n\n\nObviously, I'm pretty new to the whole field, so links to similar projects or papers would be great too.",
"date": "2021-10-05"
},
{
"vote": 4,
"title": "German POS Corpus for Commercial use",
"text": "I'm trying to find a German corpus with POS tags that can be used for commercial purposes. I know about the TIGER corpus for which you could get a commercial license at leat in theory... however they haven't responded in months. Is there any alternative?",
"date": "2021-10-05"
},
{
"vote": 14,
"title": "Phone interview for Language Engineer job at Amazon",
"text": "I have one coming up soon and have no idea how to prepare for it or what kind of questions I should expect. I tried to search reddit and only found posts about onsite interviews. If anyone could share their experience I'd be very grateful.\n\n\nNot sure if important, but the job is not language-specific afaik. I was told earlier that I will be interviewed by 2 people but in the most recent email, the recruiter says \"interviewer\" singular, so not sure anymore.",
"date": "2021-10-05"
},
{
"vote": 3,
"title": "Question-Answering Model",
"text": "Hey guys! I am a bit new to NLP and Question-Answering in general. How would one create a Question-Answering model on a very specific domain? I know that there are ways to train a given model (SimpleTransformers for example) but I was wondering what you guys would suggest for such a task.",
"date": "2021-10-04"
},
{
"vote": 2,
"title": "Entity extraction from videos?",
"text": "Hi all,\n\n\nI am working on a recommendation engine which suggests the most likely related video(s) for a given news article. There is little to no metadata outside a video title so the approach that I am considering is automatically transcribing the video and then performing entity extraction on the transcript, performing the same entity extraction on the article text and comparing the two. \n\n\nMy worry is entity extraction will be impacted negatively by noisy transcription. Does anyone have any recommendations on NER from messy data or as to whether my approach to the problem of linking relevant videos to articles is with relative merit? Thanks",
"date": "2021-10-04"
},
{
"vote": 1,
"title": "I just released a \"Youtube name generator\" over the weekend by training a massive neural network",
"text": null,
"date": "2021-10-04"
},
{
"vote": 1,
"title": "followai â€” Telegram bot for your AI/data science needs",
"text": "[removed]",
"date": "2021-10-03"
},
{
"vote": 3,
"title": "Question about scraping unstructured texts using BERT",
"text": "Hello,\nFirst of all, I'm a data analyst with some data engineering background as well. I never really studied/worked with ML models...\n\n\nI am working on a project where I need to extract data from unstructured texts (PDF documents with multiple pages each). I assume it's possible to find the data I'm looking for in the texts. Since I know nothing about the text, and since it is unstructured, I looked into using BERT, pre-trained on the CoQA dataset to answer questions, based on:\nhttps://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626\n.\nI get good results from this pre-trained model if I manually locate the paragraph that contains the answer to the question, and let the model predict the answer with that paragraph as input. However, since I don't know in which paragraph the answer is hiding, this is clearly not helping me much. Some ideas I've tried:  \n\n\n\n\nSplitting the text into paragraphs and asking the model to predict an answer for the same question for each paragraph. I assume I'll get the right answer, but I won't know which one it is.... So not really helpful. (I might be able to ask the model to predict again on the outputs from theÂ  previous step, seems a bit messy but I'll try).\n\n\nExtracting a list of headers from the text (meaning the title of each paragraph), and asking the model to predict which header's paragraph might contain the answer my question. This method works in some cases, but certainly not good enough.\n\n\n\n\nIs there an elegant method you are familiar with? I'm sure I'm not the first person to try scraping large documents with BERT. Any inputs or ideas are welcome.\nThanks!",
"date": "2021-10-02"
},
{
"vote": 1,
"title": "Etymology, Yogad, Lapis - The Heranian Dome",
"text": null,
"date": "2021-10-02"
},
{
"vote": 2,
"title": "Braifun-nlp: A free Natural Language Processing tool to help Researchers brain storm their ideas (Alpha release)",
"text": null,
"date": "2021-10-02"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-10-02"
},
{
"vote": 0,
"title": "Text Classification - Sentiment Classifier without Training Data - Hugging Face NLP",
"text": null,
"date": "2021-10-01"
},
{
"vote": 8,
"title": "Get list of authors for topic in gensim atmodel",
"text": "In gensim atmodel  **get_author_topics(**\nauthor_name)\n returns the topic distribution for the selected author.\n\n\nIs there any method that given a topic, returns a list of the most probable authors?",
"date": "2021-10-01"
},
{
"vote": 1,
"title": "USA - Germany Internet",
"text": "[removed]",
"date": "2021-10-01"
},
{
"vote": 1,
"title": "Transformer NLP model similar to GPT-2 345M with nice up-to-date code base and multi-GPU training support?",
"text": "I am working on an interactive poetry project and I am searching for a model that would be easy to work with.\n\n\nI have worked on a previous project that involved a pre-trained version of the 345M GPT-2 model. It delivered great results for our use case. Larger models also worked great, but we opted for this smaller version since we had very limited compute available for inference â€” this was a personally-funded web-based application, and server time got expensive very quickly.\n\n\nI am working on a new project that both gives us the resources to train and fine-tune that model with our chosen datasets (cloud GPUs got really good and inexpensive in recent years!). We need to train it both in French and English. The datasets we have arenâ€™t huge, they have respectively about 60,000 and 8,000 literary pieces, so using a gigantic model wouldnâ€™t really be beneficial. We donâ€™t have as much of a restriction on inference compute here, as long as it can run fine on a decent CPU at a few words per second.\n\n\nMy initial thought was to simply train the same model, but the code base is somewhat old (not compatible past TensorFlow 1.15), which seems to cause issues with newer Ampere GPUs. It also doesnâ€™t support multi-GPU training. I know there is a TensorFlow 2.0 fork, and I know I could spend a bit of time getting multi-GPU working by splitting batches, but time is short, and I figure there must have been a lot of NLP code written since then.\n\n\nSo my question is: is there a nice, roughly similarly sized NLP model with a modern codebase youâ€™d recommend for this?",
"date": "2021-09-30"
},
{
"vote": 1,
"title": "How to customize UI",
"text": "Hello, im planning on creating a naural language processing ui to help me with homework, find information on the web, make calculations, and more. My only question is, can how can I make the voice the UI responds in unique, and not the same as the first siri, or whatever default voice it uses.",
"date": "2021-09-30"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-30"
},
{
"vote": 2,
"title": "Difference b/w Elasticsearch and Retriever",
"text": "I'm in the process of documenting a build of an extractive QA pipeline using haystack and elasticsearch.  From my understanding, we first take the corpus and store the documents/contexts from the corpus into a sparse (ie. elasticsearchdocumentstore) or a dense documentstore (ie. FAISS).  Once encoded, the retriever (ie. sparse or dense passage retriever) will perform a similarity search to identity top-n of most relevant documents.  The reader will then predict where in each context the answer is located.  I'm confused where elasticsearch comes into the picture.  I read that elasticsearch is the back-end search engine but isn't the retriever doing the actual searching/similarity calculations.",
"date": "2021-09-30"
},
{
"vote": 8,
"title": "Looking for best way to do embedding search in production",
"text": "Hi all,\nI came across with one problem of finding similar documents in a set of huge corpus. Looking for your help to figure out best possible solution.\n\n\nWhat I am looking for is, given a new document I want to retrieve similar documents based on the semantic similarities from a collection of documents (millions, billions in number)\n\n\nCurrently I am looking at the pre-computation of all the documents in corpus and store it somehow(maybe elastic search). Now whenever a new document comes, calculate embedding and find similar documents (with some threshold). Now since documents are huge in number and for every new document I have to calculate similarity with all documents which is way too time taking. So looking for a way to reduce complexity and latency. (Results should be achieved in less than a second)\n\n\nHelp me, if you guys know anything similar or how should I proceed with such problem.",
"date": "2021-09-29"
},
{
"vote": 2,
"title": "Google AI Introduces Translatotron 2 For Robust Direct Speech-To-Speech Translation",
"text": "The Natural Language Processing (NLP) domain is experiencing remarkable growth in many areas, including search engines, machine translation, chatbots, home assistants and many more. One such application of S2ST (speech-to-speech translation) is breaking language barriers globally by allowing speakers of different languages to communicate. It is therefore extremely valuable to humanity in terms of science and cross-cultural exchange.Â \n\n\nAutomatic S2ST systems are typically made up of a series of subsystems for speech recognition, machine translation, and speech synthesis. However, such cascade systems may experience longer latency, information loss (particularly paralinguistic and non-linguistic information), and compounding errors between subsystems.\n\n\nGoogleâ€™s recent study presents the improved version of Translatotron, which significantly enhances performance. \nTranslatotron 2\n employs a new way for transferring the voices of the source speakers to the translated speech. Even when the input speech involves numerous speakers speaking in turn, the updated technique to voice transference is successful while also decreasing the potential for misuse and better complying with our AI Principles.Â \n\n\n5 Min Read\n | \nPaper\n | \nGoogle AI Blog",
"date": "2021-09-29"
},
{
"vote": 6,
"title": "Loss stuck. Model for speech-to-text system",
"text": "Iâ€™m trying to build a speech-to-text system my data is (4 - 10 seconds audio wave files) and their transcription (preprocessing steps are char-level encoding to transcription and extract mel-Spectrograms from audio files). this is my model architecture is ( a 3 conv1d layers with positional encoding to the audio file - embedding and positional encoding to encoded transcription and then use those as input to transformer model and lastly a dense layer) the loss function is cross entropy and optimizer is Adam.\n\n\nthe problem is that the loss is always stuck at some point it starts around 3.8 (I have 46 classes) and after some batches it decreases to (e.g. 2,8) and stuck their. it bounces around that value and never decrease again. I tried changing parameters of the model, Iâ€™ve changed the optimizer and learning rate always result the same problem.\n\n\nI donâ€™t understand what Iâ€™m doing wrong \n\n\nTraining Loss",
"date": "2021-09-28"
},
{
"vote": 8,
"title": "Using NLP to parse and analyse cooking recipes.",
"text": "Hey everyone, I'm a intermediate programmer with an interest but no experience in Natural Language Processing and I was hoping to get some guidance.\n\n\nI'm trying to write a command-line program that takes plain text files of recipes and returns an analysis of potential typos in weight, volume, temperature, time, etc. For example, if a given recipe says to bake for 45 seconds instead of minutes.  \n\n\nI should also be able to query the recipe for things like \"well-cookedness\" where (given the previous example), the program would identify that the recipe produces 'uncooked' or 'undercooked' results. \n\n\nI was hoping to do all of the work in Python and I read that Python's default NLP library, the Natural Language Toolkit (NLTK) would be a good place to start.\n\n\nI am ready to learn everything as I go along but I'm hoping for guidance on the overall process of implementing such a project. Please forgive me if the following questions sound stupid ðŸ˜…:\n\n\n\n\nIs there an NLP library I should use instead of or in addition to Python's NLTK?\n\n\nWhat recommended AI or NLP techniques should I research and implement for a program like this?\n\n\nWhat would be the main stages of this program? From text analysis straight to querying data or are there some intermediate steps?\n\n\n\n\nThank you for reading up to this point and for any advice!",
"date": "2021-09-28"
},
{
"vote": 3,
"title": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
"text": "Abstract: To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of a unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.\n\n\nPaper link: \nhttps://arxiv.org/abs/2109.09519",
"date": "2021-09-27"
},
{
"vote": 5,
"title": "Need a mentor for his/her guidance in my NLP project",
"text": "Hi community! I am in search for a mentor who can guide me on how to approach for a project I want to build. My project is aimed to build a NLP model which can take information about a certain topic/query from various sources and summarises the text in a more understandable manner. The key task is the model takes a query from user and uses Google's search results to extract text from the webpages, understand the semantics and provide a more summarised and understandable output for the searched topic. As I am new to this there might be some assumptions I am making wrong or arbitrary. I don't know how should I approach this problem neither I have worked upon an NLP project before but I can learn and work for it. If anyone can mentor me for this, it'll be great.\nThanks in advance!",
"date": "2021-09-26"
},
{
"vote": 3,
"title": "Training or fine-tuning transformers on weighted sample data",
"text": "Hi there, I am wondering if there is a way to use weights (e.g. upvotes/downvotes) into the fine-tuning of GPT-2 or a different NLP algorithm.  In other words, the higher the human rating given to a sample in the corpus, the more influence it should have on the fine-tuned model.  I apologize in advance if this is very basic functionality that I'm just not aware of!",
"date": "2021-09-26"
},
{
"vote": 3,
"title": "Indox - text summarization engine",
"text": "Hi all!\nIâ€™ve developed a cutting-edge summarization engine and want to start a company that will provide AI services to customers. I dropped an article on medium \nhttps://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf\n .   \n\n\nIf you like, please, look at it, demo area on a website will be very appreciated for a feedback\nThanks!",
"date": "2021-09-26"
},
{
"vote": 8,
"title": "[Hiring] Looking for data scientists with NLP experience in USA",
"text": "Hi all, \n\n\nMy team is currently looking for data scientists with NLP experience. The role could potentially be remote from anywhere in the USA. \n\n\nAlthough the role would involve the usual data science suspects like EDA and ad hoc analysis, there would be a heavy NLP element to the role including custom NER modeling. \n\n\nIdeal candidate - have industrial data science experience and comfort with messy data. \n\n\nIf anyone is interested, pls reach out to me.",
"date": "2021-09-26"
},
{
"vote": 0,
"title": "How will machines understand people? That's how! The Folksâ€™Talks understanding test.",
"text": "https://youtube.com/watch?v=mlJakDX_93g&feature=share",
"date": "2021-09-25"
},
{
"vote": 32,
"title": "We are now publishing some downloadable NLP datasets from reddit posts and comments. First subreddits covered are /r/wallstreetbets (25K posts and 1 million comments) and /r/NoNewNormal (120k posts 2.5 million comments) for Aug 2021",
"text": null,
"date": "2021-09-24"
},
{
"vote": 5,
"title": "A Guide to Building Your First NLP Application to Detect SPAM",
"text": null,
"date": "2021-09-24"
},
{
"vote": 0,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-24"
},
{
"vote": 6,
"title": "Zero or Few Shot NER on Custom Entity",
"text": "Hey ya'll, I'm try to get a baseline for how good a zero or few shot approach would be on recognizing a custom entity (in this case job titles in german). I've been skimming through a few papers and see that it's certainly possible to do this, but I haven't seen any out-of-box type code that I could use to get a baseline on how effective it'll be. Anyone have any thought or ideas on how to approach this?",
"date": "2021-09-24"
},
{
"vote": 0,
"title": "UBIAI",
"text": "Today, text annotation tools are one of the most prominent parts of machine learning. Research areas such as search engines, chatbots, sentiment analysis, and virtual assistants require text annotation tools for better training of machine learning models.\n\n\nThe machine learning industry and AI research require a large amount of annotated data. High-quality annotated data is like a goldmine for them. However, finding and creating this enormous amount of annotated data can be an arduous task, and most of the time, expensive.\n\n\nFortunately, text annotation tools can help annotate this enormous amount of data in a matter of time. These annotation tools help with named entity recognition annotation, entity extraction, sentiment analysis, relation annotation, document classification, and more.\n\n\nFind out more here: \n\n\nhttps://ubiai.tools/",
"date": "2021-09-24"
},
{
"vote": 14,
"title": "Fine-tuning BERT models, alternatives for the last layers?",
"text": "I'm relatively new to the field of NLP, so excuse me if this is a trivial question.\n\n\nI'm fine-tuning a BERT model to do sentiment analysis, I have already succeeded. However, I find interesting that all tutorials and notebooks I found use the same layers after the BERT encoder, namely a dropout (sometimes) and a dense layer with the appropriate size for the task.\n\n\nIt is common to use different architectures for the layers after the encoder, for example, two (or more) dense layers, etc.\n\n\nThanks for any insight.",
"date": "2021-09-23"
},
{
"vote": 6,
"title": "Summarizing multiple documents into one summary",
"text": "I have found lots of info on summarizing single documents. But what I am looking for is being able to take multiple documents on the same subject and generate one summary that encompasses several different source documents.  \n\n\nThe next level of this for me would be to highlight the outlier info in the different documents.\n\n\nHas this been done? Maybe I am searching using the wrong terms to find the info...\n\n\nAny help is appreciated",
"date": "2021-09-23"
},
{
"vote": 23,
"title": "Fine-tuning GPT-J: key takeaways",
"text": "Hello all,\n\n\nWe've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at \nNLP Cloud\n. Finding the best solution was not straightforward and we had to look at things like speed, server costs, ease of development, accuracy of the fine-tuned model... It took time but we ended up with a nice setup (and we are now officially proposing GPT-J fine-tuning + automatic deployment on our platform).\n\n\nHere are our key takeaways:\n\n\n\n\nThe best methodology seems to be the one from the Mesh Transformer Jax team: \nhttps://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md\n\n\nFine-tuning on GPU is not ideal. Even several GPUs used in parallel with Deepspeed can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took 1h30 to only compute our first checkpoint (+ 80GB of RAM used...), for a training dataset made up of 20k examples. Maybe a GPU A100 would be worth a try.\n\n\nFine-tuning on TPU is very efficient but it takes a TPU v3 because TPUs v2 are running out of memory. It takes around 15mns, for a training dataset made up of 20k examples, which is really awesome.\n\n\nThe overall process is not straightforward as it takes several kind of conversions (converting the datasets to the right format, making a slim version of the model, converting the weights to Transformers...)\n\n\n\n\nIn the end this is worth the effort, because combining fine-tuning and few-shot learning makes GPT-J very impressive and suited for all sorts of use cases. \n\n\nIf you guys have different feedbacks about GPT-J fine-tuning, please don't hesitate to comment, I would love to have your opinion.\n\n\nHope you found the above useful!",
"date": "2021-09-23"
},
{
"vote": 5,
"title": "Concatenate to LSTM models",
"text": "I'm fairly new to NLP and building a model that takes two sub-models and concatenates them. The dataset has two text input columns and the predictor variable has 3 classes. Below is the code I wrote:\n\n\nmodel1 = Sequential() model1.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X1.shape[1])) model1.add(SpatialDropout1D(0.2)) model1.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n\n\nShape <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_3')>\n\n\nmodel2 = Sequential() model2.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X2.shape[1])) model2.add(SpatialDropout1D(0.2)) model2.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n\n\nShape <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_4')>\n\n\nconcat_layer = Concatenate()([model1.output, model2.output]) dense_layer = Dense(10, activation='relu')(concat_layer) output = Dense(3, activation='softmax')(dense_layer)\n\n\ninput_1 = Input(shape=(MAX_LEN,)) input_2 = Input(shape=(MAX_LEN,))\n\n\nI have set Max_LEN=250 # Both input_1 and input_2 are of shape TensorShape([None, 250])\n\n\nmodel = Model(inputs=[input_1, input_2], outputs=output)\n\n\nWhen I run the model I get the below error:\n\n\nValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 250), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=\"created by layer 'embedding_3_input'\") at layer \"embedding_3\". The following previous layers were accessed without issue: []\n\n\nWhat mistake am I making?",
"date": "2021-09-22"
},
{
"vote": 3,
"title": "Interpret 3d/2d shape from its text description",
"text": "I want to make a model that takes a text input such  as \"Make a round ball and a pyramid for me please\" and gives an output \"sphere and cone\" since they are the 3d shapes that are refereed to in the sentence. Any idea I can achieve something like this? Any links that can help me with this task?",
"date": "2021-09-22"
},
{
"vote": 3,
"title": "Pre processing text",
"text": "I am trying to clean some text from html tags however I cannot manage to remove new lines and slashes. What am I missing?\n\n\nraw text:\n\n\n'Is there an easy way to get a list of my blogs that require re-tagging?\n[\\'<div class=\"dm-section-hero--question\\\\\\\\\\\\\\_\\\\\\\\\\\\\\_body\">\\\\n                                <p>\nMost of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  I**\\\\\\**'ve done and those I haven\n\\\\\\**'t . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...\n</p><p>\nSteve.\n</p>\\\\n                            </div>\\']**'\n\n\nwhat I do:\n\n\nsoup = BeautifulSoup(raw_text)\ntext = soup.get_text()\ntext = re.sub(r&#039;[\\ \\n]{2,}&#039;, &#039; &#039;, text)\ntext = re.sub(r&#039;[\\t\\r\\n]&#039;, &#039;&#039;, text)\ntext = re.sub(r&#039;\\n&#039;, &#039; &#039;, text)\ntext.replace(&quot;\\\\n&quot;, &quot;&quot;)\n\n\n\nWhat I get:\n\n\n\"Is there an easy way to get a list of my blogs that require re-tagging?\n['\\\\n\n Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  \nI\\\\\\'ve\n done and those I \nhaven\\\\\\'t\n . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.\n\\\\n ']\n\"\n\n\nWhat I want:\n\n\n\"Is there an easy way to get a list of my blogs that require re-tagging? Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those I 've done and those I haven't . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.\"",
"date": "2021-09-22"
},
{
"vote": 11,
"title": "Is there any white paper or research paper explaining the architecture of any NLP engine like Dialogflow or LUIS?",
"text": "I tried to find on Google but couldn't find any research paper related to i design implementation of any NLP engine like Dialogflow,  LUIS etc.\nI would be really thankful if someone could provide.\nBasically I need to complete a POC for designing an NLP engine from scratch.",
"date": "2021-09-22"
},
{
"vote": 2,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-22"
},
{
"vote": 1,
"title": "Recognition of Resume and onvoice documents",
"text": "Hello, i need help\n\n\nI am asked in my internship to detect only invoice and resume documents from large amount of documents that contains numerous types.\n\n\nI am asked to build a model with NLP, so i should extract text from image or PDF than i begin the process of detection/classification\n\n\nTo be honest, i don't know from where i can start, i find it difficult task\n\n\nCan any one help me and put me in the road",
"date": "2021-09-21"
},
{
"vote": 11,
"title": "Natural language processing course - Looking for feedback",
"text": "Iâ€™m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting November 1st. \nhttps://corise.com/course/natural-language-processing\n.\n\n\nWe wanted to share what weâ€™ve learned in machine learning over the years. You can join the first run of the course (capped at about 30 students) below. If youâ€™re open to giving feedback on the class on how we can do better, happy to give a discount.",
"date": "2021-09-21"
},
{
"vote": 0,
"title": "Catogorize the Data- Topic Modelling algorithm",
"text": "Team,\n\n\nI am new to NLP , there is a requirement asked for me to categorize the data, Data which i have is just one column data in excel and these are values are user daily search criteria on google browser.\n\n\nsimply the search text done on google browser.\n\n\n&#x200B;\n\n\nI need to run a LDA (topic mapping algorithm ) on this data , so that the algorithm will classify them into some meaningful categories.\n\n\nThanks,",
"date": "2021-09-21"
},
{
"vote": 12,
"title": "NLP Bachelor's Thesis idea",
"text": "Hi everyone, I want to write my bachelor's thesis about Natural Language Processing in the context of cyber security. Is there anyone familiar with the combination of these fields (Evaluation of Logs for example)? One idea coming to mind would be using NLP on nMap-Output.",
"date": "2021-09-21"
},
{
"vote": 3,
"title": "Voice Cloning Software not limited to Text-to-Speech",
"text": "Hello everyone,\n\n\nI have been searching for a voice cloning software or process that isn't limited to Text-To-Speech output; something that allows me to clone, for example, my neighbor's voice then use that to either edit pre-recorded audio or change the voice live.\n\n\nI have come across a lot of software that clones your voice and uses text-to-speech, but this is limited by the language that the text-to-speech supports (plus it sounds a bit robotic). The other side of the coin is software that allows you to edit out audio or mask it live, but this one is limited to pre-synthesized voices included in the software.\n\n\nI have been exploring some ideas about new content for my youtube/twitch channel, and was wondering if this was possible.",
"date": "2021-09-21"
},
{
"vote": 1,
"title": "Clinical trials LM?",
"text": "Which pre-trained LMs will best suit applications for data from clinicaltrials.gov? (Like biobert, clinicalBert, UMLSBert, etc.)?",
"date": "2021-09-21"
},
{
"vote": 7,
"title": "A visual guide to filters in vector similarity search + latest advances in efficient filtering",
"text": "I put together \nan article and video covering metadata filtering\n for vector similarity search - it had lots of very insightful input from some of the industry's leading experts at \nPinecone\n, so I think this should hopefully be very useful! Pinecone also let me write a little bit about a new approach to filtering they've developed, and also test it - and from what I've seen it is super impressive, I was a little blown away haha :)\n\n\nLet me know what you think and if you have any questions!",
"date": "2021-09-20"
},
{
"vote": 4,
"title": "Get multiple translation candidates",
"text": "Does anyone know a (Statistical) Machine Translation model which is able to give me (for example) the best five translation candidates and not only one?",
"date": "2021-09-19"
},
{
"vote": 1,
"title": "I will translate English, Spanish and Portuguese languages",
"text": "[removed]",
"date": "2021-09-19"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-19"
},
{
"vote": 4,
"title": "Document IDs in the author2doc dictionary in gensim atmodel",
"text": "From the documentation:\n\n\n\n\nauthor2doc\n (\ndict of\n \n(str,\n \nlist of int),\n \noptional\n) â€“ A dictionary where keys are the names of authors and values are lists of \ndocument IDs\n that the author contributes to.\n\n\n\n\nDo the document IDs have to be the order in which the document appears in the corpus? Although it is not explicitly written in the documentation (hence why I am asking), I assume so because otherwise I wouldn't understand how the model would link authors and documents.",
"date": "2021-09-17"
},
{
"vote": 4,
"title": "Poor man's IBM Watson Concepts Model",
"text": "I am interested in how to replicate the IBM watson \"Concepts\" model, this from the outside seems a little similar to a topic model, however with all the topics labelled nicely!\n\n\nThis seems a great way to \"tag\" articles, I would be interested in suggestions of both the model itself and also any datasets which could be leveraged to train such a model.\n\n\nMy initial thoughts of a topic model or neural net for such a problem, but the fact that the output space is huge (60k+ concepts) I am not sure if that is the correct approach!\n\n\nAny ideas welcome :)\n\n\nhttps://cloud.ibm.com/apidocs/natural-language-understanding#concepts",
"date": "2021-09-17"
},
{
"vote": 5,
"title": "Is there some way to only apply the AI21 tokenizer to a bunch of texts? Have they released the tokenizer anywhere?",
"text": null,
"date": "2021-09-17"
},
{
"vote": 8,
"title": "Search-and-replace with correct grammatical case - does it exist?",
"text": "First off: I'm a professional software engineer but complete noob regarding NLP and linguistics, so please excuse if I explain myself badly or misuse any terms.\n\n\nIs there a tool, or even just a research effort, to enable a search-and-replace mechanism for single root words that results in gramatically correct results for (a) language(s) with grammatical cases? Or, to narrow it down and maybe make more clear what I mean: Is there a tool that can, in a German text replace all occurences of one noun with another noun in such a way that the grammatical case is still correct everywhere?\n\n\nFor example, let's say I have this text:\n\n\n&quot;Ich machte die Spende nur der Spende wegen.&quot;\n\n\n\nThen I replace all occurences of \"Spende\" (or rather its root) with \"Beitrag\" and would like to get as output\n\n\n&quot;Ich machte den Beitrag nur des Beitrags wegen.&quot;\n\n\n\nFor non-German speakers: The result is different from a normal search an replace because the genitive case of \"Beitrag\" is constructed differently than that of \"Spende\".\n\n\nFurther questions: \n\n\n\n\nAre there any specific or technical terms I should use to research this?\n\n\nHow hard of a problem is this? Is it something an NLP newbie like me could take on as a side project? (I imagine it being easy, as it's syntax-only and doesn't require semantic understanding of the text.)",
"date": "2021-09-16"
},
{
"vote": 13,
"title": "Want to reduce Data Annotation cost? GPT-3 can help (Research Paper Walkthrough)",
"text": "[Edited URL] This research paper from Microsoft proposes GPT-3 Language Model for Data Annotation in NLP. ðŸ”¥ The authors perform extensive experimentation to evaluate the quality of labels produced by GPT-3 and its cost-effectiveness when compared to human annotators. ðŸ”¥\nWatch paper summary at - \nhttps://youtu.be/CYD7HRIjhps",
"date": "2021-09-16"
},
{
"vote": 1,
"title": "The Role of Text Annotation Tools in Machine Learning",
"text": "Today, text annotation tools are one of the most prominent parts of machine learning. Research areas such as search engines, chatbots, sentiment analysis, and virtual assistants require text annotation tools for better training of machine learning models.\n\n\nThe machine learning industry and AI research require a large amount of annotated data. High-quality annotated data is like a goldmine for them. However, finding and creating this enormous amount of annotated data can be an arduous task, and most of the time, expensive.\n\n\nFortunately, text annotation tools can help annotate this enormous amount of data in a matter of time. These annotation tools help with named entity recognition annotation, entity extraction, sentiment analysis, relation annotation, document classification, and more.\n\n\nFind out more here: \nhttps://ubiai.tools/blog/article/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications",
"date": "2021-09-16"
},
{
"vote": 0,
"title": "What is the difference between weak supervision and distant supervision? Is it just me or is there no clear-cut definition?",
"text": null,
"date": "2021-09-16"
},
{
"vote": 0,
"title": "Sentiment Analysis using Python and Deep Learning (DistilBERT) in 3 lines of code",
"text": null,
"date": "2021-09-16"
},
{
"vote": 9,
"title": "AI21 Offers Instant Access To Its NLP Models, Jurassic-1 Large and Jurassic-1 Jumbo via AI21 Studio",
"text": "AI21\n, An Israeli AI company specializing in Natural Language Processing (NLP), has recently launched two big NLP models, \nJurassic-1 Large \nand \nJurassic-1 Jumbo\n, through an interactive web UI dubbed AI21 Studio. Unlike OpenAI, which has a limited beta, AI21 makes its models available for everyone to try out â€“ there is no waiting list.\n\n\nAs per the researcher\n, â€œUsing AI21 Studio, businesses can take advantage of text-based AI in the same way that Amazon Web Services makes cloud computing availableâ€\n\n\nNLP is an area of computer science that aims to design algorithms that can process and generate written natural language. Language Models are systems that can intake text and generate likely continuations. Language models are almost exclusively produced at big tech AI labs, out of reach of a wider developer audience, because training and deploying them into production generally requires expensive computational resources and highly sought-after AI engineers. As a result, language models are almost exclusively produced at big tech AI labs, out of reach of a wider developer audience.\n\n\n4 Min Read\n | \nPaper\n | \nGet Instant Access\n\n\n&#x200B;\n\n\nhttps://reddit.com/link/powhqf/video/vyoejsy8spn71/player",
"date": "2021-09-15"
},
{
"vote": 2,
"title": "6 Best FREE NLP Online Courses",
"text": null,
"date": "2021-09-15"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-14"
},
{
"vote": 5,
"title": "Bert WordPiece tokenizer tutorial",
"text": "Hi, I put together an \narticle\n and \nvideo\n covering the build steps for a Bert WordPiece tokenizer - I wasn't able to find a guide on this anywhere (the best I could find was BPE tokenizers for Roberta), so I figured it could be useful!\n\n\nLet me know what you think/ if you have Qs - thanks all! :)\n\n\n(If the article link shows the Medium paywall you can use \nthis link\n for free access)",
"date": "2021-09-14"
},
{
"vote": 1,
"title": "Daily summaries for selected arXiv papers",
"text": "[removed]",
"date": "2021-09-14"
},
{
"vote": 6,
"title": "Text Classification with Naive Bayes using Julia",
"text": null,
"date": "2021-09-14"
},
{
"vote": 8,
"title": "NLP Data Reduction With Similarity Detection",
"text": "TLDR: I'm trying to use natural language processing to label transaction data (topic modeling).   The problem is I have way too much data, a lot which are very similar.\n\n\nFor Example:\n\n\n\n\n\"Wells Fargo MTG PYMT #2434\"\n\n\n\"Wells Fargo MTg PYMT #4235435\"\n\n\n\n\nWhat are some efficient techniques to group like texts/transactions?\n\n\nThe long and to expand on this:\n\n\nSay I have a myriad of transaction descriptions; below is an example of what I want the final output to look like.\n\n\n\n\n\n\n\n\nTransaction Description\n\n\nLabel\n\n\n\n\n\n\n\n\nMcDonalds Debit #2343\n\n\nRestaurant\n\n\n\n\n\n\nWells Fargo Pymt\n\n\nBank\n\n\n\n\n\n\nNordstrom San Francisco #54525\n\n\nRetail\n\n\n\n\n\n\nMacy's  In-Store Purchase\n\n\nRetail\n\n\n\n\n\n\n&#x200B;\n\n\nThe problem (a good problem) is I have a lot of data.\n\n\nIn an attempt to reduce the data, I'm wondering if it's common to apply some sort of training sample reduction techniques. My thoughts are using string similarity algorithms (Jaro, Jaccard, Levenshtein, etc...) - if the strings are say 90% a match to one another, assume it's the same transaction. But this would be computationally expensive.\n\n\n&#x200B;\n\n\nWhat are efficient methods to group like texts and only take a fraction of them? Concretely, let's assume we have the following transactions:\n\n\n&#x200B;\n\n\nTransaction\n\n\nAmazon Txn #1234\n\n\nAmazon Txn #434343\n\n\nAmazon Txn #445454\n\n\nChase Bank Loan Pymnt #12\n\n\nChase Bank Loan Pymnt #1243\n\n\nWells Fargo Bank Debit -50\n\n\nWells Fargo Bank Debit -3131S\n\n\nStarbucks Main St\n\n\nStarbucks Second St\n\n\nStarbucks San Francisco\n\n\n&#x200B;\n\n\nAlbeit we have ~10 total transactions, in reality, we have 4 distinct transactions in the above example; Amazon, Wells Fargo, Chase, Starbucks. So instead of bringing in all transactions maybe only bring in 1,000 for each type?\n\n\nMaybe I'm thinking about this completely wrong and naively, but any help would be greatly appreciated.",
"date": "2021-09-13"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-12"
},
{
"vote": 6,
"title": "Classify if an input is a command or greet",
"text": "I am working on a bot, this bot should converse with the user as well as execute some software level activities. How can i identify if the input is a greet or a command? Like how does ok google identify a difference in input such as â€œheyâ€ and â€œ play a song on spotifyâ€",
"date": "2021-09-10"
},
{
"vote": 2,
"title": "Getting NLP to Challenge Misinformed Questions",
"text": null,
"date": "2021-09-10"
},
{
"vote": 53,
"title": "Facebook AI Introduces GSLM (Generative Spoken Language Model), A Textless NLP Model That Breaks Free Completely of The Dependence on Text for Training",
"text": "The recent advancements in text-based language models, such as BERT, RoBERTa, and GPT-3, have been extremely impressive. Because they can generate realistically written words from a given input, these models can be utilized for various natural language processing applications, including sentiment analysis translation information retrieval inferences summarization, among others using only a few labels or examples (e.g., BART and XLM R). However, these applications have a major limitation: the models are only suitable for languages with very large text data sets.\n\n\nFacebook AI has introduced the first high-performance NLP model, called Generative Spoken Language Model (GSLM), which leverages state-of-the-art representation learning to work with raw audio signals without labels or text. This can lead to a new era of textless applications for any language spoken on earth, even those without significant text data sets. By using GSLM, you can develop NLP models that incorporate the full range of expressivity found in spoken language.\n\n\n4 Min Read\n | \nGLSM Paper\n | \nExpressive Resynthesis Paper\n | \nProsody-Aware GSLM Paper\n | \nCode and Pretrained Models\n | \nFacebook Blog",
"date": "2021-09-10"
},
{
"vote": 16,
"title": "Salesforce Open-Sources â€˜CodeT5â€™, A Machine Learning Model That Understands and Generates Code in Real Time",
"text": "AI-powered coding tools, which use machine learning algorithms to generate code based on input data, have attracted increasing attention. In theory, these systems can reduce the time spent writing codes as well as computational and operational costs with minimal errors in output.Â \n\n\nHowever, current coding pre-training systems have many challenges. These methods heavily rely on either an encoder-only model similar to BERT or a decoder-only model like GPT. Either way, it is suboptimal for generation and understanding tasks. As an example, CodeBERT needs an additional decoder when used for tasks like code summarization. Apart from the above issue, most current methods adopt the conventional NLP pre-training techniques on source code by considering it a sequence of tokens like in natural language (NL). This largely ignores the rich structural information present in programming languages, which is vital to comprehend its semantics fully.\n\n\n3 Min Read\n | \nPaper\n | \nCode\n| \nSalesforce Blog",
"date": "2021-09-08"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-08"
},
{
"vote": 1,
"title": "Detect words based of context",
"text": "[deleted]",
"date": "2021-09-08"
},
{
"vote": 2,
"title": "Implementing Transformer Paper (Google T5 Transformer from Scratch and using it to create aâ€¦",
"text": null,
"date": "2021-09-07"
},
{
"vote": 1,
"title": "how do you keep track of translations for terminologies when writing a technical book",
"text": null,
"date": "2021-09-07"
},
{
"vote": 10,
"title": "Chrome summarizer extension (powered by DistilBERT)",
"text": null,
"date": "2021-09-07"
},
{
"vote": 3,
"title": "Any method to generate text using only keywords in GPT2?",
"text": "I am fine-tuning a GPT2 model in order to generate text. One approach I've found would work is to append the nouns at the start of the sentence and then pass it to the model for fine-tuning.\n\n\nThe text would get shortened even more due to the max_len feature. Are there any alternative approaches I can use for this task?",
"date": "2021-09-07"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-07"
},
{
"vote": 2,
"title": "Manual Data Annotation Tool for Relation Extraction",
"text": "Over the past week Iâ€™ve been looking for a manual annotation tool for relation extraction like this (e1,relation,e2). I have found UBIAI and Prodigy but they offer paid services.\n\n\nIâ€™m looking for open source tools for my project. BRAT and Inception are used for RE but Iâ€™m not considering them because of their limitations.\n\n\nLooking for open source tools for manual annotation\n\n\nThanks",
"date": "2021-09-06"
},
{
"vote": 1,
"title": "Help me build an app to help people learn fast! Hi guys, I am currently creating a new age app for language learning and I'd need your help to know exactly what you would like to see and and how it could best suit your needs :D thank you so much!! Please click the link below and fill out the form!!",
"text": "[removed]",
"date": "2021-09-06"
},
{
"vote": 1,
"title": "Project Help : Semi supervised text classifier",
"text": "Hi,\n\n\nI am planning to work on a semi supervised text classifier (NLP. Project), though I'm a master student.\n\n\nI have never worked before on semi supervised learning.\n\n\nSo if you have thought, Idea, resources, or this seems interesting to you and wanna Collab I'm happy to welcome you. \n\n\nAny kind of help will be very grateful.",
"date": "2021-09-06"
},
{
"vote": 1,
"title": "P, Q, R, S And T Are Placed In The Order Of 1 To 5. S Is At One Of The Extreme Ends. Q And R Are Neighbors And T Is 3rd To The Right Of P If R Is In Second Position, Then Who Will Be In Fourth Position? A) T B) Q C) P D) S",
"text": null,
"date": "2021-09-06"
},
{
"vote": 4,
"title": "What tech do I need to learn to programmatically parse ingredients from a recipe?",
"text": "Iâ€™m looking to create an app that can parse ingredients from recipes written in English.\n\n\nI have a general understanding that the above application is a form of NLP, but nothing more than this basic understanding. My questions are as follows:\n\n\n\n\nI think this falls in the realm of Natural Language Processing (NLP). Is that correct?\n\n\nWhat do I need to learn to write a program that can parse ingredients from recipes?\n\n\nGiven the above, does anyone have any suggested courses that I might want to take to learn what I need to learn?\n\n\nIâ€™m currently learning Kotlin â€“ can I write an app in Kotlin to parse ingredients from a recipe text?\n\n\nIf using Kotlin is a suitable approach, what technologies in addition to Kotlin should I look into for parsing ingredients from a recipe?",
"date": "2021-09-05"
},
{
"vote": 4,
"title": "Deep Natural Language Processing for LinkedIn Search Systems (Research Paper Walkthrough)",
"text": null,
"date": "2021-09-05"
},
{
"vote": 2,
"title": "Run ONNX Transformers pipelines",
"text": "&#x200B;\n\n\nhttps://reddit.com/link/pha6ef/video/4qjcbr5tsbl71/player\n\n\nThe example above shows how \ntxtai\n enables exporting models to \nONNX\n. The example loads an existing Transformers model, exports it to ONNX and then runs the sentiment analysis model with ONNX (scores are from 0-1 with 1 being most positive and 0 most negative).\n\n\nExported ONNX models can be directly loaded either with txtai or Hugging Face pipelines. Check it out!\n\n\nLink to Gist",
"date": "2021-09-03"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-09-02"
},
{
"vote": 9,
"title": "I want to start this project but I am unsure what to do",
"text": "For a bit of background, I have no experience in coding except very basic knowledge in Python but I am super willing to learn anything necessary.\n\n\nI have this idea for a competitor for Grammarly. Not a direct competitor, but one targeted at poets. I envision it being able to count syllables per line, rhymes for words, synonyms, antonyms, detect clichÃ©s and be able to guide a user to use a form of their choice. \n\n\nWith all these different components, I know it will take time, probably years to complete however I will just pick out a few to release a beta to customers to gauge interest and then carry on from there. So now you know what I want to do, how would you start this in my position? Would you dive right into the NLP part or fully learn Python then start?",
"date": "2021-08-31"
},
{
"vote": 10,
"title": "Looking for text highlighting / annotation library like displacy that can handle overlapping entities / texts",
"text": "I am looking for a  text highlighting library like displacy that can handle overlapping entities / matches. I am currently using displacy to highlight matched texts and named entities. One challenge with it is that it can't handle overlaps.\n\n\nI looked at \nhttps://github.com/tvst/st-annotated-text\n which doesn't handle overlaps either :(\nSearching for annotations excluding matplotlib results I don't get too many results.\nLooking for text highlighting libraries doesn't seem fruitful either as most of them are pdf based highlighting solutions.  \n\n\nProdigy's span manual annotation interface is an exmaple of what I am looking for \nhttps://prodi.gy/docs/api-interfaces#spans_manual\n but I am looking for a library where I can just display text like this though ideally using boxes rather than just lines",
"date": "2021-08-31"
},
{
"vote": 12,
"title": "I scrapped Amazon product reviews, I was wondering what analysis can I do on it",
"text": "I wrote a Selenium script that scrape Amazon product reviews and I want to get an insight from it, and to impress my potential employer - job interview is on Wednesday (the company buys Amazon brands). What insights and tools, l can I get from product reviews? I hope this this is the right place to ask. \n\n\nThank you for the help!",
"date": "2021-08-30"
},
{
"vote": 1,
"title": "ML Podcast!",
"text": "Hey guys! My friends and I have been working on a tech podcast and our latest episode on Machine Learning is out now!\n\n\nThis weekâ€™s episode is on Machine Learning, Artificial Intelligence and Data Science with our speaker Vaidheeswaran Archana who is an Artificial intelligence engineer at Continental and Leadership Fellow at Women Who Code.\n\n\nItâ€™ll be great if yâ€™all could check it out and get some amazing insights into the world of ML\n\n\nNow streaming on Spotify, Apple Music and all platforms that you love! Listen now: \n\n\nhttps://open.spotify.com/show/7550NpVvaE4pgaOvYo6xCp?si=OG9-FfENQxWjJ9Qd9KAJWw&nd=1\n\n\nhttps://podcasts.apple.com/in/podcast/the-techloop-podcast/id1528881215\n\n\n&#x200B;\n\n\nhttps://reddit.com/link/peng3g/video/j6t4cjlcbjk71/player",
"date": "2021-08-30"
},
{
"vote": 9,
"title": "How to extract item name from a given sentence?",
"text": "Hi,\n\n\nI am new to NLP.\n\n\nI have sentence like\n\n\n4-5 bone-in skin-on chicken thighs\n in this I need to extract  \nChicken thighs\n as Ingredient.\n\n\none more example\n\n\n2 cloves of garlic minced\n in this I need to extract \ngarlic\n as Ingredient\n\n\none final example\n\n\n2 serrano chiles minced (remove the seeds and membranes if you want it less spicy)\n in this I need to extract \nchiles\n as Ingredient\n\n\nPlease explain me a method on how to solve this using NLP.",
"date": "2021-08-30"
},
{
"vote": 2,
"title": "How can I identify flagged keywords from text?",
"text": "I have text data from expense receipts. I need to identify few items like alcohol, so I can mark those receipts.\n\n\nData format: 1 text file for each receipt text with trimmed spaces.\n\n\nIn future I might be supposed to find jewellery and cosmetics receipt types as well from their raw text.\n\n\nFor beginners I have a config file with related string / regex patterns which I am using to identifying few items.\n\n\nI need to improve performance of the system. Is there anything  I can refer for further enhancements, like a dataset for related regex patterns or list of alcohol items.\n\n\nI cannot use ML models to classify them as it will take my team some time to request for further resources.\n\n\nProgramming language: python",
"date": "2021-08-30"
},
{
"vote": 2,
"title": "Analyzing screenplay- NLP models for more than one \"author\"?",
"text": "I want to conduct a research on movie screenplay (e.g. punchline/ argument) but not sure what models I should be looking into....\n\n\nFor not the BERT models/ RNNs work towards one author. Is there any language model architecture that caters to multiple authors specifically (e.g. it can classify whether two people are having an argument based on a movie script)?",
"date": "2021-08-30"
},
{
"vote": 2,
"title": "[paper suggestion] Long text generation",
"text": "Hi there,\n\n\nI want to work on long text generation. I want to have your experiences : )\n\n\nWhich papers do you think are best/must to read? and which approaches do you think are the best?\n\n\nAlso if you know of any paper on long text generation with VAE, that would be great.\n\n\n&#x200B;\n\n\nThanks in advance.",
"date": "2021-08-30"
},
{
"vote": 7,
"title": "Automated Narrative Content from Data",
"text": "I am looking to automate some report writing based on tables of simple statistics such as percent change over time, ranking, share of total etc. I am aware of vendors that provide such services such as WordSmith, Arria and Narrative Science but am hoping to find an open source solution in Python. Is anyone aware of this NLG capability in Simple NLG, spaCy, a Markov Generator or any other option?\n\n\nThank you",
"date": "2021-08-29"
},
{
"vote": 7,
"title": "non-standard english dataset",
"text": "Hi!\n\n\nI am looking for a text dataset with any sort of non-standard english like Appalachian English or AAVE. Do you know where I could find something like this?\n\n\nThanks!",
"date": "2021-08-28"
},
{
"vote": 5,
"title": "Register for Truth and Trust Online 2021 Conference",
"text": "The \nregistration for Truth and Trust Online 2021\n is now open!\n\n\nThe annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space of online communications.\n\n\nRegister to hear from our keynote speakers \nKalina Bontcheva (Sheffield University), Gianluca Stringhini (Boston University) and Lyric Jain (LogicallyAI)\n, and join the discussions on the future of online communications! \nhttps://truthandtrustonline.com/register/\nAlso, we have a f\nee waiver program\n in place for those who may not have the financial resourcesÂ that permitÂ registration. Find the form here (theÂ deadline to apply is October 4th, 2021): \nhttps://docs.google.com/forms/d/e/1FAIpQLSdfKlquWRahjRPY0kyoinH8L1_irZXu712XLd6oaxnZwn73MQ/viewform\nÂ Looking forward to seeing you in October!",
"date": "2021-08-27"
},
{
"vote": 1,
"title": "question",
"text": "[removed]",
"date": "2021-08-27"
},
{
"vote": 5,
"title": "EMNLP 2021 Main Conference - Notification of Acceptance",
"text": "Just as the title says. You should hopefully have it in your email already.",
"date": "2021-08-26"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-08-25"
},
{
"vote": 9,
"title": "11 Best Natural Language Processing Online Courses",
"text": null,
"date": "2021-08-25"
},
{
"vote": 1,
"title": "Debate about Ethics in AI",
"text": "[removed]",
"date": "2021-08-25"
},
{
"vote": 1,
"title": "Does anyone have any experience with a good transformer for customer interaction data?",
"text": "Basically I want to build a self supervised learning algorithm to understand whatâ€™s the main topics of the calls are? I have used BERT, Universal Sentence encoder, And a few other transformers from spacy. Anybody have any good luck with anything from hugging face?",
"date": "2021-08-24"
},
{
"vote": 4,
"title": "New article on random projection for LSH + how we implement in Faiss",
"text": "Hi all, I put \nthis article (and videos)\n together covering random projection in LSH - after recently covering shingling, minhash, and the 'traditional' LSH. This approach is the most popular implementation and it's what is used over in Faiss for \nIndexLSH\n. I've included a simple Python implementation + the Faiss implementation.\n\n\nI hope you enjoy it!",
"date": "2021-08-24"
},
{
"vote": 2,
"title": "NLP techniques for converting from a direct speech to a reported speech",
"text": "Hello,\n\n\nAny idea of some NLP techniques to transform a direct speech to a reported speech ?\n\n\nExample converting : \n\"I'm learning NLP\" said a user\n to : \na user said he's learning NLP.\n\n\nI thought about paraphrasing but not sure..\n\n\nThank you!",
"date": "2021-08-24"
},
{
"vote": 3,
"title": "What is the best speech-to-text engine based on ease of use, speed, and accuracy?",
"text": "I personally am looking for a good python library for speech to text, having used a variety of difficult to set up or expensive frameworks in the past. By now I imagine we have a variety of pip installable python libraries that should be pretty fast and accurate, no?\n\n\nMost upvoted response wins?",
"date": "2021-08-23"
},
{
"vote": 23,
"title": "What company would you consider the most exciting and/or promising language technology startup today?",
"text": "(and why)",
"date": "2021-08-23"
},
{
"vote": 4,
"title": "Need help with NLP project",
"text": "I've started working on a project that requires \"freeform\" text attributes to be transformed and used as features in a classifier. It's a pretty expansive and complicated project so they will not be the only features, but currently those are the ones I'm responsible for. I've started some work on it but have currently hit a wall and I'm hoping there are some NLP experts who can provide some guidance. I'd like to keep this post as short as possible so I won't go into too much detail but more than happy to elaborate in comments or DM.\n\n\n&#x200B;\n\n\nBasically, where I'm stuck is that there are over 8,000 \"unique\" attributes, but many are variations of the same thing. Some examples, \nshort sleeve\n is also entered in as 'ss', 's/s', 'shrt sleev', 'short sleev', 'shrtslv', 'shortsleeve', etc. Or \nboxer briefs\n are also 'boxerbriefs', 'bxr briefs', 'bxrbrfs', 'b briefs', etc. They also aren't in sentences so there's not much context around them. There are also a lot of acronym such as \nTB\n for \ntoddler boy's\n or \nTG\n for \ntoddler girls\n. This isn't the extent of the issues I've run into but the one that seems to be the biggest problem for now.\n\n\n&#x200B;\n\n\nIn short, they are a mess and I'm at a loss as to how to approach this other than manually inspecting every attribute and create some sort of mapping to standardize them. But even this I'm skeptical will be a viable long term solution as the model will eventually be in production and receiving new data so if they keep these types of attributes there's no way of knowing what other creative ways people can come up with to say things like \"short sleeve.\"\n\n\n&#x200B;\n\n\nAny help at all would be immensely appreciated!",
"date": "2021-08-23"
},
{
"vote": 2,
"title": "Provision.ai MLOPS",
"text": "[removed]",
"date": "2021-08-23"
},
{
"vote": 1,
"title": "Extracting data / predicting data from word document?",
"text": "Hello guys, I wanted to ask is there an NLP, ML algorithm that takes a Word Document and extract some data from it\n\n\nFor example, let's say we have a template of a word document with only some sections, paragraphs, or phrases that changes for each document how can I extract these paragraphs/phrases\n\n\nI thought about an algorithm that memorizes the template and then whenever I feed him the new document he tries to figure out the new phrases, paragraphs but it's not practical nor optimized.\n\n\nDoes anyone have an idea or a similar had a similar situation and found a solution?",
"date": "2021-08-22"
},
{
"vote": 8,
"title": "Identify vocabulary that is characteristic of a genre",
"text": "I am working on a project where I need to produce lists of words that are characteristic of some specified genre or topic. As a crude example, if I input the genre 'fantasy' I would like to get an output something like ['magic', 'dragon', 'necromancer', 'dungeon', 'invisible', ...]. The words should be fairly specific to that genre compared to some baseline, but broad enough to include all the genre-specific vocabulary a writer would want to use. They don't have to be exclusive to a genre.\n\n\nAs a first attempt, my plan is to build a list of the genres I want to support and find some corpora representative of them. I will then build a list of unique lemmas from each corpus, excluding the functional vocabulary. I will then further exclude all the lemmas that occur frequently in some larger baseline corpus, e.g. news articles. \n\n\nHas someone attempted this before? Is there a better way?",
"date": "2021-08-22"
},
{
"vote": 23,
"title": "Visual-heavy article covering Shingling -&gt; MinHash -&gt; LSH",
"text": null,
"date": "2021-08-20"
},
{
"vote": 5,
"title": "Tips on training your own speech-to-text transcriber",
"text": "Background:\n \n\n\nI am looking at the Google \nspeech-to-text transcriber api\n and it cost about ~$1for a 1 hour transcription!! Given that we're in a virtual world where most transcription tools will be transcribing hours of content, this can lead to \n> $200/mo per user\n at 20-25 hours of transcription a week. Does anyone have any idea how other companies pay for this API?\n\n\nGCP Pricing Quote for Speech to Text API:\n\n\nhttps://cloud.google.com/products/calculator#id=96249467-014d-4aa4-a9a9-9deba0f4b8d9\n\n\nAsk:\n\n\n I am currently looking into Modzilla DeepSpeech and Kaldi. Any tips or estimates of what this would take to fine tune for phone/zoom calls?",
"date": "2021-08-19"
},
{
"vote": 7,
"title": "Which models are best for template based text generation",
"text": "Hey guys,\n\n\ncurrently, I am trying to produce Text in a Template based manner. This means that I want my output to have each the same sentence structure and have blanks filled with changing data. \n\n\n&#x200B;\n\n\nExample this could be a Template:\n\n\n___ is a ___ serving ___ in the ____ price range \n\n\n\nThe blanks should than be filled out accordingly to my dataset and result in a bunch of text generation like this:\n\n\nRomana is a restaurant serving italian food in the low price range\n\nEl toro is a coffee shop serving spanish coffee in the moderate price range\n...\n...\n\n\n\nAnd so on.  Of course the Template should not be completely static, for this have a look at ->\n\n\nThis Paper\n.  Here it showcases exactly what I want to achieve, for better illustration check out Figure 1 in this paper. We can see that the template words are interchangeable depending on the blank filled-in words.\n\n\nSadly the Paper focuses more on how to find such templates in a corpus and not so much on how to generate the text.\n\n\n&#x200B;\n\n\nI would like to know if anybody knows how text generation with such templates is achieved and which models should be preferably used (LSTM, GRU or pre-trained models like GPT 2 and BERT) ?",
"date": "2021-08-19"
},
{
"vote": 12,
"title": "New to NLP: Various questions",
"text": "I'm new to NLP, so I'm not very knowledgeable about which techniques and libraries to use and when they are more useful.\n\n\nIn particular.....\n\n\nStemming vs lemmatisation. - they both do slightly different things. Is one better than the other? Would you ever use both in the same program?\n\n\nI've heard of bag of words and word2vec, but from what I can see on the internet, these are a bit outdated now - FastText seems to be the new thing.\n\n\nI'm currently trying to make an intent-based chatbot with Catalyst (and maybe Keras.NET if it's needed). \n\n\nAre there certain techniques and software that would be ideal/optimal for this type of chatbot?\n\n\nAny advice you can provide would be helpful.\n\n\nThanks.",
"date": "2021-08-19"
},
{
"vote": 5,
"title": "Any software or APIs to listen to videos and transcribe into IPA (international phonetic alphabet)?",
"text": "I'm looking for state-of-the-art tools available to listen to a video of people talking which can automatically generate the IPA symbols for what it hears. Any suggestions? I'll consider paid solutions as well. Preferably something that tries to block out background noise and only transcribes voices.\n\n\nSide question: what's the SOTA on this? Is the best IPA auto-transcription more accurate than language transcription?",
"date": "2021-08-18"
},
{
"vote": 7,
"title": "Labelling NER, sentences vs paragraphs?",
"text": "Hi everyone,\n\n\nSo far I have just been classifying sentiment of text, but now looking to tinker with NER.  When labelling, is it preferable to label sentences?  Sometimes the text I'm examining requires context from nearby text (Sometimes whole documents are necessary for context).  Does it harm training if I put the entire document in as one task/training example?",
"date": "2021-08-17"
},
{
"vote": 1,
"title": "How to remove words from sentences that are non-English.",
"text": "Iâ€™d like to remove sentences from dataset that have  â€œHindi written in english wordsâ€ or â€œArabic written in English wordsâ€. Any suggestions ?",
"date": "2021-08-17"
},
{
"vote": 5,
"title": "What is an utterance?",
"text": "Hi all,\n\n\nI'm a bit confused with the term \"utterance\". I've found 2 examples which are contradicting:\n\n\nHere:\n\n\nhttps://www.youtube.com/watch?v=FtHNDDLHMI4\n\n\nAn utterance is a \"A continuous piece of speech without any pause in it\". In the example with the 2 sentences there can be over 20 utterances.\n\n\n&#x200B;\n\n\nBut here:\n\n\nhttps://arxiv.org/pdf/1802.08379.pdf\n (Table 3: Data format of EmotionLines)\n\n\n&#x200B;\n\n\n>speaker     Rachel\n>\n>utterance   Hi Joey! What are you doing here?\n>\n>emotion     joy\n>\n>speaker     Joey\n>\n>utterance   Uhh, well Iâ€™ve got an audition down\n>\n>the street and I spilled sauce\n>\n>all over the front of my shirt.\n>\n>You got an extra one?\n>\n>emotion     neutral\n>\n>speaker     Rachel\n>\n>utterance   Yeah, sure. Umm... here.\n>\n>emotion     neutral\n\n\nalthough it's obvious that there are pauses between the pieces of speech, the speech of each speaker is count as a single utterance.\n\n\nSo who is right? Is it defined how big is the pause?\n\n\nIs there another term for the speech block of each speaker?\n\n\nAre there any online tools or libraries that can find utterances in an audio file?",
"date": "2021-08-17"
},
{
"vote": 7,
"title": "Extracting commonly mentioned facts from a list of texts",
"text": "[deleted]",
"date": "2021-08-15"
},
{
"vote": 1,
"title": "Precision Recall Break Even Point (PRBEP) question",
"text": "Currently revising for an NLP exam at uni and Jurafsky/Manning books didn't solve the query.\n\n\nThe break even point is the value at which precision is equal to the recall. BUT if the first result is not relevant and there are other relevant results further down the line, there will be at least 2 points in the curve where precision @ k == recall @ k (see table)\n\n\nWhat would be the PRBEP in the following system: 0 or 0.25?\n\n\n&#x200B;\n\n\n\n\n\n\n\n\nPOSITION\n\n\nRELEVANCE\n\n\nPRECISION @ K\n\n\nRECALL @ K\n\n\n\n\n\n\n\n\n1\n\n\n-\n\n\n0\n\n\n0\n\n\n\n\n\n\n2\n\n\n+\n\n\n0.5\n\n\n0.25\n\n\n\n\n\n\n3\n\n\n-\n\n\n0.33\n\n\n0.25\n\n\n\n\n\n\n4\n\n\n-\n\n\n0.25\n\n\n0.25\n\n\n\n\n\n\n5\n\n\n+\n\n\n0.4\n\n\n0.5\n\n\n\n\n\n\n6\n\n\n-\n\n\n0.33\n\n\n0.5\n\n\n\n\n\n\n7\n\n\n-\n\n\n0.29\n\n\n0.5",
"date": "2021-08-15"
},
{
"vote": 3,
"title": "Tensorflow or Rasa for voice based chatbots?",
"text": "Hi guys good day, I would like to ask the experienced people here some tips. I would like to learn NLP and integrate it to my web services. I understand that advanced algebra, calculas, statistics is required. I am ok with that part as I did electronics engineering and learning advanced maths is also one or our university requirements. Just concern which one will work best in web app based application. I am planning to run my services in nodejs. Thanks in advance for your assistance.",
"date": "2021-08-14"
},
{
"vote": 11,
"title": "3 Steps Process to Make Google T5 Transformer - Train on any subreddit with ease",
"text": null,
"date": "2021-08-14"
},
{
"vote": 14,
"title": "NLP Method(s) for Finding Commonalities?",
"text": "I'm working on an NLP project and I was curious if there is a method or a Python package that will find commonalities between a group of words. Here's an example:\nInput: \"Mercury Venus Earth Mars Jupiter\"\nOutput: \"Planets\" or \"Space\"\nDoesn't have to exactly output this, but something of this nature. Any suggestions for pre-created Python libraries or implementation ideas?",
"date": "2021-08-13"
},
{
"vote": 0,
"title": "Help Wanted: Looking for a Writing Critique Database",
"text": "I am developing a tool that will help you understand where your writing could be improved. \n\n\nFor this, I am looking for a large collection of essays paired with critiques. Does anyone know of websites or datasets with examples of writing critiques? Preferably the writing would feature a range of experience levels, from early school-age writers to advanced college writers or professional authors. \n\n\nAny help is appreciated! Thanks.",
"date": "2021-08-13"
},
{
"vote": 1,
"title": "RST Compression/Tree and Syntactic Compression / Patterns",
"text": "Hi, Has anyone worked with the above topics before? I need help in their implementation in python. if you could guide me to the right resources or explain in the comments it would be great. \n\n\n&#x200B;\n\n\nPS: I tried searching myself but couldn't find insightful resourses",
"date": "2021-08-13"
},
{
"vote": 6,
"title": "TextFeatureSelectionEnsemble for scalable and higly accurate text classification",
"text": "Use of document frequency, ensembling and genetic algorithm to develop highly accurate and scalable nlp models.\n\n\nTextFeatureSelection has a new module TextFeatureSelectionEnsemble for just that.\n\n\nIt combines the power of\n\n\n\n\nDocument frequency and grid-search for feature selection for NLP models.\n\n\nEnsembling multiple NLP models\n\n\nFeature selection for ensemble model using genetic algorithm to reduce number of base learner models.\n\n\n\n\nhttps://pypi.org/project/TextFeatureSelection/",
"date": "2021-08-13"
},
{
"vote": 1,
"title": "Top NLP journals?",
"text": "[deleted]",
"date": "2021-08-12"
},
{
"vote": 7,
"title": "Paraphrasing a sentence and changing the tone of it",
"text": "I am trying to make a model that is capable of translating a sentence  into a new and a better form. I would like the model to change the tone  and also give it some character. I am using this in my web app UI,  simply allowing the users to witness new description as they refresh the  page. For example, \"You are logged out\" -> \"Looks like you have  logged out\". Something of such sort, any idea on this?",
"date": "2021-08-12"
},
{
"vote": 13,
"title": "Zero-shot learning for text classification",
"text": "Hello,\n\n\nAfter a first article about few-shot learning, I decided to write a new one about zero-shot learning applied to text classification:\n\n\nhttps://nlpcloud.io/zero-shot-learning-for-nlp-text-classification.html\n\n\nI'm not going into too many details. The idea is to explain what few-shot learning is, and how it is making text classification much more flexible, thanks to Transformers.\n\n\nFor those who heard about few-shot learning but don't know what it is exactly, I hope it will help!\n\n\nFeel free to comment!",
"date": "2021-08-12"
},
{
"vote": 15,
"title": "Best Resources to Learn Natural Language Processing(Books, YouTube...)",
"text": null,
"date": "2021-08-12"
},
{
"vote": 9,
"title": "Human Annotation Not Used in Business to Evaluate Model Performance",
"text": "Every academic paper I've read uses at least one data set that has been annotated by human(s) to set the gold standard for desired output. Then the model under evaluation is scored against that standard to produce precision and recall numbers. Simple.\n\n\nBut then you start looking at for-profit text analytics companies who actually deploy NLP models and develop their own pipelines. Most of them practically brag about the fact that they never ever use human annotation for any reason. At least, in my (possibly limited) experience.\n\n\nAll of them are constantly working to improve their ML pipelines that create insights into unstructured text data. But how on earth do they know if a change to the pipeline is having a desired or undesired result if they don't have an annotated data set against which to grade the output? I feel like I'm missing something.",
"date": "2021-08-11"
},
{
"vote": 4,
"title": "What are some good Research paper on Social Media Sentiment Analysis",
"text": "I am currently doing a minor project on bitcoin sentiment Analysis using  Twitter Data on Python and tweepy and textblob  are the main libraries that are used. I want to get a deep insight about how Subjectivity and polarity is calculated using text blob and also in general.",
"date": "2021-08-10"
},
{
"vote": 9,
"title": "Article + video on choosing the right index for similarity search",
"text": "Here's a big article\n I put together covering a few of the various indexes we can use in similarity search - like LSH, HNSW, and IVF. Tried to keep it as visual as possible! (personally, I learn \nmuch\n better with visuals)\n\n\nI hope you like it, there's a video in there too if you're not a big fan of articles, thanks all! :)",
"date": "2021-08-09"
},
{
"vote": 19,
"title": "Fine-tuning GPT-J-6B",
"text": "Through the use of DeepSpeed, one can fine-tune GPT-J-6B given they have high-end(though still relatively affordable) hardware. This video goes over how to do so in a step-by-step fashion.\n\n\nhttps://youtu.be/fMgQVQGwnms",
"date": "2021-08-08"
},
{
"vote": 10,
"title": "Coherence score for Top2Vec models",
"text": "I am using Top2Vec, which I am finding to be a really cool package by  \nu/ddangelov\n.   I am trying to calculate the coherence score for a number of models   with different hdbscan parameters (specifically min_cluster_size).   However, I am running into problems doing so. Specifically, it seems the   gensim package will fail on words in the topics not present in the   dictionary.\n\n\nHas anyone tried this before? Could you share your code? The closest I could find to answer my question is \nhere\n (\nthis\n is what I'm hoping to see eventually).",
"date": "2021-08-06"
},
{
"vote": 1,
"title": "Project to identify",
"text": "Hello, I need a project to identify the language a document is written in. The suggested programming language is Python. Hoping for the suggentions.",
"date": "2021-08-05"
},
{
"vote": 1,
"title": "Readefine - Reword the Internet",
"text": "Hi there,\n\n\nI recently built a browser extension called Readefine that lets you reword the internet. Check it out: \nhttps://www.getreadefine.com\n\n\nBy default, Readefine automatically simplifies thousands of words and phrases, but you can also create your own personal dictionary or contribute and use community dictionaries that reword certain domains - it's even used for passively learning the vocabulary of other languages (members of the community have created Spanish and Numu, or Paiute, dictionaries so far), sort of like Toucan, but created by members of the Readefine community.\n\n\nReadefine is available as an extension on Chrome, Safari, Firefox, and Edge, and it's also available as an iOS Safari extension if you're on the iOS 15 Beta. You can download it on your browser at \nhttps://www.getreadefine.com/installreadefine",
"date": "2021-08-05"
},
{
"vote": 1,
"title": "Large parallel corpus of English and Farsi text",
"text": "[removed]",
"date": "2021-08-05"
},
{
"vote": 45,
"title": "Google AI Introduces Two New Datasets, â€˜TimeDialâ€™ and â€˜Disfl-QAâ€™, For Conversational NLP (Natural Language Processing)",
"text": "Natural language processing (NLP) has made significant advancements in recent years, with applications in learning, comprehending, and generating human language content. However, one of the greatest challenges in NLP is designing conversational bots that can understand and reason about distinct linguistic phenomena specific to natural speech.\n\n\nPeople do not always plan out precisely what they will say, and disfluencies, or interruptions in speech, are common in spontaneous conversations. Simple disfluencies (such as interjections, repetitions, restarts, or corrections) interrupt the flow of a sentence, while more complicated semantic disfluencies modify the underlying meaning of a phrase. Furthermore, understanding a conversation frequently requires an awareness of temporal linkages and relationships between events, such as whether one incident precedes or follows another.Â \n\n\nQuick Read: \nhttps://www.marktechpost.com/2021/08/05/google-ai-introduces-two-new-datasets-timedial-and-disfl-qa-for-conversational-nlp-natural-language-processing/\n \n\n\nPaper (TIMEDIAL): \nhttps://arxiv.org/abs/2106.04571\n\n\nGithub (TIMEDIAL): \nhttps://github.com/google-research-datasets/timedial\n\n\nPaper (Disfl-QA): \nhttps://arxiv.org/abs/2106.04016\n\n\nGithub (Disfl-QA): \nhttps://github.com/google-research-datasets/disfl-qa\n\n\nGoogle Blog: \nhttps://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html",
"date": "2021-08-05"
},
{
"vote": 5,
"title": "Finding a English Wikipedia dump",
"text": "I need help finding a complete English Wikipedia dump, text only.\n\n\nI downloaded the \nenwiki-latest-pages-articles.xml.bz2\n from \nhere\n (around 18GB), assuming it is the latest English dump with all pages and articles.\n\n\nWith the help of \nwikiextractor\n, i was able to query it and process the dump. However, when i start inspecting it, some articles are empty. For example \nAccessibleComputing\n should not be empty, but the dump gave:\n\n\n&lt;page&gt;\n    &lt;title&gt;AccessibleComputing&lt;/title&gt;\n    &lt;ns&gt;0&lt;/ns&gt;\n    &lt;id&gt;10&lt;/id&gt;\n    &lt;redirect title=&quot;Computer accessibility&quot; /&gt;\n    &lt;revision&gt;\n      &lt;parentid&gt;854851586&lt;/parentid&gt;\n      &lt;timestamp&gt;2021-01-23T15:15:01Z&lt;/timestamp&gt;\n      &lt;contributor&gt;\n        &lt;username&gt;Elli&lt;/username&gt;\n      &lt;/contributor&gt;\n      &lt;minor /&gt;\n      &lt;comment&gt;shel&lt;/comment&gt;\n      &lt;model&gt;wikitext&lt;/model&gt;\n      &lt;format&gt;text/x-wiki&lt;/format&gt;\n      &lt;text bytes=&quot;111&quot; xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Computer accessibility]]\n\n{{rcat shell|\n{{R from move}}\n{{R from CamelCase}}\n{{R unprintworthy}}\n}}&lt;/text&gt;\n      &lt;sha1&gt;kmysdltgexdwkv2xsml3j44jb56dxvn&lt;/sha1&gt;\n    &lt;/revision&gt;\n  &lt;/page&gt;\n\n\n\nCould i get a little help on finding the complete dump?",
"date": "2021-08-05"
},
{
"vote": 3,
"title": "Which is the best opensource model for text generation?",
"text": "Mission: to create an opensource chatbot which responds using natural language generation.  If you have some guidelines please share. I am open for collaboration( **let's build it**), you can text me or comment. \n\n\nIt will be so wonderful of you if you can share related articles or throw some light on possible road blocks.\n\n\nI 'll be 26 on this 31st and I want to complete it before that.",
"date": "2021-08-05"
},
{
"vote": 8,
"title": "In-depth Lab-Based training Work Shops",
"text": "Hi All,\n\n\nI'm looking for Lab-based workshops that would be similar to those offered in Nvidias Deep learning Institue. Linked an example of what is covered in one of them.\n\n\nLink\n\n\nSpecifically looking for workshops that go beyond the introduction of concepts and are more offering an in-depth insight into a specific topic within NLP, e.g. Named Entity Recognition. Where the theory is covered but also some practical implementation. \n\n\nOnline, In-person, free or paid is all fine.\n\n\nAny recommendations would be much appreciated.\n\n\nThanks",
"date": "2021-08-05"
},
{
"vote": 1,
"title": "TensorFlow 2.0 - Three Best In-Breed Programs for Career Growth",
"text": "[removed]",
"date": "2021-08-03"
},
{
"vote": 2,
"title": "Chrome plugin based on GPT-J which lets you generate text",
"text": "https://chrome.google.com/webstore/detail/type-j/femdhcgkiiagklmickakfoogeehbjnbh\n\n\nIt allows you to generate infinite amount of text suggestions\n\n\nWhen typing in textarea, press TAB to see a suggestion. Press arrow down to show next suggestion, or arrow right to increase suggestion size. Repeat until you find a text that suits you\n\n\nYou can try it out in the comments\n\n\nPlease let me if you find if useful for any case",
"date": "2021-08-03"
},
{
"vote": 1,
"title": "What language model is Siri built on?",
"text": "title. \n\n\nhelp, please",
"date": "2021-08-03"
},
{
"vote": 20,
"title": "Facebook AI Releases â€˜VoxPopuliâ€™, A Large-Scale Open Multilingual Speech Corpus For AI Translations in NLP Systems",
"text": "With the wide-scale use of speech recognition and translation technologies, these AI systems can be implemented in many different languages. But at this point, they are only available for a handful of widely spoken languages like English or Mandarin â€“ thereâ€™s still plenty to do before it will work with all 6,500+ other human tongues.\n\n\nFacebook AI is releasing, \nVoxPopuli\n, a collection of audio recordings in 23 languages with 400,000 hours to help accelerate the development of new NLP systems. The VoxPopuli data set also includes transcribed speeches from 15 different languages and oral interpretation into 17 target language written translations for over 1,800 total hours.\n\n\nQuick Read: \nhttps://www.marktechpost.com/2021/08/02/facebook-ai-releases-voxpopuli-a-large-scale-open-multilingual-speech-corpus-for-ai-translations-in-nlp-systems/\n \n\n\nGithub: \nhttps://github.com/facebookresearch/voxpopuli\n?\n\n\nPaper: \nhttps://arxiv.org/abs/2101.00390",
"date": "2021-08-02"
},
{
"vote": 6,
"title": "How to Label Text Classification Training Data â€” With AI",
"text": null,
"date": "2021-08-01"
},
{
"vote": 3,
"title": "Fine Tuning BERT for making concept maps for input text",
"text": "[deleted]",
"date": "2021-07-31"
},
{
"vote": 0,
"title": "How to fine-tune a transformer like GPT-J or GPT-Neo on numerical scores of some variable?",
"text": "I don't think fine tuning these models with a score in the text is a good idea given how poorly GPT-3 performs on math.\n\n\nHowever, OpenAI does have a feature where you can do a semantic search and they return a number of how similar a piece of text is to a given piece of text. \n\n\nThey may be doing this perhaps via embeddings, or via probabilities, I have no idea.\n\n\nBut what I am most interested in is fine-tuning based on a number, somehow externally. (Meaning the number is not in the text data), something akin to contrastive learning or something. \n\n\nFor example:\n\n\n&#x200B;\n\n\n\n\n\n\n\n\nSentence 1\n\n\nSentence 2\n\n\nDifference of Opinion\n\n\n\n\n\n\n\n\nI'm a republican\n\n\nI'm a supporter of Bernie Sanders\n\n\n-1\n\n\n\n\n\n\nI think that science should study itself\n\n\nIt's possible to know if something is true simply by looking at the evidence. Scientists may have biases but that's not that significant.\n\n\n-.5\n\n\n\n\n\n\nWhy? \n\n\nI don't just want to measure the difference of opinion, there are lots of other scenarios where I want to measure some number relative to 2 sentences that is NOT similarity and where semantic similarity would fail. \n\n\nWhy do I want to fine-tune GPT-J instead of use another specialized huggingface model or make my own model?\n\n\nBecause it has read so much, I believe it'll perform \nway\n better at this task than any model I could train on my own because it can recognize entities and \nso much more known unknowns.",
"date": "2021-07-31"
},
{
"vote": 27,
"title": "Announcing Hora 0.1.0, an blazingly fast approximate nearest neighbor search algorithm library",
"text": "I'm glad to announce we have released Hora0.1.0, an approximate nearest neighbor algorithm library, which is written in rust. and focus on the approximate nearest neighbor search field, we have already implemented HNSW(Hierarchical Navigable Small World Graph Index) index, SSG(Satellite System Graph)index, PQIVF(Product Quantization Inverted File) index, BruteForceIndex, and other indexes are coming. and we use SIMD to accelerate the performance, make it blazingly fast.\n\n\nour slogan is \"hora search everywhere\", which means hora can be deployed in any OS platform, Including already supported PC OS, (Linux, Mac OS, Windows), will support portable device OS(IOS and android), and even will support embedded systems(no_std). and we would support many language bindings, including Python, Javascript, Java, Ruby, Swift, and R. thanks to the LLVM great portable feature, we can make it happen.\n\n\ngithub:\n \nhttps://github.com/hora-search/hora\n\n\nhomepage:\n \nhttps://horasearch.com/\n\n\npython library:\n \nhttps://github.com/hora-search/horapy\n\n\nyou can easily install horapy:\n\n\npip install -U horapy\n\n\nfor \nhorapy\n demo, you can check our github\n\n\nhere is our online demo (you can find it on our homepage)\n\n\n**ðŸ‘©Â Face-Match [**\nonline demo\n] (have a try!)\n\n\nhttps://reddit.com/link/ov7akh/video/y2tjq791xje71/player\n\n\n**ðŸ·Â Dream wine comments search [**\nonline demo\n] (have a try!)\n\n\nhttps://reddit.com/link/ov7akh/video/yv9n7ne3xje71/player\n\n\nhere is the features:\n\n\nPerformant\n: 1. SIMD-Accelerated (packed_simd), 2. Stable algorithm implementation, 3. Multiple threads design\n\n\nReliable and Productive\n: 1. Rust compiler secures all code, 2. Memory managed by Rust for all language libs such as horapy, 3. Broad testing coverage, 4. Well documented, 5. Elegant and simple API, easy to learn\n\n\nPortable\n: 1. Support Windows, Linux, and OS X, 2. No heavy dependency, such as BLAS\n\n\nMultiple Languages Support\n: 1. Rust, 2. Python, 3. Javascript\n\n\nMultiple Distances Support\n: 1. Dot Product Distance, 2. Euclidean Distance, 3. Manhattan Distance, 4. Cosine Similarity\n\n\nwe are pretty glad to have you participate, any contributions are welcome, including the documentation and tests. We use GitHub issues for tracking suggestions and bugs, you can do the Pull Requests, Issue on the github, and we will review it as soon as possible.\n\n\ngithub:\n \nhttps://github.com/hora-search/hora",
"date": "2021-07-31"
},
{
"vote": 16,
"title": "Class imbalance issues in NER task",
"text": "My question is - how do we deal with class imbalance issues in NER tasks? What is the standard way?\n\n\nI have been searching for this but most answers were saying that it is an issue that is not easy to solve. I have to admit my lack of research skills but i really could not find an answer to this question. \n\n\nSpecifically, if the number of Entities is small, it is very easy for the \"O\" tag to be dominant (in my case it is 99%). I tried taking weighted loss, or cutting up the sentences into small pieces but to no fruitful results.\n\n\nNER tasks are such an iconic NLP task so i presume there is some sort of a standard way to deal with it. If there isnt any, i would like to hear how u fellow researchers approached it! Thank you so much in advance!!\n\n\n[EDIT] just to add a little bit more, i identified the proportion of occurrences of each tokens and inverted that to give weights to the loss. The model does not overfit this way, but the loss doesnt drop",
"date": "2021-07-30"
},
{
"vote": 0,
"title": "Getting Started with Google BERT: Build and train state-of-the-art natural language processing models using BERT PDF - Click on Open and then click on Download, Saving you from Chaos",
"text": "[deleted]",
"date": "2021-07-30"
},
{
"vote": 15,
"title": "BERT: Why are special tokens are not appearing as predictions in masked language modelling?",
"text": "[deleted]",
"date": "2021-07-30"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-07-30"
},
{
"vote": 17,
"title": "We built the Italian CLIP ðŸ‡®ðŸ‡¹",
"text": "During the HuggingFace JAX Community Week, we had the chance to specialize OpenAI's CLIP for the Italian language ðŸ¤Œ\n\n\nCLIP (Contrastive Languageâ€“Image Pre-training) is one of the most recent multi-modal models that connect images and text. The original model was limited to English, so we decided to extend its capabilities.\n\n\nThe model is already available online! HF hub: \nhttps://huggingface.co/clip-italian/clip-italian\n\n\nThe demo and paper links are coming out soon!\nGitHub repo: \nhttps://github.com/clip-italian/clip-italian\nTwitter: \nhttps://twitter.com/peppeatta/status/1419593282682773507\n\n\nLook at some lit ðŸ”¥ results on Image Retrieval, Zero-Shot Classification, and a cool \"localization\" feature we added for interpretability ðŸ”Ž\nhttps://imgur.com/a/Yej1Lab\n\n\nLeave us a GitHub starâ­ï¸ or share this post to support this work!ðŸ¤—",
"date": "2021-07-29"
},
{
"vote": 5,
"title": "What are some good examples of using semantic role labeling for downstream sentence classification tasks?",
"text": "Working on a project where we want to classify whether sentences are about say, a patient's housing status, or the housing status of the patient's \nfamily/friends\n (or not about housing status at all). Manager thinks SRL would be useful for this, and I'm wondering if it's going to be worth the trouble. Not sure it'll be any better than just throwing some big DNN like CNN, LSTM, or BERT at this and making it a three-way sentence classification task.\n\n\nSo, I'm wondering what are some good papers showing how incorporating SRL features improve performance on some downstream classification task? I'm especially interested in seeing \nhow\n SRL features are incorporated.\n\n\nThanks!",
"date": "2021-07-29"
},
{
"vote": 2,
"title": "Language Detection - Pre Trained Models",
"text": "Hi guys, does anyone recommend any pre-trained models for language detection?\n\n\nI have currently tried:\n\n\n\n\nfasttext\n\n\npolyglot\n\n\nlangdetect\n\n\nlangid\n\n\nspark-nlp\n\n\n\n\nAnd yet all of those have a difficult time distinguishing between Portuguese and Spanish. I know its something prone to those type of misclassification but its there some library/modules/pre-trained models for language detection that I have missed? It doesn't need to be python specific.\n\n\nThanks",
"date": "2021-07-29"
},
{
"vote": 7,
"title": "DeepPavlov Community Call #11: TripPy for Goal-Oriented Chatbots, Multitask BERT, and Relation Extraction",
"text": "Happening right now! \n\n\nOur \n#GSoC\n Students present their work they've done at \n@deeppavlov\n in the last couple months!  \n\n\nCome join us to learn about \n#OpenSource\n Relation Extraction, TripPy architecture implementation in Go-Bot, and Multitask BERT! \n\n\nhttps://www.youtube.com/watch?v=Ud2vsSw__Dk",
"date": "2021-07-29"
},
{
"vote": 4,
"title": "Masked language modelling with specific entities or POS",
"text": "For a MLM task say with this input â€œThere are 42 [MASK] in [MASK]?â€. Is it possible to constrain the predicted [MASK] token to be a specific entity or POS? For example the second [MASK] token i would like the prediction to be a company name only so it may output something like:\n\n\n\n\nâ€œThere are 42 employees in Microsoft?â€\n\n\nâ€œThere are 42 developers in Microsoft?â€\n\n\nâ€œThere are 42 employees in Apple?â€\nâ€¦ etc\n\n\n\n\nI was thinking to use another model which is already capable of doing NER tasks correctly (classify company names) to jointly generate the sentence, but don't really know how to do it. I'm using the huggingface transformers library.\n\n\nAny possible solutions would be appreciated, thanks.",
"date": "2021-07-29"
},
{
"vote": 19,
"title": "UK PhD Opportunity: Text mining the impact of SARS-CoV-2 mutations from the research literature at University of Glasgow",
"text": null,
"date": "2021-07-28"
},
{
"vote": 3,
"title": "Detecting phrases that fit a CFG",
"text": "Hello. I'm trying to start a small based project in order to learn NLP hands-on and I would appreciate some help. I have: a very simple CFG that describes the structure of some artificial language. I would like to: parse a corpus, and detect phrases that fit the CFG and map them to the CFG. So I would like to extract from a natural language phrase \"Even in the best weather a BMW can appear from nowhere and crash your car\" something like [car1=BMW] [will sometimes] [hit] [car2=your car]. Phrases that do not fit the CFG should be discarded.\n\n\nI'm a newbie and I wouldn't want to reinvent the wheel. I'll be grateful for any pointers to papers, directions, etc. Should I just read Jurafsky until I know enough about constituency grammars etc?",
"date": "2021-07-28"
},
{
"vote": 1,
"title": "Can anyone see their EMNLP reviews/responses?",
"text": "[removed]",
"date": "2021-07-28"
},
{
"vote": 1,
"title": "It is important that the training data of a chatbot differentiate between interlocutors?",
"text": "Hello everyone, I am recently learning NLP with Python, i have a question that has been generated in relation to the development of a chatbot, if a traditional model is used for training, or a lstm, or a transformer, is it important for the model that training data are divided by interlocutor?, that is, I have some voice recordings, where more than two people talk about a certain thematic, I want to train a chatbot with those recordings, therefore I wanted to know itself it is important that those recordings/texts are separated by those people who spoke because perhaps you want to give a priority to one of those people because it has more knowledge about the thematic, have some information about this, examples? , Thank you.",
"date": "2021-07-27"
},
{
"vote": 0,
"title": "DFM --&gt; Topic model",
"text": "Hi, I've been trying all day to convert my DFM -> topic model. Im using the Quanteda tutorials. The code works all the way until:\n\n\ndtm= convert(dfm, to= \"topicmodel\")\n\n\nupon which, I get the following error message:\n\n\nError in convert(dfm, to = \"topicmodel\") : unused argument (to = \"topicmodel\")\nhelp??",
"date": "2021-07-27"
},
{
"vote": 11,
"title": "word2vec as a public notebook and/or service",
"text": "Hi, I love \nobservablehq.com\n for making visualisation and the low effort of not having to install any tools. I wanted to do some dataviz on words, and needed a way of embedding them. I could not find a free word2vec service so I made one myself, and maybe it is of interest to other people? It's all MIT licensed and the full steps of building the service are in the notebook. Happy to chat about architecture\n\n\nWhat I really like is being able to lean on the Observable visualisation tools so you can generate all kinds of pretty visualisations using the inbuilt Observable tooling without much effort\n\n\nAnyway the notebook is here (pictures are in there)\n\n\nhttps://observablehq.com/@endpointservices/word2vec\n\n\nAlso as it is a service you can also access it directly\n\n\ncurl &#039;https://webcode.run/notebooks/@endpointservices/word2vec/deploys/word2vec/mods/X/secrets/endpointservices_secretadmin_service_account_key?words=king,queen&#039;",
"date": "2021-07-27"
},
{
"vote": 8,
"title": "nltk pos tag with restriction on classes?",
"text": "Hi, newbie post here, I'm doing some scripts for data processing, I would need to perform Part of Speech tagging on some single lexical units, the problem is that some of them in English are both nouns (NN) and form of verbs (VB) e.g. \"holding\" and since they are presented as single words without context they are labeled as NN, while I know that they are VB from design.\n\n\nSince all the data that i have are only prepositions or verbs I would like to know if there was a way to use the pos_tag function restricting the classes of choice in some way, excluding the possibility to have e.g. NN and limiting the choice to preps and verbs.\n\n\nThanks for your help.\n\n\n(ps. I could do it with a list of prepositions I think, but it would be better to use nltk library for the kind of project I'm working on)",
"date": "2021-07-26"
},
{
"vote": 1,
"title": "Whole sentence rather than word frequency nltk?",
"text": "*still a beginner \n\n\nIs there a way to find the frequency usage of an entire sentence, rather than just a word in nltk?",
"date": "2021-07-25"
},
{
"vote": 2,
"title": "Deciding between two programs?",
"text": "[deleted]",
"date": "2021-07-25"
},
{
"vote": 1,
"title": "BERT: Is it possible to filter the predicted tokens in masked language modelling?",
"text": "[deleted]",
"date": "2021-07-25"
},
{
"vote": 2,
"title": "Dealing with non-deterministic result",
"text": "Hi all!\n\n\nI train a transformer seq2seq model to convert natural language to sequence of code. I use accuracy metric to measure the performance (I consider the generated sequence is valid if the generated sequence match with the ground truth). \n\n\nHowever, I have a hard time dealing with non-deterministic result. I use TensorFlow Keras. I compare 2 different approaches, but the different is not that much (ranging from 3% to 5%). I am wondering how do you guys report this non-deterministic result? I heard some papers use t-test or confidence interval, but I am not sure and need a clarification.\n\n\nThank you!\n\n\nNb: I know I can set the seed, but I think this is quite tricky.",
"date": "2021-07-25"
},
{
"vote": 2,
"title": "Databases for NLP in Spanish",
"text": "Hi,\n\n\nWhere can I find databases to do experiments on Spanish written language?\n\n\nIs this 2018  \nrepository\n repository up to date?",
"date": "2021-07-24"
},
{
"vote": 13,
"title": "Does a Master's in NLP/CL carry the same weight as a Master's in AI or ML?",
"text": "Generally, industry positions ask for people with graduate degrees in Computer Science, Statistics, Engineering, AI, or related subdisciplines, especially for positions like ML Engineer or Applied Scientist. Is a Master's in NLP or CL going to be valued as less rigorous or less relevant, or on par with these degrees, given that these degrees generally have a substantial linguistics background too? I'm specifically asking for general AI positions and if specialising in NLP would be detrimental.",
"date": "2021-07-24"
},
{
"vote": 1,
"title": "Pointers on making a grammer checker",
"text": "Spell checking is dead easy but grammar, a bit like the Grammarly service, how would one go about doing that with NLTK for python?  I'm probably going to go towards the the more rule end of things and not a neural network knowing what best fits for a certain type of text",
"date": "2021-07-23"
},
{
"vote": 1,
"title": "Is there a project or whatever it is that can convert results of SQL queries to natural language?",
"text": "Hi, I'm currently doing my graduation project, and I might gonna create some tools using text-to-sql.\n\n\nIs there a project or whatever it is that can convert results of SQL queries to natural language?",
"date": "2021-07-23"
},
{
"vote": 1,
"title": "Spacy transformer model - word embedding",
"text": "[removed]",
"date": "2021-07-22"
},
{
"vote": 1,
"title": "[D] Few Shot Learning - NLP",
"text": "Hi, Can you guys suggest to me research papers related to few-shot learning in NLP (to be specific in sentence similarity detection)?  \n\n\nThanks in advance.",
"date": "2021-07-22"
},
{
"vote": 10,
"title": "Are there any good tutorials for distilling LMs (i.e. GPT2)?",
"text": "I'm fine-tuning a larger GPT-2 model to a specific task but would like to distill it into a smaller model. I have a rough idea of how distillation works, but would love to find a hands-on tutorial. Any recommendations? thanks",
"date": "2021-07-20"
},
{
"vote": 3,
"title": "Generalization through Memorization: Nearest Neighbor Language Models (Research Paper Walkthrough)",
"text": null,
"date": "2021-07-20"
},
{
"vote": 9,
"title": "Spellchecker for NLP pipeline?",
"text": "I have a pipeline of NLP tasks set up in R, but the spellchecker dictionary I'm using (\nhunspell\n) marks a lot of correctly spelled words as misspelled. The spellchecker's only purpose in my task is to give a report of how many misspelled words there are. I think that the main issue is the wide variety of words that go through the pipeline. \n\n\nGiven the variety of (English) text that goes through this pipeline, any recommendations for a good alternative free or paid spellchecker dictionary? Hunspell has worked for ease of use, but it isn't as effective as I need it to be.",
"date": "2021-07-19"
},
{
"vote": 2,
"title": "Where to find a dictionary of (market,technology,business) tokens?",
"text": "Hey, how are you guys?\nI am currently developing a tokenizer that has to deal with tokens like\n\n\nA/B, z/OS, Vue.js, 9box\n that can't be spitted\n\n\nand I think that having a dictionary/vocabulary of such terms in hand could be pretty helpful, since I would know which tokens I'm allowed to split or not. \n\n\nI just can't find something like it online. Do you know any resource like this one? Where can I find it?",
"date": "2021-07-19"
},
{
"vote": 4,
"title": "Where to start",
"text": "I don't have much background in ML or NLP but I was hoping to find a tool to assist me in a game I'm making. I need some direction as to where to start looking/learning. I need the system to classify user input, especially questions, into exact parts so I can use that information to create a unique response. I also need to know whether or not I'm able to add context to the system(names of people places and things in the game world) to make it more accurate. Any help is much appreciated!",
"date": "2021-07-18"
},
{
"vote": 5,
"title": "SpaCy, how to create a pattern to match a string caught via SpeechRecognition?",
"text": "Hello there! I am having a little bit of problems figuring out how to approach the problem.\n\n\nFACT: I'm building an App for a role playing game (GURPS), who tracks damage  dealt to enemies from players. App itself is quite done, and i used  PySimpleGUI for the graphic interface. Next step, is to integrate vocal  commands, in order to enter input not from keyboard, but from voice  (because there are several inputs, so, why not?). So, i used SpeechRecognition library to catch the voice input, creating a  string variable who stores the input from user. Now I'm working on the  second part: from the string, extract the inputs. The last part would be  to stored those inputs into a dictionary and use it as input for my  functions.\n\n\nWHAT I'M TRYING TO ACHIEVE I'm having a lot of problem designing the matches with SpaCy. Because i  think there are no databases to train a NN or a ML model for my tasks,  i'm using \nRules Matching\n. In this way, any sentence has to be structurated in a certain way, in order to extract token as i'd wish. An example of sentece is this one: \"You hit the enemy zombie one, that has vulnerability 2, to the head, with a large piercing attack, dealing 8 damage\".\n\n\nThe inputs i have to extract are following:\n\n\n&#x200B;\n\n\n\n\nenemy hit: zombie one (is the enemy hit, and in the dataframe  created could be present zombie_1, zombie_2, etc..., in general,  multiple zombie, with a sequential number attached. Still try to  understand if will be a better idea to name them as zombie1, zombie2...)\n\n\nvulnerability \"number\"\n\n\nlocation hit: head in this case, but could be \"right arm\", that i  can't extract because tokenization sees them as 2 tokens instead of 1\n\n\nlarge penetration: type of attack (the easiest case in something  like \"cutting\", or \"crushing\", one word, easy to take, but i didn't find  any way to extract these tokens togheter, becuase how tokenization  works)\n\n\ndamage 8: the damage dealt\n\n\n\n\nPROBLEMS: I'm currently using DependencyMatcher. Main problems are:\n\n\n&#x200B;\n\n\n\n\nBecause tokenization works on single word, in the case said above, i  would lose the second part (right arm, extract only arm; large  penetration, only penetration).\n\n\nCan't generalize my pattern and i'm not sure \"DependencyMatcher\" is the right tool here. I'm working with Italian Language, but i'm testing in English for semplicity. My current script for english language is:\n\n\n\n\n&#x200B;\n\n\nstring = &quot;You hit the enemy zombie one, that has vulnerability 2, to the head, with a large piercing attack, dealing 8 damage.&quot;\n    nlp = spacy.load(&quot;en_core_news_sm&quot;)\n    doc = nlp(string)\n    # for token in doc:\n    #     print(token.text, token.dep_)\n    \n# i&#039;m going to create 2 lists with all words of body locations hit and type of attacks, in order to find the words via &quot;LOWER&quot; or &quot;LEMMA&quot; dependency (first part of list is in english, second part in italian)\n   \n    body_list_words = [&quot;Body&quot;, &quot;Head&quot;, &quot;Arm_right&quot;, &quot;Arm_left&quot;, &quot;Leg_right&quot;, &quot;Leg_left&quot;, &quot;Hand_right&quot;, &quot;Hand_left&quot;, &quot;Foot_right&quot;, &quot;Foot_left&quot;,\n                 &quot;Groin&quot;, &quot;Skull&quot;, &quot;Vitals&quot;, &quot;Neck&quot;, &quot;corpo&quot;, &quot;testa&quot;, &quot;braccio destro&quot;, &quot;braccio sinistro&quot;, &quot;gamba destra&quot;, &quot;gamba sinistra&quot;,\n                       &quot;mano destra&quot;, &quot;mano sinistra&quot;, &quot;piede destro&quot;, &quot;piede sinistro&quot;, &quot;testicoli&quot;, &quot;cranio&quot;, &quot;vitali&quot;, &quot;collo&quot;]\n\n    attack_type_words = [&quot;cutting&quot;, &quot;impaling&quot;, &quot;crushing&quot;, &quot;small penetration&quot;, &quot;penetration&quot;, &quot;big penetration&quot;, &quot;huge penetration&quot;,\n                          &quot;burning&quot;, &quot;explosive&quot;, &quot;tagliente&quot;, &quot;impalamento&quot;, &quot;schiacciamento&quot;, &quot;penetrazione minore&quot;, &quot;piccola penetrazione&quot;,\n                          &quot;penetrazione&quot;, &quot;penetrazione maggiore&quot;, &quot;enorme penetrazione&quot;, &quot;infuocati&quot;, &quot;esplosivi&quot;]\n\n\n    ###############\n    # Trovare i match\n    ##############\n    matcher = DependencyMatcher(nlp.vocab)\n    # I&#039;m starting finding the verb\n    patterns = [{&quot;RIGHT_ID&quot;: &quot;anchor_verbo&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;VERB&quot;}},\n    \n    # Looking for Obj (word: enemy)\n                {&quot;LEFT_ID&quot;: &quot;anchor_verbo&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;obj_verbo&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;obj&quot;}},\n\n    # Looking for the name of the enemy: zombie1\n                {&quot;LEFT_ID&quot;: &quot;obj_verbo&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;type_enemy&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nmod&quot;}},\n    \n     # Looking for word: vulnerability\n                {&quot;LEFT_ID&quot;: &quot;anchor_verbo&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;vulnerability&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;LEMMA&quot;: &quot;vulnerability&quot;}},\n\n    #Looking for number associated to Vulnerability\n                {&quot;LEFT_ID&quot;: &quot;vulnerability&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;num_vulnerability&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nummod&quot;}},\n\n    #location of body hit\n                {&quot;LEFT_ID&quot;: &quot;anchor_verbo&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;location&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;LOWER&quot;: {&quot;IN&quot;: body_list_words}}},\n\n   # Looking for word: attack, in order to find the type of attack\n                {&quot;LEFT_ID&quot;: &quot;anchor_verbo&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;attack&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;NOUN&quot;}},\n\n    #Looking for type of attack\n                {&quot;LEFT_ID&quot;: &quot;attack&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;type_attack&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;LEMMA&quot;: {&quot;IN&quot;: attack_type_words}}},\n\n    #Looking for word: damage in order to extract the number\n                {&quot;LEFT_ID&quot;: &quot;attack&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;word_damage&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nmod&quot;}},\n\n    # Looking for the number\n                {&quot;LEFT_ID&quot;: &quot;word_damage&quot;,\n                 &quot;REL_OP&quot;: &quot;&gt;&gt;&quot;,\n                 &quot;RIGHT_ID&quot;: &quot;num_damage&quot;,\n                 &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nummod&quot;}}\n\n                ]\n\n    matcher.add(&quot;Inputs1&quot;, [patterns])\n    matches = matcher(doc)\n\n    match_id, token_ids = matches[0]\n    matched_words = []\n    for i in range(len(token_ids)):\n        #print(patterns[i][&quot;RIGHT_ID&quot;] + &quot;:&quot;, doc[token_ids[i]].text)\n        matched_words.append(doc[token_ids[i]].text)\n    \n#########\n# Now i&#039;m creating the dictionary, deleting first element\n#########\n    index_to_remove = [0]\n    for index, elem in enumerate(index_to_remove):\n        del matched_words[elem]\n    print(matched_words)\n\n    input_dict = {matched_words[0]: matched_words[1], &quot;location&quot;: matched_words[4], matched_words[5]: matched_words[6],\n                  matched_words[7]: matched_words[8], matched_words[2]: matched_words[3]}\n\n    #print(input_dict)\n    return input_dict\n\n\n\nGeneral problem to solve: any complex words that should groupped  togheter (as for \"right arm\", \"left leg\", \"large penetration\") can't be  extract in this way (only arm, leg or penetration would be returned).\n\n\nCan you help me? Thanks!",
"date": "2021-07-18"
},
{
"vote": 3,
"title": "Simple stemmers",
"text": "Hi! I'm working on a project and I need to write a stemmer - I've done some research and it seems like the Porter stemmer algorithm is the simplest one (relative to the others).\n\n\n&#x200B;\n\n\nHowever even then, it's still complex - are there any simpler stemmers (which might not perform as accurately), but still do reasonably well?",
"date": "2021-07-18"
},
{
"vote": 8,
"title": "What are your favorite conversational datasets? (multiple domains)",
"text": "I am working on a finetuning project with GPT-3 and I need conversational data! I would like data that contains multiple domains, such as medical, scientific, interpersonal, and fictional. Preferably open source.",
"date": "2021-07-17"
},
{
"vote": 27,
"title": "We made a search engine for Stack Overflow",
"text": "Our search engine, \nsearchoverflow.com\n, is built with NLP in mind. We are using Elastic Search and our own custom algorithms to help our users find what they are searching for. \n\n\nWe thought that this subreddit may be interested in it and we would be happy to answer any questions that you all may have.",
"date": "2021-07-14"
},
{
"vote": 0,
"title": "Turning Game Semantics into a real world application",
"text": "Hi guys, next semester there will be a lecture about â€œGame theory and agent-based modelling in linguisticsâ€ at my university and I was wondering if I should take a look at it. During my research, I have read about game semantics but havenâ€™t found anything â€œpracticalâ€. Do you maybe know some papers where an application has been implemented based on game semantics? I am not really a fan of theoretical concepts and want to focus on stuff I can use in the real world.",
"date": "2021-07-14"
},
{
"vote": 0,
"title": "4 Popular Techniques to Measure the Readability of a Text Document",
"text": null,
"date": "2021-07-14"
},
{
"vote": 2,
"title": "SpaCy, dependency matcher for multiple dependencies",
"text": "Hello there!\n\n\nI'm trying to figure out how to find matches, using DependencyMatcher, in the case i have a token with more than one dependency. For example, i have this sentence:\n\n\nsentence = a tree is hit in the branch with a crushing attack\n\n\n&#x200B;\n\n\nthe visualitazione can be found here: \nhttps://imgur.com/TfKr6iI\n\n\n&#x200B;\n\n\nNow, my code is:\n\n\nimport spacy\nfrom spacy import displacy\nfrom spacy.matcher import DependencyMatcher\n\n\nstring = &quot;a tree is hit in the branch with a crushing attack&quot;\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\ndoc = nlp(string)\n\nmatcher = DependencyMatcher(nlp.vocab)\n# Only run nlp.make_doc to speed things up\npatterns = [{&quot;RIGHT_ID&quot;: &quot;anchor_verb&quot;,\n             &quot;RIGHT_ATTRS&quot;: {&quot;ORTH&quot;: &quot;hit&quot;}},\n\n            {&quot;LEFT_ID&quot;: &quot;anchor_verb&quot;,\n             &quot;REL_OP&quot;: &quot;&gt;&quot;,\n            &quot;RIGHT_ID&quot;: &quot;subj_verb&quot;,\n             &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nsubjpass&quot;}},\n\n            {&quot;LEFT_ID&quot;: &quot;anchor_verb&quot;,\n             &quot;REL_OP&quot;: &quot;&gt;&quot;,\n             &quot;RIGHT_ID&quot;: &quot;location_verb&quot;,\n             &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;prep&quot;}}]\n\nmatcher.add(&quot;Inputs1&quot;, [patterns])\nmatches = matcher(doc)\n\nmatch_id, token_ids = matches[0]\nmatched_words = []\nfor i in range(len(token_ids)):\n    print(patterns[i][&quot;RIGHT_ID&quot;] + &quot;:&quot;, doc[token_ids[i]].text)\n    matched_words.append(doc[token_ids[i]].text)\n\nIn this way, i extract &quot;hit&quot;, &quot;tree&quot; and &quot;in&quot;.\nPoint is, the word &quot;hit&quot;, has 2 dependency (prep): &quot;in&quot; and &quot;with&quot;. How can i extract the with? Because in this way, from the chunk:\n{&quot;LEFT_ID&quot;: &quot;anchor_verb&quot;,\n             &quot;REL_OP&quot;: &quot;&gt;&quot;,\n             &quot;RIGHT_ID&quot;: &quot;location_verb&quot;,\n             &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;prep&quot;}}\ni can only extract the first prep, the token &quot;in&quot;, but if i wish to extract &quot;with&quot;?",
"date": "2021-07-14"
},
{
"vote": 1,
"title": "Web app for doing coreference resolution and outputting file in \".conll\" format",
"text": "Hi guys,\n\n\nIs there a web app online that does coreference resolution and then automatically outputs the resolved text in \".conll\" format? Thanks.",
"date": "2021-07-14"
},
{
"vote": 15,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-07-14"
},
{
"vote": 22,
"title": "Scalable Search With Facebook AI's FAISS",
"text": null,
"date": "2021-07-13"
},
{
"vote": 3,
"title": "I got access to the Copilot!",
"text": null,
"date": "2021-07-13"
},
{
"vote": 1,
"title": "What are my options for deploying pre-trained language models?",
"text": "I have pretrained T5 and Roberta model fine-tuned to solve some business analytics task. I want to deploy the models and I am not sure where to start, Heroku doesn't seem to support GPU hardware and the Ram is too small. \n\n\nIdeally if there is a platform where you can pay as you use, because inference would be ran once a day, so most of the time the models would be idle.\n\n\nThank you.",
"date": "2021-07-13"
},
{
"vote": 10,
"title": "How hard is it to speech recognize lyrics in a song and transcribe them to text?",
"text": "I'm trying to figure this out using Python and am struggling. The package \nSpeechRecognition\n, using Google's api's,  can readily speech recognize normal speaking voice and transcribe that into text fine, but struggles when fed any parts of a song, even with singers singing very clearly.\n\n\nIs this like an impossible task/would require a very sophisticated neural network beyond what these packages offer? Or is there another way to do this (has to be in Python) that would actually work?",
"date": "2021-07-12"
},
{
"vote": 18,
"title": "Automatic Question Generation (with transformers) and its challenges!",
"text": null,
"date": "2021-07-12"
},
{
"vote": 4,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-07-11"
},
{
"vote": 3,
"title": "Best Computer Linguistics bachelor in Europe?",
"text": "I've been looking for this information, but the discussions that I found are pretty old and limited to either MA programmes or just Germany. I was considering studying it in Heidelberg, as in Italy there is no specific undergraduate course. Thank you for answering.",
"date": "2021-07-11"
},
{
"vote": 11,
"title": "Suggestions on where to find good data sets? In particular, for training aspect based sentiment analysis models?",
"text": "So, I'm just wondering, what are the best places to find datasets for model training? I'm already tried \nhttps://paperswithcode.com/\n, is there a better place to look? Point being, I'm trying to train my model to do document level aspect based sentiment analysis, and the only decent dataset I can find for it is the one from semeval 2015/2016, however, there's a lot of issues with this dataset, so I was hoping to find a better one. Any ideas on where to look?",
"date": "2021-07-10"
},
{
"vote": 1,
"title": "SMOTE for Document Classification?",
"text": "[deleted]",
"date": "2021-07-09"
},
{
"vote": 1,
"title": "Sentiment Analysis with multiple emotions",
"text": "Hello everyone. I am relatively new in the field of ML/NLP and as one of my first projects I'm working on a sentiment analysis model for movies. Basically it takes movie descriptions, genres, ratings... And figures out how the audience will feel after watching it (happy, sad, angry, scared...). However I couldn't find a text corpus in python that goes past the positive/negative/neutral sentiments. I would appreciate any recommendations for corpora that fit what I want to do, and failing that any guidance on how to create my own corpus using a words/sentiments dataset I found online, or any other way I can use that dataset to aid in my task. Thanks for any help in advance",
"date": "2021-07-09"
},
{
"vote": 18,
"title": "Most influential work NLP + cognitive science",
"text": "It seems like most of the most highly cited professors/labs working in the intersection of NLP and cognitive linguistics or cognitive science are mostly in the US and concentrated in a handful of schools like CMU, MIT, Princeton, UMD (maybe I am missing many - I am not super familiar). What are the biggest names in this subfield (inside and also outside of the US)? Beyond standard NLP venues like ACL/EMNLP or workshops, I imagine they also publish in cognitive science journals -- I am not sure how to figure out which works are prominent and where the field is heading, so if anyone has a few prominent labs/people to follow, that would be appreciated a lot!",
"date": "2021-07-08"
},
{
"vote": 24,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-07-06"
},
{
"vote": 8,
"title": "NLP-powered Article Title Generator",
"text": "Hi all! Here's an article title generator I just made. It uses NLP to come up with unique titles for articles, blogs etc., just by pasting in the content of the article. It's still learning, so the results are hit or miss, but please give it a try! The more you use it, the better it will get.\n\n\nhttps://tools.hyperwrite.ai/titlegenerator",
"date": "2021-07-06"
},
{
"vote": 20,
"title": "Training and Testing an Italian BERT - Transformers From Scratch Series",
"text": null,
"date": "2021-07-06"
},
{
"vote": 26,
"title": "GPT-J for text generation on NLPCloud.io",
"text": "Hi everyone.\n\n\nSince the release of GPT-J, I worked hard in order to add it to \nNLPCloud.io\n for text generation. \n\n\nThis is done now and the infrastructure is stabilized but that was tricky. So I thought I would share here my key takeaways, in case it can help some of you:\n\n\n\n\nOn CPU, the model needs around 40GB of memory to load, and then around 20GB during runtime.\n\n\nOn CPU, a standard text generation (around 50 words) takes approximately 12 CPUs for 11 seconds\n\n\nOn a GPU, the model needs around 40GB of memory to load, and then around 3GB during runtime + 24GB of GPU memory. For a standard text generation (around 50 words), the latency is around 1.5 secs\n\n\n\n\nThe 2 main challenges are the high amount of RAM needed for startup, and then high amount of GPU memory needed during runtime which is quite impractical as most affordable NVIDIA GPUs dedicated to inference, like Tesla T4, only have 16GB of memory...\n\n\nIt's very interesting to note that, during my tests, the latency was pretty much the same as GPT-Neo 2.7B on the same hardware, but accuracy seems of course much better.\n\n\nIf some of you also ran these kinds of benchmarks on GPT-J I'd love to see if we're aligned or not!",
"date": "2021-07-05"
},
{
"vote": 1,
"title": "Best language learning apps of 2021",
"text": "[removed]",
"date": "2021-07-05"
},
{
"vote": 8,
"title": "Any guidance on converting Question and Answer text to coherent prose text?",
"text": "Hello, \n\n\nI have a particular NLP problem I'm trying to solve where I want to take a series of questions with answers and convert them into prose to summarize in statement form all that was discussed.\n\n\n&#x200B;\n\n\nTo illustrate I would like to take question and answers like this:\n\n\nQ: How long ago did your cough start?\nA: Three days ago.\n\n\nQ: Do you have a fever?\nA: No.  \n\n\nAnd convert this into prose text that states something like:\n\"The cough started three days ago.  There is no fever.\"  \n\n\nI understand this is a challenging NLP task.  I have a background in programming, but not really in NLP.  Can anyone recommend a python library that could do this or offer any guidance at all.  It would be appreciated!\n\n\nThanks!",
"date": "2021-07-04"
},
{
"vote": 23,
"title": "Can anyone point me in the direction of some good portofilio's that highlight Text Minining, NLP, and Sentiment Analysis?",
"text": "I have been working on some projects and have a background in linguistics. I wanted to see viable porfolio examples, so I can use them for inspiration!  Any help is appreciated!",
"date": "2021-07-02"
},
{
"vote": 3,
"title": "Seeking OCR review paper (spark OCR, keras-ocr, tesseract, google vision, EasyOCR)",
"text": "I'm seeking a review paper comparing accuracy/speed of contemporary OCR methods.\n\n\n(for context, I'm working on a project where I'm extracting text from 112,000,000 pdf pages (pdf -> png) and want to select the most-accurate, then fastest method)\n\n\nOpen to all thoughts/ideas, thanks.\n\n\n&#x200B;\n\n\nResources available:\n\n\n14 CPU Cores @ 3.6 GHz\n\n\n2x 3060TI(s)\n\n\n(cannot use cloud instances)",
"date": "2021-07-02"
},
{
"vote": 12,
"title": "BERT: Why does my masked language model not output emojis after adding them to my tokenizer?",
"text": "[deleted]",
"date": "2021-07-02"
},
{
"vote": 0,
"title": "Guide To Sentiment Analysis Using BERT",
"text": null,
"date": "2021-07-02"
},
{
"vote": 1,
"title": "How to make text-summarizing model predict for a single input?",
"text": "So my LSTM-based model takes in a review and summarizes it in a few words. These are the steps I take \non a single input\n after cleaning the text:\n\n\ntokenizer = Tokenizer()  \nsequence = tokenizer.texts_to_sequences([input_string])  \nsequence = pad_sequences(sequences, maxlen = 35) \n\n\n\nSince  \ntexts_to_sequences\n takes in a list of strings, I did \ntexts\\_to\\_sequences(\\[input\\_string\\])\n after reading \nthis answer\n to a similar question (and going through \nthis blogpost\n). However, I still get an error that 'NoneType' object has no attribute 'lower'. Looking through the stack trace, it seems that a function called \ntext\\_to\\_word\\_sequence\n in the keras library is being called, and it's taking in a parameter \ntext\n which seems to be a \nNoneTypeObject\n. How can I fix this issue?",
"date": "2021-07-02"
},
{
"vote": 2,
"title": "Do new datasets have a higher chance of being accepted into conferences?",
"text": "[deleted]",
"date": "2021-07-01"
},
{
"vote": 4,
"title": "How to process Reddit comments for stock symbols?",
"text": "[deleted]",
"date": "2021-07-01"
},
{
"vote": 2,
"title": "Ludicrous BERT Search Speeds",
"text": null,
"date": "2021-07-01"
},
{
"vote": 8,
"title": "Term Extraction Package Demo on Heroku",
"text": "Hey guys, about a year ago I posted my Python package PyATE, which has implementations of automated term extraction (ATE) algorithms at \nhttps://www.reddit.com/r/LanguageTechnology/comments/hpg7h1/term_extraction_package_in_python/\n. To showcase the algorithms, I made a web app at \nhttps://pyate-demo.herokuapp.com/\n, for easier access so users don't have to download the package to test it out. I thought I would share this here as it could potentially help out engineers and researchers.\n\n\n(Also as this web app is hosted on Heroku free tier, it may take 30 seconds to launch since the app may have fallen asleep.)",
"date": "2021-07-01"
},
{
"vote": 11,
"title": "The Illustrated Wav2vec: How to predict the future of an audio sequence",
"text": null,
"date": "2021-06-29"
},
{
"vote": 24,
"title": "3 Vector-based Methods for Similarity Search (TF-IDF, BM25, SBERT)",
"text": null,
"date": "2021-06-29"
},
{
"vote": 2,
"title": "Is there any room for NPL startups?",
"text": "I mean, it seems like the state-of-the-art in NLP and AI in general is led by the top companies. I had the idea of starting something related but I can't really identify a business model that adds something new to the scene. I've been working on different models but it seems like this field has to do more with research, being the money reserved for a few top companies.\n\n\nSorry about my English and my pesimism, just got rejected for 849483984 time.",
"date": "2021-06-29"
},
{
"vote": 13,
"title": "Imbalanced Dataset in NLP",
"text": "Hi all,\n\n\nJust wanted to inquire if there anybody has leads on best practices to work with imbalanced dataset and text classification use cases.\n\n\nTIA\n\n\nEdit --\n\n\nInitial process assessment has lead me to understand that in a multi-class classification case for a banking firm's Customer support, there will be certain imbalance in the email data available.\n\n\nSo as an example, there are more emails on a daily basis related to technical bits about applications/FAQ questions related to the bank's operations. Whereas there will be less emails when it comes to customers reporting fraud. Till now the SMEs have informed me the ratio of emails would be 10:1 for Fraud cases.",
"date": "2021-06-29"
},
{
"vote": 3,
"title": "Does Any Research Exist on LSH for Strings (edit distance)?",
"text": "I have a model where we currently handle out of vocab tokens using hashing. That works better than you would expect but i am looking for ways to hash strings so that the likelihood of collision is higher for strings that are more similar, either in terms of edit distance, or same character ngram overlap (e.g. jaccard). One method would be to do LSH on the character ngrams. Are there any known methods for automatically hashing similar strings into the same hash buckets? Either learned (data dependent) or determinstic (like LSH)?",
"date": "2021-06-29"
},
{
"vote": 1,
"title": "English bachelors programs in CompLing or Linguistics in the EU?",
"text": "[deleted]",
"date": "2021-06-28"
},
{
"vote": 0,
"title": "How to create a virtual assistant with Python",
"text": null,
"date": "2021-06-27"
},
{
"vote": 2,
"title": "Which ML algorithm is used by Kira for their smart fields",
"text": "Kira software calls it smart fields to filter passages from contracts to identify sections like \"Termination for convenience\", \"Notice\". \"automatic contract renewal\" and about 1000 other smart fields (they call it like that).\n\n\nDoes anyone know what kind of ML algorithm they use? It looks very fast what they do. I guess it is something like relation extraction but it covers whole paragraphs in this example.\n\n\nHere is a product walkthrough for the how it works in their software.\n\n\nhttps://www.youtube.com/watch?v=2qqVeqPzTuw",
"date": "2021-06-26"
},
{
"vote": 1,
"title": "Command line tools for NLP on tweets",
"text": "[removed]",
"date": "2021-06-26"
},
{
"vote": 3,
"title": "Why do you need a threshold when tokenizing a text corpus?",
"text": "So I'm a self-learning NLP and came across \nthis kaggle notebook\n that does text summarization using an LSTM. When it makes an `orderedDict` of words to integers, there's some code that apparently calculates the percentage of rare words in the vocabulary. Why is there a threshold value of 4 there? As far as I can see, the word to integer mappings are arbitrary (unless each integer = number of times the word was repeated), so the threshold value of 4 seems a bit arbitrary to me. \n\n\nThanks in advance for helping :)",
"date": "2021-06-26"
},
{
"vote": 28,
"title": "Call for Participation to NL-Augmenter ðŸ¦Ž â†’ ðŸ",
"text": "Hi  r/LanguageTechnology Members!\n\n\nWe, a team of researchers spanning Google AI Language, UW, CMU and 7 other institutions organizing NL-AugmenterÂ ðŸ¦Ž â†’ ðŸ“·.Â , are now inviting  transformation submissions to the same!  \n\n\nAll submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework. NL-AugmenterÂ ðŸ¦Ž â†’ ðŸ“· is a part of the wider GEM benchmark,  \nGEM (Generation, Evaluation, Metrics)\n workshop at ACL, 2021 and their future iterations.\n\n\nThe NL-Augmenter is a collaborative effort intended to add transformations of datasets dealing with natural language. Transformations augment text datasets in diverse ways, including: introducing spelling errors, translating to a different language, randomizing names and numbers, paraphrasing ... and whatever creative augmentation you contribute to the benchmark.Â We invite submissions of transformations to this framework by way of GitHub pull request, through \nSeptember 1, 2021\n.Â \nAll submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework.\n\n\nProject:Â \nhttps://github.com/GEM-benchmark/NL-Augmenter\n\n\nWeÂ strongly believe that the benefits of open science should reachÂ everyone and hence we are making this effort to reach you. We also encourage you to share this with other researchers inÂ your department whoÂ would benefit fromÂ this open collaboration. To know more about theÂ framework, check our \nmotivation and review criteria\nÂ and some ofÂ \nour recent work\n.\n\n\nOrganizers:\n\n\nKaustubh Dhole (Amelia R&D) , Sebastian Gehrmann (Google AI Language), Varun Gangal (LTI, Carnegie Mellon University), Jascha Sohl-Dickstein (Google Brain), Tonghuang Wu (University of Washington), Simon Mille (Universitat Pompeu Fabra)Â , Zhenhao Li (Imperial College, London), Saad Mahmood (Trivago R&D), Aadesh Gupta (Amelia R&D), Samson Tan (Salesforce Research), Jinho Choi (Emory University)",
"date": "2021-06-26"
},
{
"vote": 8,
"title": "The loss jump up suddenly",
"text": "Hi everyone,\n\n\nRecently, when I train a Transformer for summarization, I meet some problems. At first, the loss decline as normal, then, suddenly, it jumps up a lot (not gradually increase, just changes in a step), and then, it begins to decline again.  I am sure the data is correct, and I found when I use a larger batch size, this phenomenon will occur later (but still occur). \n\n\nWhat should I do?\n\n\nThank you very much!",
"date": "2021-06-26"
},
{
"vote": 6,
"title": "BERT + clustering(KMeans)",
"text": "Hi, folks!\n\n\nI'm new to NLP and topic modeling and in one of my tests I decided to use BERT+clustering(KMeans), and I'm getting unexciting results.\n\n\nMy code is as below:\n\n\nBERT and cluster functions\n\n\ndef bert_model(data):\n    model = SentenceTransformer(&#039;bert-base-multilingual-uncased&#039;)\n    vec = np.array(model.encode(data, show_progress_bar=False))\n    return vec\n\ndef cluster(vec, k):\n    cluster_model = KMeans(k)\n    cluster_model.fit(vec)\n    lbs = cluster_model.predict(vec)\n    return lbs\n\n\n\nAutoencoder to reduce dimensionality\n\n\n#BERT\nbert_vec = bert_model(data.full_text)\n\n#Autoencoder\nAE = Autoencoder()\nAE.fit(bert_vec)\nvec_autoencoder = AE.encoder.predict(bert_vec)\n\n\n\nClustering and calculating metrics\n\n\nn_topics = 12\ntokens = data_tokens.full_text\ndictionary = corpora.Dictionary(tokens)\n    \n#Clustering\nlbs = cluster(vec_autoencoder, n_topics)\n\n#Lists of metrics \ncoherence = get_coherence(tokens, lbs, dictionary)\nsilhouette = silhouette_score(vec_autoencoder, lbs)\n\n\n\nAs a result I am having a coherence of 0.374 and a silhouette of 0.089. Is there anything wrong with my code or is there anything I can do to improve these results without modifying the structure?\n\n\nObs.: I'm using BERT fromSentenceTransformer and calculating coherence and silhouette with gensim and sklearn, respectively.",
"date": "2021-06-25"
},
{
"vote": 7,
"title": "Classify this NLP-Problem, using BERT?",
"text": "Hello guys,\n\n\nright now I am in the process of deciding on my master thesis topic. Right now I and my Professor are thinking about the possibility to have a large dataset of product requirements given in a natural language. My task would be to develop a domain-specific language (DSL) for these requirements and afterwards to develop a NN that gets the dataset containing these requirements as an input and transform each requirement into a requirement in this DSL that I have to develop beforehand.\n\n\nAn Example for such a DSL would be presented in this Paper: \nA Textual Domain Specific Language for Requierement Modeling\n \n\n\n(It's not exactly the same but a good showcase of the DSL might be derived) \n\n\nFor the people that don't want to read the Paper there is an example given like this:\n\n\nNatural Language Req: \"The Lateral and Vertical 'ERC' and'WRD' labels shall be displayed in a green color\"\n\n\nThe NN should then take this requirement as an input and generate the following requirements:\n\n\nReq1: \nThe\n Lateral_ERC_Label \nshall be displayed in\n green\n\n\nReq2: \nThe\n Lateral_WRD_Label \nshall be displayed in\n green\n\n\nReq3: \nThe\n Vertical_ERC_Label \nshall be displayed in\n green\n\n\nReq4: \nThe\n Vertical_WRD_Label \nshall be displayed in\n green\n\n\n&#x200B;\n\n\nAs we can see the DSL, in this case, has the keywords \"The\" and \"shall be displayed in\" (Bolded) that stay the same for each generated requirement, and in between are the specifications for the requirement.\n\n\nNow the question is, how is this achievable? My professor said I should look into BERT for this task but my research has led me to the conclusion that BERT is not really suitable for text generation.\n\n\nOn the other hand, I am not really sure if this above-mentioned problem can be classified as text generation. It looks like a combination Problem of Text extraction ( extracting the necessary keywords) and text generation.\n\n\nBut I am not really sure if I am even looking and researching in the right direction right now. So I would really appreciate it if somebody would be able to help me out here and giving me a push in the right direction",
"date": "2021-06-25"
},
{
"vote": 1,
"title": "Open-source PHI de-identification tool",
"text": "Hi all, is there an out-of-the-box system available for healthcare domain de-identification? Specifically, it should remove Protected Health Information (PHI).\n\n\nIs open source that would be great. Otherwise, are there any paid ones? \n\n\nI know only about \nhttps://www.johnsnowlabs.com/spark-nlp-health/",
"date": "2021-06-25"
},
{
"vote": 1,
"title": "pretrain BERT for poetry with custom corpus data from scratch",
"text": "I have raw plain text data in sql db.\n\n\nnow i want to make a model out of it. for that i need to pretain BERT (specially for hindi language).\n\n\nalso, i wish to add more data in future. \n\n\nso guys can you guide me for simple basic steps? which technology is future proof?\n\n\ni know there are huggingface, google examples. but i just want to do it on my own local machine without these apis.\n\n\n&#x200B;\n\n\nwell my goal is to generate a poem / quote / or a sentimental paragraph after inserting some hints or category.\n\n\n&#x200B;\n\n\nsorry for poor english",
"date": "2021-06-25"
},
{
"vote": 1,
"title": "Sources for spam classification",
"text": "Hi,\nI've started doing some small project for spam detection, but while researching, the most common file is the 'SMSSpamCollection' that is used in vast majority of documents... but it's only in english.\n\n\nI wanted to use the approach to create one model by language, based on the detection made by langdetect library in python, but the problem is that.... I found no databases to use for other common languages like german, italian, spanish, portuguese, etc.\n\n\nIs there any place I should start looking at?\n\n\nThanks in advance",
"date": "2021-06-24"
},
{
"vote": 15,
"title": "When is it better to use nn.embedding instead of pre-trained word embeddings?",
"text": "Hi all. I'm learning NLP, and currently working on the implementation of a neural machine translator using PyTorch.\n\n\nI'm wondering if it is better to use pre-trained word embeddings from word2vec/GloVe or just the embedding layer provided by PyTorch, which, as far as I understand, learns the embeddings during training. \n\n\nIn your experience when is it better to use pre-trained embeddings over learnable embeddings and why?\n\n\nI suspect the use cases of pre-trained embeddings depend on the task to accomplish, for example, for summarization Id say its better to use pre-trained embeddings, but perhaps there are some rules of thumb given by practical experience you can share.\n\n\nThanks!",
"date": "2021-06-24"
},
{
"vote": 1,
"title": "Build a Custom Transformer Tokenizer - Transformers From Scratch #2",
"text": null,
"date": "2021-06-24"
},
{
"vote": 3,
"title": "What do negative sentence similarity values mean?",
"text": "Iâ€™m a newbie to using the Google Universal Sentence encoder, and have a question about some of the results Iâ€™m getting.\n\n\nPreviously, we used Encoder 2 with Javascript and never saw negative sentence similarity values. However, Iâ€™m now using Encoder 4 with Python, and some sentence similarity values are negative. This is true, for instance, of the sentence pair â€œGet the facts.â€ and â€œHi Frank, itâ€™s Bob.â€\n\n\nIâ€™m calculating semantic textual similarity as follows. (This is based on the tensorflow hub tutorial for Encoder 4 at \nhttps://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n.)\n\n\na = &quot;Get the facts.&quot; \nb = &quot;Hi Frank, it&#039;s Bob.&quot; \nmodel = hub.load(MODEL_DIRECTORY) \nembeddings = model([a, b]) \nresults = numpy.inner(embeddings, embeddings)\nsimilarity = results[0,1]\n\n\n\nThe value I get is -0.036280043.\n\n\nAm I calculating semantic textual similarity correctly? What is the range for similarity values? Do negative values have any special significance?",
"date": "2021-06-23"
},
{
"vote": 2,
"title": "Linguistic comparison in python",
"text": "(Note: I posted this on r/learnpython but was told this subreddit is better)\n\n\nI  am doing a project where I compare different texts from different eras of English and trying to see how similar they are. However, I'm not sure what is the best way to go forward with it. I am thinking of using  word2vec but after that, I'm lost.\n\n\nAny and all advice would be appreciated.",
"date": "2021-06-23"
},
{
"vote": 10,
"title": "Data to text generation - are any deep learning techniques possible without hallucination or should I look at other methods?",
"text": "Hi all,\n\n\nI'm currently exploring creating automatically generated short articles from structured data about live traffic incidents. I can access the data, I.e where the incident was, length of delay, congestion plus a brief description, but have been having trouble finding the best NLG methodology to ensure that it stays factual. I performed some minor experiments with some deep learning algorithms but was getting some strange hallucinations which I certainly don't want when trying to create factual articles (I.e. Adding drama and injuries to the crash).\n\n\nWould love to hear any advice on good ways forward, ideally wouldn't like to do a pure templated method!",
"date": "2021-06-23"
},
{
"vote": 4,
"title": "Is there any work that shows/mentions that MRC/QA works particularly well for longer text?",
"text": "I'm reading a lot of papers that report high/robust performance on NLP tasks when those tasks are formulated as machine reading comprehension (MRC) or question answering (QA).\n\n\nWhat I'm wondering is, is there any particular work that mentions that MRC works particularly better for longer contexts?",
"date": "2021-06-23"
},
{
"vote": 2,
"title": "Looking for a technical cofounder for a product around voice/upskilling",
"text": "Hi all,\n\n\nI am a second-time founder, with a total of 20 yr experience across product mgmt, engineering, and sales. For my latest startup, already launched, I am looking for a technical co-founder with a background in ML / NLP tools (I am thinking DialogFlow currently)\n\n\nThe first tool to build is a reverse interview bot, and then graduate to simulators for training customer-centric roles like sales/customer success/service, etc\n\n\nThe general area of the startup is upskilling/training/ed-tech/early-talent/recruiting. Would anyone here like to explore more?",
"date": "2021-06-22"
},
{
"vote": 23,
"title": "How-to Use HuggingFace's Datasets - Transformers From Scratch #1",
"text": null,
"date": "2021-06-22"
},
{
"vote": 23,
"title": "Visualising words weight - BERT Sentiment Analysis",
"text": "I have finetuned a BERT model for sentiment analysis. Is it now possible to extract and visualise which words have the strongest effect to make the sentiment positive or negative?\n\n\nThank you!",
"date": "2021-06-22"
},
{
"vote": 11,
"title": "Gpt 2 - fine tuning 124 M vs 355M",
"text": "Can someone please explain me what difference does it make to fine tune gpt2 124M and 355M.\nI get it that 355m was pretrained on larger dataset.\nBut when I am fine tuning it, how does the model size matter.?\nThanks",
"date": "2021-06-21"
},
{
"vote": 20,
"title": "Noob question: Are semantic understanding tasks hard because we can't provide machines with physical/mental association of text to real world objects?",
"text": "From what I understand, we learn a word by associating its symbol (text) or sound (speech) with a real world phenomenon. After learning basic English for example, I know the text \"table\" is about this common four legs object and not some four legs animal. Even with a word like \"abstract\", we can still understand it by mentally \"relating\" it to everything around us. We know whether something is abstract or not. By interacting with the things that the words are used to describe, we are able to create mental connections, which lead to the understanding of textual contents semantically. \n\n\nSo, is it the lacking of this interaction with the environment preventing us from achieving high results in semantic related tasks?",
"date": "2021-06-21"
},
{
"vote": 1,
"title": "â£ï¸ ðŸ’Ÿ ðŸ’– Nude Server Code",
"text": null,
"date": "2021-06-20"
},
{
"vote": 8,
"title": "Masked Language Models Vs Causal Language Models in NLP #shorts",
"text": null,
"date": "2021-06-19"
},
{
"vote": 5,
"title": "Does removing (people, countries name, etc ...) help with reducing noise for sentiment analysis task ?",
"text": "I'm using Bert model to do sentiment analysis and I'm wondering if I remove names of (people, countries and organizations name, etc ) would help reduce the noise, and make the model focus more on the actual sentiment related words in the text.\n\n\nI'm asking this because I've noticed that for example, if certain names appears to much in angry related sentences, that the model would consider that name as an important word to base its prediction on !\n\n\nDoes performing NER and POS to capture and remove such names is something that is beneficial. How do you guys deal with such thing ?",
"date": "2021-06-19"
},
{
"vote": 3,
"title": "ML Illustration/Visualizing Software Question",
"text": "What software did the author use to create the wonderful visualizations in this Transformer writeup? I would love to recreate his work. \n\n\nhttps://jalammar.github.io/illustrated-transformer/",
"date": "2021-06-18"
},
{
"vote": 5,
"title": "Megatron-LM - Annotated Paper!!",
"text": "Megatron-LM provides a simple yet innovative approach on how to parallelize models to train large (multi-billion parameters) language models and efficiently use GPU memory during scaling. The key point is that it does not require any major modifications (like compilation or an entirely new framework) to implement this in the existing code. It also suggested a small modification in the BERT architecture which allowed BERT to scale effectively to parameter sizes that did not perform well on before.\n\n\nI will focus more on papers on model scaling techniques in the upcoming few annotated papers as I want to gain more idea about this area. Check out the annotated paper below - \n\n\nAnnotated Paper -  \nhttps://github.com/shreyansh26/Annotated-ML-Papers/blob/main/MegatronLM.pdf",
"date": "2021-06-18"
},
{
"vote": 1,
"title": "Legal Translation Services",
"text": null,
"date": "2021-06-18"
},
{
"vote": 1,
"title": "Infer Demographics Based on Names?",
"text": "Hi all. I was given a task where I have to infer the demographics (gender, race, ethnicity) of a name. Any SOTA that I can grab for use?",
"date": "2021-06-18"
},
{
"vote": 14,
"title": "Language Classification for Swahili in Python",
"text": "I want to do some simple language classification in Swahili.  What is the easiest and best way to do this? \n\n\nAbout Swahili - Swahili is a very agglutinative language so what would be many tokens in english is often one token in Swahili.  A single verb can have thousands of possible variants depending on prefix or suffix.  I have managed to piece together a parser which cam capture the verb lemmas.\n\n\nMy Use Case - I want to have a simple binary classification on pieces od text that are 20-100 words, but also would like to do category classification of text with 10 or so targets. I could train the data but hope to get results after like 50 datapoints per class.\n\n\nSolutions? - I was thinking SVM or naive bayes based on simple token recognition together with TD-IDF?  Should I do more?  Any easy low hanging fruits?  Is there easy stuff i can use in Spacy or NLTK packages?  HELP!",
"date": "2021-06-17"
},
{
"vote": 2,
"title": "Call for book chapter for CRC Press: NLP",
"text": "We are pleased to invite book chapters for our book on 'Natural Language Processing and Information Retrieval: Principles and Applications' in CRC Press: Taylor and Francis.\n\n\nThere is no processing / publication charge for this book. However, the acceptance is purely based on the merit/ quality of the manuscript. The length of a book chapter should be between 15 to 25 pages. The manuscript should be submitted in word file (.doc format).\n\n\nSubmission Link:Â \nhttps://easychair.org/conferences/?conf=nlpir20210",
"date": "2021-06-17"
},
{
"vote": 5,
"title": "BERT Next Sentence Prediction: How to do predictions?",
"text": "[deleted]",
"date": "2021-06-17"
},
{
"vote": 4,
"title": "Japanese Frequency on the Fly - Chrome Extension",
"text": "I recently made a Chrome Extension that parses the body HTML for Japanese and displays frequencies (Kanji, Words). Here is a video demoing it (about 45 seconds): \nhttps://www.youtube.com/watch?v=Skhc9zpR9yU\n\n\n&#x200B;\n\n\nI used it to quickly get an overview of the language found in a webpage. \n\n\nHere is the Chrome store link (free): \nhttps://chrome.google.com/webstore/detail/tango/childjcagcbmjcphnacbgmlbeddmgljd/related\n\n\n&#x200B;\n\n\nIt's no Yomichan, but I find it interesting for reading stuff online to see what is most important. Or you could scroll all the way down to find some uncommon words!\n\n\n&#x200B;\n\n\nGitHub: \nhttps://github.com/LexingtonWhalen/TanGoChrome",
"date": "2021-06-17"
},
{
"vote": 37,
"title": "Explain Like I'm 5 Bot, Powered by NLP",
"text": "Hi all! Here's an NLP-powered explain like I'm five app I just made. It uses advanced AI to explain any topic simply, in a few sentences. It's still learning, so the results are hit or miss, but please give it a try! The more you use it, the better it will get.\n\n\nhttps://tools.hyperwrite.ai/eli5",
"date": "2021-06-17"
},
{
"vote": 5,
"title": "Why do some works anonymize the entities when performing relation extraction?",
"text": "I'm currently doing some work in relation extraction and have noticed that many works (e.g., the TACRED paper) claim to use entity anonymization. For example, if the entity in question is \"John Cena\" then they'd replace it with \"PER-SUBJ.\"\n\n\nDoes anybody know why this may be the case? The TACRED paper claims that this 1) helps provide entity type information and 2) prevent models from overfitting to specific entity names.\n\n\nHowever, I've also noticed that many recent works don't perform this masking strategy. They simply put entity markers on the front and back without any masking. Is this simply a trend that went out of fashion?",
"date": "2021-06-17"
},
{
"vote": 7,
"title": "Fast text transfer learning",
"text": "I have a trained fasttext model. Can I extract its word embedding and use that embedding for another   task?\n\n\nThe task is given a morphological variant of a word(Ex: coming, comes..) predict the base version of that word(Ex: come). I have the required dataset. But is it possible to do this transfer learning for fast text?",
"date": "2021-06-16"
},
{
"vote": 1,
"title": "Airy - Open Source conversational platform &amp; unified messaging APIs",
"text": "[removed]",
"date": "2021-06-16"
},
{
"vote": 2,
"title": "Similarity Score of a Document and Keywords",
"text": "I have a list of documents (which are online articles) and I would like to test their similarity against a list of keywords. The trick is that the documents were scraped from the internet using those keywords, so more often than not, they have some sort of similarity. \n\n\nThe problem is, sometimes articles are not exactly what you're looking for (e.g an advertisement that had one of the keywords or a completely different topic article but it unfortunately had the same keyword)\n\n\nI have tried huggingface's \"zero-shot-classification\" to try to see which keywords the model classifies that document as, but this gave me 2 problems: \n\n\n\n\nThe model was taking very long to classify (because the labels are a lot)\n\n\nI realized this still doesn't help me with the underlying problem of trying to find which document is more likely to be relevant to those keywords\n\n\n\n\n&#x200B;\n\n\nAny guidance or help would be appreciated. Thank you.",
"date": "2021-06-16"
},
{
"vote": 7,
"title": "Machine translation of a new language",
"text": "In Northern Ireland the government has recently approved plans to translate government business into the Ulster-Scots language.\n\n\n \nhttps://www.bbc.co.uk/news/uk-northern-ireland-57491212\n \n\n\nI'm not convinced that there exists sufficient translation expertise, there are only a few tens of thousands of speakers of Ulster-Scots, only a few dozen writers, and maybe less than ten with any experience of translation.\n\n\nSo, in order to carry out translations the government might have to rely on technology. Ulster Scots isn't supported by Google Translate and there are very few corpora of contemporary Ulster Scots or any variety of Scots that can be used to train translation software.\n\n\nHowever, I've been compiling a corpus of 21st century Scots texts for the last year or so just as a hobby. I don't have any practical experience of NPL, but I would like to understand what I'd need to do to turn by collection of text files into something that can be used to train machine translation, to produce something that can be offered to the Northern Irish government.",
"date": "2021-06-16"
},
{
"vote": 1,
"title": "Looking for automatic cropping for pattern recognition",
"text": "[removed]",
"date": "2021-06-16"
},
{
"vote": 6,
"title": "How to fine-tune with BertForPretraining",
"text": null,
"date": "2021-06-15"
},
{
"vote": 1,
"title": "Speech AI â€“ how to improve call center sales performance",
"text": "[removed]",
"date": "2021-06-15"
},
{
"vote": 1,
"title": "Should you use MT5 instead of T5 for every non-english task?",
"text": "I was looking at the T5 transformer model and was thinking about using it for a german generation task. This model seems really powerful but it is only trained for English. Then I found the multilingual T5 model which supports many languages. Is this way better for the non-english tasks (for example paraphrase generation in my case)? I am confused because the T5 model is able to translate from English to German, it can be trained on other languages etc.",
"date": "2021-06-15"
},
{
"vote": 1,
"title": "Researchers From Bangladesh University And UCLA Use AI To Develop A Framework (Text2App) To Create Android Apps From Text Descriptions",
"text": "A team of Researchers at BUET (Bangladesh University of Engineering and Technology) and UCLA (University of California- Los Angeles) has created a \nframework that can be used to develop Android applications from text descriptions.\n\n\nAccording to Masum Hasan, a researcher who carried out the study, their team wondered whether a full-fledged software could be built from natural language specification. Almost all the existing models for creating software based on text descriptions are based on end-to-end neural machine translation (NMT) models, which are similar to the one behind Google Translate. Usually, these models use NMT frameworks to translate human language into a source code.\n\n\nSummary: \nhttps://www.marktechpost.com/2021/06/14/researchers-from-bangladesh-university-and-ucla-use-ai-to-develop-a-framework-text2app-to-create-android-apps-from-text-descriptions/\n\n\nPaper: \nhttps://arxiv.org/pdf/2104.08301.pdf\n\n\nGithub: \nhttps://text2app.github.io/",
"date": "2021-06-15"
},
{
"vote": 5,
"title": "Fine-tuning the larger GPT Neo models(1.3B and 2.7B) with a Jupyter notebook",
"text": "Normally, only fine-tuning the 125M GPT Neo model is possible due to the fact that even that model uses over 10GB of VRAM, and the 1.3B and 2.7B taking much more. By following this video and the provided Jupyter notebook, it is possible to fine-tune the larger GPT Neo models on high-end consumer hardware or on cheap cloud options\n\n\nhttps://www.youtube.com/watch?v=Igr1tP8WaRc",
"date": "2021-06-14"
},
{
"vote": 9,
"title": "Edinburgh or U Washington NLP Masters? accepted and need help deciding.",
"text": "Hi, I'm into both U of Washington and U of Edinburgh for NLP Master's and Master's of Speech and Language Processing, respectively. Washington is remote and tuition is 40k (and 1 year in duration if full-time) and Edinburgh seems to be 1 year + dissertation at 60k for tuition. I was hoping to get some insight on which path would be more lucrative considering the costs, as well as U wash having an internship option. I am certain I want to continue onwards to industry as opposed to PHD if that helps. My concerns are getting a degree from an overseas uni, as well as slight concern about the tuition. I am a new grad (May 2021) and realized I need further education for most NLP positions in the US. \n\n\nThank you everyone in this community for your help!",
"date": "2021-06-14"
},
{
"vote": 2,
"title": "Gpt 2 124m using transformers",
"text": "I downloaded gpt 124 M and i was able to run the interactive sample file.\nHow can I run 124M using transformers.\nWhile running:\nAuto tokenizer. Frompretrained(\"c:\\path\\124M\")\n\n\nI get an error :configuration not found.\n\n\nNote:124 M contains encoder. Json, hoarams. Json,... Etc.",
"date": "2021-06-14"
},
{
"vote": 1,
"title": "Looking for medical audio/transcription dataset for training STT model with DeepSpeech.",
"text": "Iâ€™m working on training a STT model with DeepSpeech that is based on a pre-trained model but will specialize in transcribing medical conversation (diseases, medications, etc). The most frustrating part so far has been a lack of medical audio data for training. Iâ€™m looking for high quality audio on the scale of thousands of hours with accurate transcriptions. Does anyone know if such a dataset exists?",
"date": "2021-06-14"
},
{
"vote": 9,
"title": "BERT: How to do sentiment analysis on a custom dataset?",
"text": "[deleted]",
"date": "2021-06-14"
},
{
"vote": 5,
"title": "Methods for measuring OCR accuracy?",
"text": "[removed]",
"date": "2021-06-14"
},
{
"vote": 17,
"title": "Knowledge Graph",
"text": "Hi , iâ€™m working on a project to build knowledge graph , so that the relationships within the document will help us to query documents with those attributes from the entire corpus.\nWould like some help what methods can be used to mine relationships from the document.",
"date": "2021-06-13"
},
{
"vote": 1,
"title": "How to pass BERT output to dense layer",
"text": "I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is `batch,sequence length, 768 `. So I am confused about how to pass it to the dense layer",
"date": "2021-06-13"
},
{
"vote": 1,
"title": "How to do batch tokenization in BERT auto-tokenizer",
"text": "[deleted]",
"date": "2021-06-13"
},
{
"vote": 0,
"title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough)",
"text": "[deleted]",
"date": "2021-06-13"
},
{
"vote": 1,
"title": "Text Classification using spaCy v3.0 transformers in Python | Natural Language Processing Tutorial",
"text": "[deleted]",
"date": "2021-06-13"
},
{
"vote": 1,
"title": "Using gpt neo checkpoints",
"text": "Hi,\nI downloaded gpt neo from theeye.eye on my pc.\nIt downloaded a various checkpoints.\nHow do i use them? ... Because in order too load and use model I'd need encoder. Json, pytorch. Bin, etc..",
"date": "2021-06-13"
},
{
"vote": 2,
"title": "Extract text from image which is printed columnwise",
"text": "Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? \n\n\nThanks in advance!",
"date": "2021-06-12"
},
{
"vote": 8,
"title": "A Search Engine with a normalized scoring function",
"text": "Hey!\nI am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here (\nhttps://github.com/quantleaf/quantleaf-language-documentation\n). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it â€œzero-to-oneâ€ score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.\n\n\nI have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)\n\n\nhttps://quantleaf.github.io/probly-search-demo/\n\n\nWhen searching â€œGarlic Chickenâ€, the first two results are:\n\n\nFor the â€œzero-to-oneâ€ scoring\nâ€œGarlic Chickenâ€ score 1.\nâ€œGarlic-Sherry Chickenâ€ score: 0.7307692307692308\n\n\nFor BM25 (standard parameters)\nâ€œGarlic Oven Fried Chickenâ€ score 8.564332563809089\nâ€œGarlic Chickenâ€œ score 8.455662889754347\n\n\nFor the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.\n\n\nThe search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).\n\n\nLibrary source code: \nhttps://github.com/quantleaf/probly-search\nDemo source code: \nhttps://github.com/quantleaf/probly-search-demo\n\n\nI am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)",
"date": "2021-06-12"
},
{
"vote": 1,
"title": "A Search Enginge with a normalized scoring function",
"text": "[deleted]",
"date": "2021-06-12"
},
{
"vote": 0,
"title": "Tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using R",
"text": "Hey, I've created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: \nhttps://statisticsglobe.com/replace-missing-values-by-column-mean-in-r",
"date": "2021-06-11"
},
{
"vote": 1,
"title": "8 Types of Sampling Techniques",
"text": "[removed]",
"date": "2021-06-10"
},
{
"vote": 2,
"title": "Speech summarization datasets",
"text": "Hi everyone,\n\n\nI'm currently learning about text summarization and I'd really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).\n\n\nDoes anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.\n\n\nThank you!",
"date": "2021-06-10"
},
{
"vote": 19,
"title": "Clustering word vectors for topic discovery without the actual documents (overlapping &amp; hierarchical)",
"text": "Hi everyone,\n\n\nFor my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).\n\n\nThe problem is that because I don't have documents it's not straightforward to apply topic modeling ideas here.\n\n\nI know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there's little research on hierarchical overlapping clustering.\n\n\nI've been also thinking to leverage community detection methods on graphs, as it's possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there's no a more natural choice before pursuing this.\n\n\nWould appreciate any ideas, thank you!",
"date": "2021-06-10"
},
{
"vote": 6,
"title": "Unsupervised Relation Extraction using BERT attention scores",
"text": "Hey, \n\n\nGiven that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  \n\n\nObviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.",
"date": "2021-06-09"
},
{
"vote": 6,
"title": "Relationship extraction",
"text": "I would like to extract relationships in plain text between two named entities.  \n\n\nI first wanted to try with Machine Learning but it's complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I'm trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).\n\n\nDo you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? \n\n\nThaaaanks :)",
"date": "2021-06-09"
},
{
"vote": 5,
"title": "How to build a model for german paraphrase generation?",
"text": "I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus_paraphrase or Vamsi/T5_Paraphrase_Paws.\n\n\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) \n\n\n\nI can't find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!",
"date": "2021-06-09"
},
{
"vote": 0,
"title": "Benefits of Using PHP for Web Development",
"text": null,
"date": "2021-06-09"
},
{
"vote": 3,
"title": "Need direction related a project, like what topics(study material) should I look into.",
"text": "I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.",
"date": "2021-06-08"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-06-08"
},
{
"vote": 5,
"title": "Clustering latent representation vectors with a size less than the number of clusters",
"text": "I'm doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.\n\n\nThe methodology I'm following is \nthis\n.\n\n\nThen the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.\n\n\nWith this I have the error:\n\n\nValueError: n_samples=5 should be >= n_clusters=7.\n\n\nDoes anyone know how I could fix this?",
"date": "2021-06-08"
},
{
"vote": 1,
"title": "Clustering latent representation vectors with a size less than the number of clusters \"[Discussion]\" \"[Project]\"",
"text": "[removed]",
"date": "2021-06-08"
},
{
"vote": 2,
"title": "FREE WEBINAR - Automating Data Annotation with MicroModels - Automating processes within the workflow to improve efficiency &amp; guarantee high quality",
"text": null,
"date": "2021-06-08"
},
{
"vote": 24,
"title": "Job opportunity: Assistant Professor in Speech Technology at the University of Groningen (the Netherlands)",
"text": "Very interesting job to teach at a new \nMSc. in Voice technology\n!\n\n\nKey points:\n\n\n\n\nEnglish language program\n\n\n80-100% full-time position (depending on how many classes you want to teach)\n\n\nBalance between teaching and research is 60/40 (really!)\n\n\n\n\nIn addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):\n\n\nâ— Speech Synthesis I and II\nâ— Speech Recognition I and II\nâ— Python for Voice Technology (and Intro to Python at the undergraduate level)\nâ— Machine Learning for Voice Technology\n\n\nMore details\n  (qualifications, application procedure, etc.)\n\n\nDeadline: 13 June 11:59pm (CEST - European time)",
"date": "2021-06-08"
},
{
"vote": 2,
"title": "google-research/mozolm - A language model serving library, with middleware functionality including mixing of probabilities from disparate base language model types and tokenizations along with RPC client/server interactions.",
"text": null,
"date": "2021-06-07"
},
{
"vote": 26,
"title": "John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more!",
"text": null,
"date": "2021-06-07"
},
{
"vote": 12,
"title": "Master's degree in Computational Linguistics (Stuttgart)",
"text": "Hello everyone!\n\n\nI'm an Italian uni student, and I'm going to graduate in foreign languages and literature in October (English and German).\n\n\nI'm interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don't know whether I stand any chance of getting accepted into this programme.\n\n\nI took a couple of linguistics exams, but since in Italy Bachelors of linguistics don't exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I'm still afraid that my degree won't be enough to satisfy their admission requirements.\n\n\nI know the basics of programming (Python), but I actually don't know what I could do so as to stand a better chance of getting accepted. \n\n\nIs there anything else I could do?",
"date": "2021-06-06"
},
{
"vote": 3,
"title": "Debiasing large pretrained language models using distributional control",
"text": null,
"date": "2021-06-06"
},
{
"vote": 0,
"title": "Help",
"text": "[removed]",
"date": "2021-06-06"
},
{
"vote": 47,
"title": "More than 10K of you downloaded the free NLP transformers course... Wow!",
"text": "Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, r/learnmachinglearning and r/Python being two.\n\n\nThree days and 10823 downloads later, here we are! I thought we'd be lucky to hit 1K!\n\n\nIncredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I'm looking forward to working with all the students and helping you guys out, just please don't all ask me questions at once! ðŸ˜¬\n\n\nThanks all, truly humbled by the response - it's really \nreally\n cool, it has blown my mind.\n\n\nFor any of you that are still interested, I will leave a final discount link \nhere\n, thanks all!",
"date": "2021-06-06"
},
{
"vote": 4,
"title": "Fill mask with subwords",
"text": "[deleted]",
"date": "2021-06-06"
},
{
"vote": 9,
"title": "Does anyone know of coreference resolution tools where you can specify the entity?",
"text": null,
"date": "2021-06-06"
},
{
"vote": 5,
"title": "How to use BERT multilingual embedding",
"text": "I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). \nI first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.\n\n\nThen i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.\nSo, does anybody know how to use the embeddings from bert multilingual model.",
"date": "2021-06-05"
},
{
"vote": 3,
"title": "Incorrect tags when parsing test case",
"text": "Iâ€™m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). Iâ€™ve been playing around with Pythonâ€™s spaCy library and noticed that when I process the following string, I get incorrect tags:\n\n\nâ€œUser enters Password and press tab keyâ€\n\n\nâ€œpressâ€ is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been â€œpressEDâ€ and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?",
"date": "2021-06-05"
},
{
"vote": 1,
"title": "bachelor degree in computational linguistic course in germany",
"text": "[deleted]",
"date": "2021-06-05"
},
{
"vote": 1,
"title": "FREE 11 Hour NLP Transformers Course (Next 3 Days Only)",
"text": "[deleted]",
"date": "2021-06-04"
},
{
"vote": 3,
"title": "Training T5 model in just 3 lines of code with ONNX Inference",
"text": null,
"date": "2021-06-04"
},
{
"vote": 19,
"title": "Does a truly comprehensive rule-based grammar for the English language exist? Or is there any (recent) study that discusses their limitations?",
"text": "Hey,\n\n\nI'm working with rule-based language models and I'm wondering if there is anything that could be called a \"comprehensive set of rules\"  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I'd love to be proven wrong!\n\n\nI'm know of Lambeks Pregroup grammar and Montague grammar but I'm not sure if any of those formalism captures the English language without major flaws.\n\n\nI appreciate any references!",
"date": "2021-06-04"
},
{
"vote": 5,
"title": "Can you guys help me understand how word embeddings is not a symbolic representation of language?",
"text": "I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?",
"date": "2021-06-04"
},
{
"vote": 0,
"title": "Is there a cloud service where I can run my LDA code at a good speed?",
"text": "I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.\n\n\n&#x200B;\n\n\nEdit: I use gensim in python",
"date": "2021-06-04"
},
{
"vote": 1,
"title": "Advice for how to approach classifying apartment posts on facebook?",
"text": "I want to develop software that helps streamline connecting people with roomates/apartments. Right now I'm using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I'd like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.\n\n\nMy background is in computer vision for robotics with CNNs, so this is a totally different domain I'm not familiar with. From the research I've done so far, it sounds like I should look into entity recognition and relationship extraction. But I'm not sure what models are good for that, how much labeled data I need to get started.\n\n\nI'd be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.\n\n\nWhat models should I look into? What data labeling services/tools? How should I approach this?",
"date": "2021-06-04"
},
{
"vote": 5,
"title": "Build and query a book similarity index",
"text": "&#x200B;\n\n\nhttps://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player\n\n\nThis application builds a local txtai index using book data from \nopenlibrary.org\n. It supports natural language queries to find the best matching books.\n\n\nApplications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn't expect users to speak in an abrupt way that is typical with traditional token-based search systems.\n\n\nGitHub: \nhttps://github.com/neuml/txtai\nApplication: \nhttps://github.com/neuml/txtai/blob/master/examples/books.py",
"date": "2021-06-03"
},
{
"vote": 10,
"title": "What is happening when you say \"play me butter\" to your Alexa/Google/Siri?",
"text": "Hi,\n\n\nIt might be a novice question, but could be a deeper one as well.\n\n\nAnyone out there please guide me with what is actually happening when you say,\n\n\nPlay me Butter\n\n\nto your Alexa/Google/Siri?\n\n\nWhat I know is that, the ML/NLP engine under the hood will\n\n\n\n\nClassify the intent -> Music Play\n\n\nRecognize what \"Butter\" means -> It is a song by BTS\n\n\nTell \"here's Butter by BTS\" to the user\n\n\nAnd actually play it.\n\n\n\n\nBut this is just the high-level sketch. For example, the word \"Butter\" was meaning the food ingredient before BTS launched the single.\n\n\nHow will the engine behind update their Knowledge Base according to new releases? In a timely manner?\n \n\n\nI can't imagine it's being done manually. And,\n\n\nWhat will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it'll be done with a single classifier.\n\n\nThanks a ton in advance!",
"date": "2021-06-03"
},
{
"vote": 25,
"title": "Text classification: When to use sequence models over bag-of-words model?",
"text": "In the recent update on his upcoming book \"Deep Learning with Python. 2nd edition\" F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  > 1500 one should use a sequence model, if  it is < 1500, then one should use a bag-of-bigrams. \n\n\nIt seems they didn't publish this finding in a research paper but only in a \nGoogle guide to text classification\n (without any names except 'Google'). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments \" across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures\" (\nsource\n). \n\n\nAs this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?",
"date": "2021-06-03"
},
{
"vote": 1,
"title": "Readlist of NLP Research Paper Summary Blog",
"text": "[deleted]",
"date": "2021-06-03"
},
{
"vote": 1,
"title": "Readlist of NLP Research Paper Summary Blog",
"text": "[deleted]",
"date": "2021-06-03"
},
{
"vote": 8,
"title": "Assessing the â€œValueâ€ of a question.",
"text": "Hi, All. I am new to NLP but have a problem I'd like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.\n\n\nDid a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in '?'. So, that's all the questions.\n\n\nIn 3M+ utterances, there are a bunch of questions. Many are...not good questions.\n\n\n'Beth, are you out there?'\n\n\n'What do you say, Bob?'\n\n\n'Is that right?'\n\n\nFewer are actually good or more meaningful questions.\n\n\n'Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?'\n\n\n\"The issue in the Hollywood writer's strike is, do writers get paid for work that winds up online?\"\n\n\nIn general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).\n\n\nAnyways, I always open to learn something new. If anyone can point me in the right direction, i'd appreciate it. Thanks!\n\n\n&#x200B;\n\n\nCode thus far for anyone interested:\n\n\nimport nltk\n\n\nfrom nltk.corpus import stopwords\n\n\nfrom nltk import word_tokenize\n\n\nfrom nltk import sent_tokenize\n\n\nimport numpy as np\n\n\nimport pandas as pd\n\n\nnltk.download\n(&#039;nps_chat&#039;)\n\n\n&#x200B;\n\n\nnpr = pd.read_csv(&#039;C:\\Users\\...\\Desktop\\Python Scripts\\Data\\utterances.csv&#039;)\n\n\nnpr = npr[npr[&#039;utterance&#039;].notna()]\n\n\n&#x200B;\n\n\ntokens = npr[&#039;utterance&#039;].apply(lambda x: sent_tokenize(x))\n\n\n#put tokens into the DF\n\n\nnpr[&#039;utterance_tokenized&#039;] = tokens\n\n\n#build lists of question sentences\n\n\nis_question = npr[&#039;utterance_tokenized&#039;].apply(lambda x:\n\n\n[q for q in x if &#039;?&#039; in q])\n\n\n#put questions into the DF\n\n\nnpr[&#039;questions&#039;] = is_question\n\n\n#identify non-empty lists of questions\n\n\nhas_question = npr[&#039;questions&#039;].apply(lambda x: True if (len(x) &gt; 0) else False)\n\n\n#put boolean results into data frame\n\n\nnpr[&#039;has_questions&#039;] = has_question",
"date": "2021-06-02"
},
{
"vote": 1,
"title": "What is Annotation Tool",
"text": "[removed]",
"date": "2021-06-02"
},
{
"vote": 2,
"title": "Sentiment Analysis with flair on poetry - model evaluation",
"text": "Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  \n\n\nIf someone could please help me, that would be great. Thank you!",
"date": "2021-06-02"
},
{
"vote": 17,
"title": "Few Shot Learning in BERT",
"text": "Hi, I'm interested to know whether the few-shot learning can be done on BERT for sentence similarity.  \n\n\nUpdate: Found the pipeline in hugging face for zero-shot learning.\nhttps://huggingface.co/facebook/bart-large-mnli\nBut still, I'm not sure about the few-shot though.",
"date": "2021-06-02"
},
{
"vote": 1,
"title": "How to Build Your Own Real Estate Application like Zillow and Make it Thrive?",
"text": null,
"date": "2021-06-02"
},
{
"vote": 5,
"title": "Assigning a category to a word",
"text": "Hey there,\nI am waiting currently waiting my master's thesis and I had an idea for an analysis. \nI have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don't really get \"super-categories\" of these words. \nI would love to have something like:\nf(\"cars-for-sale.com\")=  \"car sales website\".\nAm I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.",
"date": "2021-06-01"
},
{
"vote": 5,
"title": "Minimum text length in diary entries for detecting emotions/sentiment and classifying levels of mental health issues",
"text": "Hi, \n\n\nI am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.\n\n\nThanks!",
"date": "2021-06-01"
},
{
"vote": 3,
"title": "[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video]",
"text": null,
"date": "2021-06-01"
},
{
"vote": 1,
"title": "Kabirdas",
"text": null,
"date": "2021-06-01"
},
{
"vote": 19,
"title": "Facebook AI Demonstrates How The Power Of Transfer Learning Can Boost Code Autocompletion Accuracy By Over 50%",
"text": "Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmersâ€™ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.\n\n\nIn a \npaper\n, a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.\n\n\nSummary: \nhttps://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/\n\n\nPaper: \nhttps://arxiv.org/pdf/2105.05991.pdf",
"date": "2021-06-01"
},
{
"vote": 7,
"title": "Your opinions: Where are conversational interfaces actually useful?",
"text": "Hi. I'm working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).\n\n\nSo, in the spirit of the \"are chatbots just a fad?\" I would like to know your opinions on the following subjects:\n\n\nAre the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is \"cool\" but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don't?",
"date": "2021-05-31"
},
{
"vote": 6,
"title": "[2105.13626] ByT5: Towards a token-free future with pre-trained byte-to-byte models",
"text": null,
"date": "2021-05-31"
},
{
"vote": 9,
"title": "Into NLP - Part-of-speech tagging",
"text": null,
"date": "2021-05-31"
},
{
"vote": 4,
"title": "BERT - Annotated Paper + Paper Summary",
"text": "Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.\n\n\nAs a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!\n\n\nPaper Summary -  \nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\nAnnotated Paper -  \nhttps://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf",
"date": "2021-05-31"
},
{
"vote": 2,
"title": "Structuring free text, then performing analysis vs. Performing analysis on unstructured free text",
"text": "Hi reddit, NLP newbie here\n\n\nI am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. \n\n\nFor context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?",
"date": "2021-05-31"
},
{
"vote": 2,
"title": "Python bindings for LibreTranslate",
"text": null,
"date": "2021-05-31"
},
{
"vote": 1,
"title": "PytPython bindings for LibreTranslate",
"text": "[deleted]",
"date": "2021-05-30"
},
{
"vote": 1,
"title": "Training data creation for BERT",
"text": "[deleted]",
"date": "2021-05-30"
},
{
"vote": 27,
"title": "Facebook AI releases Dynaboard: A New Evaluation platform for NLP Models",
"text": "Last year, Facebook AI released \nDynabench\n, a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called \nDynaboard\n. Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.\n\n\nDynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.\n\n\nFull Summary: \nhttps://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/\n\n\nGithub: \nhttps://github.com/facebookresearch/dynalab\n\n\nPaper: \nhttps://dynabench.org/dynaboard.pdf",
"date": "2021-05-30"
},
{
"vote": 13,
"title": "Spacy Matcher, POS Tagging and Grammatical Errors",
"text": "Hi everyone, \n\n\nI'm a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. \n\n\nSo I'm actually looking for suggestions regarding a particular POS tagging problem as seen below:\n\n\n\"I think very good idea.\" \n\n\nThere, the learner obviously means \"I think -it is a- very good idea\" but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. \n\n\nHow could one deal with such sentences from a matching/POS tagging perspective? \n\n\nI have been considering \"next word generation\" but so far I haven't tested it properly. So any suggestion is welcome. \n\n\nThanks in advance big time.",
"date": "2021-05-30"
},
{
"vote": 2,
"title": "Loading mbart-large-50-one-to-many-mmt is very slow",
"text": "Whenever i try to run :\nmodel = MBartForConditionalGeneration.from_pretrained(\" [local path]/mbart-large-50-one-to-many-mmt\")\nMy computer ether freezes or it takes 15-20 minutes to load the model.\n\n\nI am using it for translation\nCode:Â \nhttps://huggingface.co/facebook/mbart-large-50-one-to-many-mmt\n\n\nAny solution fo this?\n\n\n-Thanks",
"date": "2021-05-30"
},
{
"vote": 1,
"title": "FastSpeech2 DTW computing, and other TTS evaluation methods",
"text": "[deleted]",
"date": "2021-05-28"
},
{
"vote": 1,
"title": "60sec highlights - ExpireSpan",
"text": "[deleted]",
"date": "2021-05-28"
},
{
"vote": 20,
"title": "Any free open-access NLP annotation tools?",
"text": "I am looking for a web based tool for an NLP annotation task (similar to docanno for example). \n\n\nThe catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.\n\n\nAny tool that comes close to this criteria?",
"date": "2021-05-28"
},
{
"vote": 2,
"title": "Training Question Answering Models from Synthetic Data (Research Paper Summary)",
"text": "[deleted]",
"date": "2021-05-28"
},
{
"vote": 3,
"title": "Semantic similarity between programming languages and math terminology",
"text": "I've wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like 'vector' and 'matrix' and programming terminology like 'list/array' and '2d array'. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I'd want a binary output that takes two strings as input to compare.\n\n\nQuestion is what would be the approach to a problem like this, would it need carefully labelled data ('translating' the terminology between the two topics) or is a self-supervised method at all possible? I've only recently got into data so I could be way off here on what is possible and what is not.",
"date": "2021-05-28"
},
{
"vote": 2,
"title": "[P] Do Context-Aware Translation Models Pay the Right Attention?",
"text": null,
"date": "2021-05-28"
},
{
"vote": 8,
"title": "What is considered as a small learning rate?",
"text": "I using the BERT transformer to solve the classification problem.\n\n\nI am confused with the size of the learning rate of the BERT\n\n\nThe author suggests of using one of the following parameters\n\n\nlearning rates: 3e-4, 1e-4, 5e-5, 3e-5\n\n \n\n\n\nI know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.\n\n\n&#x200B;\n\n\nWhen  I use a learning rate of 1e-5 I get the following result\n\n\n{&quot;train&quot;: {&quot;eval_examples_count&quot;: 8548, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.721, &quot;f1_macro&quot;: 0.7201, &quot;accuracy&quot;: 0.7255, &quot;roc_auc&quot;: 0.8883}, &quot;time_spent&quot;: &quot;0:02:06&quot;}}\n{&quot;valid&quot;: {&quot;eval_examples_count&quot;: 2849, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.6766, &quot;f1_macro&quot;: 0.6784, &quot;accuracy&quot;: 0.6816, &quot;roc_auc&quot;: 0.8545}, &quot;time_spent&quot;: &quot;0:00:42&quot;}}\n{&quot;test&quot;: {&quot;eval_examples_count&quot;: 2850, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.7003, &quot;f1_macro&quot;: 0.7008, &quot;accuracy&quot;: 0.7046, &quot;roc_auc&quot;: 0.8685}, &quot;time_spent&quot;: &quot;0:00:42&quot;}}\n\n\n\nHowever when I use 5e-5:\n\n\n{&quot;train&quot;: {&quot;eval_examples_count&quot;: 8548, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.1617, &quot;f1_macro&quot;: 0.1647, &quot;accuracy&quot;: 0.3269, &quot;roc_auc&quot;: 0.5159}, &quot;time_spent&quot;: &quot;0:02:07&quot;}}\n{&quot;valid&quot;: {&quot;eval_examples_count&quot;: 2849, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.1743, &quot;f1_macro&quot;: 0.1704, &quot;accuracy&quot;: 0.3412, &quot;roc_auc&quot;: 0.5321}, &quot;time_spent&quot;: &quot;0:00:42&quot;}}\n{&quot;test&quot;: {&quot;eval_examples_count&quot;: 2850, &quot;metrics&quot;: {&quot;f1_weighted&quot;: 0.1758, &quot;f1_macro&quot;: 0.1705, &quot;accuracy&quot;: 0.3435, &quot;roc_auc&quot;: 0.5208}, &quot;time_spent&quot;: &quot;0:00:42&quot;}}\n\n\n\nIT DOES NOT LEARN ANYTHING\n\n\n&#x200B;\n\n\nSo I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? \n\n\nWhat is the ascending order of these learning rates?\n\n\nWhich one considered as big and which is considered as small?",
"date": "2021-05-28"
},
{
"vote": 22,
"title": "GPT-2 - Annotated Paper + Paper Summary",
"text": "The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.\n\n\nI went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!\n\n\nPaper Summary -  \nLanguage Models are Unsupervised Multitask Learners\n\n\nAnnotated Paper -  \nhttps://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf",
"date": "2021-05-27"
},
{
"vote": 6,
"title": "Extracting topics from 250k facebook posts",
"text": "Hi there!\n\n\nI'm working on a project in which I need to extract the topics from a collection of 250K facebook posts. \n\n\nCan someone recommend (in high level) what's the best approach to do something like that? Ways that will generate the best results.\n\n\nThanks!",
"date": "2021-05-26"
},
{
"vote": 1,
"title": "Deep Learning for Natural Language Processing",
"text": "[removed]",
"date": "2021-05-24"
},
{
"vote": 1,
"title": "Controllable Text Summarization in Python based on CtrlSum",
"text": null,
"date": "2021-05-24"
},
{
"vote": 33,
"title": "EleutherAI Develops GPT-3â€™s Free Alternative: GPT-Neo",
"text": "In todayâ€™s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.\n\n\nWith the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.\n\n\nOpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.\n\n\nFull Article: \nhttps://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/\n\n\nGithub: \nhttps://github.com/EleutherAI/gpt-neo",
"date": "2021-05-24"
},
{
"vote": 6,
"title": "Tokenising SpaCy constituency parse output",
"text": "Hi all, so I'm doing some constituency parsing with SpaCy and benepar, and i'd ideally like to be able to tag the words in a sentence with their corresponding constituents. \n\n\nCurrently however, Spacy allows you to print the parse string, but I'd like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; \n\n\nimport spacy\nimport benepar\n\nnlp = spacy.load(&#039;en_core_web_md&#039;)\n\nif spacy.__version__.startswith(&#039;2&#039;):\n  nlp.add_pipe(&#039;benepar&#039;, config={&#039;model&#039;: &#039;benepar_en3&#039;})\nelse:\n  nlp.add_pipe(&quot;benepar&quot;, config={&quot;model&quot;: &quot;benepar_en3&quot;})\n\ndoc = nlp(&quot;Last Tuesday, I thought to myself that I saw a cat.&quot;)\nsent = list(doc.sents)[0]\n\nconstituents = sent._.parse_string\nprint(constituents)\n\n&gt;&gt;&gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))\n\n\n\nI'd like for each word to be tagged with it's corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?",
"date": "2021-05-24"
},
{
"vote": 2,
"title": "Parsing text for syntactic structures then tagging the text with those structures.",
"text": "[deleted]",
"date": "2021-05-24"
},
{
"vote": 1,
"title": "Interesting seminars and talks?",
"text": "I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to \nhttp://researchseminars.org/\n for language technology / computation linguistics / that sort of space?\n\n\nOr, if you don't know of a large universal one, does your research group have a website where you publish your upcoming seminars?",
"date": "2021-05-24"
},
{
"vote": 14,
"title": "One sentence highlight for every NAACL-2021 Paper",
"text": "Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.\n\n\nhttps://www.paperdigest.org/2021/05/naacl-2021-highlights/",
"date": "2021-05-24"
},
{
"vote": 15,
"title": "A Graph-based Text Similarity Method with Named Entity Information in NLP",
"text": null,
"date": "2021-05-23"
},
{
"vote": 4,
"title": "Using document formatting for NLU",
"text": "Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that thereâ€™s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.",
"date": "2021-05-21"
},
{
"vote": 5,
"title": "Conversational AI Platform",
"text": "[removed]",
"date": "2021-05-21"
},
{
"vote": 4,
"title": "Text Extraction",
"text": "New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.",
"date": "2021-05-21"
},
{
"vote": 2,
"title": "Techniques for Evaluating Text Generation Systems",
"text": "[deleted]",
"date": "2021-05-20"
},
{
"vote": 2,
"title": "A New Google Research Introduces FNet By Replacing Self-Attention Sublayers With Simple Linear Transformations Achieving 92% Accuracy and Runs Up To Seven Times Faster On GPUs And Twice As Fast On TPUs (Paper link in comments)",
"text": null,
"date": "2021-05-20"
},
{
"vote": 2,
"title": "Is this approach to summary production novel?",
"text": "Hello! Been playing around with summary generators, and I'm pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author's typical writing style. Basically I'm just trying to find out if there's a word for this algo yet or not. Project is called Bite, and can be found \nhere\n. \n\n\n&#x200B;\n\n\nSay we have the following corpus:  \n\n\n\"Mary had a little lamb.\"  \n\n\nBite will tokenize the sentence like so:  \n\n\n\n\nmary had\n\n\nmary had a \n\n\nmary had a little\n\n\nmary had a little lamb\n\n\nhad a\n\n\nhad a little\n\n\nhad a little lamb\n\n\na little\n\n\na little lamb\n\n\na lamb\n\n\n\n\nNext, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. \n\n\n&#x200B;\n\n\nIs there a word for this type of categorization?",
"date": "2021-05-19"
},
{
"vote": 3,
"title": "How to detect brainwashing in texts - seems like an interesting tool",
"text": "[removed]",
"date": "2021-05-19"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-05-19"
},
{
"vote": 29,
"title": "MUM: a new AI milestone for understanding information",
"text": null,
"date": "2021-05-18"
},
{
"vote": 16,
"title": "Deploying a transformer-based text classification NLP model with FastAPI",
"text": "Hello all,\n\n\nRecently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook's Bart Large MNLI model).\n\n\nFastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the \nNLPCloud.io\n API.\n\n\nHere's the article:\n\n\nhttps://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html\n\n\nI'd love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?\n\n\nThanks!",
"date": "2021-05-18"
},
{
"vote": 7,
"title": "Explanability for Transformers with Transformers-Interpret â€” A Model Explainability Tool",
"text": null,
"date": "2021-05-18"
},
{
"vote": 1,
"title": "Learning Spanish",
"text": null,
"date": "2021-05-18"
},
{
"vote": 2,
"title": "Spacy 3.0 multi class Textcat",
"text": "If anyone has any experience with this I would love to pick your brain / ask you a little assistance. \n\n\nBeen struggling to correctly initialize my model.",
"date": "2021-05-17"
},
{
"vote": 19,
"title": "Text Summarisation Techniques in NLP",
"text": null,
"date": "2021-05-17"
},
{
"vote": 13,
"title": "Automatic question generation from input paragraph",
"text": "I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,\n\n\n\n\nsentence selection\n\n\ncomplex sentence simplification\n\n\nsentence classification based on POS and NE tagged information\n\n\n\n\nI would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.",
"date": "2021-05-17"
},
{
"vote": 15,
"title": "New to NLP. Looking for library recommendations.",
"text": "Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.\n\n\nI'm familiar with Python but don't really know my way around what NLP libraries are out there as of 2021. \nDoes anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?\n\n\nThank you in advance.",
"date": "2021-05-16"
},
{
"vote": 1,
"title": "6 Exceptional GitHub Repos for All Developers â€” Part 1",
"text": "[removed]",
"date": "2021-05-16"
},
{
"vote": 9,
"title": "CQP vs LexiDB",
"text": "Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB's CQP.\n\n\nThese tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for \"the <adjective> *\" and the tool will quickly return a list of matches.\n\n\nI have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can't get it to work.",
"date": "2021-05-15"
},
{
"vote": 10,
"title": "10 popular Keyword Extraction Techniques in NLP",
"text": "This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. \n\n\nHere, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! ðŸŽ‰ \n\n\nhttps://link.medium.com/4ah0jdgXhgb",
"date": "2021-05-15"
},
{
"vote": 0,
"title": "Join our language learning based community. Learn &amp; teach languages by speaking &amp; chatting. Meet people from all over the world. 40+ languages &amp; more than 1000 language-lover!",
"text": "[deleted]",
"date": "2021-05-15"
},
{
"vote": 9,
"title": "Ides for making a podcast summarizer?",
"text": "In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.\n\n\nMy thought is to divide the problem into two main steps:\n\n\n\n\nAudio to transcript\n\n\nTranscript summation/extraction of main topics\n\n\n\n\nWhat ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?\n\n\nSuper excited to hear your thoughts!",
"date": "2021-05-15"
},
{
"vote": 7,
"title": "Large summarization dataset in Portuguese",
"text": "I'm looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset (\nhttps://huggingface.co/datasets/cnn_dailymail\n) but in Portuguese.\n\n\nDoes anyone know of such a dataset?",
"date": "2021-05-14"
},
{
"vote": 4,
"title": "Evolution of NLP search methods and the latest method - Neural Search. What is it and how to get started with it",
"text": null,
"date": "2021-05-14"
},
{
"vote": 3,
"title": "SpaCy 3.0 Text Classifier",
"text": "Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling Iâ€™m making a mistake regarding the multiple classes",
"date": "2021-05-14"
},
{
"vote": 6,
"title": "After-the-fact conversational topic analysis?",
"text": "I've developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they've been introduced) and save the interactions and the full text of the dialog between them by date.  \n\n\nI am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of \"categorize\" the subjects that interest various folks that the robots talk with.  \n\n\nI've found things like this:  \nhttps://convokit.cornell.edu/documentation/tutorial.html\n\n\nThese seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.",
"date": "2021-05-13"
},
{
"vote": 2,
"title": "Anyone here have experience with the Siri Production Studio at Apple?",
"text": "[deleted]",
"date": "2021-05-13"
},
{
"vote": 4,
"title": "Parrot: Paraphrase based utterance augmentation framework | Python #NLP",
"text": "[deleted]",
"date": "2021-05-13"
},
{
"vote": 3,
"title": "Webpage summarization: Given the URL of a company webpage, what does the company do?",
"text": "I'm faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?\n\n\nAre there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.",
"date": "2021-05-13"
},
{
"vote": 2,
"title": "Linguistics student looking for advice to advance in NLP/Computational Linguistics",
"text": "Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?",
"date": "2021-05-13"
},
{
"vote": 31,
"title": "Cognitive Scientist Terrence Deacon Says Current AI Lacks Symbolic Representations &amp; True Language Comprehension Abilities",
"text": null,
"date": "2021-05-12"
},
{
"vote": 1,
"title": "Are AAAI and IAAI tier I conferences?",
"text": "Are \nIAAI\n INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?",
"date": "2021-05-12"
},
{
"vote": 8,
"title": "Leveraging BERT for Extractive Text Summarization on Lectures",
"text": null,
"date": "2021-05-12"
},
{
"vote": 1,
"title": "Machine learning workflows to summarize, translate, transcribe and more",
"text": "[deleted]",
"date": "2021-05-12"
},
{
"vote": 2,
"title": "NLP Keywords to Sentences Text Generation with {keytotext}",
"text": null,
"date": "2021-05-11"
},
{
"vote": 22,
"title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking",
"text": null,
"date": "2021-05-11"
},
{
"vote": 28,
"title": "Generating Sentences from Keywords using Transformers",
"text": "â€œkeytotextâ€ introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. \n\n\nFor example- \nInput: India, Capital, New Delhi\nOutput: The capital of India is New Delhi.\n\n\nInteresting? Then read this walkthrough, \nBlog: \nhttps://lnkd.in/dS9_AH7\n\nGitHub: \nhttps://github.com/gagan3012/keytotext",
"date": "2021-05-10"
},
{
"vote": 3,
"title": "NER for Resume parsing",
"text": "Hello,\n\n\nI need to do resume parsing with French resumes. I will use Prodigy (\nprodi.gy\n) to annotate my dataset and make my model.\n\n\nMy question is : is it better to annotate with the full resume or annotate line by line  ,\n\n\nfor me, the position of the text in the full document is an information but maybe spacy don't care.\n\n\nThanks !",
"date": "2021-05-10"
},
{
"vote": 2,
"title": "Is there a python library or API that is able to check the grammar of a sentence?",
"text": null,
"date": "2021-05-09"
},
{
"vote": 10,
"title": "Pooled Contextualised Embeddings for NER | Research Papers Summary 017",
"text": null,
"date": "2021-05-09"
},
{
"vote": 7,
"title": "Text Generation using GPT-Neo",
"text": null,
"date": "2021-05-09"
},
{
"vote": 8,
"title": "[Q] Why is interpretability important in natural language processing? This is easier to answer for models that make high stakes decisions (e.g., surgical risk assessment; self-driving car slamming brakes; etc.,), but I would like to understand why we care about interpretability in NLP.",
"text": null,
"date": "2021-05-08"
},
{
"vote": 0,
"title": "MLE/NLP Engineer Positions at Big Tech",
"text": "Hi,\n\n\nI'm currently a rising senior majoring in CS at a top 10 university in the US. I'm debating between graduating just with bachelor's or pursuing a master's in NLP/Human Language Technology (would only take me extra two semesters). I'm mainly interested in NLP, Text Mining and recommendation systems (haven't taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I'll have access to during my masters. If I graduate just with a bachelor's, then I'm considering an SWE role at big techs. If I end up doing a master's, then an applied ML position (MLE, NLP engineer, etc) at big techs.\n\n\nI have taken some courses related to NLP/DL; I enjoyed them, but at the moment I'm not sure if I liked it enough to do a master's in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.\n\n\nI'm wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?\n\n\nThank you!",
"date": "2021-05-08"
},
{
"vote": 2,
"title": "Are there any open-source aspect-oriented sentiment analysis APIs out there?",
"text": "Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I'd like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.\n\n\nThanks in advance.",
"date": "2021-05-07"
},
{
"vote": 2,
"title": "Recursive neural networks",
"text": "Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).\n\n\nDid anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?\n\n\nI am trying to do that on a \"pyramid\" binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.",
"date": "2021-05-07"
},
{
"vote": 18,
"title": "NLPCloud.io for transformer-based models in production (PyTorch and TensorFlow)",
"text": "Some of you might already know the \nNLPCloud.io\n project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn't have the skills to deploy their new models to production... \n\n\nI had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I'm happy to announce that, after weeks of hard work, \nit is now possible to deploy your own transformer-based models\n to \nhttps://nlpcloud.io\n , whether they are running on PyTorch or TensorFlow!\n\n\nIt can be useful in 2 situations:\n\n\n\n\nYou developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don't have advanced DevOps skills, etc.)\n\n\nYou already use one of the Hugging Face pre-trained models on \nNLPCloud.io\n. It's not working so bad but you want to fine-tune them in order to adapt them to your own needs.\n\n\n\n\nYou can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.\n\n\nInternally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don't hesitate to ask, I will be glad to comment).\n\n\nFor more details here is the API documentation: \nhttps://docs.nlpcloud.io/#upload-your-transformers-based-model\n\n\nI really hope you will like it and find it useful!\n\n\nI would love to have your opinion on this new feature. Please don't hesitate to answer this post!",
"date": "2021-05-07"
},
{
"vote": 1,
"title": "graph2vec: Learning Distributed Representations of Graphs | ML with Graphs (Paper Walkthrough)",
"text": "[deleted]",
"date": "2021-05-06"
},
{
"vote": 8,
"title": "Something like style transfer for language processing?",
"text": "This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person's style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!",
"date": "2021-05-06"
},
{
"vote": 2,
"title": "[Request] Curated Advanced NLP Resources",
"text": "Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)\nI think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they've changed the way we approach NLP problems today.\n\n\nI could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about \nNLP Progress\n and \nThe Super Duper NLP Repo\n.\n\n\nI need it for a project, and any help is greatly appreciated.",
"date": "2021-05-05"
},
{
"vote": 1,
"title": "I am working on a topic modelling paper and I need your help",
"text": "[removed]",
"date": "2021-05-05"
},
{
"vote": 36,
"title": "Emotion and Empathy in NLP",
"text": "Hey everyone!\n\n\nI'm a master's student currently doing my research in the field of NLP and I've recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I've made a simple \nlist of papers\n I've read so far (not complete yet). Feel free to let me know if you're interested or if you have any opinions or suggestions. Thanks and wish you have a great day.",
"date": "2021-05-05"
},
{
"vote": 8,
"title": "MILES â€” A language-agnostic text simplifier using multilingual BERT",
"text": "[deleted]",
"date": "2021-05-04"
},
{
"vote": 4,
"title": "EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings",
"text": null,
"date": "2021-05-04"
},
{
"vote": 0,
"title": "Chatbot with R",
"text": "Hi everyone!  \n\n\nI need to create a Bot that uses NLP with R!  \n\n\nhas anyone ever done this?  \n\n\nThank you so much!",
"date": "2021-05-04"
},
{
"vote": 1,
"title": "How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?",
"text": "How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?\n\n\n\n\n&#x200B;\n\n\nSuppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is \"He loves life and laughter.\" The tokens are:\n\n\n&#x200B;\n\n\nS1_life1-life, S1_life2-life, S1_life3-life, S1_life4-life. I need to write a praat code that will separate the \"life\" from the sentence text grid.\n\n\n&#x200B;\n\n\n\n\nWill the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, \"Life has a different meaning in the mountains.\" In here, they keyword is at the beginning of the sentence.",
"date": "2021-05-04"
},
{
"vote": 3,
"title": "Inevitable Manual Work involved in NLP",
"text": "I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? \n\n\nFor example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this \"class 1\") or a non-serious condition (let's call this \"class 0\"). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. \n\n\nThe problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as \"class 1\" or \"class 0\". For example, for \"class 0\" : one of the doctors could clearly write at the end of a report \"all medical tests were conducted and the results and were all negative\", and another doctor could end the report by saying \"the patient should seriously consider changing their lifestyle and eat healthier food. benign.\" . \n\n\nIn this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a \"serious condition\" or a \"non-serious condition\"? I was thinking of using something like \"sentiment analysis\" to capture the \"mood\" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is \"dark\" (serious condition) or \"light\" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?\n\n\nIn the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a \"serious\" or a \"non-serious\" condition?\n\n\nPS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",
"date": "2021-05-04"
},
{
"vote": 10,
"title": "Auto detect sources in a news text",
"text": "[deleted]",
"date": "2021-05-04"
},
{
"vote": 1,
"title": "Stop Word and Tokenization (with R)",
"text": "I am learning about NLP and trying to understand how to tokenize text and remove stop words.\n\n\nI tried the following line of code (from \"quanteada) and got the following error:\n\n\nfirst_method <- tokens(tdm) %>% tokens_remove(stopwords(\"en\"), pad = TRUE)\n\n\nError: tokens() only works on character, corpus, list, tokens objects.\n\n\nHas anyone ever gotten this error before?\n\n\nI posted the full details to my question over here: \nhttps://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects\n\n\nThanks",
"date": "2021-05-03"
},
{
"vote": 13,
"title": "The giant leaps in language technology -- and who's left behind",
"text": null,
"date": "2021-05-02"
},
{
"vote": 2,
"title": "How to output attention for each token in Transformer binary classification model",
"text": "[deleted]",
"date": "2021-05-02"
},
{
"vote": 1,
"title": "Extracting keywords with their frequencies in sklearn LDA?",
"text": "[deleted]",
"date": "2021-05-02"
},
{
"vote": 20,
"title": "LINE: Large-scale Information Network Embedding (Machine Learning with Graphs)",
"text": null,
"date": "2021-05-02"
},
{
"vote": 5,
"title": "What model to create a sentence generator?",
"text": "Hello,\nI'm new to this field.\nWhat model would you recommend for creating single sentences? I'm looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.",
"date": "2021-05-01"
},
{
"vote": 1,
"title": "Finding attributes of a character (person) from a text",
"text": "[deleted]",
"date": "2021-04-30"
},
{
"vote": 1,
"title": "Masterâ€™s Thesis in Computational Phonology",
"text": "Hey NLPeople,\n\n\n&#x200B;\n\n\nat the end of the year, Iâ€™m planning on writing my Masterâ€™s Thesis. My Masterâ€™s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.\n\n\nBecause we live in a productivity-driven dystopia apparently Iâ€™m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.\n\n\n&#x200B;\n\n\nDo you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?\n\n\n&#x200B;\n\n\nIâ€™m really only looking for general inspiration right now.\n\n\n&#x200B;\n\n\nThanks a lot!",
"date": "2021-04-30"
},
{
"vote": 2,
"title": "Concerns about studying NLP",
"text": "I asked you guys about the opinion of choosing which domain to \n\n\nchoose between NLP and CV few weeks ago.\n\n\n&#x200B;\n\n\nI finally decided to study NLP area more deeply.\n\n\n&#x200B;\n\n\nBut I have concerns. It is really difficult to get accepted in \n\n\nNLP Labs and artificial intelligence graduate school.\n\n\n&#x200B;\n\n\nSo I made two plans if I failed to have master degree in graduate school.\n\n\n&#x200B;\n\n\nPlan A\n\n\nStudying NLP in data science graduate school\n\n\nThis plan have risk; there are no lab in this graduate school \n\n\nso I have to study NLP by myself.\n\n\n&#x200B;\n\n\nPlan B\n\n\nStudying CL(Computational Linguistics) in language graduate school\n\n\nAlso this plan have risk; the same reason with Plan A\n\n\n&#x200B;\n\n\nThere are not that many NLP labs in Korea, so I wonder if I go to\n\n\nplan A and B, I can work in NLP field.\n\n\n&#x200B;\n\n\nThe best way is to read articles published by data and language graduate school, \n\n\nand NLP lab in AI graduate school and compare it, but the sad thing is\n\n\nI don't have enough time left to contact labs and graduate schools above.\n\n\n&#x200B;\n\n\nI guess that there are some people who graduated those graduate school \n\n\nthat I mentioned, or at least worked with the people who went to Plan A and Plan B.\n\n\n&#x200B;\n\n\nI am looking forward to have any advice from you guys.\n\n\nI appreciate to the people who gave me advice last time when I asked about\n\n\nthe prospective domain to choose.\n\n\n&#x200B;\n\n\nThank you for reading my post and wish you guys all have good days today !",
"date": "2021-04-30"
},
{
"vote": 1,
"title": "BeGig Hackathon",
"text": "[removed]",
"date": "2021-04-30"
},
{
"vote": 6,
"title": "Parallelization for Author-topic models (atmodel) in Gensim",
"text": "While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues",
"date": "2021-04-29"
},
{
"vote": 7,
"title": "Looking for Datasets on Meronyms and Hypernyms - Full sentences",
"text": "Hi everyone, \n\n\nI am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I've found only sets of entity pairs and \nmy research requires full sentences.\n \n\n\nAny suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!\n\n\nThanks",
"date": "2021-04-28"
},
{
"vote": 10,
"title": "Call for Teachable NLP Challenge",
"text": "Hi NLP lovers,\n\n\nI found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?\n\n\nTeachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.\n\n\n\n\nWhen: 05/05/2021 - 05/18/2021 11:59 EDT\n\n\nHow: You just need to submit your AI model link and explanations on your AI (Good example: \nhttps://forum.ainetwork.ai/c/ai-showcase/11\n)\n\n\nPrizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)\n\n\n\n\nTo participate, submit your info via \nhttps://forms.gle/XfUuNSS2heAn7JtH7\n. You will receive an invitation email!\n\n\nCheck how Teachable NLP works: \nhttps://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65\nOr watch a 1-minute tutorial video: \nhttps://youtu.be/hzujZOT1qz8",
"date": "2021-04-28"
},
{
"vote": 1,
"title": "PEGASUS: Pre-training with Gap-Sentences for Abstractive Summarization | Research Paper Walkthrough",
"text": "[deleted]",
"date": "2021-04-27"
},
{
"vote": 1,
"title": "NEED HELP! Searching for similar gods / System of classification and comparison of gods according to the texts of different mythologies.",
"text": "[removed]",
"date": "2021-04-26"
},
{
"vote": 1,
"title": "Gradio Web Demo for SimCSE: Simple Contrastive Learning of Sentence Embeddings",
"text": "[deleted]",
"date": "2021-04-26"
},
{
"vote": 5,
"title": "What specific math subjects do I need to fully understand every NLP/Computational Linguistics journal article (espeically the most important ones)?",
"text": "Please state specific math subjects in the format used below (these are simply examples).\n\n\n\n\nCalculus 1\n\n\n\n\nCalculus 2\n\n\n\n\nCalculus 3\n\n\n\n\nCalculus 4\n\n\n\n\nDiscrete Math\n\n\n\n\nIntroduction to Probability\n\n\n\n\nIntroduction to Statistics\n\n\n\n\nStatistical Inference\n\n\n\n\nLinear Algebra\n\n\n\n\n\n\nThank You",
"date": "2021-04-26"
},
{
"vote": 7,
"title": "BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015",
"text": null,
"date": "2021-04-25"
},
{
"vote": 11,
"title": "Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough)",
"text": null,
"date": "2021-04-25"
},
{
"vote": 7,
"title": "Feedback for NLP project",
"text": "[deleted]",
"date": "2021-04-25"
},
{
"vote": 17,
"title": "I fine-tuned a language model on left and right leaning political commentary on Reddit",
"text": "https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57\n\n\nInformation:\nFor my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of \"left\" and \"right\" leaning subreddits to classify comments and subreddit's.\n\n\nI mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.\n\n\nI used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.\n\n\nI also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.\n\n\nDisclaimer:\nThe venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.\n\n\nView this project: \nhttps://reddit-political-analysis.com/",
"date": "2021-04-25"
},
{
"vote": 3,
"title": "Best way to find the (best) solution to any task?",
"text": "What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.",
"date": "2021-04-24"
},
{
"vote": 1,
"title": "Text Classification using Convolutional Neural Network with TensorFlow 2.1 in Python | #NLProc",
"text": "[deleted]",
"date": "2021-04-24"
},
{
"vote": 23,
"title": "Latest trends in topic modelling?",
"text": "Any expert in the domain here?\n\n\nI had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there's no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?",
"date": "2021-04-24"
},
{
"vote": 29,
"title": "Introducing mlconjug3. A Python library to conjugate verbs in French, English, Spanish, Italian, Portuguese and Romanian (with more in beta version) using Machine Learning techniques.",
"text": "Hi everybody!\n\n\n&#x200B;\n\n\nA couple of years ago, I made a \npost to announce the release of mlconjug\n,  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.\n\n\nSince then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.\n\n\nIt has also been used in several Academic publications, for example: \n\"Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems\"\n where it is used for automatically grading students essays, and  United States citizenship exam questions.\n\n\nI released a new and improved version of the software, called \nmlconjug3\n as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).\n\n\nThose new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.\n\n\nYou can install mlconjug3 from \nPyPi\n or \nAnaconda\n.\n\n\nSome of the features of mlconjug3 are the following:\n\n\n\n\nEasy to use API.\n\n\nIncludes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.\n\n\nEasily train new models or add new languages.\n\n\nEasily integrate MLConjug in your own projects.\n\n\nCan be used as a command line tool.\n\n\n\n\nI invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.\n\n\nThanks Everyone,\n\n\nPeace,\n\n\nSekouDiaoNlp",
"date": "2021-04-23"
},
{
"vote": 11,
"title": "AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System",
"text": "I didn't build it. But I found it interesting. What could be other alternative approach to solve this problem?\n\n\nResearch Paper",
"date": "2021-04-22"
},
{
"vote": 5,
"title": "Any idea what technology google map uses to tag reviews into categories",
"text": "I was reading reviews for a food outlet and saw tags like \"wrap\" \"hummus\" \"chicken\" \"lady\" \"platter\" \"sauce\"",
"date": "2021-04-22"
},
{
"vote": 3,
"title": "Tutorial for coreference resolution with BERT?",
"text": "[deleted]",
"date": "2021-04-22"
},
{
"vote": 1,
"title": "Help with designing chatbot for my college course",
"text": "[deleted]",
"date": "2021-04-21"
},
{
"vote": 1,
"title": "Implementing Deep Learning Algorithms with TensorFlow 2.0",
"text": "[removed]",
"date": "2021-04-21"
},
{
"vote": 2,
"title": "Using Deep Learning Based On Semantic Features for Emotion Classification in Tweets",
"text": "Hello\n\n\nI am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.",
"date": "2021-04-21"
},
{
"vote": 52,
"title": "Free NLP assignments",
"text": "If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: \nhttps://openclass.ai/catalog/nlp\n\n\nThese assignments leverage learning principles proven to optimize knowledge retention, and identify & bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.\n\n\nOur mission with this project is to improve & democratize education. We're looking to grow our community of NLP educators, so if you're interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)",
"date": "2021-04-21"
},
{
"vote": 2,
"title": "Is causal language modeling (CLM) vs masked language modeling (MLM) a common distinction in NLP research?",
"text": "The \nhuggingface documentation\n states:\n\n\n>GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.\n\n\nIs this a common distinction you'd find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don't understand why you would call BERT & co. \"masked language models\", since causal language models do the actual masking in next token prediction.",
"date": "2021-04-21"
},
{
"vote": 3,
"title": "How to add the Gradient Accumulation to my model?",
"text": "I am following this tutorial  \ndp_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb at master Â· deepmipt/dp_tutorials Â· GitHub\n\n\nHowever, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I canâ€™t find any tutorial that explains it well.\n\n\n&#x200B;\n\n\nI already reduced the batch size however it did not helped that much\n\n\nThank you.",
"date": "2021-04-20"
},
{
"vote": 3,
"title": "For binary classification, is it better to train with more focused examples?",
"text": "I'm in the process of labeling data that I'll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  \n\n\nIf I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  \n\n\nOr is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is \"A\")?  \n\n\nIf I keep the paragraphs together, I don't want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?",
"date": "2021-04-20"
},
{
"vote": 2,
"title": "Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough)",
"text": null,
"date": "2021-04-20"
},
{
"vote": 5,
"title": "Named Entity Recognition and possible extensions such as Non-noun phrases",
"text": "Hey, people! Hope you are all well!\n\n\nI've been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.\n\n\nAs a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  \n\n\nAlso, if there are other interesting extensions you might know or believe may occur, please share!",
"date": "2021-04-19"
},
{
"vote": 3,
"title": "NLP and Skill Transfer",
"text": "I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? \n\n\nThank you very much! \n\n\nNote: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .\n\n\nEdit: language mistakes",
"date": "2021-04-19"
},
{
"vote": 17,
"title": "NLP Help | Scraping Question",
"text": "Hello, \n\n\nI have a few hundred txt documents, each containing a few sentences about someone's history with substance abuse. \n\n\nBased on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: \nstatus\n (e.g., past, current, none), \nmethod\n (e.g., inhale, chew), \namount\n (e.g., 2 packs, 3-4 glasses), and \nfrequency\n (e.g., a day). \n\n\nExample txt file: \"He does not use tobacco. He sometimes drinks wine. He does not use drugs ever.\" \n\n\nMy dilemma:\n again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), \nbut the wording/sentence structure varies a lot\n --> the \nstatus\n entity, for example, could see 'past', 'currently', 'still', 'sometimes', 'on weekends', 'back in 1984', etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. \nAny ideas?\n\n\nThank you so much for your consideration.",
"date": "2021-04-19"
},
{
"vote": 1,
"title": "Deciding between Saarland and Gothenburg",
"text": "Hello everyone!\n\n\nOver the last few months I've been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I'm left with the choice between programs at Saarland University and the University of Gothenburg.\n\n\nMy research so far has favored Saarland: it's often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).\n\n\nWhile this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program's study coordinator, Saarland does not offer individual support for placement, and it appears that the university's relationship with local companies is not a strong as Gothenburg's appears to be.\n\n\nMy question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?\n\n\nThank you in advance!",
"date": "2021-04-18"
},
{
"vote": 3,
"title": "I just found this insane machine learning Humble Bundle on Twitter. Just wanted to share it with you guys!",
"text": null,
"date": "2021-04-18"
},
{
"vote": 2,
"title": "Word embeddings and neural machine translation",
"text": "Can someone help me understand what is the purpose of word embeddings in neural machine translation models? \n\n\nI understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn't the neural model learn everything itself without needing any embedding space?",
"date": "2021-04-18"
},
{
"vote": 15,
"title": "Generating Datasets with Pretrained Language Models",
"text": "In our most recent paper, we introduce \"Datasets from Instructions\" (DINO ðŸ¦•) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.\n\n\nThis is an early draft, so I'd be very happy to hear your thoughts ðŸ˜Š\n\n\nðŸ“„ Paper: \nhttps://arxiv.org/abs/2104.07540\n\n\nðŸ–¥ï¸ Code: \nhttps://github.com/timoschick/dino",
"date": "2021-04-18"
},
{
"vote": 4,
"title": "Does anyone know of any biomedical entity search engines out there?",
"text": "I'm doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the \nBiomedical Entity Search Tool (BEST)\n. Polysearch is also a candidate, but for some reason it's not working anymore.\n\n\nI'd appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!",
"date": "2021-04-18"
},
{
"vote": 3,
"title": "Why is batch size a relevant hyperparameter for BERT?",
"text": "BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT's performance? Where else does it factor in?",
"date": "2021-04-18"
},
{
"vote": 25,
"title": "BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough)",
"text": null,
"date": "2021-04-18"
},
{
"vote": 2,
"title": "Please give me some suggestions.",
"text": "Iâ€™m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and Iâ€™m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance",
"date": "2021-04-18"
},
{
"vote": 1,
"title": "Di resources",
"text": "What are some good document intelligence resources to begin with?",
"date": "2021-04-17"
},
{
"vote": 9,
"title": "Project NLP / ML from scratch - Beginner",
"text": "Hi All, \n\n\nI am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)\nIn a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit expertsÂ´ posts in \nicobench.com\n to determine a sentiment - based on Bayes Classifier. We have used Python. \n\n\nMy goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: \n\n\n\n\nSetup scraper/crawler to obtain data from several websites\n\n\nStore the data in google firebase /firestore\n\n\nCleanup the data for ML\n\n\nCreate a training set/test set\n\n\nBuild a ML model (classification/prediction)\n\n\nMeasure effectiveness\n\n\nReport on what I did\n\n\n\n\nA) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data > store > clean > give label would take a relevant amount of time.\n\n\nB) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. \n\n\nC) Which models can be more interesting to compare? \n\n\nI thank you in advance for the attention given to this post. \n\n\nHave a good weekend",
"date": "2021-04-17"
},
{
"vote": 6,
"title": "Benchmarking of pretrained models for semantic textual similarity",
"text": "Hello community!\n\n\nSo basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it's nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)\n\n\nThe context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.\n\n\nWe are seeking recommendation:\n\n\n\n\nFor the corpora\n: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase\n\n\nFor the metrics\n: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).\n\n\nWhat metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?\n\n\n\n\n\n\n\n\noh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:\n\n\n\n\nWhat tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.\n\n\n\n\nTokenization by sentence/punctuation doesn't seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.\n\n\n&#x200B;",
"date": "2021-04-17"
},
{
"vote": 17,
"title": "Links to nlp applied projects/thesis relating to \"law/legal\"",
"text": "Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the \"LEGALBert\" paper.\n\n\nDoes anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. \n\n\nThanks",
"date": "2021-04-16"
},
{
"vote": 3,
"title": "[R] Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little",
"text": "[deleted]",
"date": "2021-04-16"
},
{
"vote": 4,
"title": "MA in Linguistic and Literary Computing (TU Darmstadt): Thoughts?",
"text": "Hello, fellow Subredditors!\n\n\nI'm curious to know if any of you are familiar with this program and if you could share your opinion about it.\n\n\nI have a BA in Linguistics and I've done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I'm interested in studying a master's degree and Germany (for many reasons) is my place of choice. I've already looked at the programs at TÃ¼bingen and Stuttgart and the info I've seen on this subreddit about those two has been very useful. However, I haven't found much about the program at Darmstadt, and I'm particularly interested in this program, given its digital humanities component.\n\n\nI would really appreciate your input! Thanks a lot!",
"date": "2021-04-16"
},
{
"vote": 1,
"title": "[R] Ubiai Survey",
"text": "[removed]",
"date": "2021-04-16"
},
{
"vote": 1,
"title": "Ubiai survey",
"text": "[removed]",
"date": "2021-04-16"
},
{
"vote": 3,
"title": "Text Generation from Another Text",
"text": "Hi everyone. I want to ask you if this is possible:\n\n\nConsider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.\n\n\nIs it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?\n\n\nE.g. consider X is a text or keywords like \"I like to swim in Maldives!\". Y is a news about Maldives. Then Z might be a generated text like \"Maldives has the most clean beaches in the world.\"\n\n\nThank you all.",
"date": "2021-04-16"
},
{
"vote": 1,
"title": "Python: Flask and speechRecognition",
"text": "[removed]",
"date": "2021-04-16"
},
{
"vote": 54,
"title": "[D] WTF these results are incredible?!! What do you guys think of this? BERT seems to learn equally well on word-shuffled sentences?",
"text": "Link to paper: \nhttps://arxiv.org/abs/2104.06644\n\n\nIs BERT just a giant bag-of-words??\n\n\nRelevant meme: \nhttps://imgur.com/a/RGUtOvt",
"date": "2021-04-15"
},
{
"vote": 1,
"title": "NLP equivalent to speaker diarization?",
"text": null,
"date": "2021-04-15"
},
{
"vote": 1,
"title": "Aspect analysis",
"text": "Best method for aspect analysis ??",
"date": "2021-04-15"
},
{
"vote": 1,
"title": "Is there a way to get document wise perplexity in gensim LDA",
"text": "For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics",
"date": "2021-04-14"
},
{
"vote": 5,
"title": "Checkout our team-based cloud-first text annotator.",
"text": "Long time lurker, first time poster!\n \n\n\notso has just launched our Annotator, and have built with the needs of many in this sub!\n\n\n&#x200B;\n\n\nA cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.\n\n\nWe have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. Weâ€™ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.\n\n\notso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.\n\n\nWhy a focus on the user experience of teams?\n\n\nText annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.\n\n\nWith this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.\n\n\nTo check it out, head to \notso.ai/annotator\n.No credit card required.",
"date": "2021-04-13"
},
{
"vote": 2,
"title": "Youtube Video Transcript Summarization with Hugging Face",
"text": null,
"date": "2021-04-13"
},
{
"vote": 5,
"title": "At which linguistic patterns and features attention heads of BERT look to ?",
"text": "Hello,\n\n\nI am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?",
"date": "2021-04-13"
},
{
"vote": 0,
"title": "hello I need DUC 2004 dataset for my master project .... if anyone share it with me i'll so thankful",
"text": null,
"date": "2021-04-13"
},
{
"vote": 12,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-04-12"
},
{
"vote": 4,
"title": "[Q:] MLM for short phrases",
"text": "Hello there,\nIm currently working on my Bachelorthesis and have only little knowledge on NLP. \nMy idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. \n\n\nBasically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.\n\n\nDoes this sound realistic?\n\n\nCould someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?\n\n\nThanks !",
"date": "2021-04-12"
},
{
"vote": 0,
"title": "Can you suggest me a Transformer Model for fine-tuning for my thesis?",
"text": "Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don't have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven't fully decided on the dataset or what classification I want to do. \n\n\n\n\nCan you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. \n\n\nThere are so many different transformer models that I can work on. Which ones do you think would be the best?\n\n\n\n\nThank you in advance!",
"date": "2021-04-12"
},
{
"vote": 14,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-04-12"
},
{
"vote": 18,
"title": "NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013",
"text": null,
"date": "2021-04-11"
},
{
"vote": 7,
"title": "How to award a score (1-100) for closeness to the right answer?",
"text": "Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.\n\n\nIf correct answer is â€œGuatemalaâ€, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. \n\n\nAre there good examples of this in the wild?",
"date": "2021-04-11"
},
{
"vote": 1,
"title": "Embedding2triples, which ML algorithm?",
"text": "I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I'm new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?",
"date": "2021-04-11"
},
{
"vote": 41,
"title": "Any good NLP roadmap?",
"text": "I'm a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.\n\n\nMay be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.\n\n\nBeginners of this community would appreciate it!",
"date": "2021-04-10"
},
{
"vote": 6,
"title": "How can pretrained models like GPT-2 learn new (domain-specific) token embeddings?",
"text": "[deleted]",
"date": "2021-04-10"
},
{
"vote": 1,
"title": "Don't understand every language in videos?",
"text": "[removed]",
"date": "2021-04-10"
},
{
"vote": 9,
"title": "when doing NLP, do you usually augment your data with a \"pre defined corpus\" specific to the field your data comes from?",
"text": "when doing NLP, do you usually augment your data with a \"pre defined corpus\" specific to the field your data comes from?\n\n\nE.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?",
"date": "2021-04-10"
},
{
"vote": 10,
"title": "[EU] Carreer advice - how to get started.",
"text": "[removed]",
"date": "2021-04-09"
},
{
"vote": 10,
"title": "[Tutorial] Neural Machine Translation With Attention With Keras",
"text": "This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.\n\n\nAdditional topics covered include:\n\n\n\n\nThe Problem With Sequence-to-Sequence Models for Neural Machine Translation\n\n\nAn Introduction to Attention Mechanisms\n\n\nCategories of Attention Mechanisms\n\n\nApplications of Attention Mechanisms\n\n\nNeural Machine Translation Using an RNN With Attention Mechanism (Keras)\n\n\n\n\nTutorial link: \nhttps://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/\n\n\nRun all of the code on a free GPU: \nhttps://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras",
"date": "2021-04-09"
},
{
"vote": 6,
"title": "NLP Problem",
"text": "Hi all - I'm working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.\n\n\nProblem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I'm trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).\n\n\nSolution: I've been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!\n\n\nI would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.",
"date": "2021-04-08"
},
{
"vote": 3,
"title": "Means to detect plagiarism in textual documents",
"text": "Hello guys, \n\n\nI came here to ask you for a help. I'm writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don't know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. \n\n\nSo the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.\n\n\nMy question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.\n\n\nI am really really desperate and any information will help me. Thank you",
"date": "2021-04-08"
},
{
"vote": 1,
"title": "Student Travel Grant for ACL/NAACL/EMNLP",
"text": "[deleted]",
"date": "2021-04-08"
},
{
"vote": 1,
"title": "Conversational AI Platform",
"text": "[removed]",
"date": "2021-04-08"
},
{
"vote": 5,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-04-08"
},
{
"vote": 1,
"title": "Domain Specific ASR",
"text": "[deleted]",
"date": "2021-04-08"
},
{
"vote": 31,
"title": "Summer schools in NLP 2021",
"text": "Hi, any leads on upcoming NLP summer schools.",
"date": "2021-04-08"
},
{
"vote": 1,
"title": "MarianMT usage with Spacy possible?",
"text": "Hi everyone,\n\n\nis it possible to use MarianMT transformer models with spacy-transformers? Similar to \nhttps://spacy.io/universe/project/spacy-transformers\nMarianMT: \nhttps://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models\n\n\nThanks!",
"date": "2021-04-08"
},
{
"vote": 2,
"title": "Can you recommend a topic for my thesis on Goal oriented dialog systems?",
"text": "[deleted]",
"date": "2021-04-07"
},
{
"vote": 1,
"title": "GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge (RESEARCH PAPER WALKTHROUGH)",
"text": "[deleted]",
"date": "2021-04-07"
},
{
"vote": 22,
"title": "Multi-Document Summarization: The Wikipedia Current Events Portal Dataset (Dataset and Colab notebook)",
"text": null,
"date": "2021-04-07"
},
{
"vote": 7,
"title": "What's the word on Potsdam's MSc Cognitive Systems?",
"text": "Hey everyone!\n\n\nI'm applying for my Master's right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who's studying at Uni-Potsdam. I've seen next to nothing about the program in this sub. If I'm being honest, that program looks the most appealing of the European programs I've come across in my research. So, to anyone who can provide some insight, I ask:\n\n\n\n\nIs it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?\n\n\nIs it technical and applied (as opposed to theoretical and scholarly)?\n\n\nWhat's the general vibe? How do you and/or the students like it?\n\n\n\n\nYou can totally stop reading now but in case anyone's interested:\nI've been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I'm Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren't due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!",
"date": "2021-04-07"
},
{
"vote": 5,
"title": "Best practices in NLP",
"text": "What's your experience in NLP and what do you think are the common strategies and best practices to follow as a beginner?",
"date": "2021-04-06"
},
{
"vote": 0,
"title": "What to Consider When Choosing Programming Language",
"text": null,
"date": "2021-04-06"
},
{
"vote": 2,
"title": "[D] Can we add CNN on top of BERT",
"text": "I have dense neural network for BERT, How do I change that to Conv1D, maxpooling, and flatten, before connected to dense layer. \n\n\n```\n\n\nclass BertBinaryClassifier(nn.Module): \ndef __init__(self, dropout=0.1): \nsuper(BertBinaryClassifier, self).__init__()         \nself.bert = BertModel.from_pretrained(&#039;bert-base-uncased&#039;)         \nself.dropout = nn.Dropout(dropout)         \nself.linear = nn.Linear(768, 1)        \n self.sigmoid = nn.Sigmoid()                   \ndef forward(self, tokens, masks=None):         \n_, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)         dropout_output = self.dropout(pooled_output)         \nlinear_output = self.linear(dropout_output)        \n prediction = self.sigmoid(linear_output)         \nreturn prediction\n # Config setting \nBATCH_SIZE = 4\n EPOCHS = 5\n\n\n\n```",
"date": "2021-04-06"
},
{
"vote": 26,
"title": "LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial",
"text": "[deleted]",
"date": "2021-04-06"
},
{
"vote": 0,
"title": "Successfully unning pre-trained LDA-Mallet model in Google Colab and infering topics of unseen documents",
"text": "[removed]",
"date": "2021-04-05"
},
{
"vote": 9,
"title": "Fuzzy Matching/Logic - Concept, Utility, Implementation, and Complex Scenarios",
"text": "Weâ€™ve been seeing a surge in the requests for Fuzzy Matching/Logic techniques. We tried to learn more about these techniques and the implementation process and have written about it. We have also covered some complex scenarios and their solution.  \n\n\nHereâ€™s a detailed view on the concept, its utility, and the implementation: \nhttps://nanonets.com/blog/fuzzy-matching-fuzzy-logic/\n \n\n\nCan anyone share their experience on implementing Fuzzy Matching in their product or solution?",
"date": "2021-04-05"
},
{
"vote": 12,
"title": "How to use spacy and a tailor-made transformer in sentiment analysis?",
"text": "Hi\n\n\nI m trying to set up a Spacy pipeline with my one transformer but it does not work as the transformer I generated is not compatible.\n\n\nDows anyone have an updated example of this pipeline set up, including the compatible method of creating the Transformer?\n\n\nThanks!",
"date": "2021-04-04"
},
{
"vote": 7,
"title": "Most efficient method for converting ~300k sentences to word embeddings?",
"text": "I'm working on a project where I need to get the word embeddings of around 300,000 sentences so that I can compare them with other sentences using cosine similarity. Since I'm dealing with so many sentences, what would be the most efficient way to get the word embeddings? So far I've been using the \nsentence-transformers\n package (my model is initialized as SentenceTransformer('bert-base-nli-max-tokens')), but using this would take around 24 hours if not more to get the word embeddings of all sentences. If that's as fast as it can get I'm fine with that, but was wondering if there's a way to do this more efficiently?",
"date": "2021-04-03"
},
{
"vote": 9,
"title": "Any good tutorial for NLP using Tensorflow for classification, word embedding, ... WITHOUT imdb dataset.",
"text": "I can't find any good tutorial to learn all these strategies with my own csv dataset with labeled reviews. As I don't have the same data structure of imdb dataset, I can't follow any tutorial based on official documentation. Every f tutorial is based on the same example!\n\n\nAnyway, I would appreciate any good resource!",
"date": "2021-04-02"
},
{
"vote": 3,
"title": "Do you work or have worked in Relation Extraction?",
"text": "[deleted]",
"date": "2021-04-01"
},
{
"vote": 8,
"title": "Beginners in NLP, need your feedback. Made this video explaining building neural semantic search using open-source project Jina(think search like google). How good is this explanation?",
"text": null,
"date": "2021-04-01"
},
{
"vote": 4,
"title": "Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper Walkthrough",
"text": null,
"date": "2021-04-01"
},
{
"vote": 36,
"title": "Trankit v1.0.0 - An open-source Transformer-based Multilingual NLP Toolkit for 56 languages is out.",
"text": "Hi everyone,\n\n\nWe just released the version 1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.\n\n\nðŸ’¥ ðŸ’¥ ðŸ’¥ The new version v1.0.0 offers:\n\n\n\n\nA trainable pipeline for fundamental NLP tasks over 100 languages\n.\n\n\n90 new pretrained transformer-based pipelines for 56 languages\n. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For \nEnglish\n, Trankit is significantly better than Stanza on sentence segmentation (\n+9.36%\n) and dependency parsing (\n+5.07%\n for UAS and \n+5.81%\n for LAS). For \nArabic\n, our toolkit substantially improves sentence segmentation performance by \n16.36%\n while \nChinese\n observes \n14.50%\n and \n15.0%\n improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found  \nhere\n .\n\n\nAuto Mode for multilingual pipelines\n. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language.  Check out how to turn on the Auto Mode \nhere\n. \n\n\nCommand-line interface\n is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily.  Check out the command-line tutorials on this \npage\n.\n\n\n\n\nTrankit is written in Python\n and can be easily installed via pip. Our code and pretrained models are publicly available at: \nhttps://github.com/nlp-uoregon/trankit\n\n\nWe also created a documentation page and a demo website for Trankit.\n\n\nDocumentation page: \nhttps://trankit.readthedocs.io/en/latest/index.html\n\n\nDemo website: \nhttp://nlp.uoregon.edu/trankit\n\n\nTechnical details about Trankit can be found in our paper: \nhttps://arxiv.org/pdf/2101.03289.pdf\n\n\nThank you for your time reading this post!\n\n\nHope you enjoy Trankit!",
"date": "2021-03-31"
},
{
"vote": 71,
"title": "A Python library to boost T5 models speed up to 5x &amp; reduce the model size by 3x.",
"text": "I wanted to share this new library I've been working on and that I open-sourced!.\n\n\nhere are some links to the library:\n\n\nðŸ’» \nGitHub Repository\n\n\nðŸ \nPyPi project\n\n\nas the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.\n\n\nThe library can be installed with \npip install fastt5\n. This code snippet from the repository's README gives a concise overview:\n\n\nfrom fastT5 import export_and_get_onnx_model\nfrom transformers import AutoTokenizer\n\nmodel_name = &#039;t5-small&#039;\nmodel = export_and_get_onnx_model(model_name)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nt_input = &quot;translate English to French: The universe is a dark forest.&quot;\ntoken = tokenizer(t_input, return_tensors=&#039;pt&#039;)\n\ntokens = model.generate(input_ids=token[&#039;input_ids&#039;],\n               attention_mask=token[&#039;attention_mask&#039;],\n               num_beams=2)\n\noutput = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\nprint(output)\n\n\n\nThe fastT5 library exports the T5 model to onnx with \npast_key_values,\n then quantizes it and runs it on onnxruntime.\n\n\nThe exported onnx models support the \ngenerate()\n method of huggingface transformers for inferencing.\n\n\nfor more information on the project refer to the repository \nhere\n.",
"date": "2021-03-31"
},
{
"vote": 1,
"title": "Information Technology Techie",
"text": null,
"date": "2021-03-31"
},
{
"vote": 4,
"title": "Probably a stupid question, but is it possible to use BERT with longer sequences of tokens?",
"text": "[deleted]",
"date": "2021-03-30"
},
{
"vote": 3,
"title": "Interactive Research Graph for New Paper \"Approximating How Single Head Attention Learns\"",
"text": "I am sharing our \nnew interactive graph\n project that maps out knowledge-paper connections for the paper \"Approximating How Single Head Attention Learns\" published earlier this month by Berkeley NLP. You can click the graph nodes to explore the most relevant papers with videos and key knowledge areas. This page also includes a list of related papers without videos. \nYou can try it here.\n\n\nI'd be curious to learn your thoughts. Do you find it helpful? How can we make it more useful for understanding new research papers/areas? Thank you!",
"date": "2021-03-30"
},
{
"vote": 1,
"title": "[Q] How to truncate text to max. permissible tokens within Huggingface Pipeline?",
"text": "At the moment, I am having to manually correct for tokens in order to force limit them to 512 tokens -- the limit for distilBERT\n\n\nCan some please guide me to a solution which works with Feature Extraction Pipeline?\n\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)\n\n\nmodel = AutoModel.from_pretrained(&quot;distilbert-base-uncased&quot;)\n\n\nmodel_use = pipeline(&#039;feature-extraction&#039;, model=model, tokenizer=tokenizer) \n\n\nembedding = model_use( text )\n\n\nThanks",
"date": "2021-03-30"
},
{
"vote": 2,
"title": "BA Ling - Exchange Program",
"text": "Hei mates, Iâ€™m currently studying a BA in Linguistics and planning doing an international exchange program offered by my local faculty. Iâ€™m quite interested in the biolinguistics framework (yes, chomskyan), evolution of language experimental linguistics and language technologies. Since these fields are not deeply investigated at my university, I am looking for institutions whose programs include the topics mentioned above. Hope I can get a review/advise from any of you!! These are my options:\n\n\n\n\nUniversity College London - UK (not my best bc london is $$$$ but if u got any info i will rlly appreciate it)\n\n\nNewcastle University - UK\n\n\nLund University - Sweden\n\n\nCopenhagen University - Denmark\n\n\nUniversity of Bergen - Norway\n\n\nTÃ¼bingen UniversitÃ¤t - Germany \n\n\nUniversity of Helsinki - Finland\n\n\n\n\nTxs <3",
"date": "2021-03-30"
},
{
"vote": 7,
"title": "XLM Roberta - Maximum File Size",
"text": "Hi guys. Iâ€™m playing about with XLM Roberta Large XNLI and Python and Iâ€™m conscious it has a maximum file size of 512 tokens. \n\n\nCan anybody advice me how they are handling inputs that are larger than this? Any suggestions on code edit to only query 512 tokens to the model and ignore any more? Thanks all",
"date": "2021-03-29"
},
{
"vote": 4,
"title": "Update on my project to debug and visualize Python code by using a combination of conventional static analysis tools and the attention based AI model. - Please ask me any questions!",
"text": "I am sorry if my post doesn't sound like an innovation to you, but would like you to take a look as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!\n\n\nThe model has been trained on bug fixes in open source Github projects, and the tool itself is largely written in Python and hoping to help python coders!\n\n\nThe repository I visualized as an example is: \nhttps://metabob.com/gh/galt2x/fastapi\n\n\nThe program works best on Google Chrome, If you would like to check out the website, I linked it \nhere\n.",
"date": "2021-03-29"
},
{
"vote": 10,
"title": "Emotion detection question",
"text": "I have a data which is already labeled different type of emotions based on text context:\n\n\nSome of the label types: \n\n\nfunny, anger, boredom, empty, fun relief, sadness, happines\n\n\n\nwhen I use TFIDF alongside Logistic Regression to predict it gives me a shitty result.\n\n\nHere is the code:\n\n\n### HERE GOES PREPROCESSING FIRST\nand then\n\n#Encoding output labels &#039;sadness&#039; as &#039;1&#039; &amp; &#039;happiness&#039; as &#039;0&#039;\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(data.sentiment.values)\n\n# Splitting into training and testing data in 90:10 ratio\nX_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n\n# Extracting TF-IDF parameters\ntfidf = TfidfVectorizer(max_features=1000, analyzer=&#039;word&#039;,ngram_range=(1,3))\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_val_tfidf = tfidf.fit_transform(X_val)\n\n# Model 3: logistic regression\nlogreg = LogisticRegression(C=1,solver=&#039;lbfgs&#039;, max_iter=3000)\nlogreg.fit(X_train_tfidf, y_train)\ny_pred = logreg.predict(X_val_tfidf)\nprint(&#039;log reg tfidf accuracy %s&#039; % accuracy_score(y_pred, y_val))\n\n\n\nthis is giving me a very bad result something like 0.2342355%\n\n\nhowever, if I remove all types of emotions but leave only two. For example Happiness and Sadness,\n\n\nit gives me a better result of 0.782342342%\n\n\n&#x200B;\n\n\nWhy is this?\n\n\nHow can i make it so that the model can predict not only 2 types of emotions but also other types f emotions such as \n\"Excitement, Fun, Relief\"\n and so on?\n\n\n'",
"date": "2021-03-28"
},
{
"vote": 7,
"title": "Tunning GPT2",
"text": "I have a specific task which i would like to fine tune a pre-trained GPT2 model. I know those models require a lot of data to train, but what about fine tuning? And how can i measure if my model is actually working for this specific task? Iâ€™ve tried to fine tune with about 1600 samples, but after the third epoch the model starts to overfit",
"date": "2021-03-28"
},
{
"vote": 2,
"title": "Best description of attention?",
"text": "Where can I find the best description of the attention mechanism for NLP?  Blog post or video is good, but both are great!",
"date": "2021-03-27"
},
{
"vote": 37,
"title": "Google AI Introduces a New System for Open-Domain Long-Form Question Answering (LFQA)",
"text": "Open-domain long-on answering (LFQA) form questions a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given query and using them to generate a detailed paragraph-length answer.Â \n\n\nRecently, there has been significant progress in factoid open-domain question answering (QA). In this technique, a short phrase or entity is enough to answer a question, but significantly less work has been done in long-form question answering (LFQA). LFQA is an important task, primarily because it provides a testbed to measure generative text modelsâ€™ factuality. But, the current benchmarks and evaluation metrics arenâ€™t quite suitable for making progress on LFQA.\n\n\nIn a recent paper, \nâ€œHurdles to Progress in Long-form Question Answeringâ€,\n that is set to appear at NAACL 2021, Google.ai present a new system for open-domain long-form question answering that utilizes two recent advances in NLP: One is the state-of-the-art sparse attention models, such as Routing Transformer (RT), which allows attention-based models to scale to long sequences, and other is the retrieval-based models, like REALM, that can facilitate retrievals of Wikipedia articles related to a given query.\n\n\nShort Summary: \nhttps://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/\n\n\nGoogle blog: \nhttps://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html\n\n\nPaper: \nhttps://arxiv.org/abs/2103.06332",
"date": "2021-03-27"
},
{
"vote": 3,
"title": "Running Transformer model nearly kills my machine",
"text": "I have just a list of 43 texts. I have encoded them via \nBert Base\n model from Huggingface, limited to 512 tokens.\n\n\ntokenized = df.text.apply(lambda text: tokenizer.tokenize(text)).to_list()\ninputs = tokenizer(tokenized, is_split_into_words=True, truncation=True, max_length=512, padding=&#039;max_length&#039;, return_tensors=&#039;pt&#039;)\noutputs = model(**inputs)  # this line fills my ram\n\n\n\nI have 16 GB of RAM and another 16 GB of swap. When the third line is being run, the RAM usage goes beyond my machine's limitations. I have tried many different ways but didn't helped.\n\n\nWhat could be done? My texts have long sentences, some of them may be consist of no sentences at all; could this be the problem?",
"date": "2021-03-27"
},
{
"vote": 3,
"title": "Computing Words per Error of an N-Gram tagger using NLTK in Python?",
"text": "I've got a question - I'm trying to evaluate some sets of ngrams using a training set and a data set from a corpus and write some statistics about it. I have to include two measures: accuracy (which is easy using the \".evaluate\" module in NLTK. But I also have to find out the \"words/error\" rate of it. So for example I'm:\n\n\n\n\nusing the Brown corpus' subcorpus \"news\"\n\n\nthis supcorpus has 100,554 words in total \n\n\nI've split the corpus in to a training set of 500 sentences, and the rest are the testing set (the suborpus has 4,623 phrases in all). \n\n\nI have a partially filled chart with an example:\n\n\nThe default ngram tagger has an accuracy of 30.41% (when comparing the training and testing sets) and a error rate of \n1.4 words/error\n\n\n\n\nBut I can't figure out how this 1.4 words/error was calculated. Anyone have any ideas? I think, and I may be wrong here, that it's calculated as a function of the rest of the corpus that is NOT accurate - that is, 100%-30.41% = \n69.95%\n of the corpus. That number is then related to the total number of words somehow I think?",
"date": "2021-03-27"
},
{
"vote": 1,
"title": "Important topics in NLP",
"text": "[removed]",
"date": "2021-03-26"
},
{
"vote": 1,
"title": "Computing perplexity",
"text": "For language models, perplexity is defined as the inverse probability of the test set, normalized by the number of words. So far, so clear.\nWhat confuses me is the following section:  \n\n\n\"What we generally use for word sequence (...) is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers <s> and </s> in the probability computation. We also need to include the end-of-sentence marker </s> (but not the beginning-of-sentence marker <s>) in the total count of word tokens \nN\n.\" \n\n\n- Speech and Language Processing. Daniel Jurafsky & James H. Martin.   \n\n\nWhy do we not include the beginning-of-sentence marker?",
"date": "2021-03-26"
},
{
"vote": 2,
"title": "Extract name:value relationships from plain text",
"text": "I have a microservice that receives plain-text messages from an external API like:\n\n\n\"... You have received 1254.56 from Thomas Paine. Transaction id FGR412512 at 11:02 ...\"\n\n\nUsing Regex I can get:\n\n\ndouble amount = 1254.56;\n\n\nString transactionId = FGR412512;\n\n\nString time = 11.02;\n\n\nHowever, I would like to make my code more resilient to possible changes in text to, say, \"... at 11.02, Thomas Pain sent you 1254.56 ... \" in the future.\n\n\nIs there a Natural Language Processing library or framework that I can use to extract these relationships and convert them to variables like I do with Regex?",
"date": "2021-03-26"
},
{
"vote": 24,
"title": "SOTA for Topic Modeling",
"text": "Hi everyone,\n\n\nDoes anyone know what is the state of the art for Topic Modeling and what kind of models do they use for production at Facebook, Amazon, Google etc.? I found a couple of recent papers on NTMs, but not sure how well they work and how well they would scale.\n\n\nThere are many approaches that are quite popular (like LDA, Neural Topic Models, topic models + BERT etc), but I was quite interested in something that is quite scalable for large datasets and is easy to use for production. Also, majority of the models are unsupervised and used on \"exploratory\" basis, is there something that you would recommend when looking for a particular narrative e.g. \"PC Gaming\" or something that will give you \"pure\" topics?\n\n\nAny comments appreciated, thanks!",
"date": "2021-03-25"
},
{
"vote": 1,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2021-03-24"
},
{
"vote": 1,
"title": "stanza's Arabic language model doesn't tokenize sentences properly",
"text": "I'm trying to take Arabic text (e-mail messages, each of which are a few sentences long) and segment it all into their individual sentences.\n\n\nIt's not working. Most of the time I'm getting the entire e-mail message as my output, meaning it thinks the entire thing is one sentence, but really there are 3-5 different sentences in there.\n\n\nWhy is this not working? The stanza language models are working properly for like 7 other languages I've tried. It's not working for Arabic. Occasionally it does separate real sentences, but most of the time it just prints out 3-5 sentences as if it's one tokenized sentence. Does anyone know why the Arabic language model isn't tokenizing these e-mail messages properly?",
"date": "2021-03-24"
},
{
"vote": 21,
"title": "Production-Ready Machine Learning NLP API with FastAPI and spaCy",
"text": "Hey,\n\n\nFastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.\n\n\nThe \nNLPCloud.io\n API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:\n\n\nhttps://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/\n\n\nI'd love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I'm not aware of? Or can you think of better tools for machine learning APIs?\n\n\nThanks!",
"date": "2021-03-24"
},
{
"vote": 1,
"title": "question RoBERTa doc sentances.",
"text": "Hi everyone,\n\n\nin the Roberta paper (\nhttps://arxiv.org/pdf/1907.11692.pdf\n page 5) they say they use doc-sentences formatted inputs (512 tokens taken contiguously across multiple documents, documents are separated by a sep token). \n\n\ne.g. if I understand well: tokenized-doc 1 : 220 tokens tokenized-doc 2: 350 tokens\n\n\n-> corresponding Roberta pre-training inputs: 220 tokens from doc 1 + sep token + 291 first tokens of doc 2 \n\n\nmy question is as follows:\n\n\nIs there a way to obtain this format of input from a corpus of documents in a convenient way using pytorch or hugging face and a custom BPE tokenizer?",
"date": "2021-03-24"
},
{
"vote": 2,
"title": "How to compare a template sentence with a real sentence ?",
"text": "Hi all, does anyone knows what is the best algorithm to do something like that:\n\n\nI have a template sentence: \n\n\n\"declareÂ aÂ {memType:constant|variable}Â calledÂ {name:term} with value {value:term}\"\n\n\nThat when compared with real life sentences such as: \n\n\n\n\ndeclare a variable called A with value 32\n\n\ndeclare variable called XB with value ZA\n\n\ndeclare a constant called C with \nthe\n value 42\n\n\n\n\nShould all return true.\n\n\nI tried using regex and it works fine until it does not, because of small variations in the real life sentence such as articles (see the article \"\nthe\"\n in sentence number 3).\n\n\nMy question is: is there a better way to do that comparison ?",
"date": "2021-03-24"
},
{
"vote": 7,
"title": "BERT MLM",
"text": "Hi everyone,\n\n\nI was wondering about the masked word prediction task of BERT and how exactly it is carried out.\n\n\nIn the \nBERT paper\n the authors wrote that the masked tokens are used to predict the real tokens using a cross entropy loss. \n\n\nFurthermore they wrote \"[...]  the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\"\n\n\nSo from my understanding I would just use a fully connected layer with a softmax at the end of the BERT model to obtain a output matrix of shape \n[vocab_size, input_length]\n and then use the actual position of the masked words (15% words masked) to obtain the output matrix of shape \n[vocab_size, input_length*0.15]\n. This matrix can then be used for the (categorical) cross entropy loss.\n\n\nIs my way of reasoning correct or am I missing something important here?\n\n\n&#x200B;\n\n\nThank you in advance!",
"date": "2021-03-24"
},
{
"vote": 1,
"title": "What's the point of pre-trained tokenizers?",
"text": "I've come across the idea of pre-trained tokenizers, but I'm struggling to understand why and when these would be useful. Isn't it better if you train a tokenizer on your own training data? What if the pretrained tokenizer has only a small vocabulary or is domain-specific? Wouldn't that be more detrimental to your model?",
"date": "2021-03-24"
},
{
"vote": 1,
"title": "How I learned 8 languages easily",
"text": "[removed]",
"date": "2021-03-23"
},
{
"vote": 1,
"title": "[N] China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0'",
"text": "[removed]",
"date": "2021-03-23"
},
{
"vote": 1,
"title": "What is the latest and greatest model for natural language inference?",
"text": "[deleted]",
"date": "2021-03-23"
},
{
"vote": 3,
"title": "Interpreting the topic of a streaming text-based group chat in real-time. Is this possible?",
"text": "This may be a simple question for those with a better understanding. Iâ€™m trying to figure out if it is possible to use NLP to describe the topic of conversation in a group chat (text). For example, people in a chat are giving advice on surfing for beginners, and using machine learning we would be able to generate something like â€˜learning to surfâ€™ or â€˜beginner tips for surfingâ€™. My question for you all is (1) is this possible in real-time for an ongoing chat where the topic of conversation evolves/changes and (2) if possible, how specific can the generated topic be? Would it just be â€˜surfingâ€™ or can it be more specific such as â€˜advice on learning to surf for beginners?\n\n\nAny help would be much appreciated! Even a resource or guidance would be great if this is too simple of a question. Just havenâ€™t been able to find the answer on google searches.",
"date": "2021-03-23"
},
{
"vote": 1,
"title": "A Directory of Online Newspaper Sources for 70+ Languages",
"text": null,
"date": "2021-03-22"
},
{
"vote": 1,
"title": "[Article] Survey of Visual Question Answering",
"text": "A task that has grasped the attention of the AI community recently is that of visual question answering.\n\n\nVisual question answering involves answering questions about images in natural language. It is an interesting problem, as it combines aspects of both computer vision and natural language processing.\n\n\nThis article explores the problem of visual question answering, different approaches to solve it, associated challenges, datasets, and evaluation methods. Topics such as image featurization, question featurization, joint feature representation, and answer generation will be explained, along with a survey of recent research efforts tackling each of these problems.\n\n\nArticle link: \nhttps://blog.paperspace.com/introduction-to-visual-question-answering/",
"date": "2021-03-22"
},
{
"vote": 28,
"title": "How does Reddit show \"Trending Today\" topics? What algorithm does it use?",
"text": "The trending Today topics show up when you click search bar, and below the recent searches, appear the Trending topics. I would like to know how reddit clusters these sub-reddits, how it ranks various topics, and how it identifies various topics in the first place. The names of the algorithms used would be helpful. Thanks in advance!",
"date": "2021-03-22"
},
{
"vote": 1,
"title": "How does reddit show \"Trending Today\" topics? What algorithm does it use.",
"text": "[deleted]",
"date": "2021-03-22"
},
{
"vote": 6,
"title": "Integrating Ray Tune, Hugging Face Transformers and W&amp;B",
"text": null,
"date": "2021-03-20"
},
{
"vote": 17,
"title": "A Transformers Tutorial in Plain English To Get You Thinking",
"text": null,
"date": "2021-03-19"
},
{
"vote": 4,
"title": "What Are Some Open Source NLP Framework Pipelines For QA Task",
"text": "I am trying to build a QA pipeline. My end goal is that :\nThere are total X documents in database. given a query fetch top Y documents using some fast model (tfidf/bm25 etc) where Y is a subset of X. Then run a deep learning model (bert, longformer etc) to fetch and rank Z number of documents where Z is a subset of Y. \n\n\nI am using haystack and although I heard good things about it but for some reason its not working for me. its slower than my own pipeline, its retriever (the first/recall module which runs keyword model) is buggy (returning duplicate documents even though my data did not have any duplicate). \n\n\nSo I was wondering if there are any other tried and tested frameworks that you guys know of. \n\n\nThank you in advance.",
"date": "2021-03-19"
},
{
"vote": 1,
"title": "Anyone know where I could find a dataset that contains song lyrics and genre tags?",
"text": "Iâ€™ve been searching around but havenâ€™t had much luck on kaggle, anywhere else i could look?",
"date": "2021-03-19"
},
{
"vote": 0,
"title": "Introduction to Named Entity Recognition (NER)",
"text": "Hey!\n\n\nI made a quick article about Named Entity Recognition (NER). What is it and why is it a useful NLP technique?\n\n\nhttps://nlpcloud.io/nlp-named-entity-recognition-ner-api.html\n\n\nHope some of you will find it useful.\n\n\nIf you have any comment, please feel free!",
"date": "2021-03-19"
},
{
"vote": 21,
"title": "NLP Pipeline Completely on the GPU",
"text": "Hey all, I had to take a day off from posting our DSotD. I think sometimes the amount of information we through daily at everyone is exhausting, and it's better to not overload and instead digest.\n\n\nSo today we have an \nawesome blog post\n, which explains how we can do an entire NLP pipeline on a GPU... creating incredible fast lightening speeds for NLP solutions. Now, this is not diving into the transformer craze like BERT or GPT, so don't get your hopes up (though you can see easily where those models can be placed in the pipeline).\n\n\nWhat's interesting is the majority feel that Deep Transformers are the way to solve all NLP, but it's not always the case. \nHere\n is an example of a Nvidia Kaggle Grandmaster (and absolute Kaggle tank!) using the described pipeline above to Kaggle's Shopee - Price Match Guarantee, determining if two products are the same by their images and product description. Also, a even more streamlined Kaggle memory efficient book can be seen \nhere\n .  \n\n\nLet me know what you think, and we can get a great discussion going. Anyone want to vent about transformers being the solution to all NLP? \n(laughing face emoji, I think of Walid Saba and his many posts on LinkedIn. Give him a follow, it's amazing pure linguist information).",
"date": "2021-03-19"
},
{
"vote": 3,
"title": "Do ASIC and FPGA have any use in NLP?",
"text": null,
"date": "2021-03-18"
},
{
"vote": 2,
"title": "AI app to help you read your book smartly",
"text": "Greetings folks,\n\n\n4 months ago, I started working on a \nmachine learning based website which helps readers\n\n\nfind exiting content from a book\n or a pdf file without reading all of it.\n\n\nAnd now after working hard on it for the past few months, I have come up with a\n\n\nprototype version. You can upload any pdf and ask questions to get to the interesting part quickly.\n\n\nHere is the link :\n\n\nhttps://read-what-you-need.firebaseapp.com/\n\n\nI hope the app saves your time, and fuels your love for reading.\n\n\nHave a great day ahead!\n\n\nps: here's an account to try the app quickly, set up just for our reddit users\n\n\nusername: reddit\n\n\npassword: reddit2021\n\n\nEnjoy !",
"date": "2021-03-18"
},
{
"vote": 3,
"title": "BERT pre-training with only masked words",
"text": "Hi everyone, \n\n\ni am new to the world of NLP. For an upcoming project I would like to use BERT in order to do sentiment analysis with just the headlines of news articles. But BERT has two training tasks 1. Predict masked words and 2. Next sentence prediction. But since I will use only the headlines of an article the second training task becomes obsolet. \n\n\nMy question : Is it possible to just train BERT with the 'predict the masked words' task ?\n\n\n&#x200B;\n\n\nAnd maybe an off-topic question: Do you know how difficult it is to make a tokenizer from scratch ?\n\n\n&#x200B;\n\n\nThank you in advance!",
"date": "2021-03-18"
},
{
"vote": 1,
"title": "preprocessing reddit submissions in python (html tag stripping etc...)",
"text": "Hi there, \n\n\nI am preprocessing some reddit submission downloaded from Pushshift, which I'll later feed to a Bert sentiment encoder.\n\n\nIs there any predefined pipeline (possibly in Python) for which preprocessing steps to perform (e.g., stripping html tags, etc.)?\n\n\nThank you in advance!",
"date": "2021-03-18"
},
{
"vote": 1,
"title": "Tutorial on how to create categorical variables based on integers and numerical ranges",
"text": "Hey, I've created a tutorial on how to create categorical variables based on integers and numerical ranges using the R programming language: \nhttps://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r",
"date": "2021-03-18"
},
{
"vote": 1,
"title": "Coreference resolution with spaCy 3.0",
"text": "[deleted]",
"date": "2021-03-17"
},
{
"vote": 0,
"title": "Free GPU alternatives to Google Colab for ML/DL",
"text": "Google Colab is an undeniably popular choice for free GPU-backed Jupyter notebooks for deep learning projects, but it's not without its drawbacks.\n\n\nThis article discusses alternate sources of free GPUs in cloud-hosted Jupyter environments:\n\n\nhttps://blog.paperspace.com/best-google-colab-alternatives/\n\n\nReally curious what others' experiences with Google Colab are like, and if any of these issues resonate. Also interested in what the community has to say about the alternates presented here!",
"date": "2021-03-17"
},
{
"vote": 41,
"title": "My side project: Cloud GPUs for 1/3 the cost of AWS/GCP",
"text": "[cross posting from /r/MachineLearning]\n\n\nIâ€™ve just finished building a little side project of mine - \nhttps://gpu.land/\n.\n\n\nWhat is it?\n Cheap GPU instances in the cloud.\n\n\nWhy is it awesome?\n\n\n\n\nItâ€™s dirt-cheap. You get a Tesla V100 for $0.99/hr, which is 1/3 the cost of AWS/GCP/Azure/[insert big cloud name].\n\n\nItâ€™s dead simple. It takes 2mins from registration to a launched instance. Instances come pre-installed with everything you need for Deep Learning, including a 1-click Jupyter server.\n\n\nIt sports a retro, MS-DOS-like look. Because why not:)\n\n\n\n\nIâ€™m a self-taught ML engineer. I built this because when I was starting my ML journey I was totally lost and frustrated by AWS. Hope this saves some of you some nerve cells (and some pennies)!\n\n\nThe most common question I get is - how is this so cheap? The answer is because AWS/GCP are charging you a huge markup and Iâ€™m not. In fact Iâ€™m charging just enough to break even, and built this project really to give back to community (and to learn some of the tech in the process). \n\n\nAMA!",
"date": "2021-03-17"
},
{
"vote": 2,
"title": "An end-to-end NLP Pipeline Tutorial Applied to Landscape Restoration in Kenya",
"text": "An open-source project, where you can learn more about NLP pipelines while looking at an impactful use case.  The project was done in collaboration with a group at Stanford University, Code for Africa, WRI, and 40+ individual collaborators. \n\n\nFrom collecting and preparing more than 32.000 notices, legal entities, and court documents to build a web-based dashboard displaying land ownership in Kenya. The purpose of this project is to boost Kenyaâ€™s efforts to restore degraded land in an equitable way.\nhttps://omdena.com/blog/identifying-land-ownership/",
"date": "2021-03-17"
},
{
"vote": 1,
"title": "With GPUs, K-nearest Neighbor Algorithm Crosses the Finish Line When Others are in the Starting Blocks",
"text": "Good morning all! I'm going to try and keep up these Data Science of the Day posts. I think they are fun to share and the feedback has been great so far.\n\n\nYesterday I talked to \nu/drekalo\n in my \nr/datascience\n \npost\n(I think might have gotten removed) but I posted it in a few other subs too, here is the \npost\n in \nr/MachineLearning\n. We talked a bit about RAPIDS, which is an Nvidia library for accelerated data engineering/science (it's quit awesome). Today I want to shed light on another \nRAPIDS\n ability and that's doing classic ML algorithms \nFAST\n. This article talks about K-Nearest Neighbors (KNN) probably the most well known ML algorithm there is. One of Nvidia's Kaggle Grandmasters wrote this great \narticle\n on how RAPIDS accelerates KNN \n600x\n versus CPU. This speed up might be what many of you need to help get a benchmark on large or complicated data sets, OR get a medal in Kaggle! It's rudimentary, and not as attractive as a DL Algo like BERT, but you can use KNN for text classification to get that first step.\n\n\n&#x200B;",
"date": "2021-03-17"
},
{
"vote": 8,
"title": "Edinburgh MSc Speech &amp; Language Processing",
"text": "Hi everyone, \n\n\nI'm considering applying for this masters for September entry this year. I've had it on my mind for a few months, and really can't decide if I should focus on studying for my undergrad finals right now and wait to apply next year, or apply now so I can start asap. \n\n\nMy questions: how long did it take you to write your personal statement? What did it contain? I would really appreciate any guidance! Were you interviewed?\n\n\nAnd do you think this course will be much harder coming from a linguistics background rather than STEM? I haven't done any programming beyond tinkering in my spare time.\n\n\nthank you - any help is really appreciated",
"date": "2021-03-17"
},
{
"vote": 1,
"title": "Language Identification using XGBoost. Code for training and application of a language identification model. Trained on the WiLI-2018 database, the classifier achieves an accuracy of 85.97% on the WiLi test dataset for 235 languages.",
"text": null,
"date": "2021-03-17"
},
{
"vote": 2,
"title": "How to find Word Similarity based on spellings?",
"text": "[deleted]",
"date": "2021-03-17"
},
{
"vote": 25,
"title": "Do we really need to Dstill Language Models? Joint loss is all we need - Albert-Joint .",
"text": "Hi NLP folks,\n\n\nI have create a new model focused mainly on smaller size with decent performance. And my experiments with GLUE proved that Joint Loss is all we need. With just  \n14 million parameters and half the computation ( 6 layers instead of 12 layers\n , we got a \nGLUE score of 81.0\n, which is \n4 points\n ahead of \nDistillBERT\n which has \n60 million parameters\n and \nrequires more training ( may be few days in a single GPU )\n. The \nAlbert-Joint\n model is even better than \nMobileBERT.\n\n\nYou can read more about that \n\n\nhttps://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/\n\n\nFor Code and Models\n\n\nhttps://github.com/legacyai/tf-transformers\n\n\nBenchmarks\n\n\n&#x200B;\n\n\nPlease share feedback, comments, and raise if any issues in Github\n\n\n                                                glue_score\nAbert-Joint-layer_0\t                        0.504815\nAbert-Joint-layer_1\t                        0.682751\nAbert-Joint-layer_2\t                        0.743739\nAbert-Joint-layer_3\t                        0.773670\nAbert-Joint-layer_4\t                        0.798181\nAbert-Joint-layer_5\t                        0.810039\nAbert-Joint-layer_6\t                        0.813973\nAbert-Joint-layer_7\t                        0.822181\nAbert-Joint-layer_8\t                        0.823916\nAbert-Joint-layer_9\t                        0.823932\nAbert-Joint-layer_10\t                        0.827925\nAbert-Joint-layer_11\t                        0.821628\nDistillBert\t                                0.776625",
"date": "2021-03-17"
},
{
"vote": 4,
"title": "Anyone have experience doing TTS with Mozzilla TTS or Ossian (especially for low-resource languages)?",
"text": "Apologies, I realize this is tangentially related to this subreddit, but the TTS and NLP community is small and (perhaps I am wrong) fairly overlapped. Just looking to hear about experiences using one over another. Trying to use this on very low resource languages with a very small linguistics team for commercial purposes, so apologies for being vague. We initially tried Festival/FestVox before realizing that calling that 'a hot mess' is a serious understatement.",
"date": "2021-03-16"
},
{
"vote": 1,
"title": "Is anyone interested in training GPT-2 in your own text?",
"text": "[removed]",
"date": "2021-03-16"
},
{
"vote": 2,
"title": "Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU - Data Science of the Day",
"text": "[deleted]",
"date": "2021-03-16"
},
{
"vote": 4,
"title": "Finding the distance between two sentences that that share mostly the same words.",
"text": "I am trying to find the difference between the following examples:\n\n\n>I am writing this sentence on a Reddit thread.\n>\n>I am right in this sentence on a ready thread.\n\n\nThe output should be something like\n\n\n>I am --- this sentence on a --- thread\n\n\nor\n\n\n>{\"writing\" : \"right in\", \"Reddit\" : \"ready\"}\n\n\nI had a very uneducated idea of looping each word from one of the sentences against the other one. Then define a threshold of how many words to go forward in hope of finding the exact word in the second sentence (3-4 words for example) and give up if there is no occurrence. However this method falls short on many points.\n\n\nIs this a known NLP task? If so, I would appreciate to know what is the keyword for this problem.\n\n\nI am wondering if someone know/used/developed some algorithm/package/functionality for a similar task? I try to keep my development in Python, so I would appreciate Python suggestions as well as any algorithmic suggestions.\n\n\nThanks!\n\n\nEdit: Wow people, thanks a lot! All of the responses were helpful and creative. I have found my solution and leaving this post here for others who are looking for similar solutions\n\n\nEdit 2: Come on, guys. What's up with down voting all these people's comments? All these people were trying to help and I can tell they spend time on it. Please let's respect it, let's discuss if there is wrong information and let's be supportive to each other. We are here to share and grow, right?",
"date": "2021-03-16"
},
{
"vote": 10,
"title": "Facebook comments to build a language model",
"text": "Hello, fellow NLPers,  \n\n\nI am currently working on a low-resource language that practically has no raw corpus whatsoever.  The language in question is an Arabic dialect and it's mainly used on social media or for chatting. Consequently, I was forced to collect data from public Facebook pages in order to build a raw corpus for such dialect. The purpose behind this is to build a general transformer language model (such as BERT), that can then be used/fine-tuned on other specific tasks. The issue I have is the legality and ethicality of publishing such corpus on a scientific paper. In short here are few questions that I need some help with:  \n\n\n\n\nIs it legal/ethically correct to just publish the corpus as is for research purposes?  \n\n\nIf not, is it possible to instead apply some preprocessing techniques to the corpus in a way that makes this ethical?  \n\n\nAre there any publications that you know of that have done something similar or related to collecting data from Facebook and so forth?\n\n\n\n\nAny information you can provide is very appreciated.\n\n\nThanks a lot and have a great day.",
"date": "2021-03-16"
},
{
"vote": 1,
"title": "Are there other supervised ml tasks in NLP besides text classification?",
"text": "Iâ€™m doing some internet sleuthing but Iâ€™m mostly seeing text classification, are there other applications?",
"date": "2021-03-15"
},
{
"vote": 5,
"title": "Into NLP - Tokenization",
"text": null,
"date": "2021-03-15"
},
{
"vote": 2,
"title": "COMPARISION OF DUC DATASETS ACCURACY",
"text": "which Duc dataset gives good evaluation rouge score in text summarization? like is it DUC 2001 or DUC 2002,...?can anyone provide me a link to refer it.",
"date": "2021-03-15"
},
{
"vote": 5,
"title": "How to parse a sentence by writing grammar rules in NLTK CFG ?",
"text": "Below is the original code : \n\n\n \nimport nltk\n\n\n&#x200B;\n\n\n# flight grammar rules\n\n\nflight_grammar = nltk.CFG.fromstring(&quot;&quot;&quot;\n\n\n \nS -&gt; NP VP | VP\n\n\n \nVP -&gt; V NP | V NP PP\n\n\n \nPP -&gt; P NP\n\n\n \nNP -&gt; Prop | Det N | Det N PP\n\n\n \nV -&gt; &quot;walked&quot; | &quot;book&quot; | &quot;prefer&quot; | &quot;gave&quot; | &quot;want&quot;\n\n\n \nProp -&gt; &quot;Jack&quot; | &quot;John&quot; | &quot;I&quot; | &quot;Houston&quot;\n\n\n \nDet -&gt; &quot;a&quot; | &quot;an&quot; | &quot;the&quot; | &quot;my&quot; | &quot;that&quot;\n\n\n \nN -&gt; &quot;dog&quot; | &quot;bone&quot; | &quot;flight&quot;\n\n\n \nP -&gt; &quot;in&quot; | &quot;on&quot; | &quot;by&quot; | &quot;with&quot; | &quot;to&quot; | &quot;through&quot;\n\n\n \n&quot;&quot;&quot;)\n\n\n&#x200B;\n\n\n# make a recursive descent parser and parse the sentence\n\n\nrd_parser = nltk.RecursiveDescentParser(flight_grammar)\n\n\n&#x200B;\n\n\n#define first sentence\n\n\nsenttext = &quot;I prefer a flight through Houston&quot;\n\n\n#tokenize sentence by splitting on white space\n\n\nsentlist = senttext.split()\n\n\n# run the parse function on the tokenized sentence and print the tree strucutre\n\n\nfor tree in rd_parser.parse(sentlist):\n\n\n`print (tree)`\n\n\n\nIt is able to parse the sentence mentioned in the code, but fails to do parse this sentence : \"\"Jack walked with the dog\"\n\n\nModifying the rule to : \nVP -&gt; V NP | V NP PP | V PP\n, can parse that as well. However, I am unable to parse the following sentences : \n\n\n\"John gave the dog a bone\" , \"I want to book that flight\"",
"date": "2021-03-15"
},
{
"vote": 4,
"title": "Learning resources for chatbots / conversational AI?",
"text": "I'm looking to learn the theory, latest research, and development best practices around chatbots and conversational AI. I'm open to all mediums (e.g., books, lecture videos, courses, articles). \n\n\nSo far I'm aware of the Jurafsky & Martin \nchapter\n on it. I know Rasa should have some material floating around somewhere, but my feeling is that it'll be too specific to the Rasa library. \n\n\nAny resources y'all have really enjoyed/found useful?",
"date": "2021-03-15"
},
{
"vote": 2,
"title": "Is there a python based speaker diarization system you would recommend?",
"text": "I have audio files with two speakers and I want to have speech to text conversation. For this I plan on using Huggingface. But I also want to separate text from the two speakers so I need diarization as well.\n\n\nAny tips or suggestions based on your experience so I don't make the same mistakes. \n\n\nI see pyannote and Bob from idiap as potential options but I haven't used them before.",
"date": "2021-03-14"
},
{
"vote": 8,
"title": "Structured Nearest Neighbour Learning for Few-Shot NER | Research Papers Summary 012",
"text": null,
"date": "2021-03-14"
},
{
"vote": 0,
"title": "Seeking help in NLP assignment",
"text": "I am actively seeking help somebody who is good at NLP in finishing my assignment. The raw data is in an XML file. I need to build probably a LSTM.model. if you are interested, please ping me for more details. Thanks.",
"date": "2021-03-14"
},
{
"vote": 2,
"title": "LARA implementation",
"text": "Hi all\n\n\nI am wondering if there's any good implementation of latent aspect rating analysis (LARA). I googled it but surprisingly no libraries support it. I'm too dumb to go through paper and implement it myself. I don't even know how to prepare data for it.\n\n\nAlso now I'm curious how does the guys at Amazon do it?\n\n\nCheers",
"date": "2021-03-14"
},
{
"vote": 11,
"title": "MA in CompLing with a Linguistic background",
"text": "&#x200B;\n\n\n&#x200B;\n\n\nHello everyone,\n\n\nI am a senior student of English Language and Literature with a specialization in Linguistics. I am currently interested in pursuing a MA degree in Computational Linguistics but I am afraid that I am not qualified enough given that I lack basic programming skills.\n\n\nI have already applied to the Language Technology MA degree in Uppsala University and I am planning to apply to both Stuttgart and Tuebingen Computational Linguistics programmes.\n\n\nDo you think that is possible to be accepted with solely a linguistic background?\n\n\nDo you have any other similar MA programmes in mind?\n\n\nThank you in advance",
"date": "2021-03-13"
},
{
"vote": 2,
"title": "What are the ways to handle out of domain inputs for text classification?",
"text": "Out-domain random inputs unwantedly giving very high confidence value. What are the available ways to get rid of it? What to do when we have no negative class data or the negative class is too big to cover?",
"date": "2021-03-13"
},
{
"vote": 1,
"title": "Exploring Redundancy Reduction in Summarizing Long Documents (Research Paper Walkthrough)",
"text": "[deleted]",
"date": "2021-03-13"
},
{
"vote": 14,
"title": "Will NLP have a good future even if we reach AGI?",
"text": "I am an aspiring NLP student and I had this bizarre or some may say amateurish question. I want to know whether this field of AI has good future or not. I know prediction about AGI is like asking when will Avatar sequels come. I just wanted to know if we achieved human level in language, will the field of NLP die or will it move towards Super Intelligence?\n\n\nSo in short I want to know whether NLP will have a good future in terms of research or job opportunities in this century. If this question sounded like non sense, ignore this post, or if you are interested in this question please respond. I am struck at this question for days, so please help me with this.",
"date": "2021-03-12"
},
{
"vote": 1,
"title": "Gradient symbolic Computation... Do you know concrete implementations?",
"text": "I'm reading a couple of papers by Smolensky and Goldrick on their Gradient Symbolic Computation and similar models unifying neural networks and symbolic computation. (Ex. \nCho, Goldrick & Smolensky (2017)\n or \nGoldrick & Smolensky (2016)\n\nReally awesome! \n\n\nDoes anyone of you know where to find a concrete example implemented in Python or R?",
"date": "2021-03-12"
},
{
"vote": 0,
"title": "The Most Popular Programming Languages - 1965/2021",
"text": null,
"date": "2021-03-12"
},
{
"vote": 2,
"title": "Gpt-2 simple isnt good enough for the chatbot I want.",
"text": "So I'm running gpt-2 simple in a google colab and I have two simple problems.\n\n\nFirstly, the text generation speed is far too slow. With minimal input for the medium model I get around 12+ seconds to generate.\n\n\nSo to try and solve this problem I downgraded the model to 124M but the model wasnt capable of a basic conversation.\n\n\nI need a model that's not only capable of generating text fast but is also just as effect as gpt-2 355M if not more effective.\n\n\nI'm wondering if any of you may know a gpt-2 alternative or a way to customize gpt-2 simple to generate faster?",
"date": "2021-03-12"
},
{
"vote": 3,
"title": "Paraphrase Dataset for Slovak",
"text": "looking for a paraphrase dataset for Slovak. Any link ?",
"date": "2021-03-11"
},
{
"vote": 1,
"title": "Topic Modelling (LDA) on DUC 2004 dataset",
"text": "Hi, Has anyone here has done topic Modelling on the DUC 2004 plain text dataset? \n\n\nI'm not having good results. If you don't know about the dataset, it is similar to a news dataset. \n\n\nCan someone guide me to a quality tutorial as i have gone through every video and article and it doesn't seem cover my problem.",
"date": "2021-03-11"
},
{
"vote": 16,
"title": "SpaCy VS Transformers for NER",
"text": "It seems that both spaCy and transform based models (like \nhttps://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english\n) are suite for good entity extraction in English. So it's hard for me to know which one I should use...\n\n\nDo you have an opinion on this in terms of accuracy and performance ?\n\n\nThanks!",
"date": "2021-03-11"
},
{
"vote": 1,
"title": "Video introduction to the join functions of the dplyr package in R programming. The tutorial provides programming examples and explains the difference between inner, left, right, full, semi, and anti joins",
"text": null,
"date": "2021-03-11"
},
{
"vote": 12,
"title": "Topic Modeling using Reddit jokes",
"text": "Has anyone ever tried to do NLP Topic Modeling using Reddit jokes? Currently I'm trying to figure out the best topics for reddit jokes but I'm having a hard time how as the scores using LDA, LSI and HDP are very low. Scores are only around 0.3, 0.2 and the words are just son, man, beer. It just doesn't make sense. My dataset contains jokes from the subreddit dadjokes, 3amjokes, antijokes, darkjokes and jokes.\n\n\nHere's my code - \nhttps://github.com/ZL63388/c6_sprint4/blob/main/Reddit_Joke_Classifier.ipynb",
"date": "2021-03-11"
},
{
"vote": 2,
"title": "Turing Award Winners Yoshua Bengio, Geoffrey Hinton, and Yann LeCun to Speak at GTC21",
"text": "Hey all, I'm an NVIDIA Senior Data Scientist and have a activate personal account on the sub here and across many ML/AI/DL sub-reddits. I think some of the best discussions and debates I've been apart of or read have come on these subs. I'd love to just throw in a plug here and have you all join NVIDIA virtually for our GTC conference this year. Like the title says, the \"granddaddy's\" of modern AI are going to be speaking at the conference! This is a great opportunity to get online and listen to some quality talks and science across many domains and product talks etc.\n\n\nHere's a sign up link, I get zero commission for this, just want to share the love.\nhttps://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH\n\n\nAlso here are some bullets of what GTC has in store.\n\n\n\n\nGTC is a \nfree\n online conference and happening \nApril 12-16\n with live sessions across the world.\n\n\nKeynote\n requires \nno registration\n and happens \nMonday, April 12, at 8:30 AM PST\n, with a second broadcast at \n6:00 PM PST\n for APAC audiences.\n\n\nThe conference will have live webinars, on-demand sessions, posters. Connect With Experts, DLI (with a fee), and multiple panels that include industry pioneers, researchers, developers, start-ups, venture capitalists, and more.\n\n\nThere is an amazing line-up of speakers including Gordon Bell award winners, AI pioneers, Oscar-winning artists, and thought leaders from every industry.",
"date": "2021-03-11"
},
{
"vote": 4,
"title": "[Tutorial] Introduction to Encoder-Decoder Sequence-to-Sequence Models (Seq2Seq)",
"text": "Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers encoder-decoder sequence-to-sequence models (seq2seq) in-depth. Topics covered include:\n\n\n\n\nSeq2seq architecture\n\n\nApplications\n\n\nImplementing seq2seq for text summarization with Keras\n\n\n\n\nNote that the third point only goes up until the data loading and text processing steps. Training and inference are covered in Part 2, which is coming out on Friday. :)\n\n\nArticle link: \nhttps://blog.paperspace.com/introduction-to-seq2seq-models/\n\n\nYou can also run the full code on a free GPU: \nhttps://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models",
"date": "2021-03-10"
},
{
"vote": 21,
"title": "Semantic Search and Fuzzy string matching",
"text": "I know that bert has been used extensively for semantic search; getting similar documents, articles, etc. Word embeddings capture the context better; but is there a way to implement fuzzy string matching with bert's word embeddings, or is tfidf the better solution for that?",
"date": "2021-03-10"
},
{
"vote": 1,
"title": "Custom Word Embeddings or Word2Vec",
"text": "Hi friends,\n\n\nI have a huge documents(tickets) raised by users in company.But most of those are not more than 50 words and after preprocessing it came down to hardly 20-30 words\nIf I use Word2vec it is increasing the dimension to 300 \nStill is it advisable to go for word2vec for proper relation between words or Custom word embeddings of less dimension",
"date": "2021-03-10"
},
{
"vote": 1,
"title": "LAMA AI's weekly news, updates, and events.",
"text": "[removed]",
"date": "2021-03-09"
},
{
"vote": 1,
"title": "NLP for a general object classification?",
"text": "Hey there, I'm interested in experimenting with natural language processing to see if it's possible to perform some sort of object classification with a large dataset, most likely wikipedia. An example scenario would be where a user can formulate questions about what can and cannot be done with an object, and the model can answer these questions.\n\n\n&#x200B;\n\n\nAn example of this could be:\nUser: \"can a cup be filled with water?\"\n\n\nModel: \"yes\"\n\n\nUser: \"does a cat bark?\"\n\n\nModel: \"no\"\n\n\n&#x200B;\n\n\nI am aware that projects like GPT-3 and similar are *very* complex and can (to some degree) reason, however my intention with this is *not* to perform advanced language processing that requires advanced analysis. Also, the example provided above is not representative of what my goals are with this; the conversation may be represented with some simple class or structure that conveys the object you're asking about (cup/motorcycle), and the subject in question (can it be filled with water/does it bark). Therefore the purpose of this question does not pertain to the question asked by the user but the ability to parse text from an available database and extract useful facts and key information from the text.\n\n\n&#x200B;\n\n\nAny advice? Thanks",
"date": "2021-03-08"
},
{
"vote": 1,
"title": "Start Japanese text processing without installing any tokenizer on your local environment (tokenizers are on docker container)",
"text": null,
"date": "2021-03-08"
},
{
"vote": 10,
"title": "Intent and Action Classification, analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text and much more on NLU 1.1.3",
"text": "Intent and Action Classification,  analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text, and much more in NLU 1.1.3\n\n\nNLU 1.1.3 Release Notes\n\n\nWe are very excited to announce that the latest NLU release comes with a new pretrained Intent Classifier and NER Action Extractor for text related to\nmusic, restaurants, and movies trained on the SNIPS dataset. Make sure to check out the models hub and the easy 1-liners for more info!\n\n\nIn addition to that, new NER and Embedding models for Bengali are now available\n\n\nFinally, there is a new NLU Webinar with 9 accompanying tutorial notebooks which teach you  a lot of things and is segmented into the following parts :\n\n\n\n\nPart1: Easy 1 Liners \n\n\nSpell checking/Sentiment/POS/NER/ BERTtology embeddings\n\n\n\n\n\n\nPart2: Data analysis and NLP tasks on \nCrypto News Headline dataset\n\n\nPreprocessing and extracting Emotions, Keywords, Named Entities and visualize them\n\n\n\n\n\n\nPart3: NLU Multi-Lingual 1 Liners with \nMicrosoft's Marian Models\n\n\nTranslate between 200+ languages (and classify lang afterward)\n\n\n\n\n\n\nPart 4: Data analysis and NLP tasks on \nChinese News Article Dataset\n\n\nWord Segmentation, Lemmatization, Extract Keywords, Named Entities and translate to english\n\n\n\n\n\n\nPart 5: Train a sentiment Classifier that understands 100+ Languages\n\n\nTrain on a french sentiment dataset and predict the sentiment of 100+ languages with \nlanguage-agnostic BERT Sentence Embedding\n\n\n\n\n\n\nPart 6: Question answering, Summarization, Squad and more with \nGoogle's T5\n\n\nT5 Question answering and 18 + other NLP tasks (\nSQUAD\n / \nGLUE\n / \nSUPER GLUE\n)\n\n\n\n\n\n\n\n\nNew Models\n\n\nNLU 1.1.3 New Non-English Models\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nBengali\n\n\nbn.ner.cc_300d\n\n\n bengaliner_cc_300d\n\n\nNerDLModel\n\n\n\n\n\n\nBengali\n\n\nbn.embed\n\n\nbengali_cc_300d\n\n\nNerDLModel\n\n\n\n\n\n\nBengali\n\n\nbn.embed.cc_300d\n\n\nbengali_cc_300d\n\n\nWord Embeddings Model (Alias)\n\n\n\n\n\n\nBengali\n\n\nbn.embed.glove\n\n\nbengali_cc_300d\n\n\nWord Embeddings Model (Alias)\n\n\n\n\n\n\nNLU 1.1.3 New English Models\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nEnglish\n\n\nen.classify.snips\n\n\nnerdl_snips_100d\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.snips\n\n\nclassifierdl_use_snips\n\n\nClassifierDLModel\n\n\n\n\n\n\nNew NLU Webinar\n\n\nState-of-the-art Natural Language Processing for 200+ Languages with 1 Line of code\n\n\nTalk Abstract\n\n\nLearn to harness the power of 1,000+ production-grade & scalable NLP models for 200+ languages - all available with just 1 line of Python code by leveraging the open-source NLU library, which is powered by the widely popular Spark NLP.\n\n\nJohn Snow Labs has delivered over 80 releases of Spark NLP to date, making it the most widely used NLP library in the enterprise and providing the AI community with state-of-the-art accuracy and scale for a variety of common NLP tasks. The most recent releases include pre-trained models for over 200 languages - including languages that do not use spaces for word segmentation algorithms like Chinese, Japanese, and Korean, and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew. All software and models are free and open source under an Apache 2.0 license.\n\n\nThis webinar will show you how to leverage the multi-lingual capabilities of Spark NLP & NLU - including automated language detection for up to 375 languages, and the ability to perform translation, named entity recognition, stopword removal, lemmatization, and more in a variety of language families. We will create Python code in real-time and solve these problems in just 30 minutes. The notebooks will then be made freely available online.\n\n\nYou can watch the \nvideo here,\n \n\n\nNLU 1.1.3 New Notebooks and tutorials\n\n\nNew Webinar Notebooks\n\n\n\n\nNLU basics, easy 1-liners (Spellchecking, sentiment, NER, POS, BERT\n\n\nAnalyze Crypto News dataset with Keyword extraction, NER, Emotional distribution, and stemming\n\n\nTranslate Crypto News dataset between 300 Languages with the Marian Model (German, French, Hebrew examples)\n\n\nTranslate Crypto News dataset between 300 Languages with the Marian Model (Hindi, Russian, Chinese examples)\n\n\nAnalyze Chinese News Headlines with Chinese Word Segmentation, Lemmatization, NER, and Keyword extraction\n\n\nTrain a Sentiment Classifier that will understand 100+ languages on just a French Dataset with the powerful Language Agnostic Bert Embeddings\n\n\nSummarize text and Answer Questions with T5\n\n\nSolve any task in 1 line from SQUAD, GLUE and SUPER GLUE with T5\n\n\nOverview of models for various languages\n\n\n\n\nNew easy NLU 1-liners in NLU 1.1.3\n\n\nDetect actions in general commands related to music, restaurant, movies.\n\n\nnlu.load(&quot;en.classify.snips&quot;).predict(&quot;book a spot for nona gray  myrtle and alison at a top-rated brasserie that is distant from wilson av on nov  the 4th  2030 that serves ouzeri&quot;,output_level = &quot;document&quot;)\n\n\n\noutputs :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\ndocument\n\n\nEntities_Classes\n\n\n\n\n\n\n\n\n[1.0, 1.0, 0.9997000098228455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990000128746033, 1.0, 1.0, 1.0, 0.9965000152587891, 0.9998999834060669, 0.9567000269889832, 1.0, 1.0, 1.0, 0.9980000257492065, 0.9991999864578247, 0.9988999962806702, 1.0, 1.0, 0.9998999834060669]\n\n\n['nona gray myrtle and alison', 'top-rated', 'brasserie', 'distant', 'wilson av', 'nov the 4th 2030', 'ouzeri']\n\n\nbook a spot for nona gray myrtle and alison at a top-rated brasserie that is distant from wilson av on nov the 4th 2030 that serves ouzeri\n\n\n['party_size_description', 'sort', 'restaurant_type', 'spatial_relation', 'poi', 'timeRange', 'cuisine']\n\n\n\n\n\n\nNamed Entity Recognition (NER) Model in Bengali (bengaliner_cc_300d)\n\n\n# Bengali for: &#039;Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.&#039;\nnlu.load(&quot;bn.ner.cc_300d&quot;).predict(&quot;à§§à§¯à§ªà§® à¦¸à¦¾à¦²à§‡ à¦‡à¦¯à¦¼à¦¾à¦œà¦‰à¦¦à§à¦¦à¦¿à¦¨ à¦†à¦¹à¦®à§à¦®à§‡à¦¦ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦‰à¦šà§à¦š à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼ à¦¥à§‡à¦•à§‡ à¦®à§‡à¦Ÿà§à¦°à¦¿à¦• à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨ à¦à¦¬à¦‚ à§§à§¯à§«à§¦ à¦¸à¦¾à¦²à§‡ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦¹à¦°à¦—à¦™à§à¦—à¦¾ à¦•à¦²à§‡à¦œ à¦¥à§‡à¦•à§‡ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦®à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦Ÿ à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨&quot;,output_level = &quot;document&quot;)\n\n\n\noutputs :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_Classes\n\n\ndocument\n\n\n\n\n\n\n\n\n[0.9987999796867371, 0.9854000210762024, 0.8604000210762024, 0.6686999797821045, 0.5289999842643738, 0.7009999752044678, 0.7684999704360962, 0.9979000091552734, 0.9976000189781189, 0.9930999875068665, 0.9994000196456909, 0.9879000186920166, 0.7407000064849854, 0.9215999841690063, 0.7657999992370605, 0.39419999718666077, 0.9124000072479248, 0.9932000041007996, 0.9919999837875366, 0.995199978351593, 0.9991999864578247]\n\n\n['à¦¸à¦¾à¦²à§‡', 'à¦‡à¦¯à¦¼à¦¾à¦œà¦‰à¦¦à§à¦¦à¦¿à¦¨ à¦†à¦¹à¦®à§à¦®à§‡à¦¦', 'à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦‰à¦šà§à¦š à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼', 'à¦¸à¦¾à¦²à§‡', 'à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦¹à¦°à¦—à¦™à§à¦—à¦¾ à¦•à¦²à§‡à¦œ']\n\n\n['TIME', 'PER', 'ORG', 'TIME', 'ORG']\n\n\nà§§à§¯à§ªà§® à¦¸à¦¾à¦²à§‡ à¦‡à¦¯à¦¼à¦¾à¦œà¦‰à¦¦à§à¦¦à¦¿à¦¨ à¦†à¦¹à¦®à§à¦®à§‡à¦¦ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦‰à¦šà§à¦š à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼ à¦¥à§‡à¦•à§‡ à¦®à§‡à¦Ÿà§à¦°à¦¿à¦• à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨ à¦à¦¬à¦‚ à§§à§¯à§«à§¦ à¦¸à¦¾à¦²à§‡ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦¹à¦°à¦—à¦™à§à¦—à¦¾ à¦•à¦²à§‡à¦œ à¦¥à§‡à¦•à§‡ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦®à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦Ÿ à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨\n\n\n\n\n\n\nIdentify intent in general text - SNIPS dataset\n\n\nnlu.load(&quot;en.ner.snips&quot;).predict(&quot;I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area&quot;,output_level = &quot;document&quot;)\n\n\n\noutputs :\n\n\n\n\n\n\n\n\ndocument\n\n\nsnips\n\n\nsnips_confidence\n\n\n\n\n\n\n\n\nI want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area\n\n\nBookRestaurant\n\n\n1\n\n\n\n\n\n\nWord Embeddings for Bengali (bengali_cc_300d)\n\n\n# Bengali for : &#039;Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.&#039;\nnlu.load(&quot;bn.embed&quot;).predict(&quot;à§§à§¯à§ªà§® à¦¸à¦¾à¦²à§‡ à¦‡à¦¯à¦¼à¦¾à¦œà¦‰à¦¦à§à¦¦à¦¿à¦¨ à¦†à¦¹à¦®à§à¦®à§‡à¦¦ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦‰à¦šà§à¦š à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼ à¦¥à§‡à¦•à§‡ à¦®à§‡à¦Ÿà§à¦°à¦¿à¦• à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨ à¦à¦¬à¦‚ à§§à§¯à§«à§¦ à¦¸à¦¾à¦²à§‡ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦¹à¦°à¦—à¦™à§à¦—à¦¾ à¦•à¦²à§‡à¦œ à¦¥à§‡à¦•à§‡ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦®à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦Ÿ à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨&quot;,output_level = &quot;document&quot;)\n\n\n\noutputs :\n\n\n\n\n\n\n\n\ndocument\n\n\nbn_embed_embeddings\n\n\n\n\n\n\n\n\nà§§à§¯à§ªà§® à¦¸à¦¾à¦²à§‡ à¦‡à¦¯à¦¼à¦¾à¦œà¦‰à¦¦à§à¦¦à¦¿à¦¨ à¦†à¦¹à¦®à§à¦®à§‡à¦¦ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦‰à¦šà§à¦š à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼ à¦¥à§‡à¦•à§‡ à¦®à§‡à¦Ÿà§à¦°à¦¿à¦• à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨ à¦à¦¬à¦‚ à§§à§¯à§«à§¦ à¦¸à¦¾à¦²à§‡ à¦®à§à¦¨à§à¦¸à¦¿à¦—à¦žà§à¦œ à¦¹à¦°à¦—à¦™à§à¦—à¦¾ à¦•à¦²à§‡à¦œ à¦¥à§‡à¦•à§‡ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦®à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦Ÿ à¦ªà¦¾à¦¶ à¦•à¦°à§‡à¦¨\n\n\n[-0.0828      0.0683      0.0215     ...  0.0679     -0.0484...]\n\n\n\n\n\n\nNLU 1.1.3 Enhancements\n\n\n\n\nAdded automatic conversion  to Sentence Embeddings of Word Embeddings when there is no Sentence Embedding Avaiable and a model needs the converted version to run.\n\n\n\n\nNLU 1.1.3 Bug Fixes\n\n\n\n\nFixed a bug that caused \nur.sentiment\n NLU pipeline to build incorrectly\n\n\nFixed a bug that caused \nsentiment.imdb.glove\n NLU pipeline to build incorrectly\n\n\nFixed a bug that caused \nen.sentiment.glove.imdb\n NLU pipeline to build incorrectly\n\n\nFixed a bug that caused Spark 2.3.X environments to crash.\n\n\n\n\nNLU Installation\n\n\n# PyPi\n!pip install nlu pyspark==2.4.7\n#Conda\n# Install NLU from Anaconda/Conda\nconda install -c johnsnowlabs nlu\n\n\n\nAdditional NLU ressources\n\n\n\n\nNLU Website\n\n\nAll NLU Tutorial Notebooks\n\n\nNLU Videos and Blogposts on NLU\n\n\nNLU on Github\n\n\nSuggestions or Questions? Contact us in Slack!",
"date": "2021-03-08"
},
{
"vote": 1,
"title": "HyperBand and BOHB: understanding hyperparameter optimization algorithms",
"text": "If you want to learn about state-of-the-art hyperparameter optimization algorithms (HPO), in this article Iâ€™ll tell you what they are and how they work.\n\n\nWe cover:\n\n\n\n\nA bit about HPO Approaches \n\n\nWhat is Bayesian Optimization, and why is this method effective? \n\n\nHow do state-of-the-art Hyperparameter Optimization algorithms work? \n\n\nHyperband vs BOHB comparison\n\n\n\n\nHyperBand vs. BOHB",
"date": "2021-03-08"
},
{
"vote": 2,
"title": "PMI for WordClouds",
"text": "Hello!\n\n\nI'm currently developing an NLP project and I'm stuck on something. I'm a NLP beginner (using Python) and I have some questions I'm hoping the community might help me address.\n\n\nI've built a dataset of several hundred political speeches and I'm building Word Clouds out of the content of these speeches. To make the story short, I want to tokenize \"phrases\" or \"concepts\"; for example the name of an institution or a commonly used phrase by politicians, so that my word cloud will reflect the actual \"phrase\" or \"concept\" instead of showing me the individual words.\n\n\nI'm aware of the concept of PMI (Pointwise Mutual Information) and how it can help identify these patterns in my data. I haven't been able to find any resources that show a code pipeline to do this.\n\n\nAny suggestions would be gladly appreciated.\n\n\nThanks in advance!",
"date": "2021-03-07"
},
{
"vote": 1,
"title": "ANNOY and Semantic Search",
"text": "I am trying to build a semantic search algorithm using Bert embeddings and using annoy for indexing. AnnoyIndex() is built to take in pooled embeddings (I think); but what about word embeddings?  For example a word embedding of 100 docs of shape[100, 256, 768].  An example of working with pooled embeddings shape (100, 768): - \n\n\nfrom annoy import AnnoyIndex\n\n\n# Bert emits 768 dim vectors\n\n\nD = 768\n\n\nn_trees = 300\n\n\nann = AnnoyIndex(D, 'angular')\n\n\npooled\n\n\nfor index, embed in enumerate(pooled_embeddings):\n\n\nann.add_item(index, embed)\n\n\nann.build\n(n_trees)\n\n\n&#x200B;\n\n\nShould I flatten the word vectors? should I reduce the dimensionality in some other way?",
"date": "2021-03-07"
},
{
"vote": 22,
"title": "Embeddings of Label Components for Fine-grained NER | Research Papers Summary 011",
"text": null,
"date": "2021-03-07"
},
{
"vote": 0,
"title": "Sentiment Analysis With Transformer Models Made Easy",
"text": "[deleted]",
"date": "2021-03-07"
},
{
"vote": 8,
"title": "tNodeEmbed: Node Embeddings with Temporal Graphs | ML with Graphs (Research Paper Walkthrough)",
"text": null,
"date": "2021-03-07"
},
{
"vote": 15,
"title": "Is there a way to deploy NLP models into a Chrome extension?",
"text": "I would like to be able to process text data from newspaper sites, tweets, or whatever piece of a text an user can select from a web site and run different NLP inferences (sentiment, polarity, entity recognition, etc.) Within the client side. \n\n\nI'm relatively new and ,I don't know, may be Tensorflow could be used? \n\n\nAny thoughts?",
"date": "2021-03-07"
},
{
"vote": 8,
"title": "Is it possible to test whether a tokenizer can losslessly tokenize and detokenize a given corpus solely from its vocabulary?",
"text": "I am trying to test whether a given vocabulary list contains the tokens necessary to reconstruct a corpus of text losslessly. That is, if a tokenizer trained on a corpus of text were to attempt to tokenize the training corpus according to its associated vocabulary, would it be able to tokenize and detokenize the entire training corpus without losing any text to [unk] tokens? Are there ways to test this?\n\n\nSuppose you had a corpus of text - Iâ€™ll use a small one for this example\n\n\ncorpus = â€˜the cat in the hatâ€™\n\n\n\nSuppose you had a trained tokenizer which tokenizes according to its vocab list where each token id is simply the index of the token in the list.\n\n\nvocab = [â€˜tâ€™, â€˜hâ€™, â€˜eâ€™, â€˜ â€˜, â€˜câ€™, â€˜aâ€™, â€˜iâ€™, â€˜nâ€™]\nprint(len(vocab))\n8\ntokens = tokenize(corpus, vocab))\nprint(tokens)\n[0, 1, 2, 3, 4, 5, 0, 3, 6, 7, 3, 0, 1, 2, 3, 2, 5, 0]\nprint(â€œTokenized length: {}â€.format(len(tokens)))\nTokenized length: 18\n\n\n\nIf I then detokenize from here I can obviously reconstruct the corpus as it originally was. However, this process is suboptimal as we can combine tokens to reduce the length of the tokenized representation.\n\n\nvocab = [â€˜tâ€™, â€˜hâ€™, â€˜eâ€™, â€˜ â€˜, â€˜câ€™, â€˜aâ€™, â€˜iâ€™, â€˜nâ€™, â€˜thâ€™]\nprint(len(vocab))\n9\ntokens = tokenize(corpus, vocab))\nprint(tokens)\n[8, 2, 3, 4, 5, 0, 3, 6, 7, 3, 8, 2, 3, 2, 5, 0]\nprint(â€œTokenized length: {}â€.format(len(tokens)))\nTokenized length: 16\n\n\n\nThis is obviously still lossless, though it adds an extra token to the vocabulary list, which will increase the possibility space of a model trying to predict the next token in a sequence. But if I remove a necessary token, it becomes lossy. Supppose our tokenizer outputs â€˜[unk]â€™ for all tokens not in the vocabulary and that the â€˜[unk]â€™ token is always the last token in the vocabulary regardless of the content of the vocabulary.\n\n\nvocab = [â€˜tâ€™, â€˜hâ€™, â€˜eâ€™, â€˜ â€˜, â€˜câ€™, â€˜aâ€™, â€˜iâ€™, â€˜thâ€™]\nprint(len(vocab))\n8\ntokens = tokenize(corpus, vocab))\nprint(tokens)\n[7, 2, 3, 4, 5, 0, 3, 6, 8, 3, 7, 2, 3, 2, 5, 0]\nprint(â€œTokenized length: {}â€.format(len(tokens)))\nTokenized length: 16\n\n\n\nThe length of the representation does not change but we can tell that it is lossy just from the vocabulary, therefore the compression is not lossless.\n\n\nprint(detokenize(tokens, vocab))\n[â€˜the cat i[unk] the hatâ€™]\n\n\n\nThis example makes it easy to tell the compression is lossy because our vocabulary is small and made of suboptimally combined tokens. We can go further with the combination of the vocabulary and combine â€˜tâ€™, â€˜hâ€™, â€˜eâ€™, and â€˜ â€˜ to form a single token covering all instances of â€œthe â€œ. Since â€˜eâ€™ doesnâ€™t occur outside of its place in â€˜theâ€™ we can delete it from the vocabulary list and still maintain the same level of loss in compression, ditto â€˜iâ€™ and â€˜nâ€™.\n\n\nvocab = [â€˜tâ€™, â€˜hâ€™, â€˜the  â€˜, â€˜ â€˜, â€˜câ€™, â€˜aâ€™, â€˜inâ€™]\nprint(len(vocab))\n7\ntokens = tokenize(corpus, vocab)\nprint(tokens)\n[2, 4, 5, 0, 3, 6, 3, 2, 1, 5, 0]\nprint(len(tokens)\n11\n\n\n\nThus far this is the most optimally compressed form of the sentence (though I donâ€™t claim for it to be the most optimally compressed) and it has the smallest vocabulary size, making it easier for a model to guess the next token. In all of these examples we have been able to tell unambiguously whether or not the compression is lossless just by eyeballing it.\n\n\nHowever, there is more than one way to tokenize a sentence. If our tokenizer sees the â€˜tâ€™ and â€˜hâ€™ without the context of the â€˜e â€˜, it will run into a problem: **there is no â€˜eâ€™ in our vocabulary list** and, if not properly trained, it will be forced to replace that â€˜eâ€™ with an â€˜[unk]â€™ token, making it lossy while also expanding the length of its representation significantly.\n\n\nTherefore, even though we have just proven lossless compression is possible given this corpus and the last vocabulary list, if the recognition of the sequence by the tokenizer is poor, it wonâ€™t be capable of losslessly tokenizing even though the vocabulary list is suited for it. As corpus size increases, it becomes harder and harder to tell just by looking that the vocabulary list can losslessly reconstruct the corpus.\n\n\nWith something like SentencePiece, which defaults to a vocabulary size in the thousands, trying to reconstruct the Wiki-sentences corpus, which is millions and millions of sentences long, it becomes untenable to pore over every output vocabulary size and manually check if it can reconstruct the corpus.\n\n\nThus my question: given a vocabulary list and a corpus, is there an automatic way to tell that that it is *possible* to reconstruct the given corpus losslessly using the given vocabulary assuming a well trained tokenizer? In other words is there a function to determine if lossless tokenization is possible which returns true or false given an input corpus and vocabulary?",
"date": "2021-03-07"
},
{
"vote": 2,
"title": "Looking for En-Zh bilingual medical corpora",
"text": "Hi everyone \n\n\nI am building an MT engine and looking for En-Zh bilingual medical corpora to download. If anyone can share it here with me will be much appreciated!",
"date": "2021-03-06"
},
{
"vote": 13,
"title": "Linguistic resources?",
"text": "Hi all. I'm looking for resources that computer science students can use to learn more about linguistics. As my professor likes to put it, we're good at the P part of NLP but aren't the best at the NL part. Any help is greatly appreciated!",
"date": "2021-03-06"
},
{
"vote": 2,
"title": "Help with tf-idf for job description extraction",
"text": "Hi, this isn't really a question asking for help with code syntax, more so on the general theory / workflow side of things. \n\n\nI'm doing a small Python project where I'm trying to extract relevant job 'requirements' from UX Researcher job listings on Google. I've scraped a load of jobs, cleaned them using NLTK, Spacy etc and I have saved each job description as a text file respectively. I also made one large text file corpus with all the cleaned job descriptions in. \n\n\nI've heard that tf-idf is a good way to go about extracting just the employers' desired 'requirements' from these job descriptions (i.e., 'PhD in Psychology' or 'Agile and Scrum experience'). \n\n\nMy problem however is i'm not sure in what format to load the documents in to process using tf-idf. Should I load in all of the documents which each contain one job description, or use the one text file which has all of the documents in itself? My thinking is I need to load each individual document given that tf-idf uses document frequency right? Any help would be appreciated, thanks!",
"date": "2021-03-05"
},
{
"vote": 2,
"title": "Hosting PDF for public view: how to make it as much OCR-friendly as possible?",
"text": "Hello!\n\n\n&#x200B;\n\n\n My company is going to make available many PDF files for the public.\n\n\n&#x200B;\n\n\n Which settings/encoding should I recommend them to use in order for the software to be as much OCR-friendly as possible? For example, saving a MS Word file in PDF instead of scanning a printed image. (but that's a non-technical example)",
"date": "2021-03-05"
},
{
"vote": 2,
"title": "How does event extraction differ from information extraction/ retrieval?",
"text": "Based on some additional research I am revising a question I asked yesterday. So new question, how does event extraction differ from information extraction? Could I use information extraction type code to accomplish event extraction from a written corpus? Any help would be much appreciated!",
"date": "2021-03-05"
},
{
"vote": 1,
"title": "Visualizing the Distribution of Several $Billions of #NGO Grants through Natural Language Processing (Case Study)",
"text": "I thought this could be of interest. A team of 40+ changemakers built an NLP analysis pipeline for feature extraction, scraping Twitter, Google, and 1200 PDF files through automated APIs.   \n\n\nhttps://omdena.com/blog/nlp-analysis/",
"date": "2021-03-05"
},
{
"vote": 1,
"title": "I got ya homie!",
"text": null,
"date": "2021-03-05"
},
{
"vote": 1,
"title": "King -Man +Woman = King ?",
"text": null,
"date": "2021-03-04"
},
{
"vote": 1,
"title": "How to best determine changing news event coverageâ€” semantic similarity or no?",
"text": "I am looking to figure out a way to show how news events unfold over time : ie how early articles described an event like the us capitol riot versus what they say now. Iâ€™m guessing semantic similarity is the best way to achieve this? Does anyone have other/ better ideas? Iâ€™m fairly new to nlp and appreciate any help anyone can provide. I have gotten great help from this subreddit in the past.",
"date": "2021-03-04"
},
{
"vote": 13,
"title": "How does event extraction differ from topic modeling?",
"text": "Can anyone explain how event extraction coding techniques differ from topic modeling? A lot of the code examples Iâ€™ve looked at seem to be using topic modeling then organizing discerned topics by date. Is there more to event extraction than that? I assume there is. I feel like I must be missing something and I hope someone here has some insight. Also, if anyone has any materials regarding event extraction that are helpful I would love to hear about them. Thanks all!",
"date": "2021-03-04"
},
{
"vote": 0,
"title": "What is Ð”Ð Ð›ÐÐ”ÐÐž in Russian Language | Spoken Russian with NATIVE | Engl...",
"text": null,
"date": "2021-03-04"
},
{
"vote": 4,
"title": "Tutorial on how to perform a Shapiro-Wilk normality test",
"text": "Hey, I've created a tutorial on how to perform a Shapiro-Wilk normality test in the R programming language: \nhttps://statisticsglobe.com/shapiro-wilk-normality-test-in-r",
"date": "2021-03-04"
},
{
"vote": 1,
"title": "Best/good practices for evaluating deep learning NER models?",
"text": "Hi all, I'm supposed to evaluate a Dutch DL NER model (Huggingface) for a course, and compare it to a rule-based model. I'm feeling a bit lost and don't know where to start. Does anyone have any tips/good practices? What metrics should I use, and are there other evaluation techniques besides metrics? I appreciate all input!",
"date": "2021-03-03"
},
{
"vote": 1,
"title": "Conference for Truth and Trust Online Calls for Paper and Talk Proposal Submissions",
"text": "Are you working on fact-checking, detection of misinformation, hate speech or other problems related to trust and truth online? You can now \nsubmit a paper or talk proposal to the Truth and Trust Online 2021 (TTO 2021). \nhttps://truthandtrustonline.com/call-for-papers-2/\n\n\nThe annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space. This yearâ€™s \nTTO is virtual and will take place online Oct 7-8 2021.\n \n\n\nWe welcome technical papers of the following types: surveys, methods, reproduction papers, resource papers, case studies. \n\n\nTopics of interest include:\n\n\n\n\nMisinformation and disinformation \n\n\nTrustworthiness of COVID-19 news and guidance\n\n\nHate speech\n\n\nOnline harassment and cyberbullying\n\n\nCredibility and fake reviews\n\n\nHyper-partisanship and bias\n\n\nImage/video/audio verification\n\n\nFake amplification, polarization, and echo chambers\n\n\nTransparency in content and source moderation \n\n\nPrivacy and anonymity requirements\n\n\n\n\nWe encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.\n\n\nTechnical paper submission deadline: July 30, 2021\n\n\nTalk proposal submission deadline: August 13, 2021\n\n\nMore details can be found: \nhttps://truthandtrustonline.com/call-for-papers-2/",
"date": "2021-03-02"
},
{
"vote": 1,
"title": "NLP course",
"text": "Hi,\n\n\nI've peripherally worked with NLP in the past mainly through my work with Python and machine learning. I'd love to get more in-depth knowledge and so would my company, so much so they've offered to pay for any NLP course I want.\n\n\nDoes anyone have any good recommendations for NLP courses?",
"date": "2021-03-02"
},
{
"vote": 3,
"title": "[D] Annotation tool for entity sentiment analysis",
"text": "Hi everyone, we are a marketing company and going to start an annotation project for entity sentiment analysis. Can you please share what are best practices for starting an NLP annotation project? What is most efficient approach? What techniques are mostly used to automate the annotation process?",
"date": "2021-03-02"
},
{
"vote": 7,
"title": "Paper \"M6: A Chinese Multimodal Pretrainer\". Dataset contains 1900GB of images and 292GB of text. Models contain 10B parameters and 100B (Mixture-of-Experts) parameters. Images shown are text-to-image examples from the paper. Paper link is in a comment.",
"text": null,
"date": "2021-03-02"
},
{
"vote": 33,
"title": "Researchers From Stanford, UCI and UC Santa Barbara Conducted a Study to Understand How The mBERT Model Encodes Grammatical Features",
"text": "Over the past few decades, Deep neural network-based models have been developed to complete a broad range of tasks. Some of them are mainly designed to process and generate coherent texts in multiple languages, answer questions about a text, translate texts, and create summaries of the online content.\n\n\nSeveral Deep learning systems are already available with linguistic capabilities, for instance, text analysis tools, in the form of applications for real-time translation, and virtual assistants such as Alexa, Bixby, Siri, Google Assistant, and Cortana. Some of the above systems use a specific deep-learning model called Multilingual BERT (mBERT). mBERT is released by Google and is trained on approximately 100 languages simultaneously.Â \n\n\nPaper summary: \nhttps://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/\n \n\n\nPaper: \nhttps://arxiv.org/abs/2101.11043\n \n\n\nGithub: \nhttps://github.com/toizzy/deep-subjecthood",
"date": "2021-03-02"
},
{
"vote": 4,
"title": "LAMA AI's weekly news, updates, and events.",
"text": "Hey guys!\n\n\nThis week, LAMA (\nhttps://lamaai.io\n) has a couple of updates. Let's start with this weeks AI news!\n\n\nYou can find the video \nhere\n, but as for the key highlights:\n\n\n\n\nFacebook AI Research\n announce a new multi-modal Transformer architecture, \nUniT\n\n\nSebastian Ruder updates\n us on the latest advances in language model fine-tuning\n\n\nOpenAI\n have news about DALL-E\n\n\nGeoffrey Hinton \nproposes an idea paper\n he dubs GLOM\n\n\nStudioGAN\n is introduced: A PyTorch library for SoTA GAN models\n\n\n\n\n&#x200B;\n\n\nWould you like to know how we can use Machine Learning to detect COVID symptoms? Imperial College's \nBjÃ¶rn Schuller\n is going to be presenting  his recent and topical work on detecting COVID symptoms through the use of Computer Audition (think Computer Vision but for audio instead!). As a little introduction, BjÃ¶rn is a Full Professor at the University of Augsburg in Germany, where he is also Chair of Embedded Intelligence for Health Care and Wellbeing. He is also a Professor of Artificial Intelligence at Imperial College London and heads GLAM (Group for Language, Audio and Music). He has over 1000 publications which feature his name (ðŸ¤¯) and his recent research interests focus on audio and multi-modal approaches to emotion detection. BjÃ¶rn will be discussing his paper: \nCOVID-19 and Computer Audition\n which was written during the outbreak last year. In this paper, he overviews the usage of speech and sound analysis by artificial intelligence/machine learning to detect a presence of COVID. If you're interested in attending the talk, register on the eventbrite: \nhttps://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561\n\n\n&#x200B;\n\n\nFinally, last week we had a paper presentation on the current state of AI's progress towards Natural Language Understanding. You can find the video/talk \nhere\n! As for some key points from the talk:\n\n\n\n\n(Bender and Koller, 2020) discuss the question whether a system exposed only to the form of language in its training data, can in principle learn its meaning\n\n\nThey underline their arguments with multiple thought experiments and a comparison to human children language acquisition which is grounded in the real world and in interaction with others\n\n\nThe NLP research community is called to reflect on the current research trends and to take a more top-down approach by asking â€œwhether the hill we are climbing so rapidly is the right hillâ€\n\n\n(Linzen, 2020) discusses common evaluation practices in NLP research and their limitations\n\n\nHe proposes a new evaluation paradigm which takes into consideration pre-training corpora of different sizes, as well as normative and efficiency attributes while comparing ML models to each other.",
"date": "2021-03-01"
},
{
"vote": 14,
"title": "[Need Advice]PhD in NLP @ reputed US institute/Prof. Worth it?",
"text": "Hi, \n\n\nI recently got admitted to a good PhD program in the US to work on natural language processing. The advisor is great too. A question to this reddit community - do you think going for a PhD in this domain is worth it? \n\n\nBackground: Im already working at a great organization where my learning curve is increasing for the last two years. Im getting to work on state of the art things, I get to read papers, implement them, come up with my own architectures etc. My peers are very supportive, and Im assuming this is paving path for great industry opportunities in general. \n\n\nHowever, I applied for a PhD for two reasons: I want to learn more about the domain and stick to an area to get more expertise, understand the theory etc. And eventually I want to be a research lead, for which I think a PhD will provide me with immense credibility. However the idea of starting a PhD at 27, and going back to school and that lifestyle for another five years is very very scary. Im starting to have cold feet. \n\n\nAny words of wisdom from someone in the process, or someone who has been through this? \n\n\nThank you so much!",
"date": "2021-03-01"
},
{
"vote": 2,
"title": "Extracting information from text",
"text": "Hello,\n\n\nI am a NLP noob so please forgive my naive questions. I have a problem and looking for pointers to find a solution.\n\n\nI have a list of keywords and have loads of unstructured data. Based on the keywords in my list, I want to extract information like values from the text. For example I have a sentence like 'His age is 15'. and my keyword is 'AGE'. Now I want to extract the age and the number from the text. THe thing is since the text is unstructured, the exact semantics of the sentence would differ in different texts. So I am looking to find/implement some solution that identifies the keyword (Simple enough) and extracts the associated information. I have tried using regexp, they work sometimes and sometimes start getting too complex to extract the info. Any help would be appreciated.\n\n\nThanks",
"date": "2021-03-01"
},
{
"vote": 2,
"title": "Into NLP - Fuzzy String Matching and the Edit Distance",
"text": null,
"date": "2021-03-01"
},
{
"vote": 1,
"title": "Does anyone know of any domain-specific pretrained language models?",
"text": "Hi everyone. The title is the question. I'm just wondering whether there may be any language models like BERT that were pretrained on domain-specific corpora.\n\n\nFor example, the BioBERT model is known to have been pretrained on biomedical text, and hence achieves better performance than BERT on biomedical NLP tasks.\n\n\nWould there be anything else along the same lines? Thanks.",
"date": "2021-03-01"
},
{
"vote": 9,
"title": "Training a Multi-Label Emotion Classifier with Tez and PyTorch to detect +20 different emotions",
"text": "Hello everyone!\n\n\nI built an end-to-end tutorial to train an emotion classifier using SqueezeBERT, a state-of-the-art lightweight version of BERT.\nI got +0.9 on test data in a few lines of code.\nI thought I'd share with you my approach and the code so that you can maybe apply it to your projects.\n\n\nThe trained model is also available so that you can use it on your own data without retraining from scratch.\n\n\n\n\nHere's the code: \nhttps://github.com/ahmedbesbes/multi-label-sentiment-classifier\n\n\nand the blog post: \nhttps://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a\n\n\n\n\nPlease let me know if you have an issue\n\n\nFeedbacks more than welcome :)\n\n\nBest!",
"date": "2021-02-28"
},
{
"vote": 7,
"title": "I have an issue with GPT-2 Reading comprehension for a task.",
"text": "Scroll down to bottom for TLDR.\n\n\nSo ive been working on fine-tuning GPT2 to be capable of reading comprehension using CoQa dataset. It works good but my goal is to create a system that can read an entire book and then answer questions given about the book. The problem is there is too much data being sent into the bot within a single input. It can handle multiple paragraphs quite well and perhaps even an essay but a book is just too much data. There are a few solutions ive thought of such as using reading comprehension to create a pre-generated data set with each paragraph of the book followed by generated questions. However, even using google colab such data generation would take an excruciatingly long time. If there are 400 paragraphs it would take nearly 3 hours to finish the data generation. I also thought I could perhaps remove words like \"You, but or\" etc those kind of conjunctions and pronouns that don't at to the story. But I feel as if its comprehension capabilities may be diminished. Perhaps such a task would not be feasible until I can get my hands on GPT-3?\n\n\nAnyways ill break down the methods that I have thought of\n\n\nMethod 1: Take the specified book and allow GPT-2 to generate reading comprehension dataset from it. Then feed that data back into the model so that it learns specific things about said book. ETA 4+ hours\n\n\nMethod 2: Take the book and remove conjunctions and pronouns from the text. then do the same as method 1\n\n\nETA <2 hours\n\n\nMethod 3: No clue how this would work but perhaps I could just throw the whole book into the training data along with reading comprehension data to see if it might pick up on some of the questions Eta < 1 hour\n\n\nMethod 4: Find a dataset that specifically answers questions from a variety of books and add that to CoQa. Eta < 1 hour + how long to find said dataset.\n\n\nThe end goal is to create a system where I can have an AI answer questions about a book. Which would completely remove any need for me to read said book. Also be just as capable of reading pages from a text book and completing questions. Such a system would greatly enhance my ability to complete school work and free up hours of wasted time on learning about 50 year old books.\n\n\nTLDR:\n\n\nBook is too big to feed into GPT-2 reading comprehension model. How do I design/train my model to quickly answer questions about entire books?\n\n\nMy Question to you:\n\n\nIf you are reading, my questions to you are exactly which methods I should approach and is this challenge even feasible using the current GPT-2 model.",
"date": "2021-02-28"
},
{
"vote": 6,
"title": "Struggling With Terminology From Linguistics",
"text": "Hi all, \n\n\nStarting to make a switch toward NLP research in my Ph.D. and find I struggle to read many papers which use terminology from linguistics. Does anyone have any book suggestions that might help me better understand grammar / linguistic terminology? I am a native English speaker, I just am not familiar, for example, with Chomsky's surface vs deep structures, what frame semantics are, etc. \n\n\nThanks!",
"date": "2021-02-28"
},
{
"vote": 1,
"title": "Applying gensim topic model on new document",
"text": "Hi,\n\n\nI am currently working on a topic modelling project with gensim and i have some issues, regarding the tf_idf vectors for adding a new document.\n\n\n&#x200B;\n\n\nThe preprocessed and tokenized data is stored in a pandas dataframe (using spaCy). I splitted the data into 2 parts train and test. (2 seperate dataframes). \n\n\nGensim requires to first create a dictionary. The dictionary is expandable, so i first created it with only the train data and afterwards expand it with the test data, if there are possibly some new tokens.\n\n\nAfterward the corpus is created ( which are basically just BOW vectors). The tf-idf vectorizer is created on top of this corpus and then applyed on the corpus:\n\n\n\"\"\"\n\n\ndataset = corpora.Dictionary()\n\n\ndataset.add_documents(dataframe['tokens'].tolist())\n\n\ncorpus = [dataset.doc2bow(text) for text in dataframe['tokens'].tolist()]\n\n\ntfidf_vectorizer = TfidfModel(corpus)\n\n\ntfidf = tfidf_vectorizer[corpus]\n\n\n\"\"\"\n\n\n&#x200B;\n\n\nQuestion 1: For applying the model to a new document, i would need the tfidf vectors of the new document. Do i have to just add the document to the old corpus, create a new tfidf_vectorizer with that corpus and then just apply the vectorizer to only the new document? Or am i missing something?\n\n\nQuestion 2: As i saw in the documentation, the model is applied to a new document with the following code( i am using LsiModel):\n\n\n\"\"\"\n\n\nmodel = LsiModel(tfidf_vect[corpus], id2word=dataset)\n\n\nvector = model[tfidf_vect[new_corpus]\n\n\n\"\"\"\n\n\nPrint(vector) prints :  <gensim.interfaces.TransformedCorpus at 0x245468431>. \n\n\nHow can i visualize / just print the resulting vector ( i guess it should be the topic distribution of the new document)? Or am i doing something wrong here ?",
"date": "2021-02-28"
},
{
"vote": 29,
"title": "University of Wisconsin-Madison, UC Berkeley, and Google Brain Introduce Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention",
"text": "Early days of research in Natural language processing established long-term dependencies. It also brought the vanishing gradient problem in front of us because nascent models were handling the input sequences one by one, without parallelization. Recently, revolutionary transformer-based architectures and the self-attention mechanisms have enabled token pairsâ€™ interactions across complete sequences resulting in modeling arbitrary dependencies in a constant number of layers. The above helped achieve state-of-the-art performance across many Natural language processing tasks.\n\n\nHowever, these advantages came to a high-cost barrier because the transformer-based networksâ€™ memory and computational requirements grow quadratically with sequence length. The above results in significant efficiency bottlenecks when dealing with long sequences. Researchers from the University of Wisconsin-Madison, American Family Insurance, UC Berkeley, and Google Brain propose NystrÃ¶mformer. NystrÃ¶mformer is an O(n) approximation in both memory and time for self-attention. The above is designed to rescue us from the\nÂ quadratic cost\nÂ associated with the long input sequences\n\n\nPaper Summary: \nhttps://www.marktechpost.com/2021/02/27/university-of-wisconsin-madison-uc-berkeley-and-google-brain-introduce-nystromformer-a-nystrom-based-algorithm-for-approximating-self-attention/\n\n\nPaper: \nhttps://arxiv.org/pdf/2102.03902.pdf\n\n\nGithub: \nhttps://github.com/mlpen/Nystromformer",
"date": "2021-02-28"
},
{
"vote": 7,
"title": "Paper: Investigating the Limitations of the Transformers with Simple Arithmetic Tasks",
"text": null,
"date": "2021-02-28"
},
{
"vote": 5,
"title": "New Masters program in Voice and Speech Tech (in Europe)",
"text": "There is a new 1-year Masterâ€™s program at the University of Groningen (the Netherlands) dedicated exclusively to voice/speech tech. Focus is on speech recognition and voice synthesis. \n\n\nThis is an interdisciplinary program for students from CS, AI,  Linguistics, or similar. \n\n\nhttps://youtu.be/297BY6uTHB8\n\n\nhttps://www.rug.nl/masters/voice-technology/",
"date": "2021-02-27"
},
{
"vote": 15,
"title": "Best MOOCs/Projects for MS in CL/NLP (Linguistics Background)",
"text": "Hi everyone! \n\n\nI come from a language/linguistics background. I got my BA in Spanish with a minor in Linguistics. Iâ€™ve taken classes like Syntax, phonetics, phonology, language technology, Hispanic linguistics, and â€œunderstanding machine learning.â€\n\n\nIâ€™m looking to apply to masters in Voice Tech, comp ling or NLP this year and I would like to boost the technical side of my knowledge and CV so that the universities see that I could be a good fit for their programs. \n\n\nIâ€™ve already completed two courses on Python on Coursera and Codecademy, and I trained a language detection model using naive bayes to distinguish between Spanish and Portuguese texts. I am really eager to keep learning, I just donâ€™t know where to start or what direction to go in.\n\n\nAre there any online courses or portfolio projects you would recommend to show admissions that I have some background knowledge? Unfortunately I canâ€™t re-enroll at my university at the moment as they are already half way through the semester and it would cost thousands. \n\n\nThanks in advance!",
"date": "2021-02-27"
},
{
"vote": 1,
"title": "Guidelines PHP 8 GIG",
"text": null,
"date": "2021-02-27"
},
{
"vote": 3,
"title": "How to extract keywords important to a text classification problem?",
"text": "Hi.\n\n\nI have a text multi-class text classification problem, in which I'm trying to classify different subreddits' comments using a very simple TFIDF + PCA + SVM pipeline. What I'm really keen to know is that how different keywords in each class contribute to this classification problem. How can I do this? I have around 10 classes each having 5000 comments with 30 words in average.",
"date": "2021-02-26"
},
{
"vote": 1,
"title": "[P] KsponSpeech: 1000-hour Korean speech corpus preprocessing code.",
"text": "[removed]",
"date": "2021-02-26"
},
{
"vote": 1,
"title": "[R] AAMAS 2021: A framework for integrating gesture generation models into interactive conversational agents",
"text": null,
"date": "2021-02-26"
},
{
"vote": 0,
"title": "Emotion extraction from subtitles of the movies",
"text": "Hi everyone, \n\n\ni'm trying to implement a system that from the subtitles of movies/tv series extracts emotion that film causes. I found dataset named DailyDialog that contain 103K sentences labelled with 6 based Ekman's emotion plus no_emotion label.\n\n\nI'm trying to use BERT and i reached an 85% of accuracy. The problem is that dataset is very unbalanced. I'm trying to use BERT for data augmentation using it as masked language modelling (MLM) for generate new sentences with new words that fit in the context of the sentence. For every sentence the data aug generates ten sencentes. The problem is  that the word outputed by BERT it is not sure it is conforms to the emotion of the sencente. I can try to see with SenticNet if the word is related to the emotion of sentence.\n\n\nAnother thing is that Bert immediately goes into overfitting.\n\n\n- Anyone have any suggestions if the project is feasible?\n- Do you know any datasets related to the described task?\n- Do you have any suggestions on how to do a good data augmentation?",
"date": "2021-02-26"
},
{
"vote": 3,
"title": "Want to extract semantic meaning from implied context and norms, I need direction of what to search for",
"text": "Iâ€™m curious about determining semantic meaning in cases where context implies more than what is available in the sentence. \n\n\nFor example: â€œI only wear that shirt because she gave it to meâ€. The implied meaning is that â€œIâ€ donâ€™t like the shirt. \n\n\nIs there any dataset or tool to extract these implied or norm based meanings? Thank you!",
"date": "2021-02-26"
},
{
"vote": 8,
"title": "Train Multi-Lingual classifier for 100 languages in 1 Line, Hindi Word Embeddings, Bengali NER NYC/DC Meetup Webinar, in NLU 1.1.2",
"text": "NLU 1.1.2 Release Notes\n\n\nWe are very happy to announce NLU 1.1.2 has been released with the integration of 30+ models and pipelines Bengali Named Entity Recognition, Hindi Word Embeddings,\nand state-of-the-art transformer based OntoNotes models and pipelines from the \nincredible Spark NLP 2.7.3 Release\n in addition to a few bugfixes.\nIn addition to that, there is a \nnew NLU Webinar video\n showcasing in detail \nhow to use NLU to analyze a crypto news dataset to extract keywords unsupervised and predict sentimential/emotional distributions of the dataset and much more!\n\n\nPython's NLU library: 1,000+ models, 200+ Languages, State of the Art Accuracy, 1 Line of code - NLU NYC/DC NLP Meetup Webinar\n\n\nUsing just 1 line of Python code by leveraging the NLU library, which is powered by the award-winning Spark NLP.\n\n\nThis webinar covers, using live coding in real-time,\nhow to deliver summarization, translation, unsupervised keyword extraction, emotion analysis,\nquestion answering, spell checking, named entity recognition, document classification, and other common NLP tasks. T\nhis is all done with a single line of code, that works directly on Python strings or pandas data frames.\nSince NLU is based on Spark NLP, no code changes are required to scale processing to multi-core or cluster environment - integrating natively with Ray, Dask, or Spark data frames.\n\n\nThe recent releases for Spark NLP and NLU include pre-trained models for over 200 languages and language detection for 375 languages.\nThis includes 20 languages families; non-Latin alphabets; languages that do not use spaces for word segmentation like\nChinese, Japanese, and Korean; and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew.\nWe'll also cover some of the algorithms and models that are included. The code notebooks will be freely available online.\n\n\nNLU 1.1.2 New Models  and Pipelines\n\n\nNLU 1.1.2 New Non-English Models\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nBengali\n\n\nbn.ner\n\n\nner_jifs_glove_840B_300d\n\n\nWord Embeddings Model (Alias)\n\n\n\n\n\n\nBengali\n\n\nbn.ner.glove\n\n\nner_jifs_glove_840B_300d\n\n\nWord Embeddings Model (Alias)\n\n\n\n\n\n\nHindi\n\n\nhi.embed\n\n\nhindi_cc_300d\n\n\nNerDLModel\n\n\n\n\n\n\nBengali\n\n\nbn.lemma\n\n\nlemma\n\n\nLemmatizer\n\n\n\n\n\n\nJapanese\n\n\nja.lemma\n\n\nlemma\n\n\nLemmatizer\n\n\n\n\n\n\nBihari\n\n\nbh.lemma\n\n\nlemma\n\n\nLemma\n\n\n\n\n\n\nAmharic\n\n\nam.lemma\n\n\nlemma\n\n\nLemma\n\n\n\n\n\n\nNLU 1.1.2 New English Models and Pipelines\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.small_l2_128\n\n\nonto_small_bert_L2_128\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.small_l4_256\n\n\nonto_small_bert_L4_256\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.small_l4_512\n\n\nonto_small_bert_L4_512\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.small_l8_512\n\n\nonto_small_bert_L8_512\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.cased_base\n\n\nonto_bert_base_cased\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.cased_large\n\n\nonto_bert_large_cased\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.electra.uncased_small\n\n\nonto_electra_small_uncased\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.electra.uncased_base\n\n\nonto_electra_base_uncased\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.electra.uncased_large\n\n\nonto_electra_large_uncased\n\n\nNerDLModel\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.tiny\n\n\nonto_recognize_entities_bert_tiny\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.mini\n\n\nonto_recognize_entities_bert_mini\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.small\n\n\nonto_recognize_entities_bert_small\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.medium\n\n\nonto_recognize_entities_bert_medium\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.base\n\n\nonto_recognize_entities_bert_base\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.bert.large\n\n\nonto_recognize_entities_bert_large\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.electra.small\n\n\nonto_recognize_entities_electra_small\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.electra.base\n\n\nonto_recognize_entities_electra_base\n\n\nPipeline\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.large\n\n\nonto_recognize_entities_electra_large\n\n\nPipeline\n\n\n\n\n\n\nNew Tutorials and Notebooks\n\n\n\n\nNYC/DC NLP Meetup Webinar video analyze Crypto News, Unsupervised Keywords, Translate between 300 Languages, Question Answering, Summerization, POS, NER in 1 line of code in almost just 20 minutes\n\n\nNLU basics POS/NER/Sentiment Classification/BERTology Embeddings\n\n\nExplore Crypto Newsarticle dataset, unsupervised Keyword extraction, Stemming, Emotion/Sentiment distribution Analysis\n\n\nTranslate between more than 300 Languages in 1 line of code with the Marian Models\n\n\nNew NLU 1.1.2 Models Showcase Notebooks, Bengali NER, Hindi Embeddings, 30 new_models\n\n\n\n\nNLU 1.1.2 Bug Fixes\n\n\n\n\nFixed a bug that caused NER confidences not beeing extracted\n\n\nFixed a bug that caused nlu.load('spell') to crash\n\n\nFixed a bug that caused Uralic/Estonian/ET language models not to be loaded properly\n\n\n\n\nNew  Easy NLU 1-liners in 1.1.2\n\n\nNamed Entity Recognition for Bengali (GloVe 840B 300d)\n\n\n#Bengali for :  It began to be widely used in the United States in the early &#039;90s.\nnlu.load(&quot;bn.ner&quot;).predict(&quot;à§¯à§¦ à¦à¦° à¦¦à¦¶à¦•à§‡à¦° à¦¶à§à¦°à§à¦° à¦¦à¦¿à¦•à§‡ à¦¬à§ƒà¦¹à§Ž à¦†à¦•à¦¾à¦°à§‡ à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡ à¦à¦° à¦ªà§à¦°à¦¯à¦¼à§‹à¦—à§‡à¦° à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ à¦¶à§à¦°à§ à¦¹à¦¯à¦¼&#039;&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nentities\n\n\ntoken\n\n\nEntities_classes\n\n\nner_confidence\n\n\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà§¯à§¦\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦à¦°\n\n\n['LOC']\n\n\n0.9999\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¦à¦¶à¦•à§‡à¦°\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¶à§à¦°à§à¦°\n\n\n['LOC']\n\n\n0.9969\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¦à¦¿à¦•à§‡\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¬à§ƒà¦¹à§Ž\n\n\n['LOC']\n\n\n0.9994\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦†à¦•à¦¾à¦°à§‡\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦®à¦¾à¦°à§à¦•à¦¿à¦¨\n\n\n['LOC']\n\n\n0.9602\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡\n\n\n['LOC']\n\n\n0.4134\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦à¦°\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦ªà§à¦°à¦¯à¦¼à§‹à¦—à§‡à¦°\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¶à§à¦°à§\n\n\n['LOC']\n\n\n0.9999\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\nà¦¹à¦¯à¦¼\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\n['à¦®à¦¾à¦°à§à¦•à¦¿à¦¨ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦·à§à¦Ÿà§à¦°à§‡']\n\n\n'\n\n\n['LOC']\n\n\n1\n\n\n\n\n\n\nBengali Lemmatizer\n\n\n#Bengali for :  One morning in the marble-decorated building of Vaidyanatha, an obese monk was engaged in the enchantment of Duis and the milk service of one and a half Vaidyanatha. Give me two to eat\nnlu.load(&quot;bn.lemma&quot;).predict(&quot;à¦à¦•à¦¦à¦¿à¦¨ à¦ªà§à¦°à¦¾à¦¤à§‡ à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥à§‡à¦° à¦®à¦¾à¦°à§à¦¬à¦²à¦®à¦£à§à¦¡à¦¿à¦¤ à¦¦à¦¾à¦²à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¸à§à¦¥à§‚à¦²à§‹à¦¦à¦° à¦¸à¦¨à§à¦¨à§à¦¯à¦¾à¦¸à§€ à¦¦à§à¦‡à¦¸à§‡à¦° à¦®à§‹à¦¹à¦¨à¦­à§‹à¦— à¦à¦¬à¦‚ à¦¦à§‡à¦¡à¦¼à¦¸à§‡à¦° à¦¦à§à¦—à§à¦§ à¦¸à§‡à¦¬à¦¾à§Ÿ à¦¨à¦¿à¦¯à§à¦•à§à¦¤ à¦†à¦›à§‡ à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥ à¦—à¦¾à§Ÿà§‡ à¦à¦•à¦–à¦¾à¦¨à¦¿ à¦šà¦¾à¦¦à¦° à¦¦à¦¿à§Ÿà¦¾ à¦œà§‹à¦¡à¦¼à¦•à¦°à§‡ à¦à¦•à¦¾à¦¨à§à¦¤ à¦¬à¦¿à¦¨à§€à¦¤à¦­à¦¾à¦¬à§‡ à¦­à§‚à¦¤à¦²à§‡ à¦¬à¦¸à¦¿à§Ÿà¦¾ à¦­à¦•à§à¦¤à¦¿à¦­à¦°à§‡ à¦ªà¦¬à¦¿à¦¤à§à¦° à¦­à§‹à¦œà¦¨à¦¬à§à¦¯à¦¾à¦ªà¦¾à¦° à¦¨à¦¿à¦°à§€à¦•à§à¦·à¦£ à¦•à¦°à¦¿à¦¤à§‡à¦›à¦¿à¦²à§‡à¦¨ à¦à¦®à¦¨ à¦¸à¦®à§Ÿ à¦•à§‹à¦¨à§‹à¦®à¦¤à§‡ à¦¦à§à¦¬à¦¾à¦°à§€à¦¦à§‡à¦° à¦¦à§ƒà¦·à§à¦Ÿà¦¿ à¦à¦¡à¦¼à¦¾à¦‡à§Ÿà¦¾ à¦œà§€à¦°à§à¦£à¦¦à§‡à¦¹ à¦¬à¦¾à¦²à¦• à¦¸à¦¹à¦¿à¦¤ à¦à¦•à¦Ÿà¦¿ à¦…à¦¤à¦¿ à¦¶à§€à¦°à§à¦£à¦•à¦¾à§Ÿà¦¾ à¦°à¦®à¦£à§€ à¦—à§ƒà¦¹à§‡ à¦ªà§à¦°à¦¬à§‡à¦¶ à¦•à¦°à¦¿à§Ÿà¦¾ à¦•à§à¦·à§€à¦£à¦¸à§à¦¬à¦°à§‡ à¦•à¦¹à¦¿à¦² à¦¬à¦¾à¦¬à§ à¦¦à§à¦Ÿà¦¿ à¦–à§‡à¦¤à§‡ à¦¦à¦¾à¦“&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nlemma\n\n\ndocument\n\n\n\n\n\n\n\n\n['à¦à¦•à¦¦à¦¿à¦¨', 'à¦ªà§à¦°à¦¾à¦¤à¦ƒ', 'à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥', 'à¦®à¦¾à¦°à§à¦¬à¦²à¦®à¦£à§à¦¡à¦¿à¦¤', 'à¦¦à¦¾à¦²à¦¾à¦¨', 'à¦à¦•', 'à¦¸à§à¦¥à§‚à¦²à¦‰à¦¦à¦°', 'à¦¸à¦¨à§à¦¨à§à¦¯à¦¾à¦¸à§€', 'à¦¦à§à¦‡à¦¸à§‡à¦°', 'à¦®à§‹à¦¹à¦¨à¦­à§‹à¦—', 'à¦à¦¬à¦‚', 'à¦¦à§‡à¦¡à¦¼à¦¸à§‡à¦°', 'à¦¦à§à¦—à§à¦§', 'à¦¸à§‡à¦¬à¦¾', 'à¦¨à¦¿à¦¯à§à¦•à§à¦¤', 'à¦†à¦›à§‡', 'à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥', 'à¦—à¦¾', 'à¦à¦•à¦–à¦¾à¦¨', 'à¦šà¦¾à¦¦à¦°', 'à¦¦à§‡à¦“à§Ÿà¦¾', 'à¦œà§‹à¦¡à¦¼à¦•à¦°', 'à¦à¦•à¦¾à¦¨à§à¦¤', 'à¦¬à¦¿à¦¨à§€à¦¤à¦­à¦¾à¦¬', 'à¦­à§‚à¦¤à¦²', 'à¦¬à¦¸à¦¾', 'à¦­à¦•à§à¦¤à¦¿à¦­à¦°à¦¾', 'à¦ªà¦¬à¦¿à¦¤à§à¦°', 'à¦­à§‹à¦œà¦¨à¦¬à§à¦¯à¦¾à¦ªà¦¾à¦°', 'à¦¨à¦¿à¦°à§€à¦•à§à¦·à¦£', 'à¦•à¦°à¦¾', 'à¦à¦®à¦¨', 'à¦¸à¦®à§Ÿ', 'à¦•à§‹à¦¨à§‹à¦®à¦¤', 'à¦¦à§à¦¬à¦¾à¦°à§€', 'à¦¦à§ƒà¦·à§à¦Ÿà¦¿', 'à¦à¦¡à¦¼à¦¾à¦¨à§‹', 'à¦œà§€à¦°à§à¦£à¦¦à§‡à¦¹', 'à¦¬à¦¾à¦²à¦•', 'à¦¸à¦¹à¦¿à¦¤', 'à¦à¦•', 'à¦…à¦¤à¦¿', 'à¦¶à§€à¦°à§à¦£à¦•à¦¾à§Ÿà¦¾', 'à¦°à¦®à¦£à§€', 'à¦—à§ƒà¦¹', 'à¦ªà§à¦°à¦¬à§‡à¦¶', 'à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸', 'à¦•à§à¦·à§€à¦£à¦¸à§à¦¬à¦°', 'à¦•à¦¹à¦¾', 'à¦¬à¦¾à¦¬à§', 'à¦¦à§à¦‡', 'à¦–à¦¾à¦“à§Ÿà¦¾', 'à¦¦à¦¾à¦“à§Ÿà¦¾']\n\n\nà¦à¦•à¦¦à¦¿à¦¨ à¦ªà§à¦°à¦¾à¦¤à§‡ à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥à§‡à¦° à¦®à¦¾à¦°à§à¦¬à¦²à¦®à¦£à§à¦¡à¦¿à¦¤ à¦¦à¦¾à¦²à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¸à§à¦¥à§‚à¦²à§‹à¦¦à¦° à¦¸à¦¨à§à¦¨à§à¦¯à¦¾à¦¸à§€ à¦¦à§à¦‡à¦¸à§‡à¦° à¦®à§‹à¦¹à¦¨à¦­à§‹à¦— à¦à¦¬à¦‚ à¦¦à§‡à¦¡à¦¼à¦¸à§‡à¦° à¦¦à§à¦—à§à¦§ à¦¸à§‡à¦¬à¦¾à§Ÿ à¦¨à¦¿à¦¯à§à¦•à§à¦¤ à¦†à¦›à§‡ à¦¬à§ˆà¦¦à§à¦¯à¦¨à¦¾à¦¥ à¦—à¦¾à§Ÿà§‡ à¦à¦•à¦–à¦¾à¦¨à¦¿ à¦šà¦¾à¦¦à¦° à¦¦à¦¿à§Ÿà¦¾ à¦œà§‹à¦¡à¦¼à¦•à¦°à§‡ à¦à¦•à¦¾à¦¨à§à¦¤ à¦¬à¦¿à¦¨à§€à¦¤à¦­à¦¾à¦¬à§‡ à¦­à§‚à¦¤à¦²à§‡ à¦¬à¦¸à¦¿à§Ÿà¦¾ à¦­à¦•à§à¦¤à¦¿à¦­à¦°à§‡ à¦ªà¦¬à¦¿à¦¤à§à¦° à¦­à§‹à¦œà¦¨à¦¬à§à¦¯à¦¾à¦ªà¦¾à¦° à¦¨à¦¿à¦°à§€à¦•à§à¦·à¦£ à¦•à¦°à¦¿à¦¤à§‡à¦›à¦¿à¦²à§‡à¦¨ à¦à¦®à¦¨ à¦¸à¦®à§Ÿ à¦•à§‹à¦¨à§‹à¦®à¦¤à§‡ à¦¦à§à¦¬à¦¾à¦°à§€à¦¦à§‡à¦° à¦¦à§ƒà¦·à§à¦Ÿà¦¿ à¦à¦¡à¦¼à¦¾à¦‡à§Ÿà¦¾ à¦œà§€à¦°à§à¦£à¦¦à§‡à¦¹ à¦¬à¦¾à¦²à¦• à¦¸à¦¹à¦¿à¦¤ à¦à¦•à¦Ÿà¦¿ à¦…à¦¤à¦¿ à¦¶à§€à¦°à§à¦£à¦•à¦¾à§Ÿà¦¾ à¦°à¦®à¦£à§€ à¦—à§ƒà¦¹à§‡ à¦ªà§à¦°à¦¬à§‡à¦¶ à¦•à¦°à¦¿à§Ÿà¦¾ à¦•à§à¦·à§€à¦£à¦¸à§à¦¬à¦°à§‡ à¦•à¦¹à¦¿à¦² à¦¬à¦¾à¦¬à§ à¦¦à§à¦Ÿà¦¿ à¦–à§‡à¦¤à§‡ à¦¦à¦¾à¦“\n\n\n\n\n\n\nJapanese Lemmatizer\n\n\n#Japanese for :  Some residents were uncomfortable with this, but it seems that no one is now openly protesting or protesting.\nnlu.load(&quot;ja.lemma&quot;).predict(&quot;ã“ã‚Œã«ä¸å¿«æ„Ÿã‚’ç¤ºã™ä½æ°‘ã¯ã„ã¾ã—ãŸãŒ,ç¾åœ¨,è¡¨ç«‹ã£ã¦åå¯¾ã‚„æŠ—è­°ã®å£°ã‚’æŒ™ã’ã¦ã„ã‚‹ä½æ°‘ã¯ã„ãªã„ã‚ˆã†ã§ã™ã€‚&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nlemma\n\n\ndocument\n\n\n\n\n\n\n\n\n['ã“ã‚Œ', 'ã«ã‚‹', 'ä¸å¿«', 'æ„Ÿ', 'ã‚’', 'ç¤ºã™', 'ä½æ°‘', 'ã¯ã‚‹', 'ã„ã‚‹', 'ã¾ã™ã‚‹', 'ãŸã‚‹', 'ãŒã‚‹', ',', 'ç¾åœ¨', ',', 'è¡¨ç«‹ã¤', 'ã¦ã‚‹', 'åå¯¾', 'ã‚„ã‚‹', 'æŠ—è­°', 'ã®ã‚‹', 'å£°', 'ã‚’', 'æŒ™ã’ã‚‹', 'ã¦ã‚‹', 'ã„ã‚‹', 'ä½æ°‘', 'ã¯ã‚‹', 'ã„ã‚‹', 'ãªã', 'ã‚ˆã†', 'ã§ã™', 'ã€‚']\n\n\nã“ã‚Œã«ä¸å¿«æ„Ÿã‚’ç¤ºã™ä½æ°‘ã¯ã„ã¾ã—ãŸãŒ,ç¾åœ¨,è¡¨ç«‹ã£ã¦åå¯¾ã‚„æŠ—è­°ã®å£°ã‚’æŒ™ã’ã¦ã„ã‚‹ä½æ°‘ã¯ã„ãªã„ã‚ˆã†ã§ã™ã€‚\n\n\n\n\n\n\nAharic Lemmatizer\n\n\n#Aharic for :  Bookmark the permalink.\nnlu.load(&quot;am.lemma&quot;).predict(&quot;áˆ˜áŒ½áˆá‰áŠ• áˆ˜áŒ½áˆá áŠ¡ áŠ• áŠ áˆµá‹«á‹›á‰µ áŠ áˆµá‹«á‹ áŠ§ áŠ£á‰µ á¢&quot;)\n\n\n\noutput  :\n\n\n\n\n\n\n\n\nlemma\n\n\ndocument\n\n\n\n\n\n\n\n\n['\n', 'áˆ˜áŒ½áˆá', 'áŠ¡', 'áŠ•', '\n', 'áŠ áˆµá‹«á‹', 'áŠ§', 'áŠ£á‰µ', 'á¢']\n\n\náˆ˜áŒ½áˆá‰áŠ• áˆ˜áŒ½áˆá áŠ¡ áŠ• áŠ áˆµá‹«á‹›á‰µ áŠ áˆµá‹«á‹ áŠ§ áŠ£á‰µ á¢\n\n\n\n\n\n\nBhojpuri Lemmatizer\n\n\n#Bhojpuri for : In this event, participation of World Bhojpuri Conference, Purvanchal Ekta Manch, Veer Kunwar Singh Foundation, Purvanchal Bhojpuri Mahasabha, and Herf - Media.\nnlu.load(&quot;bh.lemma&quot;).predict(&quot;à¤à¤¹ à¤†à¤¯à¥‹à¤œà¤¨ à¤®à¥‡à¤‚ à¤µà¤¿à¤¶à¥à¤µ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤¸à¤®à¥à¤®à¥‡à¤²à¤¨ , à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤² à¤à¤•à¤¤à¤¾ à¤®à¤‚à¤š , à¤µà¥€à¤° à¤•à¥à¤à¤µà¤° à¤¸à¤¿à¤‚à¤¹ à¤«à¤¾à¤‰à¤¨à¥à¤¡à¥‡à¤¶à¤¨ , à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤² à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤®à¤¹à¤¾à¤¸à¤­à¤¾ , à¤…à¤‰à¤° à¤¹à¤°à¥à¤« - à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤•à¥‡ à¤¸à¤¹à¤­à¤¾à¤—à¤¿à¤¤à¤¾ à¤¬à¤¾ à¥¤&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nlemma\n\n\ndocument\n\n\n\n\n\n\n\n\n['à¤à¤¹', 'à¤†à¤¯à¥‹à¤œà¤¨', 'à¤®à¥‡à¤‚', 'à¤µà¤¿à¤¶à¥à¤µ', 'à¤­à¥‹à¤œà¤ªà¥à¤°à¥€', 'à¤¸à¤®à¥à¤®à¥‡à¤²à¤¨', 'COMMA', 'à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤²', 'à¤à¤•à¤¤à¤¾', 'à¤®à¤‚à¤š', 'COMMA', 'à¤µà¥€à¤°', 'à¤•à¥à¤à¤µà¤°', 'à¤¸à¤¿à¤‚à¤¹', 'à¤«à¤¾à¤‰à¤¨à¥à¤¡à¥‡à¤¶à¤¨', 'COMMA', 'à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤²', 'à¤­à¥‹à¤œà¤ªà¥à¤°à¥€', 'à¤®à¤¹à¤¾à¤¸à¤­à¤¾', 'COMMA', 'à¤…à¤‰à¤°', 'à¤¹à¤°à¥à¤«', '-', 'à¤®à¥€à¤¡à¤¿à¤¯à¤¾', 'à¤•à¥‹', 'à¤¸à¤¹à¤­à¤¾à¤—à¤¿à¤¤à¤¾', 'à¤¬à¤¾', 'à¥¤']\n\n\nà¤à¤¹ à¤†à¤¯à¥‹à¤œà¤¨ à¤®à¥‡à¤‚ à¤µà¤¿à¤¶à¥à¤µ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤¸à¤®à¥à¤®à¥‡à¤²à¤¨ , à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤² à¤à¤•à¤¤à¤¾ à¤®à¤‚à¤š , à¤µà¥€à¤° à¤•à¥à¤à¤µà¤° à¤¸à¤¿à¤‚à¤¹ à¤«à¤¾à¤‰à¤¨à¥à¤¡à¥‡à¤¶à¤¨ , à¤ªà¥‚à¤°à¥à¤µà¤¾à¤‚à¤šà¤² à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤®à¤¹à¤¾à¤¸à¤­à¤¾ , à¤…à¤‰à¤° à¤¹à¤°à¥à¤« - à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤•à¥‡ à¤¸à¤¹à¤­à¤¾à¤—à¤¿à¤¤à¤¾ à¤¬à¤¾ à¥¤\n\n\n\n\n\n\nNamed Entity Recognition - BERT Tiny (OntoNotes)\n\n\nnlu.load(&quot;en.ner.onto.bert.small_l2_128&quot;).predict(&quot;&quot;&quot;William Henry Gates III (born October 28, 1955) is an American business magnate,\n software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,\n  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,\n   while also being the largest individual shareholder until May 2014.\n    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;\n     it went on to become the world&#039;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.\n     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time\n      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.\n He gradually transferred his duties to Ray Ozzie and Craig Mundie.\n  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&quot;&quot;&quot;,output_level = &quot;document&quot;)\n\n\n\noutput  :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_classes\n\n\n\n\n\n\n\n\n[0.8536999821662903, 0.7195000052452087, 0.746...]\n\n\n['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'DATE', 'GPE', 'GPE']\n\n\n['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s', '1980s', 'Seattle', 'Washington', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico']\n\n\n\n\n\n\nNamed Entity Recognition - BERT Mini (OntoNotes)\n\n\nnlu.load(&quot;en.ner.onto.bert.small_l4_256&quot;).predict(&quot;&quot;&quot;William Henry Gates III (born October 28, 1955) is an American business magnate,\n software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,\n  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,\n   while also being the largest individual shareholder until May 2014.\n    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;\n     it went on to become the world&#039;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.\n     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time\n      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.\n He gradually transferred his duties to Ray Ozzie and Craig Mundie.\n  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&quot;&quot;&quot;,output_level = &quot;document&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_classes\n\n\n\n\n\n\n\n\n[0.835099995136261, 0.40450000762939453, 0.331...]\n\n\n['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico']\n\n\n['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'ORG', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'ORG', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE']\n\n\n\n\n\n\nNamed Entity Recognition - BERT Small (OntoNotes)\n\n\nnlu.load(&quot;en.ner.onto.bert.small_l4_512&quot;).predict(&quot;&quot;&quot;William Henry Gates III (born October 28, 1955) is an American business magnate,\n software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,\n  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,\n   while also being the largest individual shareholder until May 2014.\n    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;\n     it went on to become the world&#039;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.\n     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time\n      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.\n He gradually transferred his duties to Ray Ozzie and Craig Mundie.\n  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&quot;&quot;&quot;,output_level = &quot;document&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_classes\n\n\n\n\n\n\n\n\n[0.964900016784668, 0.8299000263214111, 0.9607...]\n\n\n['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico']\n\n\n['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE']\n\n\n\n\n\n\nNamed Entity Recognition - BERT Medium (OntoNotes)\n\n\nnlu.load(&quot;en.ner.onto.bert.small_l8_512&quot;).predict(&quot;&quot;&quot;William Henry Gates III (born October 28, 1955) is an American business magnate,\n software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,\n  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,\n   while also being the largest individual shareholder until May 2014.\n    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;\n     it went on to become the world&#039;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.\n     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time\n      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.\n He gradually transferred his duties to Ray Ozzie and Craig Mundie.\n  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&quot;&quot;&quot;,output_level = &quot;document&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_classes\n\n\n\n\n\n\n\n\n[0.916700005531311, 0.5873000025749207, 0.8816...]\n\n\n['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico']\n\n\n['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'PERSON', 'DATE', 'GPE', 'GPE']\n\n\n\n\n\n\nNamed Entity Recognition - BERT Base (OntoNotes)\n\n\nnlu.load(&quot;en.ner.onto.bert.cased_base&quot;).predict(&quot;&quot;&quot;William Henry Gates III (born October 28, 1955) is an American business magnate,\n software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,\n  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,\n   while also being the largest individual shareholder until May 2014.\n    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;\n     it went on to become the world&#039;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.\n     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time\n      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.\n He gradually transferred his duties to Ray Ozzie and Craig Mundie.\n  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&quot;&quot;&quot;,output_level = &quot;document&quot;)\n\n\n\noutput :\n\n\n\n\n\n\n\n\nner_confidence\n\n\nentities\n\n\nEntities_classes\n\n\n\n\n\n\n\n\n[0.504800021648407, 0.47290000319480896, 0.462...]\n\n\n['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico']\n\n\n['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE']\n\n\n\n\n\n\nNLU Installation\n\n\n# PyPi\n!pip install nlu pyspark==2.4.7\n#Conda\n# Install NLU from Anaconda/Conda\nconda install -c johnsnowlabs nlu\n\n\n\nAdditional NLU ressources\n\n\n\n\nNLU Website\n\n\nAll NLU Tutorial Notebooks\n\n\nNLU Videos and Blogposts on NLU\n\n\nNLU on Github",
"date": "2021-02-25"
},
{
"vote": 3,
"title": "Does anyone know where I could get some tagged pos/neg/neutral restaurant reviews?",
"text": "I want to create a sentiment analysis for a class using NLTK but not on movie reviews or Twitter data which seem to be the two most popular. Does anyone know where I could find a large set of sentiment tagged restaurant reviews? Thank you",
"date": "2021-02-24"
},
{
"vote": 2,
"title": "How do cross-lingual encoders work?",
"text": "Hello! I have a small question about cross lingual encoders (after having read numerous papers and going through so much code, I am still a little lost about something pretty basic lol)\n\n\nOnce we train cross lingual word embeddings (using a mapping method like \nVecMap\n), we get two resultant word embeddings: source_mapped and target_mapped. So when we use a \"cross-lingual\" encoder and we just copy the source_mapped embedding parameters into the encoder, how are we utilising the cross-lingual signal exactly? I would understand if the initial encoder mapping is done like source_mapped->target_mapped->decoder hidden state->backward pass->....-> trained model, but that doesn't seem to be the case. So if someone has experience with cross-lingual encoders, could you explain how exactly it would work/be implement after getting the source and target embeddings mapped (similarly, how it would work if they were trained in a joint fashion)? Thank you so much!!",
"date": "2021-02-24"
},
{
"vote": 29,
"title": "Recent Advances in Language Model Fine-tuning",
"text": null,
"date": "2021-02-24"
},
{
"vote": 0,
"title": "Have you ever had the opportunity to be in Europe and learn Slovak? ðŸ˜„",
"text": null,
"date": "2021-02-23"
},
{
"vote": 6,
"title": "Should you standardize/normalize embeddings when using them with a classifier?",
"text": "Standardizing/normalizing is known to help learning when fitting a model using gradient descent.\n\n\nWhen using a mix of embeddings and non-text features, how should you approach standardization/normalization? \n\n\na) Standardize/normalize everything per usual.\n\n\nb) Standardize/normalize non-text features, but leave the word embeddings alone.\n\n\nc) Don't standardize/normalize anything.\n\n\nSo far I'm leaning towards option c. My reasoning:\n\n\n- For embeddings extracted from transformers-based models, there's no sensible notion of mean and standard deviation (in the case of standardization) or maximum or minimum (in the case of normalization).\n\n\n- For a fixed vocabulary of word vectors (such as from word2vec), gensim supports norming all the vectors, but I worry that this procedure would lose useful information contained in the embeddings.\n\n\n- If you standardize/normalize non-text features only, the model would likely update weights associated with the word embeddings faster due to them likely being on a larger scale, which would hurt learning.",
"date": "2021-02-23"
},
{
"vote": 1,
"title": "A unique self-assessment algorithm App that simulates changes taking place in personality over time based",
"text": "https://youtu.be/IeACb0nqK24",
"date": "2021-02-23"
},
{
"vote": 12,
"title": "Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough)",
"text": null,
"date": "2021-02-22"
},
{
"vote": 3,
"title": "Looking for a code base to implement multi-task learning in NLP",
"text": "I am looking for library or code base to implement MTL. Any ideas on this \n\n\nThings on my checklist.\n\n\n\n\nShould be able to mix different datasets.\n\n\nAble to accumulate gradients for separate task and propagate.\n\n\nAbility to define custom layers on pretrained models.\n\n\nSingle head or specific heads for MTL.\n\n\nImplement my custom loss",
"date": "2021-02-22"
},
{
"vote": 16,
"title": "How to use Instance-based Learning to improve the INTERPRETABILITY of NER models | Research Papers Summary 009",
"text": null,
"date": "2021-02-21"
},
{
"vote": 1,
"title": "How Natural Language Processing Will Change The World",
"text": "[deleted]",
"date": "2021-02-21"
},
{
"vote": 0,
"title": "Clustering software?",
"text": "[deleted]",
"date": "2021-02-21"
},
{
"vote": 5,
"title": "Future advancements and unsolved problems",
"text": "I am curious to hear about your opinions regarding future advancements and possible solutions to the issues that exist now in the field of language technology.\n\n\nWhat do you see as the main issues and do you think something radically has to change in the way we process language data in order to solve those issues? \n\n\nDo you think that new paradigms will emerge in the years to come? I guess that's impossible to predict but do you see a tendency towards new ways of processing languages?\n\n\nWhat are you excited about the most?",
"date": "2021-02-21"
},
{
"vote": 1,
"title": "How do you download data from the Linguistic Data Consortium?",
"text": "Hi. I'm currently trying to download the TACRED dataset. According the Stanford NLP's website, you can download it for free from LDC if you're an LDC member or pay $25.\n\n\nAt the TACRED page on LDC (\nhttps://catalog.ldc.upenn.edu/LDC2018T24\n) I see at the bottom \"Available Media\" and nothing under \"View Fees.\" I don't see any way to download the data though.\n\n\nOn my account page it says that the administrator for my school hasn't approved my account yet, but would this matter? I'm still an LDC member even if I'm not officially verified to be a member of my school, right?\n\n\nAny tips are appreciated. Thanks.",
"date": "2021-02-21"
},
{
"vote": 2,
"title": "Does anybody know of relation extraction datasets that are at the document level?",
"text": "Hey guys. Working on a document-level relation extraction (DocRE) task and I'm wondering what kind of datasets there may be.\n\n\nIn the general domain I've only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I'm wondering what else may be out there that I've missed.\n\n\nThanks!",
"date": "2021-02-21"
},
{
"vote": 4,
"title": "Do you think OpenAI's GPT3 is good enough to pass the Turing Test? / The world's largest scale Turing Test",
"text": null,
"date": "2021-02-19"
},
{
"vote": 12,
"title": "Finding good NLP developers?",
"text": "Hi everyone, any idea as to where I can find a good NLP developer? I tried Upwork but it seems like there are more AI generalists than NLP specialists.\n\n\nSpecifically, \n\n\nI need a AI/NLP Developer with AWS Lambda experience to train and deploy GPT-2 model. \n\n\nEnd goal to make a chatbot that will be used as a â€˜friendâ€™ for lonely people (see \nwww.replika.ai\n [advanced] and chatty-app \nhttps://apps.apple.com/ca/app/chatty-your-robot-friend/id1020581003\n [less advanced]). The chatty-app is based on GPT-2 architecture and is what the first iteration should be similar to.\n\n\nThe first iteration of this project will be a multi user web app that will communicate back and forth with a machine learning API (accessible via high performance gPRC protobufs) running on AWS lambda endpoint. The final product should be maintained 100% by amazon so I wonâ€™t need a system engineer/admin.\n\n\nThe GPT-2 model itself will need to be fine tuned on an empathetic dialogue corpus (which I will provide). To keep server costs low, the training should be done via GPU but once trained, the model should use CPU. I am open to any suggestions that will help keep on-going operational server costs lows (for ex. Imposing a slight delay 1-2 seconds between user questions and chatbot response). Ideally the architecture should allow for:\n\n\n\n\nmulti-user support;\n\n\nup/down voting of responses so that the model can adapt to each user (via beam search [ex. You could create a tensor of weights assigned to multiple outputs from each question, have the model generate multiple output sequences via beam search, and then promote or penalize the weight associated with that response based on the upvote/downvote]); and\n\n\nallow for transfer learning so that the model output can be biased towards end userâ€™s topical and stylistic preferences via Q&A script (or another method).\n\n\n\n\nAny suggestions regarding the overall app and NLP architecture are welcome.",
"date": "2021-02-19"
},
{
"vote": 15,
"title": "Some questions about Spacy vs Hugging face transformers, fine-tuning and wav2vec.",
"text": "I am new to the NLP game and exploring the available options. I have stumbled across both Spacy and Hugging Face Transformers as python packages that seem applicable to my use cases. However, I am having a surprisingly hard time differentiation between the two packages. I would like to hear your input on the differences between Spacy and Hugging Face and perhaps some use cases in which you would prefer on over the other.\n\n\nA second question relates to the fine-tuning of the models. It is my understanding that both Spacy and Hugging Face typically require fine-tuning before reasonable accuracy can be expected on domain-specific use cases. Could anyone give an estimate of the number of labeled text files one should expect to need for fine-tuning a model? Again, I am having a hard time finding an estimate for these numbers as most blogs use pre-existing datasets with large amounts of data.\n\n\nFinally, a third question relates to the Wav2Vec 2 model, which can transcribe audio into text. It is my understanding that this model was trained on multiple languages. However, on \nhuggingface.co/models\n, I am only finding english models at the moment. Is there some way in which I could use Wav2Vec (preferably with the hugging face package) to transcribe for example French texts?\n\n\nI would very much appreciate it if you could share your expertise and help me to navigate the woods here.",
"date": "2021-02-19"
},
{
"vote": 0,
"title": "Suppressing false alarms with capturing information from unstructured corpus.",
"text": "[deleted]",
"date": "2021-02-19"
},
{
"vote": 9,
"title": "GPT-3, Esq? Evaluating AI Legal Summaries",
"text": "Hi all,\n\n\nAs an attorney & ML enthusiast alarmed by the prospect of losing my profession to AI, I ran a few experiments of legal summaries using GPT-3 that I thought the community might find interesting. My comments follow. I would love to hear your thoughts, particularly anyone who has had success using GPT-3 for high-accuracy summarization.\n\n\nhttp://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3_esq_-_evaluating_ai_legal_summaries.pdf",
"date": "2021-02-18"
},
{
"vote": 1,
"title": "Split audio based on text tokenization",
"text": "I have a long audio message and the text. I want to split the sentence in the sentence bases on full stop and also split the relevant audio path. Can anyone recommend the relevant source",
"date": "2021-02-18"
},
{
"vote": 42,
"title": "NLP Infrastructure with Docker Swarm, Docker Compose, and Traefik",
"text": "Hey, I know this is more of a devops thing, but as more and more people are asking questions about how to deploy their NLP models to production and which kind of infrastructure they should set up, I thought I would share 2 articles I wrote about that recently:\n\n\nContainer orchestration with Docker Swarm: \nhttps://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/\n\n\nRouting requests to the right NLP model with Traefik: \nhttps://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/\n\n\nI'm basically talking about how we're doing things behind the hood at \nNLP Cloud\n, where each spaCy NLP model is running inside its own container.\n\n\nI hope some of you will find these posts useful.",
"date": "2021-02-18"
},
{
"vote": 6,
"title": "Best OCR converter for magazine images?",
"text": "My grandma has a Canadian pen pal but she doesnâ€™t speak English so I translate for her. She recently received magazines from her pen pal and asked me if I could translate them for her. She really enjoys the images and the overall magazine style so I wanted to keep that format. I figured converting it would do the trick but all online converters just scramble and mess up the resulting word document. Here is a sample of the scanned pdf file of the magazine \nhere\n\nThanks for any help you can give!\n\n\nP.S: I would really prefer to have a free converter since I donâ€™t normally use them for such a hard task.",
"date": "2021-02-18"
},
{
"vote": 1,
"title": "Learn languages in just 10 minutes a day!",
"text": "[removed]",
"date": "2021-02-18"
},
{
"vote": 5,
"title": "Information Retrieval and Event Prediction from Unstructured Document Corpus",
"text": "[deleted]",
"date": "2021-02-18"
},
{
"vote": 1,
"title": "Dehyphenation",
"text": "Hi,\n\n\nAre there any available libraries for the task of word dehyphenation (removing unwanted hyphens within words, when they are a result of document hyphenation)? This occurs especially when converting PDFs to TXTs.\n\n\nThank you!\n\n\n&#x200B;\n\n\nEDIT: I'm not working with PDFs; i just have plain text which is already in the hyphenated format (without newline or space-newline after the hyphen). Therefore words like \"bag-of-words\" and \"fu-ture\" are not differentiated in any way. I've now just tried a vocabulary-based approach, which is basically: given a hyphenated word in the \"x-y\" form, if it is more frequent in the corpus in its \"xy\" form, dehyphenate to \"xy\". Seems to work. Any other suggestions?",
"date": "2021-02-17"
},
{
"vote": 1,
"title": "How we automate the process to analyze an influencer's personality in order to fit with my brand using NLP ? [Discussion]",
"text": "[removed]",
"date": "2021-02-15"
},
{
"vote": 4,
"title": "How to source support for a project",
"text": "Hi all, I am looking to source some support to do some text summarization and sentiment analysis from some transcripts of interviews. \n\n\nI have a pretty small budget but I was wondering if this community could guide me to somewhere or someone who could take this on?",
"date": "2021-02-15"
},
{
"vote": 0,
"title": "Discovering topics in NEW document using Mallet",
"text": "I've been playing around with Mallet and I know how to discover the per-document topic distributions, per-topic word distributions, and how to find similar documents within an existing corpus.\n\n\n&#x200B;\n\n\nBut what I'm not clear on and can't find anywhere is how to deal with NEW documents without running the entire topic model again.\n\n\n&#x200B;\n\n\nSo given a new document, discovering what topics are within in and also similar documents within the base corpus.\n\n\n&#x200B;\n\n\nCan anyone offer some light on this?",
"date": "2021-02-14"
},
{
"vote": 12,
"title": "How we have been evaluating Knowledge Graph Completion models INACCURATELY | Research Papers Summary 008",
"text": null,
"date": "2021-02-14"
},
{
"vote": 1,
"title": "How to train large models on a normal laptop?",
"text": "Hi, so if you've seen a previous post of mine, I mentioned a class project where we were designing a new model. The problem here is, that our advisory board wants us to show a comparison between the results of this model and existing models in the field.\n\n\nThe Transformer and the Reformer are two such models, among others. The problem is that I assume it would be impossible to train them for an extended period of time, on a dataset on the scale of enwik9 or so. My laptop specs are 8GB RAM with 256 GB SSD and I'm not really sure about the GPU but I think there is some sort of NVidia GeForce.\n\n\nColab crashed the last time I tried to train the model. And we can't really afford to shell out a lot for cloud instances, being undergrads on a budget.\n\n\nDoes anybody have any suggestions about what to do?",
"date": "2021-02-14"
},
{
"vote": 1,
"title": "Happy Valentine's, everyone! This year, I used ML to solve me being single by programming an autocomplete program using Markov Chains and RNNs trained on romantic movies! The results were REALLY unexpected so I turned that whole experience into a video. I hope y'all like it!",
"text": "[deleted]",
"date": "2021-02-14"
},
{
"vote": 4,
"title": "MS in CL at Montclair?",
"text": "Has anyone here done the MS in Computational linguistics at Montclair? Or is currently in the program? Iâ€™m considering applying but I would like to know if anyone would recommend it/what they think about it. \nThanks!",
"date": "2021-02-13"
},
{
"vote": 2,
"title": "Any standard textbooks on Bangla Natural Language Processing or other Indian languages? --Where data preprocessing is nicely explained.",
"text": null,
"date": "2021-02-13"
},
{
"vote": 1,
"title": "Neural Query Expansion for Code Search",
"text": "Searching repositories of existing source code for code snippets is a key task in software engineering. Earlier techniques like Neural Code Search(NCS), takes in a natural language query and outputs relevant code snippets, often suffers incase of short queries or query that have vague intent. Researchers propose NQE for expanding search queries to improve results. ðŸ¤ \n\n\nBlog- \nhttps://link.medium.com/uFDypem4Pdb",
"date": "2021-02-13"
},
{
"vote": 19,
"title": "New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5, and more in 1 line of Python code with NLU 1.1.1!",
"text": "John Snow Labs NLU 1.1.1 : New multilingual models, Spark 2.3 support, new tutorials and more!\n\n\nNLU 1.1.1 Release Notes\n\n\nWe are very excited to release NLU 1.1.1!\nThis release features 3 new tutorial notebooks for Open/Closed book question answering with Google's T5, Intent classification, and Aspect Based NER.\nIn Addition, NLU 1.1.0 comes with  25+ pre-trained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages from the \namazing Spark2.7.2 release\n\nFinally, NLU now supports running on Spark 2.3 clusters.\n\n\nNLU 1.1.0 New Non-English Models\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nArabic\n\n\nar.ner\n\n\narabic_w2v_cc_300d\n\n\nNamed Entity Recognizer\n\n\n\n\n\n\nArabic\n\n\nar.embed.aner\n\n\naner_cc_300d\n\n\nWord Embedding\n\n\n\n\n\n\nArabic\n\n\nar.embed.aner.300d\n\n\naner_cc_300d\n\n\nWord Embedding (Alias)\n\n\n\n\n\n\nBengali\n\n\nbn.stopwords\n\n\nstopwords_bn\n\n\nStopwords Cleaner\n\n\n\n\n\n\nBengali\n\n\nbn.pos\n\n\npos_msri\n\n\nPart of Speech\n\n\n\n\n\n\nThai\n\n\nth.segment_words\n\n\nwordseg_best\n\n\nWord Segmenter\n\n\n\n\n\n\nThai\n\n\nth.pos\n\n\npos_lst20\n\n\nPart of Speech\n\n\n\n\n\n\nThai\n\n\nth.sentiment\n\n\nsentiment_jager_use\n\n\nSentiment Classifier\n\n\n\n\n\n\nThai\n\n\nth.classify.sentiment\n\n\nsentiment_jager_use\n\n\nSentiment Classifier (Alias)\n\n\n\n\n\n\nChinese\n\n\nzh.pos.ud_gsd_trad\n\n\npos_ud_gsd_trad\n\n\nPart of Speech\n\n\n\n\n\n\nChinese\n\n\nzh.segment_words.gsd\n\n\nwordseg_gsd_ud_trad\n\n\nWord Segmenter\n\n\n\n\n\n\nBihari\n\n\nbh.pos\n\n\npos_ud_bhtb\n\n\nPart of Speech\n\n\n\n\n\n\nAmharic\n\n\nam.pos\n\n\npos_ud_att\n\n\nPart of Speech\n\n\n\n\n\n\nNLU 1.1.1 New English Models and Pipelines\n\n\n\n\n\n\n\n\nLanguage\n\n\nnlu.load() reference\n\n\nSpark NLP Model reference\n\n\nType\n\n\n\n\n\n\n\n\nEnglish\n\n\nen.sentiment.glove\n\n\nanalyze_sentimentdl_glove_imdb\n\n\nSentiment Classifier\n\n\n\n\n\n\nEnglish\n\n\nen.sentiment.glove.imdb\n\n\nanalyze_sentimentdl_glove_imdb\n\n\nSentiment Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.classify.sentiment.glove.imdb\n\n\nanalyze_sentimentdl_glove_imdb\n\n\nSentiment Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.classify.sentiment.glove\n\n\nanalyze_sentimentdl_glove_imdb\n\n\nSentiment Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.classify.trec50.pipe\n\n\nclassifierdl_use_trec50_pipeline\n\n\nLanguage Classifier\n\n\n\n\n\n\nEnglish\n\n\nen.ner.onto.large\n\n\nonto_recognize_entities_electra_large\n\n\nNamed Entity Recognizer\n\n\n\n\n\n\nEnglish\n\n\nen.classify.questions.atis\n\n\nclassifierdl_use_atis\n\n\nIntent Classifier\n\n\n\n\n\n\nEnglish\n\n\nen.classify.questions.airline\n\n\nclassifierdl_use_atis\n\n\nIntent Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.classify.intent.atis\n\n\nclassifierdl_use_atis\n\n\nIntent Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.classify.intent.airline\n\n\nclassifierdl_use_atis\n\n\nIntent Classifier (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.ner.atis\n\n\nnerdl_atis_840b_300d\n\n\nAspect based NER\n\n\n\n\n\n\nEnglish\n\n\nen.ner.airline\n\n\nnerdl_atis_840b_300d\n\n\nAspect based NER (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.ner.aspect.airline\n\n\nnerdl_atis_840b_300d\n\n\nAspect based NER (Alias)\n\n\n\n\n\n\nEnglish\n\n\nen.ner.aspect.atis\n\n\nnerdl_atis_840b_300d\n\n\nAspect based NER (Alias)\n\n\n\n\n\n\nNew Easy NLU 1-liner Examples :\n\n\nExtract aspects and entities from airline questions (ATIS dataset)\n\n\n    \nnlu.load(&quot;en.ner.atis&quot;).predict(&quot;i want to fly from baltimore to dallas round trip&quot;)\noutput:  [&quot;baltimore&quot;,&quot; dallas&quot;, &quot;round trip&quot;]\n\n\n\nIntent Classification for Airline Traffic Information System queries (ATIS dataset)\n\n\n\nnlu.load(&quot;en.classify.questions.atis&quot;).predict(&quot;what is the price of flight from newyork to washington&quot;)\noutput:  &quot;atis_airfare&quot;\t\n\n\n\nRecognize Entities OntoNotes - ELECTRA Large\n\n\n\nnlu.load(&quot;en.ner.onto.large&quot;).predict(&quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London.&quot;)\t\noutput:  [&quot;Johnson&quot;, &quot;first&quot;, &quot;2001&quot;, &quot;eight years&quot;, &quot;London&quot;]\t\n\n\n\nQuestion classification of open-domain and fact-based questions Pipeline - TREC50\n\n\nnlu.load(&quot;en.classify.trec50.pipe&quot;).predict(&quot;When did the construction of stone circles begin in the UK? &quot;)\noutput:  LOC_other\n\n\n\nTraditional Chinese Word Segmentation\n\n\n# &#039;However, this treatment also creates some problems&#039; in Chinese\nnlu.load(&quot;zh.segment_words.gsd&quot;).predict(&quot;ç„¶è€Œï¼Œé€™æ¨£çš„è™•ç†ä¹Ÿè¡ç”Ÿäº†ä¸€äº›å•é¡Œã€‚&quot;)\noutput:  [&quot;ç„¶è€Œ&quot;,&quot;,&quot;,&quot;é€™æ¨£&quot;,&quot;çš„&quot;,&quot;è™•ç†&quot;,&quot;ä¹Ÿ&quot;,&quot;è¡ç”Ÿ&quot;,&quot;äº†&quot;,&quot;ä¸€äº›&quot;,&quot;å•é¡Œ&quot;,&quot;ã€‚&quot;]\n\n\n\nPart of Speech for Traditional Chinese\n\n\n# &#039;However, this treatment also creates some problems&#039; in Chinese\nnlu.load(&quot;zh.pos.ud_gsd_trad&quot;).predict(&quot;ç„¶è€Œï¼Œé€™æ¨£çš„è™•ç†ä¹Ÿè¡ç”Ÿäº†ä¸€äº›å•é¡Œã€‚&quot;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\nToken\n\n\nPOS\n\n\n\n\n\n\n\n\nç„¶è€Œ\n\n\nADV\n\n\n\n\n\n\nï¼Œ\n\n\nPUNCT\n\n\n\n\n\n\né€™æ¨£\n\n\nPRON\n\n\n\n\n\n\nçš„\n\n\nPART\n\n\n\n\n\n\nè™•ç†\n\n\nNOUN\n\n\n\n\n\n\nä¹Ÿ\n\n\nADV\n\n\n\n\n\n\nè¡ç”Ÿ\n\n\nVERB\n\n\n\n\n\n\näº†\n\n\nPART\n\n\n\n\n\n\nä¸€äº›\n\n\nADJ\n\n\n\n\n\n\nå•é¡Œ\n\n\nNOUN\n\n\n\n\n\n\nã€‚\n\n\nPUNCT\n\n\n\n\n\n\nThai Word Segment Recognition\n\n\n# &#039;Mona Lisa is a 16th-century oil painting created by Leonardo held at the Louvre in Paris&#039; in Thai\nnlu.loadnlu.load(&quot;th.segment_words&quot;).predict(&quot;Mona Lisa à¹€à¸›à¹‡à¸™à¸ à¸²à¸žà¸§à¸²à¸”à¸ªà¸µà¸™à¹‰à¸³à¸¡à¸±à¸™à¹ƒà¸™à¸¨à¸•à¸§à¸£à¸£à¸©à¸—à¸µà¹ˆ 16 à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸”à¸¢ Leonardo à¸ˆà¸±à¸”à¸‚à¸¶à¹‰à¸™à¸—à¸µà¹ˆà¸žà¸´à¸žà¸´à¸˜à¸ à¸±à¸“à¸‘à¹Œà¸¥à¸¹à¸Ÿà¸£à¹Œà¹ƒà¸™à¸›à¸²à¸£à¸µà¸ª&quot;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\ntoken\n\n\n\n\n\n\n\n\nM\n\n\n\n\n\n\no\n\n\n\n\n\n\nn\n\n\n\n\n\n\na\n\n\n\n\n\n\nLisa\n\n\n\n\n\n\nà¹€à¸›à¹‡à¸™\n\n\n\n\n\n\nà¸ à¸²à¸ž\n\n\n\n\n\n\nà¸§\n\n\n\n\n\n\nà¸²\n\n\n\n\n\n\nà¸”\n\n\n\n\n\n\nà¸ªà¸µà¸™à¹‰à¸³\n\n\n\n\n\n\nà¸¡à¸±à¸™\n\n\n\n\n\n\nà¹ƒà¸™\n\n\n\n\n\n\nà¸¨à¸•à¸§à¸£à¸£à¸©\n\n\n\n\n\n\nà¸—à¸µà¹ˆ\n\n\n\n\n\n\n16\n\n\n\n\n\n\nà¸—à¸µà¹ˆ\n\n\n\n\n\n\nà¸ªà¸£à¹‰à¸²à¸‡\n\n\n\n\n\n\nà¹‚\n\n\n\n\n\n\nà¸”\n\n\n\n\n\n\nà¸¢\n\n\n\n\n\n\nL\n\n\n\n\n\n\ne\n\n\n\n\n\n\no\n\n\n\n\n\n\nn\n\n\n\n\n\n\na\n\n\n\n\n\n\nr\n\n\n\n\n\n\nd\n\n\n\n\n\n\no\n\n\n\n\n\n\nà¸ˆà¸±à¸”\n\n\n\n\n\n\nà¸‚à¸¶à¹‰à¸™\n\n\n\n\n\n\nà¸—à¸µà¹ˆ\n\n\n\n\n\n\nà¸žà¸´à¸žà¸´à¸˜à¸ à¸±à¸“à¸‘à¹Œ\n\n\n\n\n\n\nà¸¥à¸¹à¸Ÿà¸£à¹Œ\n\n\n\n\n\n\nà¹ƒà¸™\n\n\n\n\n\n\nà¸›à¸²à¸£à¸µà¸ª\n\n\n\n\n\n\nPart of Speech for Bengali (POS)\n\n\n# &#039;The village is also called &#039;Mod&#039; in Tora language&#039; in Bengali \nnlu.load(&quot;bn.pos&quot;).predict(&quot;à¦¬à¦¾à¦¸à¦¸à§à¦¥à¦¾à¦¨-à¦˜à¦°à¦—à§ƒà¦¹à¦¸à§à¦¥à¦¾à¦²à¦¿ à¦¤à§‹à§œà¦¾ à¦­à¦¾à¦·à¦¾à§Ÿ à¦—à§à¦°à¦¾à¦®à¦•à§‡à¦“ à¦¬à¦²à§‡ ` à¦®à§‹à¦¦ &#039; à§·&quot;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\ntoken\n\n\npos\n\n\n\n\n\n\n\n\nà¦¬à¦¾à¦¸à¦¸à§à¦¥à¦¾à¦¨-à¦˜à¦°à¦—à§ƒà¦¹à¦¸à§à¦¥à¦¾à¦²à¦¿\n\n\nNN\n\n\n\n\n\n\nà¦¤à§‹à§œà¦¾\n\n\nNNP\n\n\n\n\n\n\nà¦­à¦¾à¦·à¦¾à§Ÿ\n\n\nNN\n\n\n\n\n\n\nà¦—à§à¦°à¦¾à¦®à¦•à§‡à¦“\n\n\nNN\n\n\n\n\n\n\nà¦¬à¦²à§‡\n\n\nVM\n\n\n\n\n\n\n`\n\n\nSYM\n\n\n\n\n\n\nà¦®à§‹à¦¦\n\n\nNN\n\n\n\n\n\n\n'\n\n\nSYM\n\n\n\n\n\n\nà§·\n\n\nSYM\n\n\n\n\n\n\nStop Words Cleaner for Bengali\n\n\n# &#039;This language is not enough&#039; in Bengali \ndf = nlu.load(&quot;bn.stopwords&quot;).predict(&quot;à¦à¦‡ à¦­à¦¾à¦·à¦¾ à¦¯à¦¥à§‡à¦·à§à¦Ÿ à¦¨à¦¯à¦¼&quot;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\ncleanTokens\n\n\ntoken\n\n\n\n\n\n\n\n\nà¦­à¦¾à¦·à¦¾\n\n\nà¦à¦‡\n\n\n\n\n\n\nà¦¯à¦¥à§‡à¦·à§à¦Ÿ\n\n\nà¦­à¦¾à¦·à¦¾\n\n\n\n\n\n\nà¦¨à¦¯à¦¼\n\n\nà¦¯à¦¥à§‡à¦·à§à¦Ÿ\n\n\n\n\n\n\nNone\n\n\nà¦¨à¦¯à¦¼\n\n\n\n\n\n\nPart of Speech for Bengali\n\n\n\n# &#039;The people of Ohu know that the foundation of Bhojpuri was shaken&#039; in Bengali\nnlu.load(&#039;bh.pos&#039;).predict(&quot;à¤“à¤¹à¥ à¤²à¥‹à¤— à¤•à¥‡ à¤®à¤¾à¤²à¥‚à¤® à¤¬à¤¾ à¤•à¤¿ à¤¶à¥à¤²à¥€à¤² à¤¹à¥‹à¤–à¤¤à¥‡ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤•à¥‡ à¤¨à¥€à¤‚à¤µ à¤¹à¤¿à¤² à¤œà¤¾à¤ˆ&quot;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\npos\n\n\ntoken\n\n\n\n\n\n\n\n\nDET\n\n\nà¤“à¤¹à¥\n\n\n\n\n\n\nNOUN\n\n\nà¤²à¥‹à¤—\n\n\n\n\n\n\nADP\n\n\nà¤•à¥‡\n\n\n\n\n\n\nNOUN\n\n\nà¤®à¤¾à¤²à¥‚à¤®\n\n\n\n\n\n\nVERB\n\n\nà¤¬à¤¾\n\n\n\n\n\n\nSCONJ\n\n\nà¤•à¤¿\n\n\n\n\n\n\nADJ\n\n\nà¤¶à¥à¤²à¥€à¤²\n\n\n\n\n\n\nVERB\n\n\nà¤¹à¥‹à¤–à¤¤à¥‡\n\n\n\n\n\n\nPROPN\n\n\nà¤­à¥‹à¤œà¤ªà¥à¤°à¥€\n\n\n\n\n\n\nADP\n\n\nà¤•à¥‡\n\n\n\n\n\n\nNOUN\n\n\nà¤¨à¥€à¤‚à¤µ\n\n\n\n\n\n\nVERB\n\n\nà¤¹à¤¿à¤²\n\n\n\n\n\n\nAUX\n\n\nà¤œà¤¾à¤ˆ\n\n\n\n\n\n\nAmharic Part of Speech (POS)\n\n\n# &#039; &quot;Son, finish the job,&quot; he said.&#039; in Amharic\nnlu.load(&#039;am.pos&#039;).predict(&#039;áˆáŒ… áŠ¡ áŠ• áˆ¥áˆ« á‹ áŠ• áŠ áˆµáŒ¨áˆ­áˆµ áŠ§á‹ áŠ£áˆ áŠ§áˆ á¢&quot;&#039;)\n\n\n\nOutput:\n\n\n\n\n\n\n\n\npos\n\n\ntoken\n\n\n\n\n\n\n\n\nNOUN\n\n\náˆáŒ…\n\n\n\n\n\n\nDET\n\n\náŠ¡\n\n\n\n\n\n\nPART\n\n\náŠ•\n\n\n\n\n\n\nNOUN\n\n\náˆ¥áˆ«\n\n\n\n\n\n\nDET\n\n\ná‹\n\n\n\n\n\n\nPART\n\n\náŠ•\n\n\n\n\n\n\nVERB\n\n\náŠ áˆµáŒ¨áˆ­áˆµ\n\n\n\n\n\n\nPRON\n\n\náŠ§á‹\n\n\n\n\n\n\nAUX\n\n\náŠ£áˆ\n\n\n\n\n\n\nPRON\n\n\náŠ§áˆ\n\n\n\n\n\n\nPUNCT\n\n\ná¢\n\n\n\n\n\n\nNOUN\n\n\n\"\n\n\n\n\n\n\nThai Sentiment Classification\n\n\n#  &#039;I love peanut butter and jelly!&#039; in thai\nnlu.load(&#039;th.classify.sentiment&#039;).predict(&#039;à¸‰à¸±à¸™à¸Šà¸­à¸šà¹€à¸™à¸¢à¸–à¸±à¹ˆà¸§à¹à¸¥à¸°à¹€à¸¢à¸¥à¸¥à¸µà¹ˆ!&#039;)[[&#039;sentiment&#039;,&#039;sentiment_confidence&#039;]]\n\n\n\nOutput:\n\n\n\n\n\n\n\n\nsentiment\n\n\nsentiment_confidence\n\n\n\n\n\n\n\n\npositive\n\n\n0.999998\n\n\n\n\n\n\nArabic Named Entity Recognition (NER)\n\n\n# &#039;In 1918, the forces of the Arab Revolt liberated Damascus with the help of the British&#039; in Arabic\nnlu.load(&#039;ar.ner&#039;).predict(&#039;ÙÙŠ Ø¹Ø§Ù… 1918 Ø­Ø±Ø±Øª Ù‚ÙˆØ§Øª Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¯Ù…Ø´Ù‚ Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù† Ø§Ù„Ø¥Ù†ÙƒÙ„ÙŠØ²&#039;,output_level=&#039;chunk&#039;)[[&#039;entities_confidence&#039;,&#039;ner_confidence&#039;,&#039;entities&#039;]]\n\n\n\nOutput:\n\n\n\n\n\n\n\n\nentity_class\n\n\nner_confidence\n\n\nentities\n\n\n\n\n\n\n\n\nORG\n\n\n[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]\n\n\nÙ‚ÙˆØ§Øª Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n\n\n\n\n\n\nLOC\n\n\n[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]\n\n\nØ¯Ù…Ø´Ù‚\n\n\n\n\n\n\nPER\n\n\n[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]\n\n\nØ§Ù„Ø¥Ù†ÙƒÙ„ÙŠØ²\n\n\n\n\n\n\nNLU 1.1.0 Enhancements :\n\n\n\n\nSpark 2.3 compatibility\n\n\n\n\nNew NLU Notebooks and Tutorials\n\n\n\n\nOpen and Closed book question Ansering\n\n\n[Aspect based NER for Airline ATIS]New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5  and more in 1 line of Python code with NLU 1.1.1!(\nhttps://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/classifiers/intent_classification_airlines_ATIS.ipynb\n)\n\n\nIntent Classification for Airline emssages ATIS\n\n\n\n\nInstallation\n\n\n# PyPi\n!pip install nlu pyspark==2.4.7\n#Conda\n# Install NLU from Anaconda/Conda\nconda install -c johnsnowlabs nlu\n\n\n\nAdditional NLU ressources\n\n\n\n\nNLU Website\n\n\nAll NLU Tutorial Notebooks\n\n\nNLU Videos and Blogposts on NLU\n\n\nNLU on Github",
"date": "2021-02-13"
},
{
"vote": 1,
"title": "How should I go about creating an API to summarize text automatically?",
"text": "I would like to take the abstractive approach. Eager to learn more about this.",
"date": "2021-02-12"
},
{
"vote": 2,
"title": "[N] ICMI 2020 Best Paper | Gesticulator: A framework for semantically-aware speech-driven gesture generation",
"text": null,
"date": "2021-02-12"
},
{
"vote": 2,
"title": "Text-generation frontiers",
"text": "Hello all. Im a data science graduate student and NLP research assistant looking to write my thesis about generative language models. \n\n\nAs I understand it, the current field is largely concerned with large scale pre-trained transformer-based models such as the GPT and BERT projects. \n\n\nIt seems that the scale and computational cost of such models sets very high barriers of entry for researchers aspiring to challenge state-of-the-art or do interesting research in this sub-field. \n\n\nSo, Iâ€™m curious as to what you may consider interesting frontiers within text-generation/generative models. In your opinion, is there anything that would be worth exploring in this subfield, that do not require access to GPT-level resources?\n\n\nSo far I have considered a comparative study of traditional models (word2vec etc) and these pre-trained models on the new Allen Institute \nGENIE testbank\n. However, Iâ€™d also very much like to build my own model somehow. \n\n\nIn relation, are there any other NLP frontiers that you might suggest for a thesis project? \n\n\nThanks for reading and for you time. Any inputs would be appreciated and I would be happy to discuss with further with anyone interested. Naturally, I donâ€™t mind sharing credit either.",
"date": "2021-02-12"
},
{
"vote": 18,
"title": "NLP work in politics/intelligence",
"text": "Hi! I will be starting grad school to get my MS in CS this fall and am planning on specializing in NLP and working as an NLP engineer afterwards. I have always been very interested in politics + government work as well and I imagine there's need for NLP work in intelligence. Does anyone in this sub have any experience in this field? Or knows how much opportunity there is, or anything else worth sharing?\n\n\nAlso a bit more generally, how prevalent are CS/NLP jobs related to the humanities? For example, working in a technical role on investigative work for a news company. So roles working in politics/social/advocacy/news (esp if they require writing and communication alongside the CS). Maybe this is more applicable to leadership positions? But is it plausible that I could get a job in these fields/roles after grad school?",
"date": "2021-02-12"
},
{
"vote": 6,
"title": "Try to improve a DL model with Attention",
"text": "Hello everybody,\n\n\nI've been working with a deep learning model (a parser) for some time to make dependency parsing.\n\n\nIn brief, the model is composed of several embeddings that are passed to a BiLSTM and finally to two classifiers (one for the labels and one for the positions). An idea to improve the model is to integrate a self-attention mechanism to BiLSTM. In particular, the attempts I have made are:\n\n\n\n\nadd self-attention to BiLSTM using this \ngist\n\n\nadd multi head (self) attention using the \nfunction\n provided by pytorch\n\n\n\n\nThe results obtained are not very comforting ... with self-attention I have a very slight improvement in performance, while with multi head (self) attention I have no improvements.\n\n\nBeing self-attention mechanisms, in the multi head the vectors corresponding to Q (query), K (key) and V (value) are the same and correspond to the BiLSTM output. What I did was to vary the number of heads (2, 4, 6, 8, 10, 12: no improvement). \n\n\nThe only other attempt I can think of is to use a \nBatchNormalization\n layer after the BiLSTM and before the attention or after the attention, but honestly I doubt it will do anything ...\n\n\nObviously, adding an attention mechanism does not mean that the model necessarily improves. But I don't know exactly how to justify it. Anyone have any ideas? Or, does anyone have any ideas on how I might try to insert self attention / multi head self attention into BiLSTM?\n\n\nIf you have read this far, thank you very much!",
"date": "2021-02-12"
},
{
"vote": 12,
"title": "Which are some good papers on political science applications of NLP?",
"text": "I'm looking for some papers applying NLP to politics.\n\n\nIt doesn't matter if it is new or old, groundbreaking or not, as long as you enjoyed it, I'd like to know about it!",
"date": "2021-02-12"
},
{
"vote": 10,
"title": "Recommendations for OpenSource NLP Projects to contribute at",
"text": "Hello everyone! I would like to ask you all for recommendations on some interesting NLP projects that are running at the moment and need contributors. I do not have extensive experience, but I'd love a starting point. \nFeel free to advertise your open source project here\n!",
"date": "2021-02-12"
},
{
"vote": 6,
"title": "Are there more practical tools for KNN searches and storing documents/embeddings?",
"text": "I have a semantic searcher that retrieves the most similar documents, and then the similarity for each segment (which are sentences). I have been using FAISS for the KNN search over the documents, then I retrieve the sentences to do a dot product operation and highlight the most similar ones.\n\n\nI also need to keep my data either pickled, in hdf5, or in json, to be able to build the searcher, but after a while, it takes too long to rebuild. I need to rebuild the searcher because I don't have the option to neither update nor delete from a FAISS index, and I don't think I can save the FAISS index either - can't pickle because it yields an error related to using C++ objects.\n\n\nI also have to have the searcher object in memory (plus the embedding model itself, which is unavoidable). If you have tried Whoosh (probably is similar to Elastic_Search), it creates an index file which opened/closed per search/data_manipulation. I can update and delete documents easily, and besides being fast, doesn't have concurrency problems either, I'd love an option like this.\n\n\nEDIT: another issue I have with FAISS (and other KNN I have tried at the time), is that they don't handle IDs, instead, they retrieve positions and I need to keep a mapping to the original data.",
"date": "2021-02-11"
},
{
"vote": 17,
"title": "Need Reviews Of Saarland and Stuttgartâ€™s Masters NLP Program",
"text": "Saarland nlp program = language science and technology\nStuttgartâ€™s nlp program= computational linguistics.\n\n\nI have got admission in stuttgart nlp program and have applied in saarlandâ€™s and will also probably get an admit from there too. So i will be deciding between them for masters.\n\n\nBoth programs â€œlookâ€ great (by module book)  but i need review of real experience of studying there. I guess since saarland is more like a university town hence there will be less work opportunities in the CS or NLP field outside university, compared to stuttgart. But on the other hand saarland is cheaper than stuttgart.\n\n\nAny other thing i should know about before making the decision. Also, note that i think first 1 or 2 semesters will be online because of covid.\n\n\nTIA.\n\n\nEdit: I am unable to find people from stuttgart university in this program (not just on this post, but generally). so if you know someone from this program, can you kindly ask them to help me out here. thanks",
"date": "2021-02-11"
},
{
"vote": 14,
"title": "Researchers from the University of Sheffield &amp; Beihang University Introduce a New Approach Based on Transfer Learning to Automate Historical Text Summarization",
"text": "The researchers at the University of Sheffield, Beihang University, and Open Universityâ€™s Knowledge Media Institute have introduced historical text summarization task, where documents in historical forms of a language are summarised in the corresponding modern language.\n\n\nThe process of text summarization is a fundamentally important routine to historians and digital humanities researchers. Historical text summarization is regarded as a particular case of cross-lingual summarization. However, summarizing and interpreting historical documents can cost a lot of time and effort, even for experts. This is due to the limited historical and modern language corpora and cultural and linguistic variations over time.\n\n\nPaper summary: \nhttps://www.marktechpost.com/2021/02/10/researchers-from-the-university-of-sheffield-beihang-university-introduce-a-new-approach-based-on-transfer-learning-to-automate-historical-text-summarization/\n\n\nPaper: \nhttps://arxiv.org/pdf/2101.10759.pdf\n\n\nGithub: \nhttps://github.com/Pzoom522/HistSumm",
"date": "2021-02-11"
},
{
"vote": 6,
"title": "Large archive of papers on machine translation starting from the beginning of the field",
"text": null,
"date": "2021-02-11"
},
{
"vote": 2,
"title": "I see mean pool and max pool output of transformer models as representations of passages of text, why not concatenate the two?",
"text": "I actually tried this and the representation seems to perform worse than using just one of these. Is there a theoretical reason why this might be so?",
"date": "2021-02-10"
},
{
"vote": 1,
"title": "Top 10 Uses of Python Language in Business | The Daily Show With Erfan |...",
"text": null,
"date": "2021-02-10"
},
{
"vote": 14,
"title": "LibreTranslate - Free and Open Source Machine Translation API",
"text": null,
"date": "2021-02-10"
},
{
"vote": 9,
"title": "Intro to Embodied AI: How to combine NLP, CV, and RL",
"text": null,
"date": "2021-02-09"
},
{
"vote": 2,
"title": "Looking for dataset with empathic language",
"text": "Does anyone know of publicly available data which includes labels for levels of empathy/kindness/compassion in text?\n\n\nI am trying to reproduce old results with new data.",
"date": "2021-02-08"
},
{
"vote": 10,
"title": "Detect hate speech using a Transformer with only a few lines of code.",
"text": null,
"date": "2021-02-08"
},
{
"vote": 2,
"title": "Where should I start?",
"text": "Hi, I'd like to learn NLP by using python3. What should I do to learn? What do I have to know before start? Should I firstly check AI? It's related? Could you share documents or complete course videos. Thanks. ðŸ˜Š",
"date": "2021-02-08"
},
{
"vote": 2,
"title": "Amazon Alexa Team -- Data Linguist Roles",
"text": "Alexa team is hiring \"Data Linguists\", are these literal manual annotation roles? Is Alexa literally hand-tuned based manual decision trees?\n\n\n> Sample JD Description\n\n\n> Amazon is seeking a Data Linguist-II to join our Amazon Devices Team. This role focuses on language data, primarily in the areas of text annotation and general data analysis deliverables.\n\n\n>The Data Linguist-II must have a passion for data, efficiency, accuracy, and should be capable of:\n\n\n>â€¢ Handling unique data analysis requests from a range of data customers\n\n\n>â€¢ Providing data quality expertise to other team members and coaching improvements\n\n\n>â€¢ Delivering high quality work under aggressive deadlines\n\n\n>â€¢ Working autonomously with minimum direction\n\n\n>â€¢ Building a thorough understanding of conventions and providing support to global sites\n\n\n>â€¢ Understanding changes to conventions deployed in response to customersâ€™ requests and modifying workflows accordingly\n\n\n>â€¢ Contributing to process improvements to reduce handling time and improve output\n\n\n>â€¢ Improving software tools by identifying bugs and suggesting enhancements\n\n\n>â€¢ Diving deep into issues and implementing solutions independently\n\n\n>â€¢ Proactively addressing issues and problems\n\n\n>â€¢ Keeping up with changing project conventions and priorities\n\n\n>Basic Qualifications\n\n\n>â€¢ Bachelorâ€™s degree\n\n\n>â€¢ 2+ years of experience working with written language data, including experience with annotation and other forms of data markup\n\n\n>â€¢ 1+ year(s) of experience working with command line interfaces and basic UNIX commands\n\n\n>â€¢ Near-native level fluency in one or more non-English language\n\n\n>â€¢ Business level fluency in English\n\n\n>â€¢ Working knowledge of Microsoft Office products (e.g. Word, Excel, Access, etc.)\n\n\n>Preferred Qualifications\n\n\n>â€¢ Bachelorâ€™s degree in Linguistics or Computational Linguistics\n\n\n>â€¢ Interest in semantics and related areas\n\n\n>â€¢ Comfortable working with text from various languages and dialects\n\n\n>â€¢ Ability to quickly grasp technical concepts and learn in-house data processing tools\n\n\n>â€¢ Strong analytical, problem-solving, and critical-thinking skills\n\n\n>â€¢ Practical knowledge of data processing needs and trade-offs\n\n\n>â€¢ Detail-oriented with excellent communication and strong organizational skills\n\n\n>â€¢ Team player with exceptional interpersonal skills and a solution-oriented attitude\n\n\n>â€¢ Comfortable working in a fast-paced, highly-collaborative, dynamic work environment\n\n\n>â€¢ Willingness to support several projects at one time and to accept reprioritization as necessary",
"date": "2021-02-07"
},
{
"vote": 8,
"title": "Studying linguistic features in emotion analysis with deep learning",
"text": "Hello every one \n\n\nI am new in NLP , and I want to perform emotion analysis especially (worry) emotion. I have collected tweets related to COVID-19  to classify into 2 classes with deep learning. Someone said you have to make a linguistic study related to the Worry concept to use it as linguistic features and get good results. Does anyone know any resources that can help me or any linguistic resources? Thanks",
"date": "2021-02-07"
},
{
"vote": 1,
"title": "Need help with NLU task of getting insights into a real short story",
"text": "Hello, I have experimented with an Natural Language Understanding task on a real short story.  The original text is as follows:\n\n\n&#x200B;\n\n\n>\"The Doctor's Word\", Short Story From \"Maguldi Days\", by R.K. Narayan (1972):\n>\n>People came to him when the patient was on his last legs. Dr Raman often burst out, 'Why couldn't you have come a day earlier?' The reason was obvious - visiting fee twenty-five rupees, and more than that, people liked to shirk the fact that the time had come to call in Dr Raman; for them there was something ominous in the very association. As a result, when the big man came on the scene it was always a quick decision one way or another. There was no scope or time for any kind of wavering or whitewashing. Long years of practice of this kind had bred in the doctor a certain curt truthfulness; for that very reason his opinion was valued; he was not a mere doctor expressing an opinion but a judge pronouncing a verdict. The patient's life hung on his words. This never unduly worried Dr Raman. He never believed that agreeable words ever saved lives. He did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours. However, when he glimpsed the faintest sign of hope, he rolled up his sleeve and stepped into the arena: it might be hours or days, but he never withdrew till he wrested the prize from Yama's hands.\n>\n>Today, standing over a bed, the doctor felt that he himself needed someone to tell him soothing lies. He mopped his brow with his kerchief and sat down in the chair beside the bed. On the bed lay his dearest friend in the world: Gopal. They had known each other for forty years now, starting with their kindergarten days. They could not, of course, meet as much as they wanted, each being wrapped in his own family and profession. Occasionally, on a Sunday, Gopal would walk into the consulting room and wait patiently in a corner till the doctor was free. And then they would dine together, see a picture and talk of each other's life and activities. It was a classic friendship, which endured untouched by changing times, circumstances and activities.\n>\n>In his busy round of work, Dr Raman had not noticed that Gopal had not called in for over three months now. He only remembered it when he saw Gopal's son sitting on a bench in the consulting hall one crowded morning.Â  Dr Raman could not talk to him for over an hour. When he got up and was about to pass on to the operating room, he called up the young man and asked, 'What brings you here, sir?' The youth was nervous and shy. 'Mother sent me here.'\n>\n>'What can I do for you?'\n>\n>'Father is ill...'\n>\n>It was an operation day and he was not free till three in the afternoon. He rushed off straight from the clinic to his friend's house, in Lawley Extension.\n>\n>Gopal lay in bed as if in sleep. The doctor stood over him and asked Gopal's wife, 'How long has he been in bed?'\n>\n>'A month and a half, Doctor.'\n>\n>'Who is attending him?'\n>\n>'A doctor in the next street. He comes down once in three days and gives him medicine.'\n>\n>'What is his name?' He had never heard of him. 'Someone I don't know, but I wish he had had the goodness to tell me about it. Why, why couldn't you have sent me word earlier?'\n>\n>'We thought you would be busy and did not wish to trouble you unnecessarily.' They were apologetic and miserable. There was hardly any time to be lost. He took off his coat and opened his bag. He took out an injection tube, the needle sizzled over the stove. The sick man's wife whimpered in a corner and essayed to ask questions.\n>\n>'Please don't ask questions,' snapped the doctor. He looked at the children, who were watching the sterilizer, and said, 'Send them all away somewhere, except the eldest.'\n>\n>He shot in the drug, sat back in his chair and gazed at the patient's face for over an hour. The patient still remained motionless. The doctor's face gleamed with perspiration, and his eyelids drooped with fatigue. The sick man's wife stood in a corner and watched silently. She asked timidly, 'Doctor, shall I make some coffee for you?' 'No,' he replied, although he felt famished, having missed his midday meal. He got up and said, 'I will be back in a few minutes. Don't disturb him on any account.' He picked up his bag and went to his car. In a quarter of an hour he was back, followed by an assistant and a nurse. The doctor told the lady of the house, ' I have to perform an operation.'\n>\n>'Why, why? Why?' she asked faintly.\n>\n>'I will tell you all that soon. Will you leave your son here to help us, and go over to the next house and stay there till I call you?'\n>\n>The lady felt giddy and sank down on the floor, unable to bear the strain. The nurse attended to her and led her out.\n>\n>At about eight in the evening the patient opened his eyes and stirred slightly in bed. The assistant was overjoyed. He exclaimed enthusiastically, 'Sir, he will pull through.' The doctor looked at him coldly and whispered, ' I would give anything to see him pull through but, but the heart...'\n>\n>'The pulse has improved, sir.'\n>\n>'Well, well,' replied the doctor. 'Don't trust it. It is only a false flash-up, very common in these cases.' He ruminated for a while and added, 'If the pulse keeps up till eight in the morning it will go on for the next forty years, but I doubt very much if we shall see anything of it at all after two tonight.'\n>\n>He sent away the assistant and sat beside the patient. At about eleven the patient opened his eyes and smiled at his friend. He showed a slight improvement, he was able to take in a little food. A great feeling of relief and joy went through the household. They swarmed around the doctor and poured out their gratitude. He sat in his seat beside the bed, gazing sternly at the patient's face, hardly showing any signs of hearing what they were saying to him. The sick man's wife asked, 'Is he now out of danger?' Without turning his head the doctor said, 'Give glucose and brandy every forty minutes; just a couple of spoons will do.' The lady went away to the kitchen. She felt restless. She felt she must know the truth whatever it was. Why was the great man so evasive? The suspense was unbearable.. Perhaps he could not speak so near the patient's bed. She beckoned to him from the kitchen doorway. The doctor rose and went over. She asked, 'What about him now? How is he?' The doctor bit his lips and replied, looking at the floor, 'Don't get excited. Unless you must know about it, don't ask now.' Her eyes opened wide in terror. She clasped her hands together and implored, 'Tell me the truth.' The doctor replied, 'I would rather not talk to you now.' He turned round and went back to his chair. A terrible wailing shot through the still house; the patient stirred and looked about in bewilderment. The doctor got up again, went over to the kitchen door, drew it in securely and shut off the wail.\n>\n>When the doctor resumed his seat the patient asked in the faintest whisper possible, 'Is that someone crying?' The doctor advised, 'Don't exert yourself. You mustn't talk.' He felt the pulse. It was already agitated by the exertion. The patient asked, 'Am I going? Don't hide it from me.' The doctor made a deprecating noise and sat back in his chair. He had never faced a situation like this. It was not in his nature to whitewash. People attached great value to his word because of that. He stole a look at the other. The patient motioned a finger to draw him nearer and whispered, 'I must know how long I am going to last. I must sign the will. It is all ready. Ask my wife for the despatch box. You must sign as a witness.'\n>\n>'Oh!' the doctor exclaimed. 'You are exerting yourself too much. You must be quieter.' He felt idiotic to be repeating it. 'How fine it would be,' he reflected, 'to drop the whole business and run away somewhere without answering anybody any question!' The patient clutched the doctor's wrist with his weak fingers and said, 'Ramu, it is my good fortune that you are here at this moment. I can trust your word. I can't leave my property unsettled. That will mean endless misery for my wife and children. You know all about Subbiah and his gang. Let me sign before it is too late. Tell me...'\n>\n>'Yes, presently,' replied the doctor. He walked off to his car, sat in the back seat and reflected. He looked at his watch. Midnight. If the will was to be signed, it must be done within the next two hours, or never. He could not be responsible for a mess there; he knew the family affairs too well and about those wolves, Subbiah and his gang. But what could he do? If he asked him to sign the will, it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival. he got down from the car and went in. He resumed his seat in the chair. The patient was staring at him appealingly. The doctor said to himself, 'If my word can save his life, he shall not die. The will be damned.' He called, 'Gopal, listen.' This was the first time he was going to do a piece of acting before a patient, simulate a feeling and conceal his judgement. He stooped over the patient and said, with deliberate emphasis, 'Don't worry about the will now. You are going to live. Your heart is absolutely sound.' A new glow suffused the patient's face as he heard it. He asked in a tone of relief, 'Do you say so? If it comes from your lips it must be true...' The doctor said, 'Quite right. You are improving every second. Sleep in peace. You must not exert yourself on any account. You must sleep very soundly. I will see you in the morning.' The patient looked at him gratefully for a moment and then closed his eyes. The doctor picked up his bag and went out, shutting the door softly behind him.\n>\n>On his way home he stopped for a moment at his hospital, called out his assistant and said, 'That Lawley Extension case. You might expect the collapse any second now. Go there with a tube of --- in hand, and give it in case the struggle is too hard at the end. Hurry up.'\n>\n>Next morning he was back at Lawley Extension at ten. From his car he made a dash for the sick bed. The patient was awake and looked very well. The assistant reported satisfactory pulse. The doctor put his tube to his heart, listened for a while and told the sick man's wife, 'Don't look so unhappy, lady. Your husband will live to be ninety.' When they were going back to the hospital, the assistant sitting beside him in the car asked, 'Is he going to live, sir?'\n>\n>'I will bet on it. He will live to be ninety. He has turned the corner. How he has survived this attack will be a puzzle to me all my life,' replied the doctor.\n\n\n&#x200B;\n\n\nNext, I have prepped the data by removing punctuation marks and capital letters etc, resulting in a cleaned up version like this:\n\n\n&#x200B;\n\n\n>people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier the reason was obvious - visiting fee twenty-five rupees and more than that people liked to shirk the fact that the time had come to call in dr raman for them there was something ominous in the very association as a result when the big man came on the scene it was always a quick decision one way or another there was no scope or time for any kind of wavering or whitewashing long years of practice of this kind had bred in the doctor a certain curt truthfulness for that very reason his opinion was valued he was not a mere doctor expressing an opinion but a judge pronouncing a verdict the patients life hung on his words this never unduly worried dr raman he never believed that agreeable words ever saved lives he did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours however when he glimpsed the faintest sign of hope he rolled up his sleeve and stepped into the arena: it might be hours or days but he never withdrew till he wrested the prize from yamas hands today standing over a bed the doctor felt that he himself needed someone to tell him soothing lies he mopped his brow with his kerchief and sat down in the chair beside the bed on the bed lay his dearest friend in the world: gopal they had known each other for forty years now starting with their kindergarten days they could not of course meet as much as they wanted each being wrapped in his own family and profession occasionally on a sunday gopal would walk into the consulting room and wait patiently in a corner till the doctor was free and then they would dine together see a picture and talk of each others life and activities it was a classic friendship which endured untouched by changing times circumstances and activities in his busy round of work dr raman had not noticed that gopal had not called in for over three months now he only remembered it when he saw gopals son sitting on a bench in the consulting hall one crowded morning dr raman could not talk to him for over an hour when he got up and was about to pass on to the operating room he called up the young man and asked what brings you here sir the youth was nervous and shy mother sent me here what can i do for you father is ill it was an operation day and he was not free till three in the afternoon he rushed off straight from the clinic to his friends house in lawley extension gopal lay in bed as if in sleep the doctor stood over him and asked gopals wife how long has he been in bed a month and a half doctor who is attending him a doctor in the next street he comes down once in three days and gives him medicine what is his name he had never heard of him someone i dont know but i wish he had had the goodness to tell me about it why why couldnt you have sent me word earlier we thought you would be busy and did not wish to trouble you unnecessarily they were apologetic and miserable there was hardly any time to be lost he took off his coat and opened his bag he took out an injection tube the needle sizzled over the stove the sick mans wife whimpered in a corner and essayed to ask questions please dont ask questions snapped the doctor he looked at the children who were watching the sterilizer and said send them all away somewhere except the eldest he shot in the drug sat back in his chair and gazed at the patients face for over an hour the patient still remained motionless the doctors face gleamed with perspiration and his eyelids drooped with fatigue the sick mans wife stood in a corner and watched silently she asked timidly doctor shall i make some coffee for you no he replied although he felt famished having missed his midday meal he got up and said i will be back in a few minutes dont disturb him on any account he picked up his bag and went to his car in a quarter of an hour he was back followed by an assistant and a nurse the doctor told the lady of the house i have to perform an operation why why why she asked faintly i will tell you all that soon will you leave your son here to help us and go over to the next house and stay there till i call you the lady felt giddy and sank down on the floor unable to bear the strain the nurse attended to her and led her out at about eight in the evening the patient opened his eyes and stirred slightly in bed the assistant was overjoyed he exclaimed enthusiastically sir he will pull through the doctor looked at him coldly and whispered i would give anything to see him pull through but but the heart the pulse has improved sir well well replied the doctor dont trust it it is only a false flash-up very common in these cases he ruminated for a while and added if the pulse keeps up till eight in the morning it will go on for the next forty years but i doubt very much if we shall see anything of it at all after two tonight he sent away the assistant and sat beside the patient at about eleven the patient opened his eyes and smiled at his friend he showed a slight improvement he was able to take in a little food a great feeling of relief and joy went through the household they swarmed around the doctor and poured out their gratitude he sat in his seat beside the bed gazing sternly at the patients face hardly showing any signs of hearing what they were saying to him the sick mans wife asked is he now out of danger without turning his head the doctor said give glucose and brandy every forty minutes just a couple of spoons will do the lady went away to the kitchen she felt restless she felt she must know the truth whatever it was why was the great man so evasive the suspense was unbearable perhaps he could not speak so near the patients bed she beckoned to him from the kitchen doorway the doctor rose and went over she asked what about him now how is he the doctor bit his lips and replied looking at the floor dont get excited unless you must know about it dont ask now her eyes opened wide in terror she clasped her hands together and implored tell me the truth the doctor replied i would rather not talk to you now he turned round and went back to his chair a terrible wailing shot through the still house the patient stirred and looked about in bewilderment the doctor got up again went over to the kitchen door drew it in securely and shut off the wail when the doctor resumed his seat the patient asked in the faintest whisper possible is that someone crying the doctor advised dont exert yourself you mustnt talk he felt the pulse it was already agitated by the exertion the patient asked am i going dont hide it from me the doctor made a deprecating noise and sat back in his chair he had never faced a situation like this it was not in his nature to whitewash people attached great value to his word because of that he stole a look at the other the patient motioned a finger to draw him nearer and whispered i must know how long i am going to last i must sign the will it is all ready ask my wife for the despatch box you must sign as a witness oh the doctor exclaimed you are exerting yourself too much you must be quieter he felt idiotic to be repeating it how fine it would be he reflected to drop the whole business and run away somewhere without answering anybody any question the patient clutched the doctors wrist with his weak fingers and said ramu it is my good fortune that you are here at this moment i can trust your word i cant leave my property unsettled that will mean endless misery for my wife and children you know all about subbiah and his gang let me sign before it is too late tell me yes presently replied the doctor he walked off to his car sat in the back seat and reflected he looked at his watch midnight if the will was to be signed it must be done within the next two hours or never he could not be responsible for a mess there he knew the family affairs too well and about those wolves subbiah and his gang but what could he do if he asked him to sign the will it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival he got down from the car and went in he resumed his seat in the chair the patient was staring at him appealingly the doctor said to himself if my word can save his life he shall not die the will be damned he called gopal listen this was the first time he was going to do a piece of acting before a patient simulate a feeling and conceal his judgement he stooped over the patient and said with deliberate emphasis dont worry about the will now you are going to live your heart is absolutely sound a new glow suffused the patients face as he heard it he asked in a tone of relief do you say so if it comes from your lips it must be true the doctor said quite right you are improving every second sleep in peace you must not exert yourself on any account you must sleep very soundly i will see you in the morning the patient looked at him gratefully for a moment and then closed his eyes the doctor picked up his bag and went out shutting the door softly behind him on his way home he stopped for a moment at his hospital called out his assistant and said that lawley extension case you might expect the collapse any second now go there with a tube of --- in hand and give it in case the struggle is too hard at the end hurry up next morning he was back at lawley extension at ten from his car he made a dash for the sick bed the patient was awake and looked very well the assistant reported satisfactory pulse the doctor put his tube to his heart listened for a while and told the sick mans wife dont look so unhappy lady your husband will live to be ninety when they were going back to the hospital the assistant sitting beside him in the car asked is he going to live sir i will bet on it he will live to be ninety he has turned the corner how he has survived this attack will be a puzzle to me all my life replied the doctor\n\n\n&#x200B;\n\n\nNext, i extracted a list of unique words used in the text, of which there are a total of 653 unique words in this short story:\n\n\n&#x200B;\n\n\n>people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier reason obvious - visiting fee twenty-five rupees and more than that liked shirk fact time had call in for them there something ominous very association as result big man scene it always quick decision one way or another no scope any kind of wavering whitewashing long years practice this bred doctor certain curt truthfulness opinion valued he not mere expressing an but judge pronouncing verdict patients life hung words never unduly worried believed agreeable ever saved lives did think business provide comforting lies matter course nature would tell truth few hours however glimpsed faintest sign hope rolled up sleeve stepped into arena: might be days withdrew till wrested prize from yamas hands today standing over bed felt himself needed someone soothing mopped brow with kerchief sat down chair beside lay dearest friend world: gopal they known each other forty now starting their kindergarten could meet much wanted being wrapped own family profession occasionally sunday walk consulting room wait patiently corner free then dine together see picture talk others activities classic friendship which endured untouched by changing times circumstances busy round work noticed called three months only remembered saw gopals son sitting bench hall crowded morning hour got about pass operating young asked what brings here sir youth nervous shy mother sent me can i do father is ill operation afternoon rushed off straight clinic friends house lawley extension if sleep stood wife how has been month half who attending next street comes once gives medicine name heard dont know wish goodness word we thought trouble unnecessarily were apologetic miserable hardly lost took coat opened bag injection tube needle sizzled stove sick mans whimpered\n\n\n&#x200B;\n\n\nThen, I have searched for all co-occuring pair of words, neighboring incidents of two-words and three-words phrases, and listed out the most common of those:\n\n\n&#x200B;\n\n\n>Index [2, 5] Co-occurence of phrase ' to the ' = 5\nIndex [2, 9] Co-occurence of phrase ' to his ' = 6\nIndex [2, 210] Co-occurence of phrase ' to be ' = 5\nIndex [5, 6] Co-occurence of phrase ' the patient ' = 15\nIndex [5, 109] Co-occurence of phrase ' the doctor ' = 23\nIndex [5, 138] Co-occurence of phrase ' the patients ' = 5\nIndex [5, 623] Co-occurence of phrase ' the sick ' = 5\nIndex [12, 13] Co-occurence of phrase ' dr raman ' = 5\nIndex [51, 5] Co-occurence of phrase ' in the ' = 15\nIndex [51, 9] Co-occurence of phrase ' in his ' = 6\nIndex [51, 22] Co-occurence of phrase ' in a ' = 8\nIndex [54, 22] Co-occurence of phrase ' for a ' = 5\nIndex [75, 7] Co-occurence of phrase ' it was ' = 7\nIndex [122, 7] Co-occurence of phrase ' he was ' = 6  \n\n\n>\n>Index [5, 6, 7] Co-occurence of phrase ' the patient was ' = 3\nIndex [5, 6, 433] Co-occurence of phrase ' the patient asked ' = 2\nIndex [5, 6, 607] Co-occurence of phrase ' the patient opened ' = 2\nIndex [5, 109, 122] Co-occurence of phrase ' the doctor he ' = 2\nIndex [5, 623, 624] Co-occurence of phrase ' the sick mans ' = 4\nIndex [6, 607, 9] Co-occurence of phrase ' patient opened his ' = 2\nIndex [9, 609, 34] Co-occurence of phrase ' his bag and ' = 2\n\n\n&#x200B;\n\n\nMy question is that I feel stuck at a dead end here, because those co-occurrences do not seem to provide any meaningful data by which we can gain insight into the story at hand.\n\n\nFor example, taking out filler words like 'to', 'in' and so forth, the most common occurring word-pair is \"The doctor\" at 23 times. The most common word-triples are only used 3 or 4 times, and not very interesting.\n\n\nWhat other tools in the Natural Language Processing toolkit can allow us to gain deeper insights into a short story such as this?\n\n\nThank you beforehand!\n\n\nCheers!",
"date": "2021-02-07"
},
{
"vote": 2,
"title": "NLP model in Python",
"text": "Hey everyone! \n\n\nI am currently trying to do a sentiment analysis of different text documents. I am using pyhton 3 and jupyter Notebook. The underlying issue is that I am completely new to programming and have probably taking too much on. I was wondering if someone could explain how I can read a .csv file into \ntextblob.de\n and actually get a sentiment analysis \n\n\n&#x200B;\n\n\nPS: sorry for all the terrible simplifications but I am a total noob in this field",
"date": "2021-02-07"
},
{
"vote": 13,
"title": "How can I look up for early-stage NLP companies?",
"text": null,
"date": "2021-02-07"
},
{
"vote": 0,
"title": "Interlingo - App de Ingles para MÃ“VIL (prÃ³ximo lanzamiento)",
"text": null,
"date": "2021-02-06"
},
{
"vote": 8,
"title": "I want to find out popular issues faced by consumers",
"text": "Hello everyone!\nI'm doing this project where, I'll be taking in consumer complaints. \nThe basic aim of my project is to highlight most common issues faced by consumers by using either keyword extraction or document clustering.\n\n\nKindly suggest some techniques/tools using which I could implement this.",
"date": "2021-02-05"
},
{
"vote": 83,
"title": "Trained a Markov Chain on a bunch of r/WSB posts and comments. Only 2-word conditional probabilities but honestly, that's all that's necessary ðŸš€ðŸš€",
"text": "First two words are the seeds\n\n\n\n\nyour wife didn't marry you because we've understood all along . Deep Fucking Value . Salute !\n\n\ngamestop is still a 300% green day . Hold the line , the funds most likely rebought\n\n\nthe retard you must not be earth shattering dump . Donâ€™t worry everyone always wipes their first\n\n\nthe retard strength in this nonsense lmao ! Priceless . Good shit Edit: In discovery we could\n\n\nstonks can only place I can warm my tendies . Can someone explain how this movie 2\n\n\nyour wife and her boyfriend extra close tonight . J . Simpson to do another paper trading\n\n\nyour wife may have covered the shorts to have joined . I canâ€™t buy more shares\n\n\nstonks can only hope for the word \"tendies\" . # HOLD HOLD HOLD ! ðŸ’ŽðŸ™Œ . Guys\n\n\ngamestop is going to $100 Million . DO NOT SELL ðŸ’ŽðŸ™ŒðŸ» , BUY ! ! ! ! Available shares just got bodied",
"date": "2021-02-04"
},
{
"vote": 11,
"title": "[Best Practices] on how to organize deep learning projects",
"text": "In this article youâ€™ll see how to structure work on deep learning projects â€” from the inception to deployment, and everything in between. You will learn:\n\n\n\n\nAbout the lifecycle of the project.\n\n\nImportance of defining an objective or goal of the project.\n\n\nCollecting data based on the requirements of the project.\n\n\nModel training and results exploration including:\n\n\nEstablishing baselines for better results.\n  -Adopting techniques and approaches from the existing open-source state-of-the-art models research papers and code repositories.\n\n\nExperiment tracking and management management\n\n\n\n\n\n\nModel refinement techniques to avoid underfitting and overfitting like:\n\n\nControlling hyperparameters\n\n\nRegularisation\n\n\nPruning\n\n\n\n\n\n\nTesting and evaluating your project before deployment.\n\n\nModel deployment\n\n\nProject maintenance\n\n\n\n\nStructuring deep learning projects",
"date": "2021-02-04"
},
{
"vote": 1,
"title": "Neural Networks Generate New Dwight Schrute Quotes",
"text": null,
"date": "2021-02-04"
},
{
"vote": 1,
"title": "I'm trying to train a new entity type in Spacy's named entity recognizer. If there is a finite amount of entities, should I go through each entity and place at the correct position in each training sentence I have for additional training data for every entity?",
"text": "[deleted]",
"date": "2021-02-04"
},
{
"vote": 14,
"title": "I have a question",
"text": "So I'm going through \nTensorFlows NLP Zero to Hero\n video playlist as an introduction to NLP.\n\n\nThe host shows the word LISTEN and talks about how it may be encoded letter by letter with ASCII.\n\n\nHe then proceeds to show the word SILENT and claims that because it contains the same letters and numbers, it is hard for us to understand the sentiment of the word.\n\n\nAm I stupid or is it easy as hell to read something in order?",
"date": "2021-02-03"
},
{
"vote": 1,
"title": "Best module for my project? Please help",
"text": "Hi there NLP friends,\nIm pretty new to python and NLP. Please help point me in the right direction.\n\n\nIm curious about potential causes to an effect. Casual inference but maybe not just directly cause and effect, and even more like potential causes that may lead to effect.\nI would want to be able to type in an effect and get a collection of all the potential causes and vice versa.\nAs well the negative or nullifying forces that may potentially oppose the effect.\nMy project needs me to type in a sentence not just a bag of words. And would return phrases and words that meet the criteria.\n\n\nEg. Forest Fire.\n Potential causes and correlations: cigarette, spark, negligence, firepit, lightening strike, fireworks, spontaneous combustion of mulch, dry weather, dry plant matter.\nPotentially causes: burn down homes, fear, \nPotential opposing elements: rain, firefighters, fire extinguisher, dump water on camp fire, careful, etc.\n\n\nPreferably trained on a blend of General academic subjects, psychology, physics, phrases and quotes, and large web commons like wiki, google, etc.",
"date": "2021-02-03"
},
{
"vote": 0,
"title": "Language acquisition by virtual agent (The Folksâ€™Talks game project)",
"text": "https://drive.google.com/file/d/1cWNqnOyowoAr3d_DJZbdIpKuvlAYu7eu/view?usp=sharing",
"date": "2021-02-03"
},
{
"vote": 19,
"title": "How to get an accurate text similarity score between very large documents (dozens or even hundreds of pages of text), when token order matters?",
"text": "Levenshtein woud be ideal, but it's not computationally feasible to compare hundreds of pages long documents with it. Hamming and other edit-based metrics look good but are also way too time consuming.\n\n\nCosine similarity is very fast, but it doesn't take into account the order of appearance of tokens, which matters to me. Same goes for Jaccard and other token-based similarity metrics I've ran across.\n\n\nIs there a fast algorithm out there which takes into account the order of the sequence?\n\n\nI don't care about semantic stuff btw, I need to compare texts superficially.",
"date": "2021-02-03"
},
{
"vote": 3,
"title": "Connection between WIKIDATA and WORDNETS",
"text": "Just a quick question.... Are wordnet synsets linked to opendata in any way like wikidata or dbpedia ?",
"date": "2021-02-03"
},
{
"vote": 14,
"title": "Masterâ€™s in NLP or CompLing in Europe (non CS background)",
"text": "Hello everyone! \nIâ€™m from the US. Recently graduated with BA in Spanish with a minor in Linguistics (3.75 GPA). I have coursework in Spanish, phonology, phonetics, syntax, language and technology and machine learning (only one class).\n\n\nIâ€™m looking for Masterâ€™s in NLP or CL in Europe that are open to applicants from a non CS background. I have intermediate level knowledge of Python and I am taking Python classes through Coursera. I have already completed a few. Are there any programs that are willing to consider applicants from a strictly a language/linguistics background?\n\n\nThank you!",
"date": "2021-02-03"
},
{
"vote": 2,
"title": "How do you explain NLP to someone who's never heard of NLP?",
"text": "I'm a marketing, college student learning about AI and NLP. I have noticed that when I try to explain NLP to my non-technical friends, they have a hard time understanding. I know this group talks about more technical stuff (and I cannot understand 90% of the conversations), but does anyone have any suggestions on how I can explain NLP to someone who's never heard of NLP?",
"date": "2021-02-02"
},
{
"vote": 5,
"title": "What is a good task to demonstrate the power of a new language modeling architecture?",
"text": "So we are doing a class project, which requires use to use NLP. We have chosen to design a model akin to the Transformer. The faculty evaluating us is a tad inexperienced with language modeling, so we would like to demonstrate the performance gains and competency of our model using a simple task, like article generation, for example.\nWhat other tasks would you recommend, especially those that have comprehensive datasets available?",
"date": "2021-02-02"
},
{
"vote": 7,
"title": "[DATASET] Massive multi-turn conversational dataset based on cleaned discord data",
"text": "This is a long-context, anonymized, clean, multi-turn and single-turn conversational dataset based on discord data scraped from a large variety of severs, big and small.\n\n\nThe goal is to use this data to pretrain a small conversational model on a big variety of data.\n\n\nThe raw data for this version contained 51,826,268 messages5103788 (regex) + 696161 (toxic)/51826268, or 0.11% of the messages were removed\nThe dataset's final size is 46,026,319 messages across 456810 conversations\n, which is reduced from 33.06 GB of raw json data to 968.87 MB\n\n\nhttps://www.kaggle.com/jef1056/discord-data",
"date": "2021-02-02"
},
{
"vote": 3,
"title": "Current state of the art for document classification?",
"text": "I have a problem where I need to classify chunks of text as to whether they pertain to the pharmaceutical industry or not. It will be a supervised learning task but I will need to go through the process of manually labeling the text to come up with the training data.\n\n\nWhat are the current state-of-the-art text classifiers and are they feasible to use given my constraints?",
"date": "2021-02-02"
},
{
"vote": 2,
"title": "Do companies offer NLP Research Internships for the summer before starting grad school?",
"text": "I have been fortunate enough to be accepted into three schools so far for an MS degree and am waiting on others. Therefore, I have begun searching for internships for this upcoming summer. I have found that most research internships only want Ph.D. students. I have tried cold-emailing, connecting, asking research scientists for a \"coffee chat,\" etc. Unfortunately, I have had no success. Is there any place I should be looking? Is it rare for people in my position not to get internships?\n\n\nI have almost two years of research experience working and leading NLP research projects. However, I do not have any publications. I am also located in the US if that makes a difference.",
"date": "2021-02-02"
},
{
"vote": 1,
"title": "How can I summarize text with an unsupervised technique these days?",
"text": "Hi, I have been reading that the state of the art for summarizing, and probably other NLP tasks are done with supervised training, with huge data sets and complex architectures like transformers with pre training. \n\n\nThat is awesome, but I don't have the GPU power nor access to the datasets to explore that part. Also, I'm OK achieving acceptable results for a start.\n\n\nThat's why I was thinking on unsupervised ways to summarize.\n\n\nCould you recommend what are the state of the art techniques to do it? Where should I start to dig in?\n\n\nThanks!\n\n\nEdit for clarification\n: I have already read about the abstractive and extractive approaches. What I would like to learn is how to do unsupervised abstractive summarization for a generic text corpora, and the concepts behind this task.",
"date": "2021-02-01"
},
{
"vote": 9,
"title": "Is there any difference between sentence embedding and document embedding?",
"text": "I thought sentence and document embeddings are different. I thought one is one that embeds sentences whereas another is one that embeds more then a sentence such as paragraph or whole document text. However, some research paper got me confusing.\n\n\nLASER\n paper describes it as sentence embeddings whereas \nT-LASER\n metions LASER as document embedding. In LASER paper, text classification experiment is done, as well as multiple text classification experiments are carrying using sentence embeddings. If there would be sentence classification, I wouldn't be confused. But don't text usually means a document and instead would be efficient to use document embeddings instead?",
"date": "2021-02-01"
},
{
"vote": 0,
"title": "How To Profit From Alexa Skill Development Using Node JS",
"text": null,
"date": "2021-02-01"
},
{
"vote": 1,
"title": "Exactly *how* is the new technology supposed to help improving search engines?",
"text": "Aside from the traditional inverted-index searches, and nearest-neighbor searches (with document/sentence embeddings), I mean.\n\n\nI am having trouble mixing the two as well, because the ML models fail when it comes to proper nouns, I need an inverted-index search. Sometimes the nearest-neighbor searchs are all over the place too, with tangentially related sentences, and it's hard to adjust a similarity threshold.\n\n\nFurthermore, I'd like to know how align (highlight) related parts of the text to the query (like Google does). I suspect this is done with expanded queries on inverted-index search, because it would be impractical to store embeddings for each word/wordpiece.\n\n\nIf that's the case, then how am I supposed to expand the searches?",
"date": "2021-02-01"
},
{
"vote": 12,
"title": "What is topic modeling?",
"text": null,
"date": "2021-02-01"
},
{
"vote": 8,
"title": "What is the latest research on NLU and decision making?",
"text": "I would be really interested if anyone a list of links/reading resources in this area as it of particular interest for a project i am working on.",
"date": "2021-02-01"
},
{
"vote": 22,
"title": "Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough)",
"text": "This paper extends BERT for doing long document classification in nlp. They propose BERT variations RoBERT and ToBERT as hierarchical enchancements for the same. They obtained a significant improvement over the baseline models.\n\n\nPaper Walkthrough: \nhttps://youtu.be/3IOl5d9PZeM\n\n\nPaper Link: \nhttps://arxiv.org/abs/1910.10781",
"date": "2021-01-31"
},
{
"vote": 1,
"title": "Partner Up for Learning",
"text": "Hello everyone, hope you doing well. I just wanted share the discord server for the people who search for learning partners. You can join server to find a partner for learning different programming languages or any topics you are interested in.\nHere is the link for the server:\n\n\nhttps://discord.gg/ayeGrsaSG2",
"date": "2021-01-31"
},
{
"vote": 7,
"title": "Research topics",
"text": "Hi all, I finished my master in nlp two years ago (worked on summarization) and I'm thinking of starting a PhD program.\nThe thing is, I'm not sure what topics I should work on. \n\n\nWhat I know is that I'd like to work on applicable topics, so nothing too theoretical. Also, leaderboard climbing using Bert in different ways is not that interesting imo.\n\n\nSo I'm basically asking what nlp domains you find interesting and you think should be great research topics.\nThanks!!",
"date": "2021-01-30"
},
{
"vote": 41,
"title": "[P] An interactive history of natural language processing, starting a long time ago",
"text": null,
"date": "2021-01-30"
},
{
"vote": 5,
"title": "Entity Resolution for Master Data Management",
"text": "Last year, we set out to create a master entity resolution tool. We wanted to identify and resolve multiple occurrences of a single entity to get a clearer picture of the information within data... but in a practical and scaleable way.\n\n\nOur blog post, \nEntity Resolution for Master Data Management\n explains the why and how behind erÂ², ThinkData's entity resolution tool.\n\n\nIf you're interested, check it out here: \nhttps://blog.thinkdataworks.com/entity-resolution-for-master-data-management",
"date": "2021-01-29"
},
{
"vote": 3,
"title": "An unexpected use case of word embeddings",
"text": null,
"date": "2021-01-29"
},
{
"vote": 11,
"title": "Rebuilding the spellchecker: Hunspell and the order of edits",
"text": null,
"date": "2021-01-29"
},
{
"vote": 2,
"title": "Which are top APIs for Indian languages mainly VR, OCR, Speech - Text - Speech?",
"text": null,
"date": "2021-01-29"
},
{
"vote": 9,
"title": "Chaining BERT multilabel classifiers.",
"text": "Say I have a two layers of labels with a label and sublabel:\n\n\nFruit.Apple\nFruit.Orange\nFruit.Banana\n\nVegetable:Cucumber\nVegetable:Squash\nVegetable:Cabbage\n\nMeat:Cow\nMeat:Chicken\nMeat:Fish\n\n\n\nThen suppose I build a dataset of \"meals\" which are combinations of the above labels:\n\n\n[Vegetable:Squash,Meat:Cow]\n[Fruit.Apple,Fruit.Orange]\n[Meat:Fish,Fruit.Orange]\n\n\n\nReally each \"meal\" is a document with a category and subcategory. If I were to train a bert classifier for guessing the ingredients in a given meal, would it be better to:\n\n\n\n\ntrain 1 classifier with 9 labels\n\n\nor build a classier with 3 labels (fruits, vegetables,meats) then build 3 additional classifier to sort the finer sublabels (fruits -> (apple, orange, banana), vegetable ->(cucumber, squash, cabbage), etc)\n\n\n\n\nNow I'd imagine if you had a large enough database, the first would probably be fine. But if you had say 20 labels each with 10 sublabels, then the second option starts to make more sense.",
"date": "2021-01-29"
},
{
"vote": 4,
"title": "Sales Conversations Datasets",
"text": "Hey everybody,\n\n\nI'm new to NLP and looking into training tensorflow using sales conversations, i.e. e-mail conversations. The problem of course is getting data... I have not been able to come up with a good possible source yet. Any ideas are appreciated :-)",
"date": "2021-01-28"
},
{
"vote": 1,
"title": "How to approach transformers and easily train deep learning models for language tasks?",
"text": "[removed]",
"date": "2021-01-28"
},
{
"vote": 32,
"title": "Towards Automated Fact-Checking",
"text": null,
"date": "2021-01-27"
},
{
"vote": 1,
"title": "Find sponsors from transcripts (yt/podcast)",
"text": "Hi there!\n\n\nFor videos on YouTube or podcasts on other platforms, I want to find all the brand(s) that are sponsoring that video or that podcast. Right now we are using fuzzy queries on description and transcript texts. But that does not cover all the cases. \n\n\nOften content creators use terms like â€œthis video is sponsored/brought to you by {brand name}â€ but not always. So we want to use nlp. \n\n\nPlease suggest any approach/idea/library or any direction to further investigation. Any help is appreciated.",
"date": "2021-01-27"
},
{
"vote": 1,
"title": "Unbalanced document length &amp; topic modeling (lda) [help]",
"text": "Hi all,\n\n\nI can't seem to form an intuition for the following settings : I would like to perform a LDA on a group of text but some document are many, many time longer than others. \n\n\nThe average is at 80 tokens, median 40 but there are document reaching up to 6000 tokens. \n\n\nMy strategy was to discard document with too few words (lower than the 25% quantile ) as they are unlikely to satisfy the assumption that \"documents are a mixture of topics\". \n\n\nBut I'm not sure how to deal with the outlier with too many words. Would the LDA be robust enough to deal with the imbalance? Or should I split large documents into smaller chunks and randomly select one chunk.\n\n\nThanks in advance.",
"date": "2021-01-27"
},
{
"vote": 14,
"title": "Is there a way to analyze a sentence to calculate whether A is beneficial to B?",
"text": "For example we use a float value to represent whether Alice is helping Bob:\n\n\n\n\nIn \nAlice gives Bob $100\n, Alice is helping Bob; (+0.5)\n\n\nIn \nAlice gives Bob a slap\n, Bob is hurt by Alice; (-0.5)\n\n\nIn \nAlice gives Bob a slap to keep him from falling sleep in the snow\n, Though Bob is hurt, Alice is saving Bob's life so it's still good for him; (+0.7)\n\n\n\n\nSo, my idea is to make a corpus based on \nFrameNet\n, giving \nbeneficial\n value for frame elements. (Assuming my input is already in the frame data structure) \n\n\nIs there any better idea or existing solution for this problem?",
"date": "2021-01-27"
},
{
"vote": 2,
"title": "Fine-Tune 11B T5 for Generation Task",
"text": "I'm trying to figure out how to fine-tune the largest version of T5 for a language generation task, but I can't seem to find any notebooks or resources to do so. Any great places to start?",
"date": "2021-01-27"
},
{
"vote": 1,
"title": "Finding adjectives, nouns, and verbs associated with a keyword - Python help",
"text": "[deleted]",
"date": "2021-01-26"
},
{
"vote": 1,
"title": "Announcing UBIAI: Easy-to-Use Text Annotation Tool",
"text": "UBIAI\n was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. We know that data labeling is the bottleneck for creating custom NLP models (NER, entity relations, classification, etc) and is here to stay. We wanted to create the most accessible, easy-to-use and automated solution at an affordable price.\n\n\nIf you are launching a new NLP project, please explore our \ntool\n (we offer free 14 day trial) and give us your feedback at \nadmin@ubiai.tools\n\n\nHere are few blog links:\n\n\n\n\nIntroducing UBIAI\n\n\nBuilding Job Entity Recognizer using Amazon Comprehend",
"date": "2021-01-26"
},
{
"vote": 1,
"title": "[Tutorial] Training, Visualizing, and Understanding Word Embeddings",
"text": "In this post, we will look at different techniques you can use to better understand how well a language model captures the contextual relationship between words. We will do this by: \n\n\n\n\nLooking at the dataset we need to train these models to see if we can come up with a simple one that helps us visualize how these models â€œlearnâ€ the relationship between different words.\n\n\nLooking at the tools and techniques you can use to track the progress of these models and monitor the results while they process our simplified dataset.\n\n\nAfter that you should hopefully be able to re-use that template for more complex models with some real life datasets.\n\n\n\n\nunderstanding word embeddings",
"date": "2021-01-26"
},
{
"vote": 3,
"title": "ERNIE-M: Multilingual Model Learns 96 Languages from Monolingual Corpora, Tops Googleâ€™s XTREME Benchmark",
"text": "ERNIE-M\n is a new multilingual model that understands 96 languages and tops \nXTREME\n, a substantial multilingual multi-task benchmark proposed by Google, Carnegie Mellon University, and DeepMind. The novel pre-training method can learn semantic alignment across multiple languages on monolingual corpora.\n\n\nPaper: \nhttps://arxiv.org/abs/2012.15674\n\n\nBlog: \nhttp://research.baidu.com/Blog/index-view?id=151",
"date": "2021-01-25"
},
{
"vote": 13,
"title": "Allen Institute launches GENIE, a leaderboard for human-in-the-loop language model benchmarking",
"text": null,
"date": "2021-01-25"
},
{
"vote": 52,
"title": "720+ new NLP models, 300+ supported languages, translation, summarization, question answering, and more with T5 and Marian models! - John Snow Labs NLU 1.1.0",
"text": "720+ new  NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models!  - John Snow Labs NLU 1.1.0\n\n\nNLU 1.1.0 Release Notes\n\n\nWe are incredibly excited to release NLU 1.1.0!\nThis release integrates the 720+ new models from the latest \nSpark-NLP 2.7.0 + releases\n\nYou can now achieve state-of-the-art results with Sequence2Sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract Named Entity in various Right to Left written languages like  Arabic, Persian, Urdu, and languages that require segmentation like Koreas, Japanese, Chinese, and many more in 1 line of code!\nThese new features are possible because of the integration of the \nGoogle's T5 models\n and \nMicrosoft's Marian models\n  transformers.\n\n\nNLU 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew.\n\n\nIn addition to this, NLU 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.\n\n\nNLU 1.1.0  New Features\n\n\n\n\n720+\n new models you can find an overview of all NLU models \nhere\n and further documentation in the \nmodels hub\n\n\nNEW:\n Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models & pipelines in 192+ languages)\n\n\nNEW:\n Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on\n\n\nNEW:\n Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages\n\n\nNEW:\n Introducing WordSegmenter model for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean\n\n\nNEW:\n Introducing DocumentNormalizer component for cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n\n\n\nTranslation\n\n\nTranslation example\nYou can translate between more than 192 Languages pairs with the \nMarian Models\n\nYou need to specify the language your data is in as \nstart_language\n and the language you want to translate to as \ntarget_language\n.\nThe language references must be \nISO language codes\n\n\nnlu.load(&#039;&lt;start_language&gt;.translate.&lt;target_language&gt;&#039;)\n\n\nTranslate English to French :\n \n\n\nnlu.load(&#039;en.translate_to.fr&#039;).predict(&quot;Hello from John Snow Labs&quot;)\n&gt;&gt;&gt; Output: Bonjour des laboratoires de neige de John!\t \n\n\n\nTranslate English to Inukitut :\n \n\n\nnlu.load(&#039;en.translate_to.lu&#039;).predict(&quot;Hello from John Snow Labs&quot;)\n&gt;&gt;&gt; Output: kalunganyembo ka mashika makamankate \n\n\n\nTranslate English to Hungarian :\n\n\nnlu.load(&#039;en.translate_to.hu&#039;).predict(&quot;Hello from John Snow Labs&quot;)\n&gt;&gt;&gt; Output: HellÃ³ John hÃ³ laborjÃ¡bÃ³l.\n\n\n\nTranslate English to German :\n\n\nnlu.load(&#039;en.translate_to.de&#039;).predict(&quot;Hello from John Snow Labs!&quot;)\n&gt;&gt;&gt; Output: Hallo aus John Schnee Labors \n\n\n\ntranslate_pipe = nlu.load(&#039;en.translate_to.de&#039;)\ndf = translate_pipe.predict(&#039;Billy likes to go to the mall every sunday&#039;)\ndf\n\n\n\n\n\n\n\n\n\nsentence\n\n\ntranslation\n\n\n\n\n\n\n\n\nBilly likes to go to the mall every sunday\n\n\nBilly geht gerne jeden Sonntag ins Einkaufszentrum\n\n\n\n\n\n\nT5\n\n\nExample of every T5 task\n\n\nOverview of every task available with T5\n\n\nThe T5 model\n is trained on various datasets for 17 different tasks which fall into 8 categories.\n\n\n\n\nText summarization\n\n\nQuestion answering\n\n\nTranslation\n\n\nSentiment analysis\n\n\nNatural Language inference\n\n\nCoreference resolution\n\n\nSentence Completion\n\n\nWord sense disambiguation\n\n\n\n\nEvery T5 Task with explanation:\n\n\n\n\n\n\n\n\nTask Name\n\n\nExplanation\n\n\n\n\n\n\n\n\n1.CoLA\n\n\nClassify if a sentence is gramaticaly correct\n\n\n\n\n\n\n2.RTE\n\n\nClassify whether if a statement can be deducted from a sentence\n\n\n\n\n\n\n3.MNLI\n\n\nClassify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).\n\n\n\n\n\n\n4.MRPC\n\n\nClassify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)\n\n\n\n\n\n\n5.QNLI\n\n\nClassify whether the answer to a question can be deducted from an answer candidate.\n\n\n\n\n\n\n6.QQP\n\n\nClassify whether a pair of questions is a re-phrasing of each other (semantically equivalent)\n\n\n\n\n\n\n7.SST2\n\n\nClassify the sentiment of a sentence as positive or negative\n\n\n\n\n\n\n8.STSB\n\n\nClassify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)\n\n\n\n\n\n\n9.CB\n\n\nClassify for a premise and a hypothesis whether they contradict each other or not (binary).\n\n\n\n\n\n\n10.COPA\n\n\nClassify for a question, premise, and 2 choices which choice the correct choice is (binary).\n\n\n\n\n\n\n11.MultiRc\n\n\nClassify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),\n\n\n\n\n\n\n12.WiC\n\n\nClassify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.\n\n\n\n\n\n\n13.WSC/DPR\n\n\nPredict for an ambiguous pronoun in a sentence what it is referring to.\n\n\n\n\n\n\n14.Summarization\n\n\nSummarize text into a shorter representation.\n\n\n\n\n\n\n15.SQuAD\n\n\nAnswer a question for a given context.\n\n\n\n\n\n\n16.WMT1.\n\n\nTranslate English to German\n\n\n\n\n\n\n17.WMT2.\n\n\nTranslate English to French\n\n\n\n\n\n\n18.WMT3.\n\n\nTranslate English to Romanian\n\n\n\n\n\n\n\n\nEvery T5 Task example notebook\n to see how to use every T5 Task.\n\n\nT5 Open and Closed Book question answering  notebook\n\n\n\n\nOpen book\n and \nClosed book\n question answering with Google's T5\n\n\nT5 Open and Closed Book question answering tutorial\n\n\nWith the latest NLU release and Google's T5 you can answer \ngeneral knowledge based questions given no context\n and in addition answer \nquestions on text databases\n.\nThese questions can be asked in natural human language and answerd in just 1 line with NLU!.\n\n\nWhat is a \nopen book question\n?\n\n\nYou can imagine an \nopen book\n question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. Kinda like bringing a history book to an history examen.\n\n\nIn \nT5&#039;s\n terms, this means the model is given a \nquestion\n and an \nadditional piece of textual information\n or so called \ncontext\n.\n\n\nThis enables the \nT5\n model to answer questions on textual datasets like \nmedical records\n,\nnewsarticles\n , \nwiki-databases\n , \nstories\n and \nmovie scripts\n , \nproduct descriptions\n, 'legal documents' and many more.\n\n\nYou can answer \nopen book question\n in 1 line of code, leveraging the latest NLU release and Google's T5.\nAll it takes is :\n\n\nnlu.load(&#039;answer_question&#039;).predict(&quot;&quot;&quot;\nWhere did Jebe die?\ncontext: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards,\n and Jebe died on the road back to Samarkand&quot;&quot;&quot;)\n&gt;&gt;&gt; Output: Samarkand\n\n\n\nExample for answering medical questions based on medical context\n\n\nquestion =&#039;&#039;&#039;\nWhat does increased oxygen concentrations in the patientâ€™s lungs displace? \ncontext: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. \nCarbon monoxide poisoning, gas gangrene, and decompression sickness (the â€™bendsâ€™) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.\n Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n&#039;&#039;&#039;\n\n\n#Predict on text data with T5\nnlu.load(&#039;answer_question&#039;).predict(question)\n&gt;&gt;&gt; Output: carbon monoxide\t\n\n\n\nTake a look at this example on a recent news article snippet :\n\n\nquestion1 = &#039;Who is Jack ma?&#039;\nquestion2 = &#039;Who is founder of Alibaba Group?&#039;\nquestion3 = &#039;When did Jack Ma re-appear?&#039;\nquestion4 = &#039;How did Alibaba stocks react?&#039;\nquestion5 = &#039;Whom did Jack Ma meet?&#039;\nquestion6 = &#039;Who did Jack Ma hide from?&#039;\n\n# from https://www.bbc.com/news/business-55728338 \nnews_article_snippet = &quot;&quot;&quot; context:\nAlibaba Group founder Jack Ma has made his first appearance since Chinese regulators cracked down on his business empire.\nHis absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.\nThe billionaire met 100 rural teachers in China via a video meeting on Wednesday, according to local government media.\nAlibaba shares surged 5% on Hong Kong&#039;s stock exchange on the news.\n&quot;&quot;&quot;\n# join question with context, works with Pandas DF aswell!\nquestions = [\n             question1+ news_article_snippet,\n             question2+ news_article_snippet,\n             question3+ news_article_snippet,\n             question4+ news_article_snippet,\n             question5+ news_article_snippet,\n             question6+ news_article_snippet,]\nnlu.load(&#039;answer_question&#039;).predict(questions)\n\n\n\nThis will output a Pandas Dataframe similar to this :\n\n\n\n\n\n\n\n\nAnswer\n\n\nQuestion\n\n\n\n\n\n\n\n\nAlibaba Group founder\n\n\nWho is Jack ma?\n\n\n\n\n\n\nJack Ma\n\n\nWho is founder of Alibaba Group?\n\n\n\n\n\n\nWednesday\n\n\nWhen did Jack Ma re-appear?\n\n\n\n\n\n\nsurged 5%\n\n\nHow did Alibaba stocks react?\n\n\n\n\n\n\n100 rural teachers\n\n\nWhom did Jack Ma meet?\n\n\n\n\n\n\nChinese regulators\n\n\nWho did Jack Ma hide from?\n\n\n\n\n\n\nWhat is a \nclosed book question\n?\n\n\nA \nclosed book question\n is the exact opposite of a \nopen book question\n. In an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.\nIn \nT5&#039;s\n terms this means that T5 can only use it's stored weights to answer a \nquestion\n and is given \nno aditional context\n.\nT5\n was pre-trained on the \nC4 dataset\n which contains \npetabytes  of web crawling data\n  collected over the last 8 years, including Wikipedia in every language.\n\n\nThis gives \nT5\n the broad knowledge of the internet stored in it's weights to answer various \nclosed book questions\n\n\nYou can answer \nclosed book question\n in 1 line of code, leveraging the latest NLU release and Google's T5.\nYou need to pass one string to NLU, which starts which a \nquestion\n and is followed by  a \ncontext:\n tag and then the actual context contents.\nAll it takes is :\n\n\nnlu.load(&#039;en.t5&#039;).predict(&#039;Who is president of Nigeria?&#039;)\n&gt;&gt;&gt; Muhammadu Buhari \n\n\n\nnlu.load(&#039;en.t5&#039;).predict(&#039;What is the most spoken language in India?&#039;)\n&gt;&gt;&gt; Hindi\n\n\n\nnlu.load(&#039;en.t5&#039;).predict(&#039;What is the capital of Germany?&#039;)\n&gt;&gt;&gt; Berlin\n\n\n\nText Summarization with T5\n\n\nSummarization example\n\n\nSummarizes\n a paragraph into a shorter version with the same semantic meaning, based on \nthis paper\n\n\n# Set the task on T5\npipe = nlu.load(&#039;summarize&#039;)\n\n# define Data, add additional tags between sentences\ndata = [\n&#039;&#039;&#039;\nThe belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaalâ€™s side currently sit two points clear of liverpool in fourth .\n&#039;&#039;&#039;,\n&#039;&#039;&#039;  Calculus, originally called infinitesimal calculus or &quot;the calculus of infinitesimals&quot;, is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally &quot;small pebble&quot; (this meaning is kept in medicine â€“ see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.&#039;&#039;&#039;\n]\n\n\n#Predict on text data with T5\npipe.predict(data)\n\n\n\n\n\n\n\n\n\nPredicted summary\n\n\nText\n\n\n\n\n\n\n\n\nmanchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .\n\n\nthe belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaalâ€™s side currently sit two points clear of liverpool in fourth .\n\n\n\n\n\n\nBinary Sentence similarity/ Paraphrasing\n\n\nBinary sentence similarity example\n\nClassify whether one sentence is a re-phrasing or similar to another sentence\nThis is a sub-task of \nGLUE\n and based on \nMRPC - Binary Paraphrasing/ sentence similarity classification \n\n\nt5 = nlu.load(&#039;en.t5.base&#039;)\n# Set the task on T5\nt5[&#039;t5&#039;].setTask(&#039;mrpc &#039;)\n\n# define Data, add additional tags between sentences\ndata = [\n&#039;&#039;&#039; sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , &quot; Rumsfeld said .\nsentence2: Rather , the US acted because the administration saw &quot; existing evidence in a new light , through the prism of our experience on September 11 &quot;\n&#039;&#039;&#039;\n,\n&#039;&#039;&#039;  \nsentence1: I like to eat peanutbutter for breakfast\nsentence2: \tI like to play football.\n&#039;&#039;&#039;\n]\n\n#Predict on text data with T5\nt5.predict(data)\n\n\n\n\n\n\n\n\n\nSentence1\n\n\nSentence2\n\n\nprediction\n\n\n\n\n\n\n\n\nWe acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .\n\n\nRather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" .\n\n\nequivalent\n\n\n\n\n\n\nI like to eat peanutbutter for breakfast\n\n\nI like to play football\n\n\nnot_equivalent\n\n\n\n\n\n\nHow to configure T5 task for MRPC and pre-process text\n\n\n.setTask(&#039;mrpc sentence1:)\n and prefix second sentence with \nsentence2:\n\n\nExample pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity\n\n\nmrpc \nsentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , &quot; Rumsfeld said . \nsentence2: Rather , the US acted because the administration saw &quot; existing evidence in a new light , through the prism of our experience on September 11&quot;,\n\n\n\nRegressive Sentence similarity/ Paraphrasing\n\n\nMeasures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.\nThis is a sub-task of \nGLUE\n and based on\nSTSB - Regressive semantic sentence similarity\n .\n\n\nt5 = nlu.load(&#039;en.t5.base&#039;)\n# Set the task on T5\nt5[&#039;t5&#039;].setTask(&#039;stsb &#039;) \n\n# define Data, add additional tags between sentences\ndata = [\n             \n              &#039;&#039;&#039; sentence1:  What attributes would have made you highly desirable in ancient Rome?  \n                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?&#039;\n              &#039;&#039;&#039;\n             ,\n             &#039;&#039;&#039;  \n              sentence1: What was it like in Ancient rome?\n              sentence2: \tWhat was Ancient rome like?\n              &#039;&#039;&#039;,\n              &#039;&#039;&#039;  \n              sentence1: What was live like as a King in Ancient Rome??\n              sentence2: \tWhat was Ancient rome like?\n              &#039;&#039;&#039;\n\n             ]\n\n\n\n#Predict on text data with T5\nt5.predict(data)\n\n\n\n\n\n\n\n\n\nsentence1\n\n\nsentence2\n\n\nprediction\n\n\n\n\n\n\n\n\nWhat attributes would have made you highly desirable in ancient Rome?\n\n\nHow I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\n\n\n0\n\n\n\n\n\n\nWhat was it like in Ancient rome?\n\n\nWhat was Ancient rome like?\n\n\n5.0\n\n\n\n\n\n\nWhat was live like as a King in Ancient Rome??\n\n\nWhat is it like to live in Rome?\n\n\n3.2\n\n\n\n\n\n\nHow to configure T5 task for stsb and pre-process text\n\n\n.setTask(&#039;stsb sentence1:)\n and prefix second sentence with \nsentence2:\n\n\nExample pre-processed input for T5 STSB - Regressive semantic sentence similarity\n\n\nstsb\nsentence1: What attributes would have made you highly desirable in ancient Rome?        \nsentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?&#039;,\n\n\n\nGrammar Checking\n\n\nGrammar checking with T5 example\n\nJudges if a sentence is grammatically acceptable.\nBased on \nCoLA - Binary Grammatical Sentence acceptability classification\n\n\npipe = nlu.load(&#039;grammar_correctness&#039;)\n# Set the task on T5\npipe[&#039;t5&#039;].setTask(&#039;cola sentence: &#039;)\n# define Data\ndata = [&#039;Anna and Mike is going skiing and they is liked is&#039;,&#039;Anna and Mike like to dance&#039;]\n#Predict on text data with T5\npipe.predict(data)\n\n\n\n\n\n\n\n\n\nsentence\n\n\nprediction\n\n\n\n\n\n\n\n\nAnna and Mike is going skiing and they is liked is\n\n\nunacceptable\n\n\n\n\n\n\nAnna and Mike like to dance\n\n\nacceptable\n\n\n\n\n\n\nDocument Normalization\n\n\nDocument Normalizer example\nThe DocumentNormalizer extracts content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n\npipe = nlu.load(&#039;norm_document&#039;)\ndata = &#039;&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;&#039;\ndf = pipe.predict(data,output_level=&#039;document&#039;)\ndf\n\n\n\n\n\n\n\n\n\ntext\n\n\nnormalized_text\n\n\n\n\n\n\n\n\n&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;\n\n\nExample This is an example of a simple HTML page with one paragraph.\n\n\n\n\n\n\nWord Segmenter\n\n\nWord Segmenter Example\nThe WordSegmenter segments languages without any rule-based tokenization such as Chinese, Japanese, or Korean\n\n\npipe = nlu.load(&#039;ja.segment_words&#039;)\n# japanese for &#039;Donald Trump and Angela Merkel dont share many opinions&#039;\nja_data = [&#039;ãƒ‰ãƒŠãƒ«ãƒ‰ãƒ»ãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¢ãƒ³ã‚²ãƒ©ãƒ»ãƒ¡ãƒ«ã‚±ãƒ«ã¯å¤šãã®æ„è¦‹ã‚’å…±æœ‰ã—ã¦ã„ã¾ã›ã‚“&#039;]\ndf = pipe.predict(ja_data, output_level=&#039;token&#039;)\ndf\n\n\n\n\n\n\n\n\n\ntoken\n\n\n\n\n\n\n\n\nãƒ‰ãƒŠãƒ«ãƒ‰\n\n\n\n\n\n\nãƒ»\n\n\n\n\n\n\nãƒˆãƒ©ãƒ³ãƒ—\n\n\n\n\n\n\nã¨\n\n\n\n\n\n\nã‚¢ãƒ³ã‚²ãƒ©\n\n\n\n\n\n\nãƒ»\n\n\n\n\n\n\nãƒ¡ãƒ«ã‚±ãƒ«\n\n\n\n\n\n\nã¯\n\n\n\n\n\n\nå¤šã\n\n\n\n\n\n\nã®\n\n\n\n\n\n\næ„è¦‹\n\n\n\n\n\n\nã‚’\n\n\n\n\n\n\nå…±æœ‰\n\n\n\n\n\n\nã—\n\n\n\n\n\n\nã¦\n\n\n\n\n\n\nã„\n\n\n\n\n\n\nã¾ã›\n\n\n\n\n\n\nã‚“\n\n\n\n\n\n\nNamed Entity Extraction (NER) in Various Languages\n\n\nNLU now support NER for over 60 languages, including Korean, Japanese, Chinese and many more!   \n\n\n\n# Extract named chinese entities\npipe = nlu.load(&#039;zh.ner&#039;)\n# Chinese for &#039;Donald Trump and Angela Merkel dont share many opinions&#039;\nzh_data = [&#039;å”çº³å¾·ç‰¹æœ—æ™®å’Œå®‰å‰æ‹‰Â·é»˜å…‹å°”æ²¡æœ‰å¤ªå¤šæ„è§&#039;]\ndf = pipe.predict(zh_data, output_level=&#039;document&#039;)\ndf\n&gt;&gt;&gt; Output : [å”çº³å¾·, å®‰å‰æ‹‰]\n\n# Now translate [å”çº³å¾·, å®‰å‰æ‹‰] back to english with NLU!\ntranslate_pipe = nlu.load(&#039;zh.translate_to.en&#039;)\nen_entities = translate_pipe.predict([&#039;å”çº³å¾·&#039;, &#039;å®‰å‰æ‹‰&#039;])\n&gt;&gt;&gt; Output :\n\n\n\n\n\n\n\n\n\nTranslation\n\n\nChinese\n\n\n\n\n\n\n\n\nDonald\n\n\nå”çº³å¾·\n\n\n\n\n\n\nAngela\n\n\nå®‰å‰æ‹‰\n\n\n\n\n\n\nNew NLU Notebooks\n\n\nNLU 1.1.0  New Notebooks for new features\n\n\n\n\nTranslate between 192+ languages with marian\n\n\nTry out the 18 Tasks like Summarization Question Answering and more on T5\n\n\nT5 Open and Closed Book question answering tutorial\n\n\nTokenize, extract POS and NER in Chinese\n\n\nTokenize, extract POS and NER in Korean\n\n\nTokenize, extract POS and NER in Japanese\n\n\nNormalize documents\n\n\nAspect based sentiment NER sentiment for restaurants\n\n\n\n\nNLU 1.1.0 New Classifier Training Tutorials\n\n\nBinary Classifier training Jupyter tutorials\n\n\n\n\n2 class Finance News sentiment classifier training\n\n\n2 class Reddit comment sentiment classifier training\n\n\n2 class Apple Tweets sentiment classifier training\n\n\n2 class IMDB Movie sentiment classifier training\n\n\n2 class twitter classifier training\n\n\n\n\nMulti Class text Classifier training Jupyter tutorials\n\n\n\n\n5 class WineEnthusiast Wine review classifier training\n\n\n3 class Amazon Phone review classifier training\n\n\n5 class Amazon Musical Instruments review classifier training\n\n\n5 class Tripadvisor Hotel review classifier training\n\n\n\n\nNLU 1.1.0 New Medium Tutorials\n\n\n\n\n1 line to Glove Word Embeddings with NLU     with t-SNE plots\n\n\n1 line to Xlnet Word Embeddings with NLU     with t-SNE plots\n\n\n1 line to AlBERT Word Embeddings with NLU    with t-SNE plots\n\n\n1 line to CovidBERT Word Embeddings with NLU with t-SNE plots\n\n\n1 line to Electra Word Embeddings with NLU   with t-SNE plots\n\n\n1 line to BioBERT Word Embeddings with NLU   with t-SNE plots\n\n\n\n\nInstallation\n\n\n# PyPi\n!pip install nlu pyspark==2.4.7\n#Conda\n# Install NLU from Anaconda/Conda\nconda install -c johnsnowlabs nlu\n\n\n\nAdditional NLU ressources\n\n\n\n\nNLU Website\n\n\nAll NLU Tutorial Notebooks\n\n\nNLU Videos and Blogposts on NLU\n\n\nNLU on Github",
"date": "2021-01-25"
},
{
"vote": 1,
"title": "FastText to Spacy for custom ner",
"text": "[deleted]",
"date": "2021-01-24"
},
{
"vote": 18,
"title": "PhD internship in NLP at industrial research venue",
"text": "Hi NLP community,\n\n\nI am a second-year PhD student in Computer Science at a prestigious university in Europe (<100 ranking). \nI would want to start planning for a PhD internship at a research institute/industry in 2022/3. However, I am not too sure how to proceed with the process and what are typical requirements. Specifically, I work on the intersection of Structured Prediction and Bayesian Deep Learning, think deriving uncertainty for Named Entity Recognition. In my first year, I published a workshop paper at ICML and a large journal paper at JMLR.\n\n\nFor example, imagine I target one of the FAANG companies, what would they at least expect me to have done, e.g., publish at top ML conferences (ACL, NeurIPS, ICML, ICLR,...)? How can I increase my chances? Can someone maybe share their experiences on this? \n\n\nI appreciate your time in replying, cheers!",
"date": "2021-01-24"
},
{
"vote": 5,
"title": "Language Understanding with Knowledge-based Embeddings (LUKE) | Research Papers Summary 005",
"text": null,
"date": "2021-01-24"
},
{
"vote": 5,
"title": "What exists in the way of open source machine translation?",
"text": "In particular, is there any rule-based system that does a deeper analysis than Apertium? I'm interested in making a translator that attempts to preserve the meter and rhyme scheme of verse.",
"date": "2021-01-23"
},
{
"vote": 0,
"title": "How to cluster documents using Word2vec and K-means",
"text": null,
"date": "2021-01-22"
},
{
"vote": 3,
"title": "Does anyone know of a certificate program affiliated with a University that teaches NLP?",
"text": "I want to learn from a live instructor instead of a coursera class. I am ok with it being online. I just want something where I can ask a instructor questions right away.",
"date": "2021-01-22"
},
{
"vote": 1,
"title": "My project to debug and visualize Python code by using a combination of conventional static analysis tools and the attention based AI model. - Please ask me any questions!",
"text": "[deleted]",
"date": "2021-01-21"
},
{
"vote": 5,
"title": "withdraw from naacl submit to acl",
"text": "Is it okay to withdraw from naacl submit to acl, looks like my reviewers are being unprofessional, and my paper also has some changes. But I am submiting to the same venue. Would that be ok?",
"date": "2021-01-21"
},
{
"vote": 23,
"title": "My professor suggested that there are few business applications for NLP. Whatâ€™s your experience?",
"text": "[deleted]",
"date": "2021-01-20"
},
{
"vote": 23,
"title": "The field of natural language processing is chasing the wrong goal",
"text": null,
"date": "2021-01-19"
},
{
"vote": 3,
"title": "GPT-2",
"text": "How can I use GPT-2 to teach a model how to summarize a piece of random text?",
"date": "2021-01-19"
},
{
"vote": 1,
"title": "Are there any companies putting NLP into ethical issues?",
"text": "My question is inspired by this list \nhere\n. Do you know about any tech company that is putting NLP into ethical applications or at least care about it?",
"date": "2021-01-19"
},
{
"vote": 11,
"title": "[R] Finding The Words To Say: Hidden State Visualizations For Language Models",
"text": null,
"date": "2021-01-19"
},
{
"vote": 1,
"title": "A Closed-Domain QA System with a Community",
"text": "[deleted]",
"date": "2021-01-18"
},
{
"vote": 3,
"title": "Confused about whether I can add special tokens to a pretrained GPT-2 tokenizer",
"text": "I am using transformers/simpletransformers.\n\n\nI have sifted through the transformers source code, but the loose inheritance it uses (where parameters are all defined at the base and not validated when they are inherited) makes it really hard to decipher what is possible. Some Github issues suggest it's possible to add tokens, some not.\n\n\nTrial-and-error setting has just run me into more errors of arguments not set properly in simpletransformers, so I'm at the stage of trying to call the base tokenizer functions directly like so:\n\n\nlmm = LanguageModelingModel(&quot;gpt2&quot;, &quot;gpt2&quot;, args=args)\n## special_tokens is a list of strings\nlmm.tokenizer.add_special_tokens({&#039;additional_special_tokens&#039;: special_tokens})\nlmm.model.resize_token_embeddings(len(lmm.tokenizer)) \nlmm.train_tokenizer(train_files=[training_file, eval_file])\nlmm.train_model(train_file=training_file, eval_file=eval_file, args=args, verbose=True)\n\n\n\nWhich fails with an unclear cuda error, unfortunately.\n\n\nI am using special tokens to segment parts of conversational text. The model is used by a reddit chatbot.\n\n\nAm I wasting my time trying to add special tokens?\n\n\nUpdate: I seem to have it working by re-setting the vocab_size argument with the updated value when calling train_model. I'll find out soon whether there is any kind of improvement.",
"date": "2021-01-18"
},
{
"vote": 2,
"title": "Recommendations for extracting data",
"text": "Dear Reddit,\n\n\nI'm trying to extract/scrape the following data at scale and was looking for recommendations from the community on the best approach. \n\n\n\n\nNews articles from the web (\nI've found 'news-please' works well. Are there others?\n )\n\n\ne-mail Newsletters\n\n\nArxiv papers   \n\n\nFinancial earnings call transcripts. \n\n\nTwitter posts (\nI think the Twitter API might be the best approach\n)\n\n\nInstagram posts and comments\n\n\n\n\nIdeally I'd prefer python libraries but I'm open to any tool that does the job. \n\n\nThank You for your help",
"date": "2021-01-18"
},
{
"vote": 1,
"title": "Text mining",
"text": "[deleted]",
"date": "2021-01-17"
},
{
"vote": 17,
"title": "Temporally-Informed Analysis of Named Entity Recognition | Research Papers Summary 004",
"text": null,
"date": "2021-01-17"
},
{
"vote": 7,
"title": "How to learn (more about) GPT-2",
"text": "Dear Reddit-Fam\n\n\nI'm looking for resources about GPT-2 (for natural language generation), especially for training, fine-tuning (maybe in another language also). It seems like documentations or courses that go in-depth are very scarce at the moment (maybe/hopefully i'm false).\n\n\nGreeeets",
"date": "2021-01-17"
},
{
"vote": 37,
"title": "How to make your NLP system multilingual",
"text": "So you have an NLP system - a chat bot, a search engine, NER, a classifier... - working well for English.\n\n\nAnd you want to make it work for other languages, or maybe for all languages.\n\n\nWe see 3 basic approaches:\n\n\n\n\nmachine-translating at inference (or query) time\n\n\nmachine-translating labelled training data (or search indices), and training a multilingual model\n\n\nzero-shot approaches with a multilingual LM like BERT or LASER\n\n\n\n\nWhen to use which approach?\n\n\nMachine-translating at inference time [2] is easiest to start with, but it's usually a bad idea.  It's the default at major US tech enterprises, from what I've seen, and even at really smart ML startups like Aylien.  And it's often suggested in this sub.\n\n\nIn Europe, where building a multilingual system is super important, we've even seen researchers \nhuman-labelling\n for every language, and ML startups \nhuman-translating\n labelled training data, or doing rules-based transliteration with \nhuman post-editing\n.\n\n\nAs a guy who thinks around the clock about machine translation risk and automation, all this unscalableness pains me to see.\n\n\nSo we have shared some open guides based on the work of our clients who implemented \nmultilingual search\n.\n\n\nNerses Nersesyan from Polixis and I will give a workshop on this at Applied Machine Learning Days in March.\n\n\nhttps://appliedmldays.org/events/amld-epfl-2021/workshops/how-to-make-your-nlp-system-multilingual",
"date": "2021-01-16"
},
{
"vote": 5,
"title": "[p] Ecco â€“ See what your NLP language model is â€œthinkingâ€",
"text": null,
"date": "2021-01-16"
},
{
"vote": 1,
"title": "T5 Discord",
"text": "[removed]",
"date": "2021-01-16"
},
{
"vote": 12,
"title": "What are non-english language missing in order to reach english-like NLP performances?",
"text": "Hi all,\nI am currently working on some NLP tasks on Italian corpuses. I am noticing that, in general, Italian language models perform worse than their english siblings on similar tasks. I Indeed found several articles online raising the problem of low performance in non english NLP models.\n\n\nSo my question is: is this only a problem of low quality /small sized training datasets? Or is it something else? How would you address this problem if you had infinite ?money?\n\n\nThanks!",
"date": "2021-01-15"
},
{
"vote": 2,
"title": "Any good model which can be used for fine-tuning",
"text": "I am doing a search NLP engine which takes short query of 6-7 tokens  of  type question or a statement which wants to find a answer from database. Can you guys give me a good starting point to have a good pretrained model for short queries and fine tune different tasks like NER, entity relationship, pos tagging. And also data must be created manually. How Fine-tuning can be done wtih  minimal training data.?",
"date": "2021-01-15"
},
{
"vote": 2,
"title": "Bullet point summarizers?",
"text": "Can anyone point me to some literature that would be talking about models where you input a text and it returns bullet point summary? I found a very little number of such. Is this even a thing?",
"date": "2021-01-15"
},
{
"vote": 6,
"title": "Upcoming shared tasks in NLP 2021",
"text": "Any shared tasks in the year 2021 for NLP happening soon. \n\n\nAnyone needs collaborator for shared task. ?",
"date": "2021-01-15"
},
{
"vote": 1,
"title": "Why are language modeling pre-training objectives considered unsupervised?",
"text": "Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it's supervised learning and if not then it's unsupervised.\n\n\nI'll take the masked language modeling (MLM) that BERT (\nDevlin et al., 2019\n) and many other subsequent language models use.\n\n\nAccording to the original paper:\n\n\n> ...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n\n\nIf we just replace a certain percentage of tokens with \n[MASK]\n randomly, don't we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn't this be considered supervised learning?\n\n\nMy argument is analogous for the next sentence prediction (NSP) task.",
"date": "2021-01-15"
},
{
"vote": 0,
"title": "Introduction to chatbots: what, why and how?[chatbot creation using dialogflow]",
"text": null,
"date": "2021-01-15"
},
{
"vote": 2,
"title": "How can I extract the topic of a sentence and then the adjective related to it in spacy?",
"text": "Hello everyone! I've posted here a couple of times and you guys helped me a lot. The last time, I tried to extract NOUN and ADJECTIVE together, but the output had a lot of errors. Now I want to try a different approach, to first identify the topic of a sentence and then extract the adjective, like this: \n\n\n&#x200B;\n\n\nThe teacher is calm and intelligent.\n\n\nThe material is bad.\n\n\nI want an output that is like:\n\n\n{teacher calm}, {teacher intelligent} and {material bad}\n\n\nCan anyone help me? \n\n\nAlso, if anyone has a different idea about how to solve this problem in a better way, I'll really appreciate your reply too!  \n\n\nThanks in advance!",
"date": "2021-01-15"
},
{
"vote": 8,
"title": "Is there any case ELMo can be a better choice than BERT/Transformer?",
"text": "Hi, here's one very simple question.\n\n\nCan you guys think of any case where ELMo can be a clearly better design choice than BERT/Transformer family? I tried, but I couldn't think of any case.",
"date": "2021-01-15"
},
{
"vote": 0,
"title": "Folksâ€™Talks video game. Graphic Research Interface. Explanatory note for current temporary interface.",
"text": null,
"date": "2021-01-14"
},
{
"vote": 45,
"title": "Article: \"Googleâ€™s new trillion-parameter AI language model is almost 6 times bigger than GPT-3\"",
"text": null,
"date": "2021-01-14"
},
{
"vote": 2,
"title": "Great papers for distantly supervised learning?",
"text": "I have been searching for papers on distant supervision for named-entity extraction on Google scholar. Does anyone have some suggestions for high impact papers that worked on distant supervision in named-entity extraction?",
"date": "2021-01-13"
},
{
"vote": 1,
"title": "Information extraction and comparison",
"text": "How to extract information from sentences which is present in one excel file and compare it with 700 standard sentences  with unique codes  which is present in another Excel sheet and assign the sentence with that particular unique code of the sentence to which it is almost similar ?",
"date": "2021-01-13"
},
{
"vote": 1,
"title": "[D] Help Document Ordering",
"text": "Hi, I am currently working on a problem to order/rank help documents in the most useful way. The ranking criteria is a bit complex.\nThe criteria should be based on :  \n\n\n\n\nhow useful it was in the past \n\n\n\n\nhow popular (or trending) it is amongst the users \n\n\n\n\nalso how much information(relative to other documents in the list) does the document have.\n\n\n\n\n\n\n As of now I was thinking I could use some weighted window method to compute the popularity and trending aspect based on click through rate or likes/dislikes the documents get over the window of time.\nI wanted to understand how I could address the information aspect of the criteria i.e what methods would I use to compare the documents based on the amount of information they would have to rank accordingly.\nI would also appreciate ideas that would help me combine scores from both these aspects to get to a final score to rank the documents.",
"date": "2021-01-13"
},
{
"vote": 1,
"title": "Create search bar NLP recommendation in Python",
"text": "Like the title says, I would like to create an NLP project which consists of a search bar and when someone types something domain specific, it can start generating suggestions. The problem is, I want these suggestions to only be domain specific. Can someone guide me to tutorials or frameworks that can help me. Thank you.\n\n\nJust as an example - I would create a bank and clients would type up specific processes in that bank. So if a client were to type \"I would like\" it would generate some suggestions like \"I would like to request a loan\" and \"I would like to speak to customer service\" and \"I would like to open a bank account\"\n\n\nThanks again.",
"date": "2021-01-12"
},
{
"vote": 6,
"title": "Is there any tool (preferably for python) that helps with extracting all entities and relationships in a path of a RDF graph?",
"text": "I know that property paths in SPARQL can aid you with that. However, you need to be really specific with your nodes and relationships in between two entity nodes that mark the start and the end of the path, if you want to extract them. I am searching for a arbitrary way to do this.\n\n\nWhat i want is basically:\n\n\nInput: start entity, amount of max hops for the path (f.e. 4), RDF knowledge graph\n\n\nExtract all paths with hops <= 4 beginning from the start entity. A hop is basically a jump between entities via SPO representation.\n\n\nOutput:\n\n\n[start entity - relationship 1.1 - entity 1.1 - relationship 2.1 - entity 2.1 - relationship 3.1 - entity 3.1 - relationship 4.1 - entity 4.1],\n\n\n[start entity - relationship 1.2 - entity 1.2 - relationship 2.2 - entity 2.2 - relationship 3.2 - entity 3.2 - relationship 4.2 - entity 4.2],\n\n\netc.\n\n\nDoes not need to be in array representation. All i want is to be able to analyze a path programmatically. Im am currently considering doing this manually via loops and multiple queries on a local RDF TTL-graph.",
"date": "2021-01-12"
},
{
"vote": 13,
"title": "T5: Exploring Limits of Transfer Learning with Text-to-Text Transformer | Research Paper Walkthrough",
"text": null,
"date": "2021-01-12"
},
{
"vote": 1,
"title": "Language Exchange Survey",
"text": "[removed]",
"date": "2021-01-12"
},
{
"vote": 8,
"title": "Is there a method for restyling a sentence given another sentence?",
"text": "Iâ€™m working on a problem where I need to take two sentences:\n\n\n\n\nInput sentence (this needs to be restyled)\n\n\nReference sentence (we match the style of this sentence)\n\n\n\n\nI need to rephrase the input sentence to be more similar to the reference sentence.\n\n\nVery simple example â€” if the reference sentence is needlessly verbose, has misspellings and lots of commas, we rephrase the input sentence to have commas and misspellings, and add words to make it more verbose.\n\n\nHow should I approach this problem?",
"date": "2021-01-11"
},
{
"vote": 3,
"title": "How to know if my language model saw a phrase during training",
"text": "Is there a way to know if there was a phrase in an LMâ€™s training data w/o looking at the training data itself? It is not as trivial as looking at the vocab: for example if the vocab contains Donald and Trump, doesnâ€™t mean that it the phrase â€œDonald Trumpâ€ was in the training data.",
"date": "2021-01-11"
},
{
"vote": 1,
"title": "Language Exchange Survey",
"text": "[removed]",
"date": "2021-01-11"
},
{
"vote": 3,
"title": "[R]: Twitter Data crawling for research",
"text": "Hello All\n\n\nI am looking to crawl data for academic research (most likely need to release/open-source the dataset).  Do you guys know the license? (I have already read their webpage, terms and condition), however, I don't find too many open source twitter data set, wondering if there is any hidden terms that I am not awared off?",
"date": "2021-01-11"
},
{
"vote": 3,
"title": "Is there an NLP technique for rephrasing question-answer pairs as a full sentence?",
"text": "Is there an NLP technique for rephrasing question-answer pairs as a full and grammatically correct sentence? For example:\n\n\nQuestion: Where does Joe live?\n\n\nAnswer: Joe lives in Los Angeles.\n\n\nIâ€™ve looked into Answer Ellipsis, which is essentially the opposite of this issue and what most Machine Reading Comprehension or Question Answering systems already generate. For example, the answer to the above with answer ellipsis would be â€œLos Angelesâ€.\n\n\nFor the sake of reader comprehension, I want to turn question-answer pairs into something understandable. What techniques exist to do this?",
"date": "2021-01-10"
},
{
"vote": 3,
"title": "How do you evaluate a seq2seq QA system?",
"text": "[deleted]",
"date": "2021-01-10"
},
{
"vote": 2,
"title": "I have been doing some sentiment analysis, and the results have got me wondering",
"text": "[deleted]",
"date": "2021-01-10"
},
{
"vote": 10,
"title": "Word Embedding",
"text": "[deleted]",
"date": "2021-01-10"
},
{
"vote": 4,
"title": "Probabilistic verbal subcategorisation frames research",
"text": "This is more so on the theoretical side of things but I'm curious to know if anybody is familiar with any research that deals with calculating the likelihood that a verb will take a particular set of arguments as it's complement, in particular the likelihood that a verb will take the overt complementiser \nthat\n as in [think + [C[that]]? \n\n\nThere's a tonne of theoretical linguistic research into this but I'm curious to learn about any different approaches.",
"date": "2021-01-09"
},
{
"vote": 4,
"title": "Extrapolating Materials Science domain knowledge through context-free embeddings.",
"text": "https://www.sciencedirect.com/science/article/pii/S2589004220311196\n\n\nThis paper talks about how context-free word embeddings can capture the domain knowledge in a corpus of papers and can be used to extrapolate the same by finding novel polymers for existing applications.\n\n\nLet me know what you think!",
"date": "2021-01-08"
},
{
"vote": 5,
"title": "How to construct a taxonomy from news articles?",
"text": "I am trying to build a taxonomic representation of football related stuffs from football news articles.",
"date": "2021-01-08"
},
{
"vote": 3,
"title": "How to Fine-tune / Use GPT-3 Alternatives for Text Generation",
"text": "[deleted]",
"date": "2021-01-07"
},
{
"vote": 23,
"title": "500 AI, Machine learning, Deep learning, Computer vision, and NLP Projects with code",
"text": null,
"date": "2021-01-07"
},
{
"vote": 1,
"title": "[R] Microsoft DeBERTa Tops Human Performance on SuperGLUE NLU Benchmark",
"text": "[removed]",
"date": "2021-01-06"
},
{
"vote": 20,
"title": "Has anyone deployed a BERT like model across multiple tasks (Multi-class, NER, outlier detection)? Seeking advice.",
"text": "So I have a custom pre-trained RoBERTa model that I want to fine tune with NER, multi class classification, and outlier / new class detection.  Currently using Huggingface Transformers for pre-training and fine-tuning.\n\n\nIâ€™ve recently realized that naively fine tuning on each task separately would require loading in 3 instances of the pre trained model.  My understanding is that fine tuning makes small adjustments to the embedding layers.  Fine tuning separately but using the initial embedding may take a hit to performance.  \n\n\nAny thoughts or experience on fine tuning across all 3 tasks simultaneously? Or maybe some hacky approach?  Also, if anyone has anyone insights on parameter tuning (specifically BPE vocab size), scheduling, or noising.  Any help would be much appreciated!",
"date": "2021-01-06"
},
{
"vote": 2,
"title": "[Tutorial] on Understanding Representation and Feature Learning With Autoencoders",
"text": "For many tasks, it is impossible to know what features should be extracted.\n\n\nAlan Turing and his colleagues deciphering the enigma code observed the patterns that were regularly appearing in the messages. This is where the idea of representation learning truly comes into view.\n\n\nIn representation learning, the machine is provided with data and it learns the representation. By reducing data dimensionality you can easier find patterns, anomalies and reduce noise.\n\n\nautoencoders tutorial",
"date": "2021-01-06"
},
{
"vote": 3,
"title": "Brat-standoff to CoNLL formatted annotations",
"text": "Hi all,\n\n\nI have written a quick script to convert Brat standoff formatted annotation files (*.ann and *.txt) to CoNLL formatted annotation files which can be used for training NER models through the HuggingFace transformers library. Came across this problem a few times so thought this might be useful to other people as well.\n\n\nhttps://github.com/pranav-s/brat2CoNLL",
"date": "2021-01-06"
},
{
"vote": 42,
"title": "Free NLP assignments",
"text": "NLP educators-- I made some free mastery-based assignments to reinforce learning in your NLP classes this semester. These assignments leverage cognitive neuroscience principles proven to optimize knowledge retention & adapt to unique student needs:\n\n\nhttps://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing",
"date": "2021-01-05"
},
{
"vote": 14,
"title": "Two models now exceed the human baselines score for the SuperGLUE natural language understanding benchmark",
"text": null,
"date": "2021-01-05"
},
{
"vote": 5,
"title": "Inquisite: Open source web interface and JSON API for Gensim summaries and Spacy+nltk analysis of texts and URLs",
"text": null,
"date": "2021-01-05"
},
{
"vote": 2,
"title": "Anything that I should be doing alongside Jurafsky &amp; Martin?",
"text": "[deleted]",
"date": "2021-01-05"
},
{
"vote": 18,
"title": "WARp - Word-Level Adversarial Reprogramming",
"text": null,
"date": "2021-01-05"
},
{
"vote": 9,
"title": "[P] 611 text datasets in 467 languages in the new v1.2 release of HuggingFace datasets library",
"text": null,
"date": "2021-01-05"
},
{
"vote": 3,
"title": "Coding Attention is All You Need in PyTorch for Question Classification",
"text": "Hi Guys,\n\n\nRecently, I have posted a series of blogs on medium regarding Self Attention networks and how can one code those using PyTorch and build and train a Classification model. In the series, I have shown various approaches to train a classification model for the dataset available \nhere\n.\n\n\nPart - 1: \nhttps://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76\n\n\nPart - 1.1: \nhttps://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757\n\n\nPart - 2: \nhttps://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a\n\n\nPart - 3: \nhttps://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451\n\n\nHave a nice read. Share if you like the content. Comment for any discussions.\n\n\nThanks",
"date": "2021-01-05"
},
{
"vote": 14,
"title": "[R] Interfaces for Explaining Transformer Language Models",
"text": null,
"date": "2021-01-05"
},
{
"vote": 1,
"title": "Language acquisition by virtual agent (The Folksâ€™Talks game project)",
"text": null,
"date": "2021-01-05"
},
{
"vote": 1,
"title": "On Generating Extended Summaries of Long Documents | Research Paper Walkthrough",
"text": "[deleted]",
"date": "2021-01-05"
},
{
"vote": 4,
"title": "Best way to encode PoS bigrams and trigrams?",
"text": "Hi everyone! I have a pandas df, each row has one or more sentences written by a certain person. I extracted several features for each row, including PoS bigrams and trigrams. Now I'd like to use all the features I've extracted to solve a multiclass classification problem. The problem is that PoS bigrams and trigrams consists of a list of tuples and the tuples consists of two or three strings (e.g. bigrams [(INTJ, PUNCT), (PUNCT, ADJ), (ADJ, PROPN)]. What would be the best way to transform these bigrams/trigrams into numbers that I can compute with a machine learning algorithm?",
"date": "2021-01-04"
},
{
"vote": 13,
"title": "Quadratic complexity of Transformers solved with Linear Transformers?",
"text": "Hey,\n\n\nI'm just wondering if the problem of the quadratic complexity of the self-Attention mechanism is considered to be solved by the introduction of, for example, Linear Transformers in 2020. I'd assume that this is probably still an open research question as I guess that the most-used state-of-the-art Transformers models still scale quadratically with respect to the input sequence length, right? Are there still valid reasons for using the vanilla self-Attention mechanism that scales quadratically instead of any linear approximation/ version of it?\n\n\nAny thoughts on that?",
"date": "2021-01-04"
},
{
"vote": 0,
"title": "Trying to identify intent, impact and duration from the text ..Any ideas please share",
"text": "I am using spacy to identify the intent using part of the speech concept, but that seems not a full proof technique",
"date": "2021-01-04"
},
{
"vote": 17,
"title": "CoreWeave has agreed to provide training compute for EleutherAI's open source GPT-3-sized language model",
"text": "Please see post \nOpen-source GPT-3 alternative coming soon?\n. (I couldn't crosspost that post here because image post crossposts are not allowed here.)",
"date": "2021-01-03"
},
{
"vote": 9,
"title": "Frontier of probabilistic NLP, will deep learning always have a final say in the field?",
"text": "Hi folks,\n\n\nI am curious to know what do you think about the title. Do you think that probabilistic ML can catch up with SOTA set up by deep learning models? Could you recommend me some more recent papers where probabilistic ML (Bayesian non-parametric models in particular) outperformed deep learning models for language tasks?",
"date": "2021-01-03"
},
{
"vote": 2,
"title": "A new NLP podcast to catch the latest and greatest works in NLP. Beginning episodes will cover papers that we liked in ACL and EMNLP on most popular NLP tasks.",
"text": null,
"date": "2021-01-02"
},
{
"vote": 1,
"title": "Making Pre-trained Language Models Better Few-shot Learners",
"text": null,
"date": "2021-01-01"
},
{
"vote": 2,
"title": "BiLSTM + Attention Layer",
"text": "Hi everyone, I am newbie in NLP. I am trying to implement word2vec, + BILSTM+Attention for binary sentiment analysis problem. I search the internet, however most of them already expired and can not be run anymore. Please help.. Thank you",
"date": "2020-12-30"
},
{
"vote": 34,
"title": "Day 365 of #NLP365 - NLP Papers Summary â€“ A Survey On Knowledge Graph Embedding",
"text": "Day 365!!!\n\n\n1 day left till the last day of #NLP365 (and 2020)!! <3\n\n\nToday's post covers a paper summary on the knowledge graph embedding survey paper! Check it out :) Tomorrow, I will wrap things up with the NLP365 project so keep an eye out for that!\n\n\nhttps://ryanong.co.uk/2020/12/30/day-365-nlp-papers-summary-a-survey-on-knowledge-graph-embedding/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out Research Papers Summary videos:\n\n\nEpisode 001 - a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4\n\n\nEpisode 002 - TransE: \nhttps://www.youtube.com/watch?v=Ka8d58M4RzQ",
"date": "2020-12-30"
},
{
"vote": 11,
"title": "Text preprocessing for transformers",
"text": "Does anyone know of a publication that explores the effect of text preprocessing on latest NLP techniques like transformers?",
"date": "2020-12-30"
},
{
"vote": 4,
"title": "How to select the number of dimensions for word embedding?",
"text": null,
"date": "2020-12-30"
},
{
"vote": 19,
"title": "Experimenting with grounded language learning to teach computers meaning",
"text": null,
"date": "2020-12-29"
},
{
"vote": 1,
"title": "Guided Topic Modeling and Model Selection",
"text": "I am working on a project where I have a set of documents that all have the same major themes, but in different levels of detail or depth. So I know the salient topics already, but when I have implemented topic modeling techniques, the results or topics I get are rather mediocre. \n\n\nI have followed some online tutorials to make sure my general pathway is correct, but what I do is text extraction (get raw text files from the docs), clean the documents, remove abnormal characters, remove stop words, create n-grams, and lemmatize after which I follow the standard procedure for Gensim LDA, and Mallet LDA. I have tried tuning these parameters but the resulting topics do not capture the salient topics which I know exist in the document. \n\n\nThe alternative path I see right now is using a Guided LDA in which I create topic vectors and use a similarity metric such as cosine similarity to get the similarity value of each topic vector to each document vector and use that as my pseudo topic modeling, however, I would prefer this result be captured in a unsupervised or semi-supervised method. \n\n\nDoes anyone have any advice on other libraries or modeling methods I could try? Or links that they think may be useful in fine-tuning this approach? Truly would appreciate any advice.",
"date": "2020-12-29"
},
{
"vote": 12,
"title": "A simplistic architecture for entity linking over knowledge graphs (Non-BERT), for QA. It also works on lowercased questions and has good zero-shot performance on unseen data and KG. The system is built on freebase (discontinued) but can be applied to Wikidata KG as well. Feedbacks are welcome",
"text": null,
"date": "2020-12-29"
},
{
"vote": 2,
"title": "Day 362 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes XIII",
"text": "Day 362.\n\n\nHappy holidays!! <3\n\n\nToday's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/27/day-362-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-xiii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out Research Papers Summary videos:\n\n\nEpisode 001 - a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4\n\n\nEpisode 002 - TransE: \nhttps://www.youtube.com/watch?v=Ka8d58M4RzQ",
"date": "2020-12-27"
},
{
"vote": 7,
"title": "Getting cogcomp-nlp with SRL to work",
"text": "I followedÂ theÂ cogcomp-nlp/pipeline/README.md (see \nhere\n) using eclipseÂ with maven to build the modules. I am also usingÂ cogcomp-nlpyÂ for python. I'd like to use the SRL models with cogcomp.\n\n\nThe issue now seems to be an issue with the remote connect to getÂ cogcomp-datastore elements likeÂ ner-model-enron-conll-all-data.zip,Â ner-model-ontonotes-all-data.zip, as well as others.\n\n\nWith the repositoryÂ m2repo (see \nhere\n)Â I could get download the files for dirs gazetteers\\1.6\\gazetteers and some others manually, but concerning the ner-models,  I could not figure out which files correspond to the different ner models (srl needs ner models first),Â for example in the repo, there is the fileÂ illinois-ner-model-3.3-CoNLL_enron.jar -> is this the zip for dirÂ cogcomp-datastore\\readonly.edu.illinois.cs.cogcomp.ner\\4.0\\ner-model-enron-conll-all-data? Also where do I need to store the models, that is what is the exact subdir and the modelname for the files? For example, theÂ illinois-ner-model-3.3-CoNLL_enron.jar containsÂ the file CoNLL_enron.model.level1.lex, is this the proper name for illinois-ner package?\n\n\nIt would be great to get cogcomp-pipeline working with the required files. Has anyone got it working or maybe a snapshot of all file ready for deployment? I worked through the java code (all dependencies for Â cogcomp-nlp/pipeline), and it seems quite hard to get it working. The demo \nhere\n was quite good, so I am quite keen to get it to work as feature in a hate speech classification task.",
"date": "2020-12-26"
},
{
"vote": 7,
"title": "Day 361 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes XII",
"text": "Day 361.\n\n\nHappy holidays!! <3\n\n\nToday's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/26/day-361-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-xii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out Research Papers Summary videos:\n\n\nEpisode 001 - a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4\n\n\nEpisode 002 - TransE: \nhttps://www.youtube.com/watch?v=Ka8d58M4RzQ",
"date": "2020-12-26"
},
{
"vote": 6,
"title": "What's the difference between NLTK's BLEU score and SacreBLEU?",
"text": "Hi. I'm curious if anyone is familiar with the difference between using NLTK's BLEU score calculation and the SacreBLEU library.\n\n\nIn particular, I'm using both library's sentence BLEU scores, averaged over the entire dataset. The two give different results:\n\n\n&gt;&gt;&gt; from nltk.translate import bleu_score\n&gt;&gt;&gt; from sacrebleu import sentence_bleu\n&gt;&gt;&gt; print(len(predictions))\n256\n&gt;&gt;&gt; print(len(targets))\n256\n&gt;&gt;&gt; prediction = &quot;this is the first: the world&#039;s the world&#039;s the world&#039;s the \\\n... world&#039;s the world&#039;s the world&#039;s the world&#039;s the world&#039;s the world&#039;s the world \\\n... of the world of the world&#039;&quot;\n...\n&gt;&gt;&gt; target = &quot;al gore: so the alliance for climate change has launched two campaigns.&quot;\n&gt;&gt;&gt; print(bleu_score.sentence_bleu([target], prediction))\n0.05422283394039736\n&gt;&gt;&gt; print(sentence_bleu(prediction, [target]).score)\n0.0\n&gt;&gt;&gt; print(sacrebleu.corpus_bleu(predictions, [targets]).score)\n0.678758518214081\n&gt;&gt;&gt; print(bleu_score.corpus_bleu([targets], [predictions]))\n0\n\n\n\nAs you can see, there's a lot of confusing inconsistencies going on. There's no way that my BLEU score is 67.8%, but it's also not supposed to be 0% (there are a lot of overlapping n-grams like \"the\").\n\n\nI'd appreciate it if anyone could shed some light on this. Thanks.",
"date": "2020-12-26"
},
{
"vote": 5,
"title": "Day 360 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes XI",
"text": "Day 360.\n\n\nMERRY CHRISTMAS everyone!!! and Happy holidays!! <3\n\n\nToday's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/25/day-360-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-xi/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out Research Papers Summary videos:\n\n\nEpisode 001 - a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4\n\n\nEpisode 002 - TransE: \nhttps://www.youtube.com/watch?v=Ka8d58M4RzQ",
"date": "2020-12-25"
},
{
"vote": 10,
"title": "What does it mean to say that BLEU is a \"corpus-level metric?\"",
"text": "Hi. I'm currently working on a machine translation project and was using BLEU. I've been using SacreBLEU but have also tried out NLTK's BLEU.\n\n\nI've read quite frequently that \"BLEU is not meant to be computed at the sentence level, but at the corpus level.\" I'm a little unsure of what that's supposed to mean. What if I just want to compare sentence by sentence? Wouldn't that be considered to be calculating at the \"sentence level?\"",
"date": "2020-12-25"
},
{
"vote": 13,
"title": "Using Pytorch model in Sentence Transformer",
"text": "Is there any I can load my pretrained Pytorch bert-base-uncased model (.bin file) to Sentence Transformers?\n\n\n modelÂ =Â SentenceTransformer(&#039;/content/pytorch-model&#039;)\n\n\n\nI am looking to convert pytorch bert model to something that is accepted by \nSentenceTransfromer\n .\n\n\nEdit\n\n\nThe author of library had posted this code in one of the issues on github.\n\n\nword_embedding_model = models.BERT(&#039;path/to/your/model/stored/using/hugginface/functions&#039;)\n\n\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n\n\nIn order for this to work I need to manually change name of \"bert-config.json\" to \"config.json\" and add a \n&quot;model_type&quot; : &quot;bert&quot;\n key-value pair to it. This works and now I can use the model as intended but I don't think this should have been necessary.",
"date": "2020-12-25"
},
{
"vote": 7,
"title": "Spotting conditionals in a sentence?",
"text": "Hi everyone!\n\n\nI'm trying to figure out a way to programmatically identify sentences that have a '\nconditional\n' in them, so that I can earmark them for conversion into rules.\n\n\nThe presence of 'If-Then' is an obvious one, but conditionals are expressed can be expressed in so many ways, like 'in order to', 'in the situation where', 'assuming that', 'can only' .... and the list goes on, seemingly infinitely.\n\n\nWhat would be a \ncatch-all\n way to know if a sentence represents conditional logic, without having to resort to word/phrase lists? Any ideas or reading suggestions would be very appreciated!",
"date": "2020-12-24"
},
{
"vote": 16,
"title": "Using BERT embeddings in the embedding layer of an LSTM",
"text": "Hi everyone, an NLP noob here working with BERT and Transformers in general for the first time. I wanted to ask if anyone has come across an implementation of an LSTM with BERT pre-trained embeddings rather than the regular word2vec or any other static embeddings? The task would involve fine tuning the embeddings as the model trains for a task such as sentence entailment. I wanted to see if there is any performance gain over using BERT embeddings rather than word2vec embeddings with the same LSTM model. Let me know if my thinking is correct! Also, if anyone has implemented such a model or knows what would be the best of doing so please let me know!",
"date": "2020-12-24"
},
{
"vote": 2,
"title": "Trying to build a lemmatizer from scratch",
"text": "I'm looking for sources that can help me build a lemmatizer from scratch. Any directions would be really helpful. I was also wondering what are some of the different methodologies (if any) that are adopted over the years in developing a lemmatizer.\n\n\nThanks in advance.",
"date": "2020-12-24"
},
{
"vote": 10,
"title": "Manual Translations Future?",
"text": "Hey all! I'm thinking about getting in to the translation industry, I love languages and linguistic study so I thought it would be a natural choice. My main concern is that with the growing presence of AI translation technology, translators will become less and less needed, I thought I would ask here as you guys literally make the technology that will inevitably kill this industry lol... what does everyone here think about this? I am really keen to pursue this path but if there is a roadblock ten years down the line then I think I should reconsider. any insights would be much appreciated, thank you!",
"date": "2020-12-23"
},
{
"vote": 7,
"title": "Can we use many [MASK] with RoBERTa model ?",
"text": "Hi guys i need your help please.\n\n\nI am using the CamemBert model from Transformers to process a French text.\n\n\nApparently, in a single treatment, only one [MASK] can be used.\n\n\nBut my work consists for example of detecting several words mask them and replacing them with other words generated by my model which is Camembert (which is a type of RoBERTa ).\n\n\nSo my question is: Can we many [MASK] in one sentence instead of one ?\n \n\n\nThanks for your help in advance .",
"date": "2020-12-23"
},
{
"vote": 6,
"title": "RENT + La Boheme + Markov = La Re Bohent (@DigitalLibretto) - An NLP Project I made for a grad school final at Tisch/NYU launching December 24th, 9PM Eastern Standard Time... code is in the link in the description. (Please be kind.) :)",
"text": null,
"date": "2020-12-23"
},
{
"vote": 3,
"title": "NLP for author disambiguation?",
"text": "Does anyone know of any research or techniques on predicting the identity of authors based on text samples?\n\n\nPerhaps as a Siamese network type problem: i.e., given two passages, are they written by the same author? Or possibly clustering/classifying a collection of text passages according to their authors.",
"date": "2020-12-22"
},
{
"vote": 1,
"title": "Day 357 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes VIII",
"text": "Day 357.\n\n\nWe are now moving onto knowledge acquisition. Today's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/22/day-357-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-viii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out Research Papers Summary videos:\n\n\nEpisode 001 - a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4\n\n\nEpisode 002 - TransE: \nhttps://www.youtube.com/watch?v=Ka8d58M4RzQ",
"date": "2020-12-22"
},
{
"vote": 6,
"title": "Understanding Facebookâ€™s ReBeL â€” A Noteworthy Step in Artificial General Intelligence",
"text": null,
"date": "2020-12-21"
},
{
"vote": 7,
"title": "Day 355 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes VI",
"text": "Day 355.\n\n\nWe are now moving onto knowledge acquisition. Today's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/20/day-355-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-vi/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out episode 001 of Research Papers Summary, covering a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4",
"date": "2020-12-20"
},
{
"vote": 4,
"title": "Looking for a simple paraphrasing library that works in spanish",
"text": "I need something solid, easy to use, e.g. I give a text, and receive a paraphrased text in return.  \n\n\ntweaking optional. Does something exist already or do I have to do heavy lifting myself?\n\n\n&#x200B;\n\n\nI've seen this project but it looks fairly old:\n\n\nhttps://github.com/vsuthichai/paraphraser\n\n\nis there something better, newer?",
"date": "2020-12-19"
},
{
"vote": 5,
"title": "Move word vectors closer to each other?",
"text": "[deleted]",
"date": "2020-12-19"
},
{
"vote": 1,
"title": "Data Augmentation in NLP",
"text": null,
"date": "2020-12-18"
},
{
"vote": 5,
"title": "Need some help in calculating confusion matrix using NLTK",
"text": "I was writing a small  twitter sentiment analysis project and used \nthis\n guide as help. I'm trying to plot the confusion matrix but I'm unable to, possibly because of the data format.\nHere's\n the same file in notebook for easier access of the full file.\n\n\nSince I wasn't able to use either NLTK or Scikit-Learn's inbuilt confusion_matrix methods, I tried to implement one on my own. The accuracy calculated through the matrix dips to around 84%, while the accuracy calculated by the inbuilt nltk function displays 99%, which led me to believe I've messed up the calculation for confusion matrix. It'd all be better if I could just use the library function. Can anyone help?",
"date": "2020-12-18"
},
{
"vote": 1,
"title": "How to get Cosine Similarity as a single value when comparing corpora? (ie, not a matrix)",
"text": "[deleted]",
"date": "2020-12-17"
},
{
"vote": 51,
"title": "HuggingFace releases new NLP model which works with structured tables on their inference API",
"text": "HuggingFace just released version v4.1.1 of their \ntransformers\n library, which includes TAPAS, a model by GoogleAI.\n\n\nThis model can be prompted with a query and a structured table, and answers the queries given the table.\n\n\nHuggingFace makes it available directly on their website to test it out: \nwhich repository is coded in Rust?\n The full release is available here: \nv4.1.1",
"date": "2020-12-17"
},
{
"vote": 5,
"title": "\"Augmenting\" clustering, is this a hack?",
"text": "Hello,\n\n\nI'm working on a document clustering task. After a lot of experimentation with different features and clustering algorithms, I've got the best result using agglomerative clustering with average linkage.\n\n\nHowever even if the clusters look great sometimes two clusters are very similar and ideally they'd be one. I can't increase the distance threshold anymore as then I start getting clusters merged when they shouldn't. Also as I mentioned above I've tested many other algorithms (DBSCAN, HDBSCAN, OPTICS, agglomerative with single and complete linkages...) but agglomerative with average linkage is the one that gives me the best results.\n\n\nSo what I've done is this: after I get the initial clusters I calculate two metrics:\n\n\n\n\nthe minimum distance between every pair of clusters (closest data points), similar to what single linkage would do\n\n\nthe distance between the most representative data points in every cluster (obtained by taking the data point with the lowest average distance to its peers in the cluster)\n\n\n\n\nIf either of those two metrics is below their respective thresholds, e.g. 0.175 for minimum distance, 0.25 for cluster \"centroid\" distance (I'm using cosine distance with values between 0 and 1), then I merge the clusters together. I do a single pass without recalculating those metrics after the merge, as that would cause too much merging.\n\n\nThis seems to achieve what I want, but I can't shake the feeling that there might be a more standard way of doing this and that I'm basically doing a hack? What do you think?\n\n\nThanks in advance.",
"date": "2020-12-17"
},
{
"vote": 4,
"title": "Can you suggest good resources(articles/udemy courses) for beginner in multi-topic classification with NLP and Python?",
"text": "Hi there!\n\n\nI am a JavaScript dev and a complete noob with NLP and all that stuff (ML, Neural Networks, etc). But nevertheless I have a task about multi-topic (category) books classification. \n\n\nI have a 30 topics(like \"AI\", \"Medicine devices\", etc)  and a bunch of already classified books for each topic (up to 600). The distribution of the classified books is pretty unequal, some topics have only 50 categorised books and others can have 500. In categorisation we are using keywords like \"machine learning\", \"big data\", \"brexit\", \"green energy\" etc. The task it to categorise the remain uncategorised books (22000 in total).\n\n\nI used this \narticle\n  and basically just copy-pasted all the code(only for Bag of Words approach) and tweaked it a bit. \n\n\nThe result is good for a small amount of topics (like 7-8), the Accuracy rate was around 0.85. But when I switched to all 30 topics, the Accuracy rate dropped from 0.85 to 0.3 (that was also the case if you use dataset from the article). I think it is because of unequal distribution of already classified books between topics. But I don't know how to fix it and what I need to do to improve the result...\n\n\nTL;DR Here is my question\n: Can you please recommend any good articles/udemy courses to multi-topic classification with NLP and Python? I have a lot of troubles in theory behind all the stuff from that article and just code copy-pasting is not working...\n\n\nDon't get me wrong, I do not have time for deep and solid foundation building around ML, deep learning and all that cool stuff, what I need is a resource that will provide me good enough understanding to do a single specific task",
"date": "2020-12-16"
},
{
"vote": 5,
"title": "Any Universal sentence encoder in C++?",
"text": "[deleted]",
"date": "2020-12-16"
},
{
"vote": 33,
"title": "Is there a list of the most important papers in NLP anywhere?",
"text": "Recently I've found this \namazing list of key papers\n in Reinforcement Learning on OpenAI's SpinningUp website. Do you know of a similar list for NLP? \n\n\nI have completed many MOOCs on the subject but I have yet to see a NLP resource similar in scale and quality to what SpinningUp is for RL. Do you know of any?",
"date": "2020-12-16"
},
{
"vote": 3,
"title": "Day 350 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Acquisition â€“ 1st Passes II",
"text": "Day 350.\n\n\nWe are now moving onto knowledge acquisition. Today's post covers my first passes on some of the knowledge acquisition papers I have listed in previous blog posts! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/15/day-350-ryans-phd-journey-literature-review-knowledge-acquisition-1st-passes-ii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out episode 001 of Research Papers Summary, covering a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4",
"date": "2020-12-15"
},
{
"vote": 1,
"title": "Recommendations regarding Annotation Software",
"text": "As the title says, I was interested in how you create training data sets. Have you tried different tools what where your experiences? Is there a free one you could recommend? I'd like to start with something free, so I can find out what kind of functionality I would like to have before I spend money on a professional tool. But generally I'm interested in your experiences.",
"date": "2020-12-15"
},
{
"vote": 3,
"title": "Negation pair dataset and Negation Detection with context",
"text": "So I am working on a task to check if two sentences are negation of one another , if two sentence mean opposite or negated. Currently, I am working with Negex , comparing the two sentences and checking if there are any negated words , and a pretrained NER tagger on FLair , negation-speculation.  Both give acceptable results. However, I was looking for a method that was more based on Context and embeddings and not just taggers and syntax . Currently I am training a Bert NER with SFU review corpus . Any other suggestion or methods you might have worked or studied of  would be helpful .",
"date": "2020-12-15"
},
{
"vote": 2,
"title": "Botpress users?",
"text": "Who here uses botpress framework? What are your thoughts on it?",
"date": "2020-12-14"
},
{
"vote": 11,
"title": "Day 348 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Representation â€“ 1st Passes VIII",
"text": "Day 348.\n\n\nToday's post covers my first passes on some of the knowledge representation papers I have listed in previous blog posts, including Knowledge Vault and CoKE! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/13/day-348-ryans-phd-journey-literature-review-knowledge-representation-1st-passes-viii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out episode 001 of Research Papers Summary, covering a survey paper on Knowledge Graphs: \nhttps://www.youtube.com/watch?v=kfDufTaMsZ4",
"date": "2020-12-13"
},
{
"vote": 29,
"title": "A pathbreaking evaluation technique for Named Entity Recognition (NER)",
"text": "Named Entity Recognition (NER) models are usually evaluated using precision, recall, F-1 score, etc. But these metrics don't tell us a lot about what factors are affecting the model performance.  \n\n\nI came across a paper, where the authors present interpretable and fine-grained metrics to tackle this problem. So I decided to write an article discussing the ideas in the paper.  \n\n\nI intend to write more such articles summarizing awesome new research papers for a broader audience. Please give it a read and tell me your feedback.  \n\n\nIf there is any other interesting paper you would like me to write about in my next blogs, please feel free to reach out to me. :D  \n\n\nhttps://charudattamanwatkar.medium.com/a-pathbreaking-evaluation-technique-for-named-entity-recognition-ner-93da4406930c",
"date": "2020-12-12"
},
{
"vote": 1,
"title": "Day 347 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Representation â€“ 1st Passes VII",
"text": "Day 347.\n\n\nToday's post covers my first passes on some of the knowledge representation papers I have listed in previous blog posts, including YAGO and NTN! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/12/day-347-ryans-phd-journey-literature-review-knowledge-representation-1st-passes-vii/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-12-12"
},
{
"vote": 2,
"title": "How useful is MinIE as Open Information Extraction system for narrative fiction and dialogue heavy texts?",
"text": "I am considering OIE tools to augment training data for Transformer-based architectures (like GPT). I stumbled upon \nMinIE\n and was quite interested in its features. When doing simple tests, I was underwhelmed with the outputs, as they were in contrast to what the paper was presenting. In particular, I first tested how minimization (Mode-> aggressive) of extractions would turn out to be with such narrative fiction sentences:\n\n\nShe was sweating freely, her pale, freckled cheeks already reddening in the sun, but her eyes were alert, scanning the trees and hollows around her.\n\n\n&#x200B;\n\n\nThis yielded the following extractions, some quite puzzling, and imho far from minimization (this may be due to conjunctions and due to the augmentation phrases separated by comma):\n\n\nExtractions:\n\n\n`Triple: &quot;She&quot;\t&quot;was&quot;\t&quot;sweating&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;She&quot;\t&quot;was sweating cheeks reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;sweating&quot;\t&quot;be reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes were alert scanning trees&quot;\t&quot;be reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes were alert scanning hollows&quot;\t&quot;be reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;sweating&quot;\t&quot;be reddening&quot;\t&quot;already&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes were alert scanning trees&quot;\t&quot;be reddening&quot;\t&quot;already&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes were alert scanning hollows&quot;\t&quot;be reddening&quot;\t&quot;already&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;her&quot;\t&quot;has&quot;\t&quot;pale&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;pale&quot;\t&quot;reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;cheeks&quot;\t&quot;reddening in&quot;\t&quot;sun&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;her&quot;\t&quot;has&quot;\t&quot;eyes&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes&quot;\t&quot;were alert scanning trees around&quot;\t&quot;her&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes&quot;\t&quot;were alert scanning hollows around&quot;\t&quot;her&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes&quot;\t&quot;were&quot;\t&quot;alert scanning trees&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;eyes&quot;\t&quot;were&quot;\t&quot;alert scanning hollows&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`  \n\n\n\n(She | was sweating | freely)\n is not contained in this big set! The 'but' has not special meaning for the generation of tuples (it could be used as separator to eliminate false combination-triples, to eliminate e.g. this (`\"eyes were alert scanning trees\"\t\"be reddening\"\t\"already\"``).\n\n\n&#x200B;\n\n\nI then tried the Attribution functionality:\n\n\nEmily thought: &#039;Nobody is perfect.&#039;\n\n\nThis has yielded:\n\n\n`Triple: &quot;Emily&quot;\t&quot;thought&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n`----------`\n\n`Triple: &quot;Nobody&quot;\t&quot;is&quot;\t&quot;perfect&quot;`\t\n\n`Factuality: (+,CT)\tAttribution: NONE`\n\n\n\nSo no attribution with 'thought'?\n\n\nIs it correct that there is a limited set of 'attribution triggers'? In direct quotes, only one sentence can be attributed, correct? These seem like unnecessary limitations, especially considering narrative text.Are there alternative OIE tools that can make attributions that are not easily found with triggers, like for the following sample (prompted by another person asking â€œHow long did you stop in Tokyo?â€):\n\n\nBill Oâ€™Rourke turned up the flight log at once. â€œTwo hours and seventeen minutes. Just a refueling stop, sir.â€ ?\n\n\nMy best attempt was simply extracting all sentences before and after dialogues to find referencing entities to attribute to.\n\n\nI had such a grouping in mind to begin attribution:\n\n\nFirst step extraction of dialogues and appending neighboring sentences (use sentence tokenizer to get first and last parts correspondingly):\n\n\n(2038, &#039; Alice said.&#039;) -&gt; Sentence before quote (here erroneous, belongs to prior)\n\n\n(2051, &#039;â€œAgain.â€&#039;)\n\n\n(2060, &#039;â€œHe has no proof of that,â€&#039;)\n\n\n(2086, &#039; Ashes said, tail whipping against the back of her neck.&#039;) -&gt; Sentence after last quote\n\n\nTry grouping that can yield attributions:\n\n\nAlice said. &#039;Again,&#039;\n\n\n&#039;He has no proof of that,&#039; Ashes said, tail whipping against the back of her neck.\n\n\nOf course, dialogues can continue with the same entity speaking, without making this attribution explicit in prior, next sentence. So one would need to collect \"groups\" of dialogues and \"groups\" of story-text to separate.\n\n\nThe groups of dialogues with neighboring story-text should be somewhat self-contained to enable attribution. So far this is my idea, but I would prefer having NLP-tools that are tried and tested, also allowing entity tracking (I would need to append coreference resolution to my \"groups\", but am unsure how to construct sentence chains to parse for entities).",
"date": "2020-12-12"
},
{
"vote": 8,
"title": "Study on Framing and Metaphors in the Covid-19 Discourse",
"text": null,
"date": "2020-12-12"
},
{
"vote": 2,
"title": "Stop Words",
"text": "Hey all, what are best practices when it comes\nTo generating stopwords?  Iâ€™m trying to do some lyric analysis and am wondering if there is some ratio of tf idf I can use or word2vec, and then split it by albums or artists as every artist (and even albums) are vastly different.",
"date": "2020-12-11"
},
{
"vote": 23,
"title": "What are some usual and unusual text pre-processing techniques used?",
"text": null,
"date": "2020-12-11"
},
{
"vote": 16,
"title": "What code smells are common in or even specific to NLP ?",
"text": "For those who might not know the term : a code smell is something that looks fishy in the way the code is written.\n\n\nIt is not necessarily a bug or an error in itself, but rather a sign that there \nmight be\n something wrong at the conceptual level, whether because it complicates or obfuscates things that should be more straightforward, or because it shows some misunderstanding of the domain of the program, or any other sign that there could be a problem with the program design. See\nThe Portland Pattern Repository Wiki\n for more details on code smells. \n\n\nAnd so, what are, in your experience, code smells you typically encounter in NLP applications ?",
"date": "2020-12-11"
},
{
"vote": 3,
"title": "Recent evaluation of Google Translate (BLEU scores)",
"text": "Can anyone point me to where I can find recent ( 2019-2020) intensive testing results on Google translate like the one published in \n2011 by Aiken & Balan\n.\n\n\nThere was a recent publication by \nGoogle (June 2020)\n stating their advances in that matter but I can't seem to find their detailed results by pairs of languages anywhere.\n\n\nI'm particularly interested in pair combinations of Arabic, French and English.\n\n\nThank you so much in advance.",
"date": "2020-12-10"
},
{
"vote": 18,
"title": "NLP Snippets: Clean and Tokenize Text With Python",
"text": null,
"date": "2020-12-10"
},
{
"vote": 2,
"title": "How do I create a custom unicode set with custom characters? From where do I start?",
"text": null,
"date": "2020-12-10"
},
{
"vote": 1,
"title": "How can I test the performance of a model for semantic search?",
"text": "I was asked, a bit in a hurry, to test how good a model was for semantic search.\n\n\nThis isn't in English, so it makes things harder. Still, I already found a STS (Semantic Textual Similarity) dataset in my language, containing 2500 pairs of sentences with respective similarity scores.\n\n\nHowever I'm not sure about the correct way to use it, nor if this is the appropriate dataset to evaluate a model in the Semantic Search task.\n\n\nMy first idea was: for each sentence pair, to test if I could retrieve the paired sentence somewhere in the top 3 search (using the model embeddings + KNN).",
"date": "2020-12-09"
},
{
"vote": 7,
"title": "SOTA methods for clause extraction / long sentence segmentation",
"text": "I'm currently working on a project involving sentence vectors (from a RoBERTa pretrained model). These vectors are lower quality when sentences are long, and my corpus contains many long sentences with subclauses.\n\n\nI've been looking for methods for clause extraction / long sentence segmentation, but I was surprised to see that none of the major NLP packages (spacy, stanza) offer this. I know it should be possible to implement from dependency parsing, but I'd rather not have to do this by hand, especially since there are all sort of edges cases that I'm unlikely to get right.\n\n\nSurely there must be modern packages/methods for this?",
"date": "2020-12-09"
},
{
"vote": 1,
"title": "LSA: Computing document - document similarity using 2 different vector representations of documents.",
"text": "[deleted]",
"date": "2020-12-09"
},
{
"vote": 4,
"title": "How to build multilingual search with translation and transliteration",
"text": null,
"date": "2020-12-09"
},
{
"vote": 1,
"title": "Day 344 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ Knowledge Representation â€“ 1st Passes IV",
"text": "Day 344.\n\n\nToday's post covers my first passes on some of the knowledge representation papers I have listed in previous blog posts, covering RotatE and RESCAL! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/09/day-344-ryans-phd-journey-literature-review-1st-passes-iv/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-12-09"
},
{
"vote": 1,
"title": "Follks'Talks concept of language acquisition by computer",
"text": "[removed]",
"date": "2020-12-09"
},
{
"vote": 13,
"title": "Any resources to intuitively understand Gibbs Sampling &amp; CRF?",
"text": "I know the MCMC inutition.",
"date": "2020-12-09"
},
{
"vote": 2,
"title": "Off-the-shelf classification models for twitter?",
"text": "Hi everyone, \n\n\nI'm a data scientist looking for 'off-the-shelf' classifiers for tweets and twitter users. Specifically, I am trying to automatically classify accounts as being corporations or individual people. \n\n\nDoes anyone know where I could find off-the-shelf algorithms like this? I know how to develop my own, I just don't have the resources to do it right now (e.g., a dataset big enough to train my own model, people to code tweets for supervised learning, time).",
"date": "2020-12-08"
},
{
"vote": 3,
"title": "[Project] RNN-Transducer Prefix Beam Search",
"text": "RNN-Transducer loss function, first proposed in 2012 by Alex Graves (\nhttps://arxiv.org/pdf/1211.3711.pdf\n), is an extension of CTC loss function. It extends CTC loss by modelling output-output dependencies for sequence transduction tasks, like handwriting recognition, speech recognition etc. As proposed originally by graves, RNN-Transducer prefix beam search algorithm is inherently sequential and slow, requiring re-computation of prediction network (LSTM based used to model output-output dependencies) for each beam.\n\n\nEven though there are fast and efficient implementations of RNN-Transducer loss function online (like \nhttps://github.com/HawkAaron/warp-transducer\n & \nhttps://github.com/iamjanvijay/rnnt\n), there arenâ€™t any optimised prefix beam search implementations. I wrote an optimised RNN-T prefix beam search algorithm with multiple modifications. Following are the major modifications I did:\n\n\n\n\nSaved the intermediate prediction network (LSTM states) on the GPU to avoid re-computation and CPU-GPU memory transfers.\n\n\nIntroduced vocabulary pruning to further speed up the decoding, without degrading WERs.\n\n\n\n\nCurrent code takes around ~100ms to decode output for audio of 5 seconds for a beam size of 10 (which is good enough to achieve production level numbers using RNN-Transducer loss function). Also, compared to CTC, RNN-T based speech recognition models (\nrecent SOTA for speech recognition by Google\n \nhttps://arxiv.org/pdf/2005.03191.pdf\n \nand\n \nhttps://arxiv.org/pdf/2005.08100.pdf\n) are recently becoming popular.\n\n\nFor the near future, I have some algorithmic optimisations in my mind. Also, I have plans for making a python wrapper for my implementation.\n\n\nMy implementation is purely in CUDA C++. Here is the link to my repo:\n \nhttps://github.com/iamjanvijay/rnnt_decoder_cuda\n\n\nPlease share the comments and any feedback.",
"date": "2020-12-08"
},
{
"vote": 19,
"title": "How to extract concept from text using NLP?",
"text": "I have seen IBM Watson NLP which has a Concept extraction from text, and that concepts are not word present in the input text. I wanted to know what algorithm they use to extract it.",
"date": "2020-12-08"
},
{
"vote": 1,
"title": "The computer acquires spoken language",
"text": "[removed]",
"date": "2020-12-08"
},
{
"vote": 1,
"title": "How is deep learning applied in the analysis of open-ended survey questions?",
"text": "Hello,\n\n\nCan someone give me an idea on how deep learning applied in the analysis of open-ended survey questions?\n\n\nMy intuition is that applying deep learning  models for analyzing the vast majority of survey questions is  infeasible since most of survey data are regarded as \"small\", that does  not fit into the \"big data\" category that we normally talk about in  natural language processing or deep learning. For example, I just think  that it is impossible to train a state-of-the-art neural NLP models like  Transformers, RNN, LSTM from a pool of survey data.\n\n\nHowever, I see several private companies which claim that they use deep  learning methods to analyze survey data (for sentiment analysis,  thematic analysis, etc). So I am just curious in what general manner  deep learning models are applied in analysis of open-ended survey  questions. (Certainly these private companies are not using anything  like Transformer to do sentiment analysis on their client's survey data?  or do these survey consulting companies tend to deal with very large survey data that is often not available in public?)\n\n\nThank you,",
"date": "2020-12-08"
},
{
"vote": 2,
"title": "Career possibilities for a linguist in NLP and language technologies field? searching for advices",
"text": "Good evening everyone, I graduated BA in foreign languages â€‹â€‹and cultures and I took a short professional course in NLP, but it is not enough and I have no programming experience (it is limited to what I have learned throughout the course).  \n\n\nI would like to know if there is a chance that I will be able to enter the job market in Europe. \nI am looking for advices on the best training courses or the best ways to actively seeking internships and employment. \nAny tips, of any type, are welcome. I would like to understand how to move and to make the right choices, I donâ€™t want to invest too many years of training if there are no prospects for me in the job market. I wouldn't have enough resources.\n\n\nMany thanks in advance those who can help me!",
"date": "2020-12-07"
},
{
"vote": 29,
"title": "Found a cool use for Word2Vec when working with Labeling Functions",
"text": "I am currently working with ways to label data, and I have employed packages like \nsnorkel\n to help create labeling heuristics for categories. One simple yet useful heuristic for baseline attributes of a category are key words. For example, if you have unlabeled news article data and know that one of your categories is about guns, you could create a labelling function that labels every document containing \"gun\", \"shot\", \"firearm\", ... etc into the \"Guns\" category. \n\n\nThis is nice as a baseline, but it is often hard to determine a good set of keywords beyond what you can think of off the top of your head. I recently trained a gensim-style Word2Vec model on the corpus I'm working with and was playing around with word similarity. I found that I could easily discover new common keywords for each category by finding words clustered around my preselected keywords. For example, I would use the similarity metric to find the word most similar to \"gun\", \"shot\", and \"firearm\", and it would return phrases like \"Prop 63\" and \"concealed\".\n\n\nAfter finding these new keywords I could add them to the labeling function, and voila, I have a broader set of key words to look out for with each category! It seems like an easy way to strengthen keyword labeling functions when your own knowledge of the domain isn't extensive, like mine in this case.",
"date": "2020-12-07"
},
{
"vote": 2,
"title": "Long-form Sentiment Analysis?",
"text": "[deleted]",
"date": "2020-12-07"
},
{
"vote": 14,
"title": "What do you think about Ethics in AI?",
"text": "Everyone might have heard about what's going on at Google with regards to the paper by Timnit Gebru. She highlighted that training large models like BERT have significant carbon footprint. What do you guys think about the entire situation?\n\n\nArticle discussing her paper",
"date": "2020-12-06"
},
{
"vote": 6,
"title": "allennlp-optuna: AllenNLP plugin for making hyperparameter optimization of your AllenNLP model easy!",
"text": null,
"date": "2020-12-06"
},
{
"vote": 1,
"title": "allennlp-optuna: AllenNLP plugin for making hyperparameter optimization of your AllenNLP model easier!",
"text": "[deleted]",
"date": "2020-12-06"
},
{
"vote": 4,
"title": "If I offer a text processing/NLP service through a webapp, how can I assure confidentiality?",
"text": "I am developing a python webapp the helps translators create glossaries from their bilingual documents. \n\n\nMost translators sign NDAs and their documents can be very confidential. I want to respect that. So, I need to make sure I implement a secure way of processing the user's documents. What would you recommend? \n\n\nIs it possible to do text processing ON the browser, so that the user does not even have to do any uploading? I'm honestly not interested in harvesting any data, I just want to offer my glossary creation service and respect the user's privacy and confidentiality.\n\n\nMaybe a webapp is not the right choice? Should I rather make a windows program that runs locally?",
"date": "2020-12-05"
},
{
"vote": 1,
"title": "What is the best way to match people's names?",
"text": "Given a list of names like:\nJohn Smith\nRobert Smith\nBob Smith\nJon Smith\nJ Smith\n\n\nWhat is the best way to work out the probability of a match? Robert and Bob might be the same person, and John and J might be the same person, but we don't know, so is there a canonical way to assign a probability to this?",
"date": "2020-12-05"
},
{
"vote": 1,
"title": "Folks'Talks Game",
"text": "[removed]",
"date": "2020-12-05"
},
{
"vote": 1,
"title": "Use 2 dataset for sentiment analytics",
"text": "I want to do a sentiment analysis on two shipping expeditions in an e-commerce. call it expedition A and B. Both of these expedition datasets have the same column, namely tweet and label (manual labeling: positive, negative, neutral). Is it permissible to combine both expedition data A and B for training?",
"date": "2020-12-05"
},
{
"vote": 1,
"title": "Training a Dutch GPT-2 base model",
"text": "[deleted]",
"date": "2020-12-04"
},
{
"vote": 2,
"title": "Twitter Sentiment Analysis - Classical Approach VS Deep Learning: A Beginner Friendly Notebook",
"text": "[deleted]",
"date": "2020-12-04"
},
{
"vote": 2,
"title": "How would you learn how to talk to tree communities?",
"text": null,
"date": "2020-12-04"
},
{
"vote": 8,
"title": "Day 338 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ List Of Temporal Knowledge Graph Papers",
"text": "Day 338.\n\n\nThis week I will be dissecting a survey paper and make blog posts about the different papers you can read in the space of knowledge graphs under different techniques. Today's a list of temporal knowledge graph papers. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/03/day-338-ryans-phd-journey-literature-review-list-of-temporal-knowledge-graph-papers/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-12-03"
},
{
"vote": 4,
"title": "Output Data",
"text": "Hello, I've been trying out my model for using SpaCy. Everything runs fine. My model trains the data but on testing, I get no output (printing doc.ents prints \"()\"). I tried an instance from the training data, still I faced the same issue. What can be the possible cause? Thanks in advance.",
"date": "2020-12-03"
},
{
"vote": 2,
"title": "Is question answering generative?",
"text": "I want to improve the question answering task, so I thought about training it like a gan. but if it would be so much better, why havent anyone done it before?",
"date": "2020-12-02"
},
{
"vote": 3,
"title": "Explained implementation of the Performer model",
"text": null,
"date": "2020-12-02"
},
{
"vote": 1,
"title": "Fictional language keyboard",
"text": "Guys, I would like to make a keyboard for a fictional language that has it's own alphabet, it has different writing rules, like, in this language \"nd\" is a single letter, so I couldn't write n, d. So i wanted to ask if you have any tips or know any programs with wich I could create a keyboard where I can set these rules.",
"date": "2020-12-02"
},
{
"vote": 0,
"title": "6 Types of Neural Networks Every Data Scientist Must Know",
"text": null,
"date": "2020-12-02"
},
{
"vote": 2,
"title": "Any efficient way to learn English linguistics quickly?",
"text": "I have to understand the dependency parsing tree to write different rules for extracting subject-predicate-object.",
"date": "2020-12-02"
},
{
"vote": 20,
"title": "[article] AI Limits: Can Deep Learning Models Like BERT Ever Understand Language?",
"text": "Itâ€™s safe to assume a topic can be considered mainstream when it is the basis for an opinion piece in the Guardian. What is unusual is when that topic is a fairly niche area that involves applying Deep Learning techniques to develop natural language models. What is even more unusual is when one of those models (GPT-3) wrote the article itself!\n\n\nUnderstandably, this caused a flurry of apocalyptic terminator-esque social media buzz (and some criticisms of the Guardian for being misleading about GPT-3â€™s ability). \n\n\nNevertheless, the rapid progress made in recent years in this field has resulted in Language Models (LMs) like GPT-3. Many claim that these LMs understand language due to their ability to write Guardian opinion pieces, generate React code, or perform a series of other impressive tasks.\n\n\nTo understand NLP, we need to look at three aspects of these Language Models:\n\n\n\n\nConceptual limits: What can we learn from text? The octopus test.\n\n\nTechnical limits: Are LMs â€œcheatingâ€?\n\n\nEvaluation limits: How good are models like BERT?\n\n\n\n\nSo how good are these models? \n\n\nCan Deep Learning Models Like BERT Ever Understand Language\n?",
"date": "2020-12-01"
},
{
"vote": 1,
"title": "Day 336 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ List Of Entity Discovery Papers",
"text": "Day 336.\n\n\nThis week I will be dissecting a survey paper and make blog posts about the different papers you can read in the space of knowledge graphs under different techniques. Today's a the list of entity discovery papers. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/12/01/day-336-ryans-phd-journey-literature-review-list-of-entity-discovery-papers/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-12-01"
},
{
"vote": 8,
"title": "Very large vocab tactics?",
"text": "Hey, running into an OOM error with my large vocab (~2.8 million tokens). Using the decoder approach of predicting the next token, so I have an output Dense layer for all tokens and am embedding size of 256.\n\n\nWhat are some strategies people are using to lower memory requirements? I think I can run it on the TESLA but not my desktop's 1070, but would like to get it onto as small a machine as possible once it's finished training (distillation, etc...) I'm using TF2.3. It's not actually language, so reducing the # of output tokens isn't as straightforward. \n\n\nI've been googling and reading some papers, but not running into much yet. Cheers.",
"date": "2020-12-01"
},
{
"vote": 5,
"title": "Quick Introduction to Apache NLPCraft",
"text": null,
"date": "2020-12-01"
},
{
"vote": 1,
"title": "Vitterbi in HMM (Part-of-Speech Tagging)",
"text": "Hello everyone,\n\n\nI'm new to Natural Language Processing, but find it a fascinating field. I'm starting from the basics and am learning about Part-of-Speech (POS) Tagging right now. One possible model to solve this task is the Hidden Markov Model using the Vitterbi algorithm. Now, I'm still a bit puzzled by the probabilities it uses. It has the transition probabilities on the one hand (the probability of a tag, given a previous tag) and the emission probabilities (the probability of a word, given a certain tag). Do I understand correctly that a sequence of POS tags is created by initialising with the transition probabilities, resulting in a tag for a certain word, based on the previous tag (so for example a noun, given that the previous tag is an article) and then corrected by the emission probability (the probability that a tag results in the word that you're trying to put a tag to)? \n\n\nTo make it more concrete, if you have a sentence like \"The man is sitting on a bench\". After you have found the tag for 'the', which is an article, you could end up with an adjective or a noun for example. Then, the alghoritm will compute the probability of an adjective, resulting in a word 'man', which will be very low and the probability of a noun resulting in the word 'man', which will be much higher. These are computed and the highest probability is then the Viterbi probability. In the end, your model will then output all the tags, based on the Viterbi probabilities. Is this correct?\n\n\nAnd something else, this might be a stupid question, but can a HMM be used on its own, without using Vitterbi? If so, what would be the difference? \n\n\nThanks :)",
"date": "2020-11-30"
},
{
"vote": 4,
"title": "Detecting when an object not present. \"There was no hot water.\" \"They did not have firewood.\"",
"text": "I'm new to NLP. I am attempting to mine a list of amenities at travel destinations and stop overs from user comments. Simple full text search in Postgres is working well to find text like \"hot showers\", \"swimming pool\", \"firewood\". But just because those words or word phrases are present, doesn't mean that a particular amenity was available. Some of the comments are like\n\n\n\n\nthere is no swing set\n\n\nthey did not have hot water\n\n\nthey're lacking covered picnic tables\n\n\nwe couldn't find any (noun)\n\n\n\n\nSo I'm trying to find when they have something and when they don't. I've been reading about \"negation\", but I'm not certain that I'm heading in the right direction. Is that where I need to be looking? I'm hoping that there are some open source (or at least free) python packages that could help me to detect these situations \"out-of-the-box\" (i.e., without a lot of custom programming). I need to get up and running fairly quickly.\n\n\nTHANKS!",
"date": "2020-11-30"
},
{
"vote": 2,
"title": "Any experience or info on the Human Language Technology MS program at University of Arizona?",
"text": "I am searching for a masters program in computational linguistics or related field and am wondering if anyone has had experience or know of this program at University of Arizona. Any info would be appreciated.",
"date": "2020-11-29"
},
{
"vote": 1,
"title": "What is the standard way to host a language vector model?",
"text": "I have developed a python application than fine-tunes machine translations with the help of locally-hosted vector language models (Facebook's fasttext). \n\n\nI'm in the process of launching the application online, but I'm not sure where should I host the models, which are pretty large (> 2GB). \n\n\nBasically, my application vectorizes bilingual strings and uses the fasttext models to perform similarity-based searches and comparisons. I have no problem running it locally, but I'm hitting a wall when thinking about how I will take this online.\n\n\nWhat is the standard and efficient way of doing this? Should I host them on the same server where the app resides? Should I use something like AWS? I will appreciate your advice.",
"date": "2020-11-29"
},
{
"vote": 11,
"title": "Day 333 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ List Of Scoring Functions Papers",
"text": "Day 333.\n\n\nThis week I will be dissecting a survey paper and make blog posts about the different papers you can read in the space of knowledge graphs under different techniques. Today's on list of scoring functions papers, covering both distance-based and semantic-matching-based. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/28/day-333-ryans-phd-journey-literature-review-list-of-scoring-functions-papers/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-11-28"
},
{
"vote": 3,
"title": "Model for systematic documents review",
"text": "I work on a project where we try to automate the process of reviewing documents. We have a bunch of ideas, but I'm very interested whether you worked on something similar or can propose your suggestions.\n\n\nThe problem is the following:\n\n\nThere is a large set of documents (relatively short, usually up to 600 words). Those documents need to be reviewed by a few criteria. Each criterion is formulated as \"Document should contain X/Y/Z and/or not contain A/B/C\". Values of XYZABC are stored in the semi-structured text but can be transferred to a list like (this can be done manually):\n\n\nInclusion criteria:\n\n\n\n\nX\n\n\nY\n\n\nZ\n\n\n\n\nExclusion criteria:\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\n\n\nThe task is to determine whether the document fulfills the specified criteria.\n\n\nThere is a training set available. However, the tricky part is the solution should generalize for new documents/criteria. So, there will be a new set of documents and criteria, and the model should be able to handle it without any fine-tuning.\n\n\nAppreciate your responses!",
"date": "2020-11-28"
},
{
"vote": 1,
"title": "Day 332 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ List Of Deep Learning &amp; Knowledge Graphs Papers",
"text": "Day 332.\n\n\nThis week I will be dissecting a survey paper and make blog posts about the different papers you can read in the space of knowledge graphs under different techniques. Today's on list of deep learning and knowledge graphs papers, from neural networks to transformers. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/27/day-332-ryans-phd-journey-literature-review-list-of-deep-learning-knowledge-graphs-papers/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nCheck out my first Youtube video on the One Concept I Wish I Knew Earlier In Life: \nhttps://youtu.be/GkxsP2JaRdY",
"date": "2020-11-27"
},
{
"vote": 16,
"title": "Doing a PhD research in NLP from a Statistics department without any connections....",
"text": "Hello,\n\n\nMy situation is a little strange because I am a Statistics PhD student but I  ended up doing research in computer science/ AI/ Deep Learning due to  my supervisor's recommendation.\n\n\nI feel strange because although I can read the NLP paper, understand  them, and come up with my own research topics (so far I have identified 3  different research topics), I am not a part of computer science  department NLP lab. My supervisor into applying/developing machine  learning algorithms for analyzing open-ended survey questions, so I  guess his work is somewhat related to NLP, but he is not really an  expert in NLP/Deep Learning. He does not have much knowledge in the  field and sometimes I feel like I am taking on this initative all by  myself. I am wondering,\n\n\n- Is it advisable for me to seek a professor from computer science NLP lab to be my co-supervisor? \n\n\n- What are the advantages of doing a NLP research in a big computer science NLP lab? \n\n\n- Is collaboration important for a PhD student to do research in NLP? I  see many PhD students who publishes at top NLP conferences are often  co-authored with multiple number of collarborators. I am just doing NLP  research with my supervisor, and my supervisor and I don't really have  any connection with NLP researchers. Should I make an effort to find NLP  researchers whom I can work with for my research (of course, I will be  doing most of the work since I want to be the first author)? \n\n\n- If I need to seek collaborator / co-supervisor who are familiar with  the field of NLP, how should I approach them? can I try sending them  emails and see if they are interested in my research? I guess the best  way is to talk to people at a conference but due to COVID-19, everything  is taking place online and I doubt whether I will be able to make any  connections. \n\n\n- Do PhD student from a big NLP lab have better computational resource  than the PhD students who does NLP research outside of those labs? My  supervisor recently set up an account for the national supercomputer (I  am in Canada, it's called SHARCNET) so that I may take an advantage of  it, but since almost every researchers in Canada have access to this  resource, when I submit the job on SLURM the queue wait time can be  long. Are PhD students from computer science NLP lab often free of this  problem? (do they have a better access to computational resource?) \n\n\n...Lots of questions! Could someone advise me on these issues? \n\n\nThank you so much for your time,",
"date": "2020-11-27"
},
{
"vote": 2,
"title": "Day 331 of #NLP365 - Ryanâ€™s PhD Journey â€“ Literature Review â€“ List Of Knowledge Graph Representation Papers",
"text": "Day 331.\n\n\nThis week I will be dissecting a survey paper and make blog posts about the different papers you can read in the space of knowledge graphs under different techniques. Today's on list of knowledge graph representation papers split by different representation techniques. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/26/day-331-ryans-phd-journey-literature-review-list-of-knowledge-graph-representation-papers/\n\n\nHave a good day people,\n\n\nRyan\n\n\n====================================================================================\n\n\nThis year has been a wild year and since we are almost approaching the end of 2020, I started to think about the next step and how I should carry on in the next year. I am considering starting a Youtube channel to better document my PhD journey as well as other areas of my life! I am also considering kickstarting a weekly newsletter to specifically drive more NLP values (+ however I can) to people in 2021. These ideas are still in the brainstorming / pilot phase but I am excited (and slightly nervous) to announce that I have made and uploaded my first video on Youtube yesterday to kick things off! Check it out here: \nhttps://youtu.be/GkxsP2JaRdY\n\n\nIf you do check out the video, remember to comment that you come from #NLP365 blog! As always, any feedback is welcomed :)",
"date": "2020-11-26"
},
{
"vote": 34,
"title": "NLP Paper Walkthroughs",
"text": "Around 4 months back, I started my YouTube Channel (TechViz - The Data Science Guy), where till now, I have discussed Detailed Research Paper Walkthroughs of papers from ML/NLP domain.\n\n\nI am writing this post to take a moment to thank each and everyone of you for supporting me through this so far awesome journey. Today, we have achieved a small milestone of 10,000 views on the channel. ðŸ‘ ðŸ˜Š \n\n\nChannel Link: \nhttps://www.youtube.com/c/TechVizTheDataScienceGuy\n\n\nI would also want to share some channel stats:\n\n\n\n\nTotal Videos: 21\n\n\nTotal Subscribers: 641\n\n\nTop-3 Geographies ðŸŒ\n - India: 22%\n - United States: 12.4%\n - Germany: 2.5%\n4.Age Distribution ðŸ‘¨\n- 18-24 years: 15.4%\n- 25-34 years: 69.3%\n- 35-44 years: 15.2%\n\n\nGender Distribution ðŸ‘§ ðŸ‘¦\n- Female: 12.5%\n- Male: 87.5%\n\n\n\n\nThe response has been great so-far from the community. \n\n\nI have also created a GitHub outpost showcasing videos with some analytics. I also plan to update other stats over there on weekly basis. Check Outpost- \nhttps://github.com/prakhar21/TechViz-TheDataScienceGuy-VideoList\n\n\nCheers!",
"date": "2020-11-26"
},
{
"vote": 7,
"title": "How can I solely get the BERT word embeddings without running the model?",
"text": "I want to modify the BM25 algorithm to also take into consideration similar words so that the top sentences will be more relavent to my second sentence. So is it possible to encode my sentence into integer tokens and then embed it using some transformer model like GPT-2 or BERT? And would I be able to do this with Huggingface Transformers?\n\n\nEdit:\nWill I also be able to only run the embedding part so that I can save some inference time?",
"date": "2020-11-26"
},
{
"vote": 22,
"title": "Just finished MA in linguistics. Looking for language-related tech work. Any software devs out there who leveraged a linguistics degree?",
"text": "tl;dr: I would like to work at a language-oriented tech company and I'm looking for ideas. \n\n\nI have worked as a software developer (mostly frontend) for the last 5 years. This year I decided to leave the software world and study linguistics again. I handed in my masters thesis draft a couple of weeks ago. I'm now looking for work as a developer again, since Covid has caused some financial pressure. It's a long shot, but ideally I would like to keep doing linguistics or have a pathway to linguistics-related tech work as a developer.\n\n\nOne thing I am not sure about is how to relate my generativist background with the statistical techniques that are popular in NLP. I have never been super interested in statistical understanding of natural language, it seems more like the domain of a data scientist. My thesis was written from a Chomskyan minimalist perspective. On the other hand there are some impressive results from word embedding and the like (It's certainly fun and interesting to apply the statistical techniques developed by others!). My tech background is mostly independent from my linguistics, but I would like to find some way to connect these two worlds. Even if I can't get a job, maybe I can contribute on some open source projects...\n\n\n\n\nIs there anyone out there working on language-related tech that doesn't involve statistical techniques/machine mearning? I'm thinking along the lines of language learning apps, ESOL or language preservation. Open source projects, startups or established companies.\n\n\n\n\nAre there any other developers like me who have linguistics degrees? How did you leverage your linguistics background in your tech career?\n\n\n\n\n\n\nThank you!",
"date": "2020-11-26"
},
{
"vote": 2,
"title": "Need - Summarisation Tools, algos and papers for Twitter dataset",
"text": "I would be glad to hear back anything in regards with the title, whether it is feasible or not, what must be my approach. How should I clean data so that the finalised must be a rough[if not very very rough] estimate of the data provided.\n\n\nWith summarisation - I mean any kind of analysis other than sentimental analysis. Like summarisation from keyword extraction, abstractive or extractive summarisation. I know it depends on data sets as well.",
"date": "2020-11-26"
},
{
"vote": 0,
"title": "a idea for better nlp",
"text": "i think i found a way to make better nlp.\n\n\nthis is what the algorithm would do.\n\n\nfor every sentence that is put into it would learn the words that come before and the words that would come after each word in the sentences.\n\n\nthis would create grammar rules.\n\n\nit would generate sentences only using the grammar rules\n\n\nexample for a sentences.\n\n\nsentence = i really like art.\n\n\n&#x200B;\n\n\ngrammar rules = i really\n\n\nreally like\n\n\nlike art \n\n\nsentence = i am really in knowing what you think.\n\n\ngrammar rules = i am\n\n\nam really\n\n\nreally in\n\n\nin knowing\n\n\nknowing what\n\n\nwhat you\n\n\nyou think \n\n\nnow i am thinking it may be able to store any amount of information using this.",
"date": "2020-11-25"
},
{
"vote": 1,
"title": "Hey tech linguists! Just got an interview for a linguist role at a FAANG company. Salary expectations?",
"text": "[deleted]",
"date": "2020-11-24"
},
{
"vote": 16,
"title": "I created a publicly available article extraction API.",
"text": "I'm quite passionate about article extraction and I spend lots of time building that. It scans the article and extract informaition such as : \n\n\n\n\nAuthor(s)\n\n\nTitle\n\n\nText\n\n\nImages \n\n\nLinks\n\n\n\n\nBut are there any uses for article extraction\n ? Do you know individuals, developers or companies that could be find this even interesting ?\n\n\nPlease leave your opinion below and any suggestion for the website/API are welcome.\n\n\n&#x200B;\n\n\nDemo ==>  \nhttps://extractit.netlify.app/",
"date": "2020-11-24"
},
{
"vote": 1,
"title": "The Most Popular Programming Languages - 1965/2020",
"text": null,
"date": "2020-11-24"
},
{
"vote": 5,
"title": "Keyword extraction from scientific reports",
"text": "Hello everyone,\n\n\nI work at an institute that is conducting research in physics, and over the years, many reports of the research have amassed. These reports are in German language and contain some meta information, experimental results in tabular form and of course a written portion. I now wanted to organize all these reports and create a tool, where reports which cover the same topics and fields of reasearch are grouped. Now I am all but an expert in NLP, so I read up on keyword extraction and tried automatically finding the topics using TD-IDF and RAKE, but the results were always unsatisfactory. The actual field of research was never a part of the result, which makes sense because it, of course, is not mentioned too often in the text itself.\n\n\nMy next idea I had was to implement a library of possible topics and weigh those words higher when found in the text. But this would be really suboptimal, since during research, new topics and keywords quickly arise and the library would need to be extended.\n\n\nBefore trying that I would like to ask here for some advice. Is it even possible to extract these topics automatically and without a predefined library? I am looking forward to any responses! :)",
"date": "2020-11-24"
},
{
"vote": 3,
"title": "[2004.15011] TLDR: Extreme Summarization of Scientific Documents",
"text": null,
"date": "2020-11-24"
},
{
"vote": 2,
"title": "How do I train a classifier that has an \"out of domain\" class, so far it's horrible at predicting the out of domain",
"text": "I have a text classification algorithm that I've made in scikit-learn. There are 5 \"real\" domains I'm looking for, let's call them A, B, C, D, and E. I have a 6th class as well, that is supposed to just catch everything that is not in any of the first five domains. I have labeled this \"OOD\", or \"out of domain\". I have pre-labeled training data for all 5 domains + the OOD class, and I gather ngrams from all sentences as features for each class. I then run these through a model to make predictions based on the ngrams seen in each sentence.\n\n\nThe model does ok predicting the first 5 domains. But it's horrendous at predicting the OOD class. Most of the OOD are misclassified as one of the other domains, and barely anything actually gets correctly classified as OOD. I think this is because all the ngrams gathered for the OOD class have no consistency like they do for the other domains which are about specific topics; if it's a class that by definition has NO specific topic, then its ngrams are going to be all over the place, right? So ngram sequences found within this class aren't going to be good predictors, right?\n\n\nThat's my line of thinking anyway. But if it's true, I have no idea what to do about it. How do I solve this problem, so that it accurately predicts my OOD class along with predicted the other 5 \"real\" domains? Am I supposed to not gather any ngrams from the OOD training sentences, and instead only gather ngrams from the 5 real domains, and defined OOD by the presence or absence of the ngrams found in the 5 real domains? I'm not sure how to do that or if I'm on the right track. Can anyone help? Thanks.",
"date": "2020-11-23"
},
{
"vote": 1,
"title": "[R] McGill University, Facebook &amp; Mila Release 14M Article NLP Pretraining Dataset for Medical Abbreviation Disambiguation",
"text": "[removed]",
"date": "2020-11-23"
},
{
"vote": 1,
"title": "Day 328 of #NLP365 - Ryanâ€™s PhD Journey â€“ Link Prediction â€“ General Architecture And Negative Sampling + LAUNCHING a Youtube Channel",
"text": "Day 328.\n\n\nToday's post is about link prediction and the importance of negative sampling. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/23/day-328-ryans-phd-journey-link-prediction-general-architecture-and-negative-sampling/\n\n\nA short announcement today! This year has been a wild year and since we are almost approaching the end of 2020, I started to think about the next step and how I should carry on in the next year. I am considering starting a Youtube channel to better document my PhD journey as well as other areas of my life! I am also considering kickstarting a weekly newsletter to specifically drive more NLP values (+ however I can) to people in 2021. These ideas are still in the brainstorming / pilot phase but I am excited (and slightly nervous) to announce that I have made and uploaded my first video on Youtube yesterday to kick things off! Check it out here: \nhttps://youtu.be/GkxsP2JaRdY\n\n\nIf you do check out the video, remember to comment that you come from #NLP365 blog! As always, any feedback is welcomed :)\n\n\nHave a good day people,\n\n\nRyan",
"date": "2020-11-23"
},
{
"vote": 19,
"title": "Anyone seen work related to data imbalance in NLP?",
"text": "Hey guys. I'm working on a project and am trying to address data imbalance and am wondering if anyone has seen work regarding this in NLP.\n\n\nA paper titled \nDice Loss for Data-imbalanced NLP Tasks\n was released in this year's ACL but other than this I haven't really come across anything recent.\n\n\nIf anyone knows of anything I'd appreciate any tips! Thanks.",
"date": "2020-11-23"
},
{
"vote": 1,
"title": "NLP Cypher Newsletter: EMNLP round up and new repos",
"text": "Weekly round-up of new NLP repos, papers and research. \n\n\nhttps://quantumstat.medium.com/the-nlp-cypher-11-22-20-eea5c8e22826",
"date": "2020-11-23"
},
{
"vote": 1,
"title": "TLDR - Extreme Summarization of Scientific Documents (Paper Explained)",
"text": null,
"date": "2020-11-22"
},
{
"vote": 9,
"title": "Why does LDA work?",
"text": "How is LDA able to discover high quality topics? From David Blei's slides he claims that it's because of competing sparsity priors between the doc-topic distributions and topic-word distributions. Is this widely accepted or has been verified? Are there other reasons why it works well?",
"date": "2020-11-22"
},
{
"vote": 2,
"title": "Day 326 of #NLP365 - Ryanâ€™s PhD Journey â€“ Nodes 2020 Notes II",
"text": "Day 326.\n\n\nToday's post covers round two of my BAD notes from Nodes 2020 event LOL The last one I promised haha\n\n\nhttps://ryanong.co.uk/2020/11/21/day-326-ryans-phd-journey-nodes-2020-notes-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-21"
},
{
"vote": 5,
"title": "NLP and copyright",
"text": "Hi,I am making a commercial app that collects news articles and summarizes them using Google Pegasus.\nCould I be facing a copyright infringement by doing that?\nDoes Pegasus create \"creatively different\" summary than the original article?",
"date": "2020-11-20"
},
{
"vote": 1,
"title": "Evaluation metrics?",
"text": "[deleted]",
"date": "2020-11-20"
},
{
"vote": 9,
"title": "Day 325 of #NLP365 - Ryanâ€™s PhD Journey â€“ Nodes 2020 Notes I",
"text": "Day 325.\n\n\nToday's post covers my BAD notes from Nodes 2020 event LOL Realised it's not go to make notes in these events...\n\n\nhttps://ryanong.co.uk/2020/11/20/day-325-ryans-phd-journey-nodes-2020-notes-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-20"
},
{
"vote": 4,
"title": "Evaluation of Text Generation - A Survey",
"text": "Natural language generation (NLG) in natural language processing is the task of generating natural language from a machine representation system such as a knowledge base or a logical form. \n\n\nEvaluating such systems is not straight forward due unavailability of any robust automatic metrics that correlates well with human decisions. Also, human evaluations encode subjectivity. \n\n\nThis survey paper puts out all the research that has happened in this domain of evaluation metric of text generation systems be it Human-centric, automatic metric, learned metrics. ðŸ”¥ \n\n\nWatch Paper Walkthrough - \nhttps://youtu.be/-CIlz-5um7U",
"date": "2020-11-20"
},
{
"vote": 8,
"title": "Finding a particular paragraph in a block of text",
"text": "I decided to post this because of seen a few others ask questions about approaches and have loved the responses from the community, so I thought I'd post what I'm working on to see what you all think.\n\n\nI'm trying to figure out the best way to identify if a particular paragraph (with some variability allowed) exists within a larger block of text. The way to think about it is if you have a bunch of documents, for now let's assume word docs (but could be ocr'd pdfs), that are similar types of documents and I want to know whether a particular paragraph (the benchmark) exists in that doc. I would also want to know if a variation of that paragraph exists. \n\n\nThis would be relatively straightforward if I just took all the paragraphs in a document and compared them to my benchmark paragraph. The issue is determining the span of text that delimits the paragraph I'm looking for in the block of text. This shouldn't be too hard to address as long as I'm working with word docs, but I'll want to expand it to OCR'd documents as well. This is also why I'm avoiding things like regex/keyword searches because they wouldn't work well with OCR'd text where there are spelling issues.\n\n\nI've tried the following two approaches, both with SpaCy:\n\n\n\n\nCalculate the cosine similarity between the benchmark paragraph and the whole document. This isn't ideal because all of the documents are very similar. The benchmark is more similar to a document that includes the benchmark than one that does not, but the approach breaks down when there are changes to other parts of the document. The scores were not good enough for me to classify documents outside of the edge cases.\n\n\nBreak the benchmark into lines or sentences and compare each one of those against each line or sentence in the document, then for each line or sentence in the benchmark take the maximum similarity score among all lines/sentences in the document. If the paragraph exists (or a very similar paragraph exists) the indices of the sentences/lines in the doc that are most similar to the benchmark sentences/lines will be roughly in order (i.e. [5,6,7,8,3,10]), whereas when the paragraph doesn't exist the list of entities will be all over the place. However, this approach is not very efficient and particularly if I want to find a paragraph in a long document or several documents.\n\n\n\n\nI'm curious what other approaches may work better, both from an efficiency and accuracy perspective.",
"date": "2020-11-20"
},
{
"vote": 4,
"title": "Using NLP for automated page navigation and web scraping",
"text": "Hi everyone. I am looking into NLP for the purposes of web scraping such that my scraper starting from a domain can navigate itself to the target page, obtain some degree of confidence that it is indeed on the target page, and only then to start extracting content. \n\n\nI do not have much experience with NLP so I thought I would ask you guys what some feasible approaches would be. \n\n\nTo give a concrete example, let's say that I have an index of software company websites and I want to automatically locate their tech support pages starting from the domain name. Taking \nadobe.com\n for example, I would want my program to look at all hyperlink texts and infer that the links corresponding to the texts \"Help Center\" and \"Enterprise Support\" (under \"Support\") are the most likely links given the goal. Once on this page, I suppose further guarantees can be made by using the text on the presumed target page.\n\n\nWhat are your guys intuitions of the best way to solve this problem? Would it be best to come up with my own corpus using known samples of link path texts that get to the target page which the program could then use to base its decisions on links for future unseen websites or is there a simpler approach?\n\n\nAny help would be appreciated. Thanks!",
"date": "2020-11-19"
},
{
"vote": 18,
"title": "Where to start with NLP and resume parsing?",
"text": "[deleted]",
"date": "2020-11-19"
},
{
"vote": 10,
"title": "just published a blog post about \"Orchestrating legal NLP services for a portfolio of use cases\"",
"text": "here is the link: \nhttps://revenkoartem.medium.com/lynx-service-platform-architecture-ac8d88c754f6\n\n\n&#x200B;\n\n\nWould be grateful for any feedback. Also if you have questions - I am glad to try and answer them.",
"date": "2020-11-19"
},
{
"vote": 1,
"title": "Sample Research Proposal",
"text": "I have a PhD interview coming up for which I am asked to submit a research proposal with the technical methods I will be using and a timeline for tasks. Anyone in the past who has written such a proposal, could you please share? It would be very helpful for me to refer to a sample.\n\n\nThank you!",
"date": "2020-11-18"
},
{
"vote": 1,
"title": "Is learning a new language worth the effort anymore (considering the time and progress of language technologies)?",
"text": "TLDR:\nIs it worth learning a new language with the current language tech and the advances that will inevitably come about?\n\n\nQUESTIONS:\nWhen will learning a new language be pointless?\nWhen will language tech take off to the point of ubiquity  and be all around better than learning a language?\nMore simply, is starting to learn a language today to late to get it to payback on the time invested?\n\n\nCONTEXT:\nI recently moved to a new country and began intensively learning Chinese as an English speaker. I have periodically been doubting the value of learning the new language at this time in history where technology is striving to make learning a new language pointless/obsolete. (specially if you already speak English or Chinese; the major languages of the world).\n\n\nAlthough the tech is not here yet in the future it WILL. The only question is when?\n\n\nI am learning the language with the objective of improving my potential value.\n\n\nI suppose the threshold for an ubiquitous language translation product would be the following:\none that is not too cumbersome to carry,\nunder 5k$,\nthat is 99% accurate/effective,\nand works seamlessly in conversation.\n(I am aware all these numbers and estimates are baseless, but I hope to give you an idea of what it would take to make learning a language pointless)",
"date": "2020-11-18"
},
{
"vote": 3,
"title": "Interpretation of User Manuals",
"text": "[deleted]",
"date": "2020-11-18"
},
{
"vote": 2,
"title": "Job market after Covid",
"text": "Hi I'll be finishing my bachelor's in computational linguistics soon but I'm a little nervous about the job market right now because of the whole covid thing. Do y'all have insights about how bad the market for NLP engineers has been affected, esp for somebody with only a bachelor's?\n\n\nBesides computational linguist, and NLP engineer, what other job titles should I look for?",
"date": "2020-11-17"
},
{
"vote": 6,
"title": "Platforms that has a good amount of biased sentences/data",
"text": "[deleted]",
"date": "2020-11-16"
},
{
"vote": 43,
"title": "I created a Discord group for studying the NLTK Book",
"text": "This is a study group for the \nNLTK Book\n, the official book for the popular NLTK toolkit. It's a book written for people who are completely new to NLP and Python. I wanted to have a place to easily discuss concepts in the book. I'm interested to see how a group like this would work! Let me know if you have any suggestions. Here's a link to the server:\n\n\n\n\nhttps://discord.gg/7XrEgH6ASt\n\n\n\n\nI am also working on a resource for people new to python / python programming for data science to get a Jupyter Notebook environment up-and-running and to use code examples / exercises in the book. Check it out over here:\n\n\n\n\nNLTK Book Resource",
"date": "2020-11-15"
},
{
"vote": 2,
"title": "Why is Googleâ€™s natural questions dataset so mich harder compared to SQuAD 2.0?",
"text": "And why arent Natural Questions used as much in production compared to SQuAD?",
"date": "2020-11-15"
},
{
"vote": 1,
"title": "[D] Mentioning MOOCs In Statement Of Purpose",
"text": null,
"date": "2020-11-15"
},
{
"vote": 16,
"title": "LibreASR â€“ An On-Premises, Streaming Speech Recognition System",
"text": null,
"date": "2020-11-15"
},
{
"vote": 1,
"title": "Day 320 of #NLP365 - Ryanâ€™s PhD Journey â€“ Introduction To GNNs",
"text": "Day 320.\n\n\nToday's post covers my slow journey towards GNNs! Any useful resources that you guys came across regarding this space, please let me know and I will check them out! \n\n\nhttps://ryanong.co.uk/2020/11/15/day-320-ryans-phd-journey-introduction-to-gnns/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-15"
},
{
"vote": 2,
"title": "spacy learning curve shared",
"text": null,
"date": "2020-11-15"
},
{
"vote": 1,
"title": "Pegasus: pre-training with extracted gap sentences for abstractive summarisation",
"text": "[removed]",
"date": "2020-11-15"
},
{
"vote": 10,
"title": "I coded (with comments) a Word2Vec model with pure (low-level) TensorFlow 2",
"text": null,
"date": "2020-11-14"
},
{
"vote": 2,
"title": "PEGASUS: Pre-training with Gap-Sentences for Abstractive Summarization | Research Paper Walkthrough",
"text": "[deleted]",
"date": "2020-11-14"
},
{
"vote": 11,
"title": "GenAug: Data Augmentation for Finetuning Text Generators",
"text": "Adapting text generators for low-resource domains? What's a good data augmentation method? Do random ones work like they do for classification? These questions and more are investigated in \"GenAug: Data Augmentation for Finetuning Text Generators\".\n\n\nPresentation Link\n\n\nPaper Link\n\n\nAbstract:  In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.\n\n\nAuthors: \nSteven Y. Feng\n, \nVarun Gangal\n, \nDongyeop Kang\n, Teruko Mitamura, \nEduard Hovy\n\n\nA Q/A session will be held by the authors at the \nEMNLP DeeLIO Workshop\n on Thursday, Nov. 19.",
"date": "2020-11-14"
},
{
"vote": 13,
"title": "UK Linguistics grad considering switch to compling/NLP",
"text": "I did a more traditional Linguistics degree which I just finished and now I am considering doing a MA in Computational Linguistics, possibly at Wolverhampton. I wanted to do some in my undergrad but unfortunately it was not offered. However, I'm not interested in academic research and not sure how easy getting a job would be afterwards in the field. I am a mature student with a background in a totally different field, and I have only basic programming skills as of yet. Does anyone have any advice? Does anyone know anything about the MA program at Wolverhampton (seems to also be options in Germany). \n\n\nMany thanks",
"date": "2020-11-13"
},
{
"vote": 8,
"title": "Day 318 of #NLP365 - Ryanâ€™s PhD Journey â€“ Future Directions In KGs",
"text": "Day 318.\n\n\nToday's post explores the future direction of KGs! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/13/day-318-ryans-phd-journey-future-directions-in-kgs/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-13"
},
{
"vote": 2,
"title": "A simple summarization app I made using the transformers library",
"text": null,
"date": "2020-11-13"
},
{
"vote": 7,
"title": "\"Where are you from?\"",
"text": "Quick intro:\nÂ Iâ€™m a CS student, and Iâ€™m faced with a problem that I believe can be solved with NLP/ML!Â Iâ€™m fairly confident w/ Python, but Iâ€™m completely new to NLP (although Iâ€™ve seen some NLP/ML code before and have a super high level idea of some NLP/ML concepts).\n\n\nMy problem:\nÂ I want to classify what country a person is from, based on their input that is in the form of a sentence, phrase, or just a few words. I searched Google, but got a tad bit overwhelmed with the amount of information / models out there - came here for a little guidance and a push in the right direction!\n\n\nSome Examples:\n \n<input> â€”> <classification>\n\n\n\n\nIâ€™m from Canada â€”> Canada\n\n\nIâ€™m not from Canada â€”> Not Canada\n\n\nI wish I was from Canada â€”> Not Canada\n\n\nCanada is where Iâ€™m from â€”> Canada\n\n\nFrom Canada â€”> Canada\n\n\nI have American blood â€”> USA\n\n\nCompletely American â€”> USA\n\n\nIâ€™m from Canada, but Iâ€™m from a lot of other places as well. â€”> Canada, Mixed\n\n\nIâ€™m partly Canadian and partly from America â€”> Canada, USA\n\n\nIâ€™m based in the US â€”> USA\n\n\nIâ€™m based in the USA â€”> USA\n\n\nIâ€™m based in America â€”> USA\n\n\nIâ€™m from California â€”> Country: USA, State: CA\n\n\nCalifornia is where Iâ€™m from in the US â€”> Country: USA, State: CA\n\n\nProudly Californian â€”> Country: USA, State: CA\n\n\nI live in California. â€”> Country: USA, State: CA\n\n\nFrom San Fransisco â€”> Country: USA, State: CA, City: San Francisco\n\n\nLove SF â€”> Country: USA, State: CA, City: San Francisco\n\n\nI look Canadian, but Iâ€™m actually American. â€”> USA\n\n\nMy mother nation is Russia. â€”> Russia\n\n\nIâ€™m half Chinese and half American. â€”> China, USA\n\n\nIâ€™m a person and Iâ€™m really cool and awesome and amazing â€”> Unknown\n\n\n\n\nSo thatâ€™s my dream goal! Any ideas on where/how to start?\n\n\nNote:\n I believe I can use a library to identify location and nationality entities, but how would I be able to distinguish correct phrases like a) \n\"I'm from\n \nx\n\" and b) \n\"I was born in\n \nx\"\n from incorrect phrases like c) \n\"I currently live in\n \nx\n\" or d) \n\"I'm not from\n \nx\n\"? Examples 'a' and 'b' tell me that the person is from \nx\n, whereas 'c' and 'd' don't tell me anything about where the person is originally from. I guess I could hard code something, but there are a lot of variations / edge cases I would have to write code for, so I thought I could use NLP/ML to help out with the highly unstructured text input data!",
"date": "2020-11-13"
},
{
"vote": 38,
"title": "One sentence highlight for every EMNLP-2020 Paper, plus code for ~70 of them",
"text": "Here is the list of all EMNLP-2020 (Empirical Methods in Natural Language Processing Conference) main track papers, and a one sentence highlight for each of them. EMNLP-2020 will be held online from Nov 16, 2020.\n\n\nHighlights:\n\n\nhttps://www.paperdigest.org/2020/11/emnlp-2020-main-track-highlights/\n\n\nCode:\n\n\nhttps://www.paperdigest.org/2020/11/emnlp-2020-papers-with-code-data/",
"date": "2020-11-12"
},
{
"vote": 10,
"title": "BM25 for document vectorization",
"text": "Hi fellow NLP'ers! \n\n\nI'm desperately looking for an implementation of Okapi BM25 for document vectorization. \n\n\nI would like to use it to transform a BoW matrix to BM25 weights and use that matrix for document clustering.\n\n\nLike these authors:  \n\n\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018029\n\n\nI know gensim supports the BM25 ranking function, but I do not immediately see how I could use this to generate a feature matrix. \n\n\nhttps://gensim.readthedocs.io/en/latest/summarization/bm25.html\n\n\nAny ideas?",
"date": "2020-11-12"
},
{
"vote": 4,
"title": "Is it possible to combine two BERT models?",
"text": "I'm sorry if this seems naive.\nSo, the idea is to use a pretrained BERT model for summarisation of Biomedical text. BioBERT is trained on vocabulary of this domain and BERT-SUM is pretrained on summarisation tasks.\nMaybe we can process the text in bioBert and then in BERT-sum, I think that embeddings are particular to one model so it isn't possible this way. Thoughts?",
"date": "2020-11-12"
},
{
"vote": 2,
"title": "Customised Sentiment Analysis - Investigation of medieval court records",
"text": "Apologies if this is the wrong place for this.\n\n\nI am investigating ways in which you can explore historic datasets using modern data science techniques. I am a historian and not a data scientist so anything I have learnt is self-taught as and when I needed it. Hence I worry that often I am missing something simple and important out.\n\n\nFor the current project I am looking at the records of a medieval court and I had the idea of carrying out a custom sentiment analysis. I generated a list of common words and assigned values to them according to a particular topic (which I had pre-determined through a close reading of the document and knowledge of the period). I then ran this  to generate values for each charge/allegation and assign categories to them based on the results. For example I had a list of words associated with violent crimes and ran that. Then another list of words associated with tax evasion or corruption and ran that.\n\n\nIs this no longer sentiment analysis? I am writing about this briefly but it occurred to me that this might not actually be sentiment analysis anymore so I thought I would ask in case there is a better term to describe it.\n\n\nAs an addendum if anyone has any ideas about how to interrogate this dataset I would be interested. The current plan is: use NER to extract names, dates, locations, and sums of money. TF-IDF. Topic modelling. Visualising offenders relations to each other and to various categories of offences through network analysis. I am at a beginner level in r and python but I am prepared to spend time learning what I need to.",
"date": "2020-11-12"
},
{
"vote": 2,
"title": "French to English models using Transformers",
"text": "I have searched for french to English models in HuggingFace but I've only found English to french models. Is there any way to use existing models to translate french sentences into english?",
"date": "2020-11-12"
},
{
"vote": 19,
"title": "Best linguistics classes to take as a Computer Science major to form NLP base?",
"text": "As the title says it, I am looking to take a class that is not normally a part of my diploma to get more of an idea of the structure/workings of linguistics.\n\n\n I want to do this because I am slowly getting into NLP and would like to have more of a base in linguistics, but I do not know if this is normally something that is \nas\n relevant to the discipline as knowledge in computer science is.\n\n\nMy main interests seem to turn around automatic summarizers, which I \nassume\n are a sub-field of NLP?\n\n\nAny recommendations for types of linguistic classes I should look into? Or any other computer science classes that are particularly relevant to this field?\n\n\n(I am a 3rd year undergrad)",
"date": "2020-11-11"
},
{
"vote": 2,
"title": "Storing a rather large (too large for a csv) amount of text which needs preprocessing/cleaning",
"text": null,
"date": "2020-11-11"
},
{
"vote": 2,
"title": "Best self-hosted tool/model for performing tone analysis?",
"text": "I have a personal project that requires the tone analysis of tens of billions of comments, obviously I cannot do this though an online service as the cost for say the watson tone analyzer is in the millions.\n\n\nI have plenty of hardware to run the computation on, but just need to find a self-hosted tool or model that I can operate on. Ideally it can also not trip up on sarcasm...\n\n\nWhat recommendations do you have?\n\n\nAlternatively if someone is interested in helping with this project, I could definitely use the help, this side of things is outside my area of expertise...",
"date": "2020-11-11"
},
{
"vote": 0,
"title": "Emergency-Ally : A NLP and ML Based Comprehensive Web App which can help you provide SOS or Emergency Level Solutions , In Case You encounter or see someone in any Danger or Disaster Situtations.",
"text": "Emergency Ally, a smart website designed to always be there for you in the time of emergency and get you out of it as soon as possible.Built using Natural Language Processing and Machine Learning,Emergency Ally is a Comprehensive Web App which can help you provide SOS or Emergency Level Solutions , In Case You encounter or see someone in any Danger or Disaster Situtations.\n\n\n\n\nInput Emergency\n\n\n\n\nClassification of emergency:Classifies your emergency to a bunch of other related emergency types\n\n\n\n\nPriority Sorting:our model understands how urgent your search is and assigns it a priority level\n\n\n\n\nLocation Based Results:Gives you location based suggestions of nearest help centers and their contact information.  \n\n\n\n\nCheck Out Our Project Here:\nhttps://github.com/harshgeek4coder/Emergency-Ally\n\n\n\n\n\n\nhttps://devfolio.co/submissions/emergency-ally",
"date": "2020-11-11"
},
{
"vote": 20,
"title": "Automatic Text Summarisation Papers",
"text": "Recently, I was reading through couple of papers for doing text Summarisation (both abstractive and extractive). Thought of recording them and putting it out for the people who are interested. Also, Please comment down some more good ones, i would love to read ðŸ¤ \n\n\nText Summarisation Playlist: \nhttps://www.youtube.com/playlist?list=PLsAqq9lZFOtV8jYq3JlkqPQUN5QxcWq0f\n\n\nP.S Link to all the original papers is in the description box of the video.",
"date": "2020-11-11"
},
{
"vote": 5,
"title": "Is it worth publishing a large state-of-the-art model with minor changes?",
"text": "Hi,\n\n\nI'm new to research and have no experience in publishing papers. I recently worked on a project in which I did some unit changes to a pipeline while keeping the rest of the model weights unchanged.\n\n\nThe original pipeline itself is quite large, having three individual BERT units, and I lack the resources to completely train the model from scratch. My metrics are very close to the original ones with much less training, and my guide is pushing me to publish the work, but I doubt if it is original enough to be accepted for a decent conference.\n\n\nI'd greatly appreciate any advice.\n\n\nThank You!",
"date": "2020-11-10"
},
{
"vote": 2,
"title": "AI memory assistant â€“ remember everything you read",
"text": null,
"date": "2020-11-10"
},
{
"vote": 3,
"title": "Accuracy difference between fine tuning and feature extraction of a pre-trained model",
"text": "I did several fine tuning and features extraction tests using BERT as a pre-trained model. The dataset I used is always the same: train set, dev set and test set are always the same in both experiments.\n\n\n- The model for fine tuning is the standard one: a classifier Linear layer above BERT.\n\n\n- The model for features extraction is the standard one for token-level tasks (e.g. NER). The output of features extraction is taken in input by a biLSTM and above it there is a Linear layer for classification.\n\n\nThe two linear layers are the same as the number of output labels.\n\n\nDuring the experiments I get an accuracy difference of about 4% in favour of feature extraction model.\n\n\nAt this point I tried to make a whole series of hypotheses to explain this fact and I would like to know if you think they are correct and which hypotheses could be the most influential.\n\n\nHypothesis number 1)\n Since my dataset is not too different from the one on which BERT was trained, the fine tuning does not bring big improvements compared to features extraction. And since the features extraction model contains a biLSTM that allows the model to understand different things about the context, then features extraction works better.\n\n\nHypothesis number 2)\n Since we are talking about a token level task, having a biSLTM in the model is a great help. In fact, this architecture is the most used in all NLP tasks.\n\n\nHypothesis number 3)\n My train dataset contains ~300k words and perhaps these are too few to make the model significantly improve since in fine tuning a minimum of 2 epochs and a maximum of 4 are recommended.\n\n\nWhat do you think about it? Do you have any other ideas?\n\n\nThanks!",
"date": "2020-11-10"
},
{
"vote": 1,
"title": "Couple of questions about siamese networks (sentence BERT)",
"text": "I'm reading the \nsentence-BERT paper\n and I understand how a siamese architecture allows a model to learn to produce sentence embeddings that can be compared by cosine similarity. \n\n\nThere are a few things I'm still not sure of:\n\n\n\n\nDuring training, the model is trained with the two sub-networks. Since these sub-networks share weights, at inference (so when we no longer have sentence pairs/triplets but just want a representation of a single sentence) is only one network is used?  \n\n\nIf we use the \"triplet loss\" mentioned in the paper, does that imply that we're using three sub-networks now? (one for each of anchor, positive, and negative sentences) And obviously triplet loss can only be used when the task we're fine-tuning on provides triplet sentences, right?\n\n\n\n\nThanks in advance for any help!",
"date": "2020-11-10"
},
{
"vote": 5,
"title": "Day 315 of #NLP365 - Ryanâ€™s PhD Journey â€“ Knowledge Acquisition I",
"text": "Day 315.\n\n\nToday's post focuses on knowledge acquisition! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/10/day-315-ryans-phd-journey-knowledge-acquisition/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-10"
},
{
"vote": 0,
"title": "â€ŽNL Extreme is an app that can split text into grammatical units and then export the results as a CSV for free.",
"text": "[deleted]",
"date": "2020-11-10"
},
{
"vote": 5,
"title": "Trying to find a set of basic relations in dependency parse trees.",
"text": "Hi.\n\n\nI'm trying to extract relations from a set of documents (only text with no metadata), and for this I'm trying to go by the rule-based approach since it seems the most feasible.\n\n\nThe problem I'm having atm, is that dependency parsed trees are so confusing, and I cannot find how should I extract relations from them. Are there any set of basic rules/guidelines to how create your desired patterns?",
"date": "2020-11-10"
},
{
"vote": 1,
"title": "How to take GCP transcribed text and produce only meaningful sentences from it?",
"text": "Hi everyone, my first time here. I'm not a trained ML or NLP engineer, but I've been using the Google Cloud Vision API for a business need. I'm trying to find the best OCR setup for my boss, whose clients are French speaking and need their handwritten forms transcribed.\n\n\nI've managed to obtain text in French from a dense passage of handwriting, but the text that is outputted by GCP is gibberish, not really French. Is there such an API that for English (or French) will take \nalready\n transcribed and parsed text and force it into only consist of sensible sentences with only French words?  Perhaps one through Google? For example, after taking a handwritten note in French, I get this passage:\n\n\n\"Arone , j'aurais simi te connestue d'une autre maniere , ton courage sercinquelle m'a et boui . Est respecej awais eh le mine , je m'en suis par sin Ton anni clano l'acriture aurait et the neereilleux mairla -pacle que de toi , cu as ti im escet ipfe prie mow . Vorre perrdast deute ans cachÃ©e dat Ãªtre . insostenalle . Mais Eu as appris ca sumontas cas Ã©preuves douloureuses . Mine si tu as restu cachÃ©e pendant tous longues annis , le pire est quand mÃªme anirÃ© . On perket traiter les magis de tous les le mal est fait at reize ans , deja la...\"\n\n\nSome of the words are clearly not French. How can I force this text to be 1) at least all real French words, and 2) meaningful sentences, even if the vocabulary of the words are incorrect?",
"date": "2020-11-10"
},
{
"vote": 1,
"title": "[R] Amazon Alexa AIâ€™s â€˜Language Model Is All You Needâ€™ Explores NLU as QA",
"text": "[removed]",
"date": "2020-11-09"
},
{
"vote": 7,
"title": "Fine-tuning for question answering in new domains.",
"text": "When fine tuning a question answering model, such as bert for question answering, on a new or unique domain, will the domain specific language impact the ability of the of model to answer questions in that domain? Or is the domain specific understanding attained during pre-training?\n\n\nWhat I mean is this- letâ€™s say you take bert for question answering, and then fine tuned it on a set of question-answers in the scientific domain, will the model be better able to answer scientific questions as opposed to if you fine tuned it on non-scientific text? Or is the ability of such models to better â€œunderstandâ€ a domainâ€™s language something that is learned during pre-training only?\n\n\nThanks!",
"date": "2020-11-09"
},
{
"vote": 5,
"title": "Automatic phonetic transcription in 50+ languages",
"text": "Hi all,\n\n\nJust wanted to share a little project I did on grapheme-to-phoneme conversion. What is perhaps special about it is the large variety of languages supported. Each language uses a sequence to sequence model trained on Wiktionary data, and some languages are trained on very little data. As such, mileage may vary, but I hope you'll find it interesting/useful none the less. Soon enough, I am planning to add homograph disambiguation as well.  \n\n\nhttp://phonemizer.com/",
"date": "2020-11-08"
},
{
"vote": 6,
"title": "Day 313 of #NLP365 - Ryanâ€™s PhD Journey â€“ Knowledge Representation Learning II",
"text": "Day 313.\n\n\nToday's post continues to explore knowledge representation learning! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/08/day-313-ryans-phd-journey-knowledge-representation-learning-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-08"
},
{
"vote": 0,
"title": "Interested to see if there's a tool for this",
"text": "Pretty simple just a tool that can process a string of text:\n\n\n\"If your dog or cat likes to gobble their feed then you need a slow feeding bowl. Gobbling the food is not necessarily a sign of healthy habits.\"\n\n\nand be able to assign a single parent topic, in this case, \"pet\".",
"date": "2020-11-07"
},
{
"vote": 6,
"title": "What algorithms can associate words together from texts?",
"text": "What algorithms can associate words together from  texts? I was wondering how \nthis\n can be achieved. Thanks.",
"date": "2020-11-07"
},
{
"vote": 0,
"title": "Best quality to number of parameters ratio",
"text": "I am creating a text summarization app, and I am wondering what is the best model that is both fast and outputs \"good\" summaries.",
"date": "2020-11-07"
},
{
"vote": 2,
"title": "Need Turkish NER Dataset",
"text": "Hello,\nIâ€™m senior computer science student, I need Turkish dataset for NER for my homework project. \n\n\nI collected the datasets used in the articles, I also implemented some web crawlers, but if you have any datasets, I would be happy if you share them with me.\n\n\nThank you.",
"date": "2020-11-06"
},
{
"vote": 5,
"title": "I've a data set is a bunch of clusters with each cluster having two values - size &amp; overlap with other clusters. I am looking for an intuitive &amp; interactive visualization to present this. It is to allow users to merge or edit clusters if needed. Any framework available for this?",
"text": null,
"date": "2020-11-06"
},
{
"vote": 5,
"title": "How to Build a Twitter Sentiment Analysis System",
"text": null,
"date": "2020-11-06"
},
{
"vote": 1,
"title": "I want to build a Google Chrome plugin like Grammarly for my local language. I'm completely new to NLP. I need guidance.",
"text": "I want to build a Google Chrome plugin like Grammarly for my local language. I want the entire feature set - spell checking, tone detection and all the premium features.\n\n\nI have a basic familiarity with machine learning from my computer science master's degree study, but I've got no experience with NLP in particular. I don't have any idea about what technologies Grammarly is based on and how to implement all of these features.\n\n\nHow would I go about doing this?",
"date": "2020-11-06"
},
{
"vote": 2,
"title": "Unsupervised Style Transfer in NLP | Research Paper Walkthrough",
"text": "Text style transfer refers to class of algorithms that manipulate original text, in order to adopt the syntactic and semantic notion of another text. ðŸ¤© \n\n\nFor example- Training a model that could re-write the text from Shakespeare style of writing like a twitter post, maybe even vice-versa. ðŸ”¥ \n\n\nWatch Paper Walkthrough at \nhttps://youtu.be/cjnk3PJljDs",
"date": "2020-11-06"
},
{
"vote": 19,
"title": "Sentiment Analysis in Python: TextBlob vs Vader Sentiment vs Flair vs Building It From Scratch",
"text": "Sentiment analysis is one of the most widely known NLP tasks. It's widely adopted and has multiple applications including analyzing user reviews, tweet sentiment, etc.\n\n\nIn practice, it boils down to the multi-class text classification where the given input text is classified into positive, neutral, or negative sentiment.\n\n\nWe asked a question:\n\n\n> What are common Python packages for sentiment analysis?\n\n\nAnswer:\n\n\n> There are three popular libraries: Textblob, VADER, Flair.\n\n\nThere is also one more approach based on training own model for it. We took closer look at each library and prepared example colab notebook for training own model.\n\n\nIf this is something youâ€™re interested in, take a second to skim through the article. Please let me know if you need more info about any approach mentioned above.\n\n\nSentiment Analysis in Python",
"date": "2020-11-05"
},
{
"vote": 3,
"title": "Differences in the different levels of tasks in NLP",
"text": "Hi everyone,\n\n\nreading and studying some NLP resources, I noticed that tasks on different levels are often referred to. \n\n\nIn particular, I have often read terms like \"token level\" tasks, \"sentence level\" tasks, \"paragraph level\" tasks and so on. What exactly is the difference between these types of tasks? Suffice it to say that, for example, in token level tasks a certain label is assigned to each token, while in sentence level tasks the label is assigned to each sentence?\n\n\nAlso, could you give me some examples of token level task, sentence level task, paragraph level task and segment level task (I don't know if there are others besides these)?\n\n\nThank you so much!",
"date": "2020-11-05"
},
{
"vote": 1,
"title": "Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper walkthrough",
"text": "[deleted]",
"date": "2020-11-05"
},
{
"vote": 1,
"title": "Shopping lists corpus",
"text": "To evaluate my classifier I need a large corpus of shopping lists, like: \"2kg of black olives, peeled\".\n\n\nIt's important for it to be in natural language, as I want to make sure my classifier works for different working styles.\n\n\nI want it to be in a very specific language (Polish), but even it was in other languages, that would help me look for a similar source in my language.\n\n\nAny ideas?",
"date": "2020-11-05"
},
{
"vote": 22,
"title": "GPT-2 successfully classified sentences into 4 categories: 3 types of gibberish and clean",
"text": "[deleted]",
"date": "2020-11-05"
},
{
"vote": 1,
"title": "twitter-lda",
"text": "[deleted]",
"date": "2020-11-04"
},
{
"vote": 1,
"title": "How can I extract aspects given a sentiment in Sentiment Analysis?",
"text": "Hi! I'm new to NLP and want to extract aspects given an sentiment. For example:\n\n\n\"The battery is good.\"\n\"The screen is awesome!\"\n\n\nIf I filter by positive, I want it to return aspects that made the sentiment positive, in this case, battery and screen.\nHow can I do this?\n\n\nWhen studying \"Aspect Based Sentiment Analysis\" - ABSA, I've only seen cases where it's needed an aspect vector firts...",
"date": "2020-11-04"
},
{
"vote": 1,
"title": "How can I extract aspect given an sentiment in Sentiment Analysis?",
"text": "[deleted]",
"date": "2020-11-04"
},
{
"vote": 14,
"title": "When finetuning BERT, are you also adjusting the weights of the BERT model or just the last layers?",
"text": null,
"date": "2020-11-04"
},
{
"vote": 2,
"title": "[Research] Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer from ACL 2020",
"text": "Paper Presentation:\n\n\nProject Page:\n\n\nAbstract:\n\n\nMultilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.\n\n\nAuthors: Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, and Ahmed Hassan Awadallah",
"date": "2020-11-04"
},
{
"vote": 3,
"title": "input on NLP tools to support reading?",
"text": "I'd be interested in suggestions for practical ways to integrate NLP tools into a desktop ebook reader intended to make it easier for language learners to read foreign language texts -- sort of a reader's workbench. Ideally it should be useful to people at all levels of language skill, from beginners who have to look up every other word, to fluent readers. The project in question is Jorkens, at \nhttps://github.com/mcthulhu/jorkens\n. It's an Electron application, which can call external applications and allows users to run Python scripts from the menu to add their own custom functionality, and has a local SQLite database to store language data. I have a number of ideas I'd like to pursue, though providing broad language support without asking users to install hundreds of additional software packages in a dozen programming languages might be a bit of a challenge. (Also, I'd rather not have to parse output from a ton of different tools.)  There are a lot of NLP tools out there, but most seem to be for one or two or a handful of languages, and a lot of NLP tools seem to be focused on English... which would still be useful for people learning English, of course. \n\n\nDictionary searches, in both the local database and online dictionaries, are an obvious requirement, and thus so is lemmatization. In the past I've tried a couple of language-specific Node modules for lemmatization and a couple of finite state transducers (not readily available for more than a handful of common European languages, though I thought about trying to compile my own from publicly available lemmatization lists). I'm currently shifting from TreeTagger, which supports quite a few languages, to Stanford NLP's Stanza, which I think supports over 60 languages, though not some of the ones I'm looking for.  Is there anything better with broad coverage that I should be looking at?  Has there been any comparative study of various lemmatizers' accuracy and speed? Speed is an issue because I want to be able to open any book and start using the reader immediately, without any visible delay for preprocessing the current chapter. \n\n\nJorkens has a translation memory or bilingual concordance database, which can sort of serve as a backup dictionary if there is enough data in it. It can also serve as a way to see usage examples.  In the future, maybe this corpus can be expanded to include other, monolingual ebooks in the user's collection; I've seen people asking about ways to search for words and phrases across a collection of books, not just within the current ebook (which is what Jorkens does now--I'll have to look into that expanded search later on).  Maybe other potential uses of a local corpus could include suggesting associated words, or words used in similar contexts. Maybe mini-translation tests... Anything else? Right now, the sentence pairs are imported manually; in the future I might consider automatically sentence-tokenizing every book opened and importing the sentences for at least a monolingual corpus, pending the addition of translations. Jorkens has a parallel book view so that the original book and a translation of it can be opened side by side; it would be very nice to be able to align and import those automatically, at least at the paragraph level, using paragraph tags. Has this been done already? \n\n\nGetting an idea of key vocabulary in advance is usually a good reading strategy. Word frequency lists are already included. Terminology extraction sounds good; maybe TBXTools.py? I've looked at a couple of RAKE implementations for extracting key phrases; node-rake was agonizingly slow, but multi-rake.py turned out to be very fast. Is there any better way to do it? I should be able to do something like a TF-IDF word cloud without too much trouble, I think, probably without going outside Node. Natural has TF-IDF support, though its tokenization apparently sucks. \n\n\nSummarization, either in the foreign language or the native one, also seems like a useful way to get a preview of the text (except, spoilers). Maybe for one chapter at a time? How well would this even perform for fiction, though? I think most examples I've seen have been for non-fiction, such as news. Are there any tools I should look at? Abstractive summarization would be preferable to extractive (TextRank), I think, but may not be feasible, or at least that's my impression. \n\n\nParsing difficult sentences is another challenge where maybe NLP could help. I really like Stanza's dependency parsing, though the output might be hard for users to get used to.  I've seen sentence diagramming tools, though mostly for English. Are there any good multilingual tools that could, for instance, convert Stanza's output to a diagram? I should note that I've only tried Stanza on a couple of short sentences so far, and have not really stress-tested it. I'll try that later. \n\n\nAnother way to approach difficult sentences might be text simplification. I've seen too many long sentences with tangled syntax and obscure vocabulary in my day; I always used to wish I had a tool that could just tell me briefly what the author was trying to say. Is there any easy way to do this, for arbitrary foreign languages? I've seen things like \nhttps://github.com/cocoxu/simplification\n. How well would the available simplification tools work on fiction? I have the impression that they were mostly trained on English Wikipedia, though I may be wrong.\n\n\nJorkens isn't yet tracking reading statistics, but eventually will track things like percentage of words looked up over time, reading speed, etc. People like being able to measure their improvement. \n\n\nAny other ideas that would be worth looking into?",
"date": "2020-11-04"
},
{
"vote": 11,
"title": "GANs in NLP",
"text": "Has anyone worked on the application of GANs in NLP for any usecase? It would be really helpful if one could share some papers in this research domain.\n\n\nThanks in advance",
"date": "2020-11-04"
},
{
"vote": 3,
"title": "Is the Longformer Transformer model considered as Transformer encoder?",
"text": "Hello,\n\n\nI know that bi-directional Transformers, such as BERT, are considered as Transformer encoder models.\n\n\nIs the Longformer model likewise considered as a Transformer encoder model?\n\n\nI am asking because the self-attention mechanism used in Longformer is different than BERT....\n\n\n&#x200B;\n\n\nThank you,",
"date": "2020-11-04"
},
{
"vote": 1,
"title": "A Quick Guide to Transformer Models",
"text": null,
"date": "2020-11-03"
},
{
"vote": 1,
"title": "[2011.00747] Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation (Oral at COLING 2020)",
"text": null,
"date": "2020-11-03"
},
{
"vote": 2,
"title": "NLP Paper Submission: How to submit the human evaluation of text",
"text": "Hello\n\n\nThis is for paper submission. I have performed human evaluation on the NLP generated text (ask 500 volunteers). How should I submit the data that I have collected during the submission of my paper to the conference?",
"date": "2020-11-03"
},
{
"vote": 21,
"title": "Language Models are Open Knowledge Graphs",
"text": "Knowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones!\n\n\nVideo Presentation Link\n\n\nPaper Link\n\n\nAbstract: This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.\n\n\nAuthors: Chenguang Wang, Xiao Liu, Dawn Song",
"date": "2020-11-03"
},
{
"vote": 3,
"title": "How to get group/parents of a word using NLP",
"text": "Iâ€™m just getting started in reading about NLP and was wondering how I can find the group a certain word is a part of. For example if I input the word â€œcarrotâ€ I want the output to be â€œvegetableâ€. Is there any way to go about doing this with pre-trained models or do I have to create my own dataset with the connections between words? \n\n\nThanks for any help!",
"date": "2020-11-03"
},
{
"vote": 5,
"title": "Day 308 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 3 â€“ Entity Resolution III",
"text": "Day 308.\n\n\nToday's post continues on entity resolution KG textbook! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/03/day-308-learn-nlp-with-me-domain-specific-kg-textbook-chapter-3-entity-resolution-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-03"
},
{
"vote": 26,
"title": "Using NLP and AI, I created an app that summarizes news from all sides. Here it shows a massive discrepancy on whether or not FBI is investigating the Biden bus incident. What do you think?",
"text": null,
"date": "2020-11-03"
},
{
"vote": 13,
"title": "Feedback on a tool to democratize NLP",
"text": "Iâ€™m working on a no-code tool (\nstratoai.com\n) with the goal of making NLP more accessible to the average person. The first feature is a simple n-gram visualization tool, but there will hopefully be much more to come. Iâ€™d be very curious if anyone finds this to be useful or has any feedback so far.",
"date": "2020-11-02"
},
{
"vote": 1,
"title": "Matching the Blanks: Question about the model in a paper about unsupervised relation extraction",
"text": "Hello, I wonder if anyone has read this paper about unsupervised relation extraction: \nhttps://arxiv.org/abs/1906.03158\n\n\nIt is a pretty good paper, it seems a company called DiffBot is using this model to drive its relation extraction.\n\n\nThe paper itself sidesteps the issue of supervised fixed length relationship classification by having a model that learns a function that determines whether or not two relation statements encode the same relation. The inner products of the relation statements should be high if the relation statements express semantically similar relations and vice versa.\n\n\nThe only input to the model is the sentence with the unknown relations, and marker information about where the entity spans are in the sentence. The model trains itself to learn a set of relations.\n\n\nWhat I do not understand is exactly what length the set of relations that it learns can be is, is this a hyperparameter that one can set? is it determined by the model. I have read the paper a few times and I cannot seem to tease this information out.\n\n\nFigure 4 shows that the model with BERT embeddings learns 64 relation types on supervised datasets, but again, how this is arrived at for this unsupervised model is unclear to me.\n\n\nHas anyone read this paper? Can someone explain how a specific number of learned relation types is arrived at for this model? what determines if its 10 types? 100 types?\n\n\nThanks guys!",
"date": "2020-11-02"
},
{
"vote": 1,
"title": "converting time period in strings to equivalent numeric days",
"text": "I am working on the classic problem of converting time periodÂ in strings to equivalent numeric days. The below table should give you a brief idea.\n\n\n\n\n\n\n\n\nInput String\n\n\nExpected Numeric Output in days\n\n\n\n\n\n\n\n\n7 weeks\n\n\n49\n\n\n\n\n\n\n100 days\n\n\n100\n\n\n\n\n\n\nfour months\n\n\n120\n\n\n\n\n\n\n3 months\n\n\n90\n\n\n\n\n\n\nmid march\n\n\n75\n\n\n\n\n\n\nDo you knowÂ any python libraries or NLP based techniques that can nearly achieve that same or a pointer to an approach that can be taken?",
"date": "2020-11-02"
},
{
"vote": 7,
"title": "Poor abstractive (or extractive) summarization models?",
"text": "Usually, the only models or architectures that get promoted are the new models or the ones that beat the sota. I'm looking for summarization models that perform poorly. Ideally it should be abstractive. I'm trying to see the differences between a 'good' vs a 'bad' summarization model beyond rouge so I need some sort of baseline or a good reference point to highlight how far NLP models have come.",
"date": "2020-11-02"
},
{
"vote": 6,
"title": "Eric Brill and His Influence of Transformer Architecture?",
"text": "I was reading a paper that was given to me in my computational linguistics class; it's called \nSome Advances in Transformation-Based Part of Speech Tagging\n by Eric Brill. It was written in 1993, and I was surprised to be reading it and see that it describes a similar architecture found in \nAttention is All You Need\n. However, Eric Brill isn't credited in the latter as far as I remember. Was Eric Brill influential at all in the development of the field? And, semi-unrelated, is there a difference that should be noted in the respective architectures? I know that encoding would be the largest difference, as Brill doesn't seem to have an encoding/decoding system with his architecture. Am I fundamentally not understanding transformers? I feel like this paper really threw me off, and any explanation, and history about the inception of transformers would be appreciated!",
"date": "2020-11-02"
},
{
"vote": 0,
"title": "I Built A Robot AI Frog That Is Your Friend",
"text": null,
"date": "2020-11-01"
},
{
"vote": 2,
"title": "Day 306 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 3 â€“ Entity Resolution II",
"text": "Day 306.\n\n\nToday's post continues on entity resolutions! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/11/01/day-306-learn-nlp-with-me-domain-specific-kg-textbook-chapter-3-entity-resolution-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-11-01"
},
{
"vote": 6,
"title": "If i have an access list of names/ids that i want detect using OCR on documents, would it be possible to use a beam search using the list as my dictionary to improve accuracy?",
"text": "As title says. \n\n\nCurrently, Iâ€™m using tesseract to to output char one by one and itâ€™s doing poorly. Iâ€™ve access to the fonts that the document is using and thinking of using feature matching techniques but donâ€™t know if this is worth to pursue. Iâ€™ve come across beam search to output most likely words used for machine translation and thought itâ€™ll be applicable to my situation. Assuming that the ids are in a particular for e.g Dk-123-poo-333. Will beam search work? \n\n\nIâ€™m new to OCR, please go easy on me.",
"date": "2020-11-01"
},
{
"vote": 2,
"title": "Day 305 of #NLP365 - Ryanâ€™s PhD Journey â€“ Why Graph Databases (Neo4j)",
"text": "Day 305.\n\n\nToday's post covers why we should consider graph databases! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/31/day-305-ryans-phd-journey-why-graph-databases-neo4j/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-31"
},
{
"vote": 1,
"title": "Applying NLP in qualitative analysis with predetermined codes?",
"text": "Noob here, a bit lost and could really use some help. I basically have to study reflective write-ups by medical students and categorise them thematically. (The categories are pre determined, so this would be a deductive analysis.) \n\n\nSo far, encountered platforms like LIWC, SEANCE, IBM Personality Insights. But not sure if I could add my own categories to 'direct' the analysis based on the literature I've reviewed. Some folks have suggested nVivo, but not sure how does that simpify the process. Would appreciate any suggestions!",
"date": "2020-10-30"
},
{
"vote": 5,
"title": "Google AI Open-Sources mT5: A Multilingual Model Trained With mC4 Corpus (101 Language Dataset)",
"text": "Google has open-sourced a model called mT5, a multilingual variant of Googleâ€™s T5 model. This model is trained on a dataset comprising over 101 languages (\nmC4 corpus\n) and contains between 300 million and 13 billion parameters (internal variables used to make predictions). It is said to have adequate capacity to learn over 100 languages without facing any remarkable conflict.\n\n\nSummary:\n \nhttps://www.marktechpost.com/2020/10/29/google-ai-open-sources-mt5-a-multilingual-model-trained-with-mc4-corpus-101-language-dataset/\n \n\n\nGitHub:\n \nhttps://github.com/google-research/multilingual-t5\n \n\n\nPaper:\n \nhttps://arxiv.org/pdf/2010.11934.pdf",
"date": "2020-10-29"
},
{
"vote": 1,
"title": "Q: what are token offsets?",
"text": "[deleted]",
"date": "2020-10-29"
},
{
"vote": 1,
"title": "speech to speech alignment datasets",
"text": "[deleted]",
"date": "2020-10-29"
},
{
"vote": 3,
"title": "How can we use one [MASK] token to represent a sentence and its inverse?",
"text": "Hi. Sorry if the title is confusing, I couldn't really find a better way to write it out. Let me elaborate.\n\n\nI'm currently trying to formulate question answering data as a cloze task. For example:\n\n\n> Chemical1 induces Disease1. --> Chemical1 [MASK] Disease1.\n\n\nThe objective would to be to predict a word like \ninduces\n, \ncauses\n, \ndevelops\n, etc. The problem is representing the opposite meaning with exactly one \n[MASK]\n. I thought of doing\n\n\n> Chemical1 [MASK] induce Disease1.\n\n\nbut that wouldn't make sense because even if we could predict \ndoes\n as being the correct word for the positive sentence, we would have to either have two \n[MASK]\n tokens for \ndoes not\n or need to chance \ninduce\n to \ninduces\n for \ndoesn&#039;t\n (I hope I'm making sense).\n\n\nWould anybody know of a way that I can go about this? Any tips or pointers are appreciated. Thanks.",
"date": "2020-10-29"
},
{
"vote": 11,
"title": "Day 302 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 2 â€“ Information Extraction IV",
"text": "Day 302.\n\n\nToday's post is the continuation of our study on knowledge graph textbook, focusing on information extraction. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/28/day-302-learn-nlp-with-me-domain-specific-kg-textbook-chapter-2-information-extraction-iv/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-28"
},
{
"vote": 1,
"title": "Does your bot need NLP? Letâ€™s understand how NLP can add more human touch to your chatbot",
"text": null,
"date": "2020-10-28"
},
{
"vote": 3,
"title": "John Snow Labs NLU 1.0.2 Release",
"text": "We are glad to announce nlu 1.0.2 is released!\n\n\nhttps://github.com/JohnSnowLabs/nlu/releases/tag/v1.0.2\n\n\nNLU 1.0.2 Enhancements\n\n\n\n\nMore semantically concise output levels sentence and document enforced :\n\n\nIf a pipe is set to output_level='document' :\n\n\nEvery Sentence Embedding will generate 1 Embedding per Document/row in the input Dataframe, instead of 1 embedding per sentence.\n\n\nEvery Classifier will classify an entire Document/row\n\n\nEach row in the output DF is a 1 to 1 mapping of the original input DF. 1 to 1 mapping from input to output.\n\n\n\n\n\n\nIf a pipe is set to output_level='sentence' :\n\n\nEvery Sentence Embedding will generate 1 Embedding per Sentence,\n\n\nEvery Classifier will classify exactly one sentence\n\n\nEach row in the output DF can is mapped to one row in the input DF, but one row in the input DF can have multiple corresponding rows in the output DF. 1 to N mapping from input to output.\n\n\n\n\n\n\n\n\n\n\nImproved generation of column names for classifiers. based on input nlu reference\n\n\nImproved generation of column names for embeddings, based on input nlu reference\n\n\nImproved automatic output level inference\n\n\nVarious test updates\n\n\nIntegration of CI pipeline with Github Actions\n\n\n\n\nNew Documentation is out!\n\n\nCheck it out here : \nhttp://nlu.johnsnowlabs.com/",
"date": "2020-10-27"
},
{
"vote": 37,
"title": "How Do You Structure and Manage NLP Projects?",
"text": "NLP projects in ML can get messy pretty quickly. You may start off clean but, for some reason or another, things get in the way.\n\n\nSo we wanted to address this issue somehow and compiled key pointers, guidelines, tips and tricks that can help you stay on top of things and keep your NLP projects (mostly) in check.\n\n\nKey points we cover:\n\n\n\n\nCreating project directory structure,\n\n\nDealing with changing data: Data Versioning,\n\n\nKeeping track of ML experiments,\n\n\nProper evaluation and managing metrics and KPIs,\n\n\nModel Deployment: how to get it right.\n\n\n\n\nHereâ€™s the article, feel free to have a look through and share some of your own tips in the comments!\n\n\nHow Do You Structure and Manage NLP Projects",
"date": "2020-10-27"
},
{
"vote": 6,
"title": "Day 301 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 2 â€“ Information Extraction III",
"text": "Day 301.\n\n\nToday's post is the continuation of our study on knowledge graph textbook, focusing on information extraction. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/27/day-301-learn-nlp-with-me-domain-specific-kg-textbook-chapter-2-information-extraction-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-27"
},
{
"vote": 3,
"title": "Reusable Snippets in Notebooks",
"text": "[deleted]",
"date": "2020-10-27"
},
{
"vote": 8,
"title": "Unsupervised text summarization and simplification tool for MSc project?",
"text": "[deleted]",
"date": "2020-10-27"
},
{
"vote": 1,
"title": "Top 4 good reasons to learn to speak French",
"text": null,
"date": "2020-10-27"
},
{
"vote": 10,
"title": "For those posting about jobs in NLP, here is one",
"text": null,
"date": "2020-10-27"
},
{
"vote": 0,
"title": "Efficient One Pass End to End Entity Linking for Questions [Explained Video]",
"text": "https://youtu.be/eXN7Bu06RjI\n\n\nHow to perform full end-to-end entity linking has always been a challenging problem in NLP. The typical approach for this is to use a model to detect entities and then employ another model to perform entity disambiguation. And this paper beautifully formulates these two steps into a single neural network model.\n\n\nPaper \nhttps://arxiv.org/abs/2010.02413\n\n\nCode \nhttps://github.com/facebookresearch/BLINK/tree/master/elq\n\n\nPaper abstract\n\n\nWe present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.",
"date": "2020-10-26"
},
{
"vote": 1,
"title": "Efficient One Pass End to End Entity Linking for Questions (Explained)",
"text": "[deleted]",
"date": "2020-10-26"
},
{
"vote": 19,
"title": "I'm testing a resource for people new to Python and interested in Natural Language Processing",
"text": "The \nNLTK Book\n  is a free online resource for learning python programming as well as  the basics of natural language processing, a sub-field of data science  that focuses on the processing and understanding of text.\n\n\nFor  a senior project at my University, I wanted to create resource that  helped students create a programming environment to run the code in the  book and provide tips on the book's exercises. The NLTK Book and the  resource I created assume no prior programming experience.\n\n\nThis  resource I am creating is still under construction and I'm looking for  feedback for it's effectiveness as a self-learning tool. Notes for  setting up the Python environment and resources for Chapter 1 and  Chapter 2 are available. Check out the Setup Notebook link to see how to  get started and consider submitting a survey response to help improve  this resource. Thank you and I hope you find this helpful!\n\n\n\n\nNLTK-Book-Resource Repository\n\n\nSetup Notebook (begin here!)\n\n\nSurvey for Resource",
"date": "2020-10-26"
},
{
"vote": 1,
"title": "Is it possible for language models to achieve language understanding? - Christopher Potts (Stanford Professor of Linguistics)",
"text": null,
"date": "2020-10-26"
},
{
"vote": 14,
"title": "Papers/articles about detecting \"new\" topics in time series data?",
"text": "Hello, I am working with user feedback for a company, and we are constantly getting new feedback every day. \n\n\nSo far, I have built some sentiment analysis models and topic models to detect common topics of previous months worth of feedback.\n\n\nThe issue is that users are often complaining about/ discussing new features that we have recently implemented, and I would like to create a flexible model that can detect when new topics arise in our feedback dataset. We typically look at feedback in groups of months, so I would like to build something that can look at this month's worth of feedback, compare it to all previous months, and determine if there is a new topic being discussed.\n\n\nWhat I have considered so far:\n\n\nLets say I have 11 months worth of feedback, and I fit a KMeans model to this data. Perhaps I can look at a datapoint in an unseen month, fit it to the vectorizer used for the cluster model, and calculate the cosine similarity between this datapoint and every cluster centroid in the model. I then sum these similarities into a single score, represented as the total similarity to all of the data in the cluster model. I do this for every other datapoint, and I find the points with the lowest scores, and use these as the points that lie the furthest outside the domain of the current cluster model. After I collect these outliers, I add this new month to the training data and generate a new topic model, which hopefully encapsulates the outliers that it detected.  \n\n\nThis is a very simple method and I can think of a few problems with it:\n\n\n\n\nThere needs to be a cutoff at which the model (or you) decides how low a datapoint's score has to be before you determine that it belongs to a topic not found in the current cluster model. \n\n\nSimply summing the cosine similarities may be too simple of an approach for building a score for each new datapoint\n\n\nStarting out with KMeans comes with its own set of problems and hyperparameters, like how to determine the correct number of clusters to begin with.\n\n\n\n\nIf anyone has any resources on how to detect unseen topics in new data, given a prior topic model, please let me know! I would love to hear any suggestions or resources you can offer me.\n\n\nThanks!",
"date": "2020-10-26"
},
{
"vote": 0,
"title": "Top 10 NLP libraries with python",
"text": null,
"date": "2020-10-25"
},
{
"vote": 0,
"title": "Day 299 of #NLP365 - Ryanâ€™s PhD Journey â€“ Cypherâ€™s Hello World â€“ Movie Graph Tutorial II",
"text": "Day 299.\n\n\nToday's post covers Cypher's Hello World tutorial on building a Movie Graph. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/25/day-299-ryans-phd-journey-cyphers-hello-world-movie-graph-tutorial-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-25"
},
{
"vote": 0,
"title": "Given a list of files' titles get the topic",
"text": "Hey Everyone\n\n\nI clustered files and would like to run a model that will receive a list of file names and return their topic. My data isn't labeled so I think the best option for me will be to use some pre-trained model that does the task, however, I'm not sure which can be useful to me. Any ideas?\n\n\nThanks :)",
"date": "2020-10-25"
},
{
"vote": 10,
"title": "Rhyme datasets",
"text": "Hi All, \n\n\nI am looking to do some modelling around rhyme detection and was wondering if any of you were aware of any good datasets which classify rhyming words? \n\n\nAny information would be greatly appreciated, thanks in advance!",
"date": "2020-10-25"
},
{
"vote": 1,
"title": "I have a bunch of phrases and a score to indicate how close each phrase with other phrases in the bunch. How do I cluster them to groups?",
"text": null,
"date": "2020-10-25"
},
{
"vote": 1,
"title": "MSc Bioinformatician asking for advice",
"text": "Greetings!\n\n\nI'm on track to graduate spring 2021 and hopefully land a job. I'd appreciate any advice. For my master's thesis I plan to analyze MIMIC IV data using NLP... original right? Haha. I got into this wanting to build a robot doctor and over the last few months have slowly realized how far away that is. That's still my dream though, building a diagnosis and treatment algorithm, and I would love to do work that moves healthcare in that direction\n\n\nSome questions I have:\n\n\n\n\nWhat I learn and accomplish the next several months is important? What languages and projects should I focus on to land a cool job?\n\n\nThere are so many companies in different niches, some huge, some sprouting (like Nuance, DeepScribe, OM1) , some fledgling. And while it is a gamble, I'm hoping that if I do the right projects (see #1) it'll help me land a position I enjoy. Any tips of finding cool NLP companies, or finding cool NLP positions? \n\n\nIn some companies, I see a choice between research roles and consultant/client-facing roles. Personally, I would prefer the prior, but of course I would still love to work in the latter. Any tips or general advice?",
"date": "2020-10-25"
},
{
"vote": 25,
"title": "Using NLP to enable flatter organizations",
"text": null,
"date": "2020-10-24"
},
{
"vote": 3,
"title": "ACL/NAACL/EMNLP Submission, code must include?",
"text": "Hi\n\n\nJust wondering for ACL/EMNLP, if there is some patent related stuff and not willing to release the source code, is it still ok?",
"date": "2020-10-23"
},
{
"vote": 2,
"title": "Text rank algorithm: How does text rank combine tokes after using page rank algorithm?",
"text": "Text rank algorithm runs pagerank on co-occuring words. What confused me is, once text rank algorithm is run on nodes which represent tokens, how are words combined to give multiword phrases?",
"date": "2020-10-23"
},
{
"vote": 5,
"title": "\"spaCy is not research software\" â€“ What do you think?",
"text": "I am starting a small research-project for which I need solid tagging and parsing. I am working with historical data so adapting the tagger and parser will be part of the project. I have worked with spaCy before and was very happy with the possibilities it gave me to adapt it to my needs and with its performance in general.\n\n\nNow I found the following quote on \nthis page\n:\n\n\n>spaCy is not research software. Itâ€™s built on the latest research, but itâ€™s designed to get things done. This leads to fairly different design decisions than \nNLTK\n or \nCoreNLP\n, which were created as platforms for teaching and research. The main difference is that spaCy is integrated and opinionated. spaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets spaCy deliver generally better performance and developer experience.\n\n\nIs there someone here who can give some advice for research projects? What makes NLTK a supposedly better-suited package for research than spaCy? Is there some context to the statement by spaCy that I am missing (researchers having requested features generally that are generally unwelcome to private customers or something in that vein)?",
"date": "2020-10-23"
},
{
"vote": 5,
"title": "IndicBERT: A multilingual language model pre-trained on 11 Regional Indian Languages",
"text": null,
"date": "2020-10-23"
},
{
"vote": 0,
"title": "Day 297 of #NLP365 - Ryanâ€™s PhD Journey â€“ Cypherâ€™s User Defined Procedures And Functions",
"text": "Day 297.\n\n\nToday's post covers Cypher's user-defined procedures and functions! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/23/day-297-ryans-phd-journey-cyphers-user-defined-procedures-and-functions/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-23"
},
{
"vote": 1,
"title": "Identify the timeline of a candidate from his/her resume",
"text": "What would be your approach to solve the above problem statement? \n\n\nFor eg.. I would like to know the timeline of the candidate from his/her academic years to date",
"date": "2020-10-23"
},
{
"vote": 5,
"title": "[P] Multimodal Emotion Recognition Competition 2020 (MERC 2020)",
"text": "Object\n\n\nThis competition aims at developing the state-of-the-art models for emotion recognition from emotion-labelled facial videos with facial, speech, and text data. Participants are expected to put their efforts for obtaining the highest accuracy of recognizing one out of 7-classes, i.e., the neural and 6 emotions.\n\n\nEvaluation\n\n\n\n\nSubmissions are evaluated by the classification accuracy on the whole three test datasets.\n\n\n\n\nPrize\n\n\nThe rank of participating teams will be determined by the order of their final scores.\n\n\n\n\n1st ranked: 2000$\n\n\n2nd ranked: 1000$\n\n\n3rd ranked: 500$\n\n\n\n\nFor more information, please visit \nhttps://evalai.cloudcv.org/web/challenges/challenge-page/692/overview\n.",
"date": "2020-10-23"
},
{
"vote": 1,
"title": "Chatting Transformer: Advanced AI Text Generation With Only a Few Lines of Code Using GPT-2",
"text": "[deleted]",
"date": "2020-10-23"
},
{
"vote": 2,
"title": "Pre-trained doc2vec",
"text": "Is there are any free pre-trained English doc2vec  model to download?",
"date": "2020-10-22"
},
{
"vote": 34,
"title": "A Visual Guide to Regular Expression",
"text": null,
"date": "2020-10-22"
},
{
"vote": 0,
"title": "Vokenization Improving Language Understanding with Visual Grounded Supervision (Paper Explained)",
"text": null,
"date": "2020-10-22"
},
{
"vote": 0,
"title": "Day 296 of #NLP365 - Ryanâ€™s PhD Journey â€“ Cypherâ€™s Datetimes And Subqueries",
"text": "Day 296.\n\n\nToday's post covers Cypher's datetimes and subqueries! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/22/day-296-ryans-phd-journey-cyphers-datetimes-and-subqueries/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-22"
},
{
"vote": 1,
"title": "Using custom model in SentenceTransformers",
"text": "[deleted]",
"date": "2020-10-22"
},
{
"vote": 19,
"title": "Good major for a future career in NLP?",
"text": "I'm obsessed with both linguistics and computer science, and so I feel like working in the field of NLP would be my dream career. My school offers a Linguistics+CS combined major, which I would take with a Stats minor. This seems perfect, but I feel like a Data Science major with a Linguistics minor would prepare me better for ML. Any advice would be much appreciated!",
"date": "2020-10-22"
},
{
"vote": 3,
"title": "How do you build a text normalizer with machine learning?",
"text": "I need text normalization that will work for various languages. Obviously, normalization has to be run language-specifically. As humans we know how to read a text with numbers, abbreviations, symbols, whatever, and rewrite it using just letters of our language and put it into a canonical form with only one possible pronunciation. But the way to normalize text is highly dependent on context, so you can't just do dictionary-mapping because that's extremely naive and will change things with no regard for context. I need something that will normalize the text fully according to context the same way a human would.\n\n\nI know that this would require lots of training data of humans who have actually manually done normalization on texts. Luckily, I have access to thousands of text normalizations that have manually been done by humans. So I have original text and a normalized version for thousands of sentences already.\n\n\nMy question is how can I use this as training data for a machine learning algorithm, so that the algorithm can then do normalizations on text in a way similar to how humans would do it? The input data would be the originals, the normalized forms, and I guess the features would be all the changes that are observed between original and normalized form, right? How do I actually defined those changes to make features, though? And what would be a good ML model to use for this?\n\n\nWould appreciate any help, thanks.",
"date": "2020-10-21"
},
{
"vote": 13,
"title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
"text": "https://arxiv.org/abs/2010.08191\n\n\n>In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for answer find- ing. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for matching. How- ever, it is difficult to train an effective dual- encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and lim- ited training data. To address these chal- lenges, we propose an optimized training ap- proach, called RocketQA, to improving dense passage retrieval. We make three major techni- cal contributions in RocketQA, namely cross- batch negatives, denoised negative sampling and data augmentation. Extensive experiments show that RocketQA significantly outperforms previous state-of-the-art models on both MS- MARCO and Natural Questions. Besides, built upon RocketQA, we achieve the first rank at the leaderboard of MSMARCO Pas- sage Ranking Task. \n\n\nBlog: \nhttp://research.baidu.com/Blog/index-view?id=148\n\n\nLeaderboard: \nhttps://microsoft.github.io/msmarco/#ranking",
"date": "2020-10-21"
},
{
"vote": 0,
"title": "Day 295 of #NLP365 - Ryanâ€™s PhD Journey â€“ Cypherâ€™s Controlling Query Processing",
"text": "Day 295.\n\n\nToday's post covers Cypher's controlling query processing! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/21/day-295-ryans-phd-journey-cyphers-controlling-query-processing/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-21"
},
{
"vote": 3,
"title": "Advices On Training Text Embedding Models",
"text": "I have several questions:\n\n\n\n\nHow do we interpret the loss? For example. If I am training a word2vec algorithm using Gensim the loss is huge even for large epochs\n\n\nWhen should I stop training?\n\n\nHow hyper parameters are tuned?\n\n\nHandling multi-word or phrases",
"date": "2020-10-21"
},
{
"vote": 2,
"title": "How to detect dates from a text file?",
"text": "I am working on speech recognition where there can be dates in the text. The dates are in very different formats like \"oh nine oh eight sixty four\" or \"eight dash twelve dash seventy nine\" or \"3rd of december nineteen ninety four\". I want them to appear in DD-MM-YYYY format or MM-DD-YYYY format. Is there a way to achieve that? I tried datefider but as the text does not have numbers it. And I haven't been able to write a code to reliably convert the written numbers to actual numbers.",
"date": "2020-10-21"
},
{
"vote": 0,
"title": "#developers human swarm intelligence predicts how long until we have natural language based coding",
"text": "Here's a screengrab: \nhttps://i.imgur.com/8P495VN.jpg\n (no way to direct link to the post yet)\n\n\nSource is from: \nhttps://voy.ai/h/developers\n\n\nBot is: \nhttps://voy.ai/beezly\n\n\nSubreddit: /r/projectvoy",
"date": "2020-10-20"
},
{
"vote": 5,
"title": "How is an ASR's output compared to ground truth for validation?",
"text": "Curious how it is done as I am interested in doing something similar. I have some manually transcribed data that contains tags for multiple speakers. I want to compare how well the out of the box ASRs (Google, AWS Transcribe) are able to diarise speakers (or in simple words identify and transcribe audio with multiple speakers). I want to compare it to the ground truth data I have and come up with a comparison metric.   \n\n\nI can use Levenshtein Distance or something like Ratcliff-Obershelp similarity as a metric. But I am trying to learn if there is a more standard way of doing this?",
"date": "2020-10-20"
},
{
"vote": 1,
"title": "Final bachelor project ideas",
"text": "[deleted]",
"date": "2020-10-20"
},
{
"vote": 10,
"title": "Algorithm/model for sentence tokenization of legal texts",
"text": "What would be the best way to tokenise sentences from legal texts which contain a variety of punctuations, linguistic structure, and atypical syntax?\n\n\nIt contains tables, frequent paragraph changes along with bullet points.\n\n\n&#x200B;\n\n\nAny help would be really appreciated. ** neophyte to NLP **",
"date": "2020-10-20"
},
{
"vote": 1,
"title": "Day 294 of #NLP365 - Ryanâ€™s PhD Journey â€“ Cypherâ€™s Filtering Query Results",
"text": "Day 294.\n\n\nToday's post covers Cypher's filtering query! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/20/day-294-ryans-phd-journey-cyphers-filtering-query-results/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-20"
},
{
"vote": 28,
"title": "Interested in a career in NLP? We're running a webinar on this topic on Oct 22",
"text": null,
"date": "2020-10-20"
},
{
"vote": 1,
"title": "Smart Lens Text Scanner OCR",
"text": "Has anyone used this app on the Android store : Smart Lens Text Scanner OCR by Duy Pham (MMlab)?\nI've used this on a sample page and compared it to AABBYY Finereader and it surprisingly did a slightly better job on the very crappy old scan I fed it. Of course, I haven't tried the other OCRs out there and I am willing to purchase one. \n\n\nWhat else have you guys tried? Does anyone know what OCR engine this app uses?",
"date": "2020-10-19"
},
{
"vote": 26,
"title": "What resources do you follow to stay up to date?",
"text": "I am looking for blogs/channels/social media, anything basically. I want someone who writes about new stuff in a clear and simple way.",
"date": "2020-10-19"
},
{
"vote": 0,
"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Paper Explained)",
"text": null,
"date": "2020-10-19"
},
{
"vote": 2,
"title": "Identifying resumes from a large set of documents",
"text": "I have about 3000 resumes (for a variety of jobs and vaiety of companies) and 4000 nonresume documents (all word docs and PDFs, and the text has been extracted from the docs).  I am wondering how I might use the 7000 labeled documents to develop a way to identify resumes in a new set of data. \n\n\nSo far, i've tried using keywords that commonly occur in resumes. It works to a certain extent, but still has precision and recall issues.\n\n\nI've also tried building classifiers using tfidf with the 7000 labeled documents I have. On the test set, the best model could reach 97+% precision and recall. However, when I apply the best model to a new dataset, while the recall was good (95%), the precision was not so good (60%). My concern with this approach is that while resumes can share very similar language/tokens/words regardless of what fields/jobs they are for, nonresumes can be much more varied (e.g. documents from a financial institution might be quite different from documents from a tech company).\n\n\nI wonder if anyone might have any ideas as how how I might approach this problem. Thanks!",
"date": "2020-10-19"
},
{
"vote": 1,
"title": "Estimating product prices",
"text": "[deleted]",
"date": "2020-10-18"
},
{
"vote": 1,
"title": "Captum &amp; FlairNLP",
"text": "Has anyone managed to use \nCaptum explainable AI\n \nwith their\n\nFlair model\n? \n\n\nThere are some offhand mentions that other \nmade it work\n, but I am definitely struggling with rewriting the in-between steps of Flair.",
"date": "2020-10-18"
},
{
"vote": 2,
"title": "Review: Evaluating Neural Toxic Degeneration in Language Models",
"text": null,
"date": "2020-10-18"
},
{
"vote": 2,
"title": "what is your career?",
"text": "Hi all, next year I am looking to do a conversion course/ specialisation course in speech and language processing and I'm wondering what careers will be available to me afterwards so I'm just wondering what people here do for a job.",
"date": "2020-10-18"
},
{
"vote": 13,
"title": "Data Augmentation Techniques for Text Classification in NLP | Research Paper Walkthrough",
"text": null,
"date": "2020-10-18"
},
{
"vote": 2,
"title": "Does anybody knows, if mining parallel corpus for english nepali language pair by paracrawl/bitextor still in progress and will be released further ?",
"text": null,
"date": "2020-10-17"
},
{
"vote": 4,
"title": "Bias in Artificial Intelligence!",
"text": "After watching Andrew Ng's course on sequence modeling, I decided to dive deeper into subjects most interesting to me as a newbie in NLP. So I started researching about a paper called,\"Man to computer programmer is as woman to homemaker.Debiasing word embeddings.\"\nI was stunned by how machines can even take out the nuances in our behavior and how our past behaviors is being amplified into our future!\nYou can watch the part1 video I made to summarize my learnings.\n\n\nAlso I'd love to know what you think about the video!  \n\n\nhttps://youtu.be/QPIv1JRIHGY",
"date": "2020-10-17"
},
{
"vote": 7,
"title": "How well does average word vectors using BERT embeddings work for document classification tasks?",
"text": "I think this is a pretty straightforward question. I have a custom classification task on web documents that I want to run using the text of the document as input. My quick approach is to break the text into sentences and for each sentence derive its averaged word embedding over the words in the sentence from the last hidden output layer of BERT (via Hugging Face, thank you!) I then average again over the sentence vectors which are then the input to a follow-on feed forward neural network trained for my custom task.\n\n\nIs this a reasonable approach? Are there better ones I could try that aren't overly complex? \n\n\nI searched the subreddit but didn't find any specific posts covering this question. I found this to be a good resource: \nhttps://www.topbots.com/document-embedding-techniques/\n but thought I'd ask this community",
"date": "2020-10-16"
},
{
"vote": 1,
"title": "Learn Python Comments",
"text": null,
"date": "2020-10-16"
},
{
"vote": 1,
"title": "career options after studying speech and language processing",
"text": "[deleted]",
"date": "2020-10-16"
},
{
"vote": 1,
"title": "POS tagger and Dependency Parser. What happens when out-of-vocab word is present in query?",
"text": "For models used by Spacy and NLTK corpora. Is there also a way to handle this. The NLP pipeline I am building for query has lots of words which won't exist in corpus of training data of State of art models we see.\n\n\nTL;DR, Handling out of vocab words for tagging Pos Tags and Dependency Parser.",
"date": "2020-10-16"
},
{
"vote": 10,
"title": "Day 289 of #NLP365 - Ryanâ€™s PhD Journey â€“ Neo4j Graph Fundamentals",
"text": "Day 289.\n\n\nToday's post covers the graph fundamental of Neo4j! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/15/day-289-ryans-phd-journey-neo4j-graph-fundamentals/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-15"
},
{
"vote": 0,
"title": "Natural Language Processing with CityFALCON",
"text": null,
"date": "2020-10-14"
},
{
"vote": 16,
"title": "Is there any specific terminology in NLP/CompLing regarding the phenomenon where entities are linked via commas or \"and\"/\"or?\" And is there a way to extract such cases?",
"text": "Hi. The title is basically the question, but let me elaborate. I'm currently working on an NLP project and have noticed that there are cases where models behave in unexpected ways when entities are connected by commas or conjunctions like \"and\" or \"or.\" One example I can think of off the bat is:\n\n\n> \nCOVID-19\n is said to cause \ncoughing\n, \nfever\n, and \nloss of smell\n.\n\n\nIn this particular case, it's not hard to see that the symptoms of COVID-19 are listed and connected via commas and the conjunction \"and,\" and I'm wondering if there's any specific terminology to refer to this.\n\n\nThe reason why I'm asking is because I've noticed that in a lot of NLP literature, authors have pointed out that models behave in unexpected or unwanted ways when entities are \"grouped\" like this. However, I haven't seen any definitive approach to formalize that.\n\n\nRegarding the second question (i.e., extracting such cases from text), has there been any work w.r.t. finding such cases? I suppose you could use a rule-based template, but that seems too limited and not very useful. I was hoping someone more knowledgeable here could perhaps suggest an automatic tool for the job.\n\n\nAny feedback or tips are appreciated. Thanks!",
"date": "2020-10-14"
},
{
"vote": 3,
"title": "COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs",
"text": null,
"date": "2020-10-14"
},
{
"vote": 16,
"title": "SummPip: Multi-Document Summarization with Sentence Graph Compression | Research Paper Walkthrough",
"text": "Found this really interesting paper that talks about doing unsupervised multi document summarisation. Also the work claims to achieve comparable results with supervised methods in this domain. Here, is the video that explains the paper in detail.\n\n\nAbstract -  Obtaining training data for multi-document summarization (MDS) is time consuming and resource- intensive, so recent neural models can only be trained for limited domains. In this paper, we propose SummPip: an unsupervised method for multi-document summarization, in which we convert the original documents to a sentence graph, taking both linguistic and deep representation into account, then apply spectral clustering to obtain multiple clusters of sentences, and finally compress each cluster to generate the final summary. Experiments on Multi-News and DUC-2004 datasets show that our method is competitive to previous unsupervised methods and is even comparable to the neural supervised approaches. In addition, human evaluation shows our system produces consistent and complete summaries compared to human written ones.\n\n\nExplainer Video - \nhttps://youtu.be/1jwUOMQVCo4\n\nOriginal Paper - \nhttps://arxiv.org/pdf/2007.08954.pdf",
"date": "2020-10-14"
},
{
"vote": 1,
"title": "[Q] - what is the best way to classify scientific documents?",
"text": "Hello,\nI am working on a project to classify scientific documents. I have tried many methods but can't seem to get a good result. \nI am hesitating between bag of words and tfidf, then wether or not using dimension reduction the what algorithm to use and hyperparameter tuning.\n\n\nCan I get advice or examples of great pipelines to learn and succeed in this project.\nThanks in advance",
"date": "2020-10-13"
},
{
"vote": 0,
"title": "Internal Content Indexing NLU Service Available on the CityFALCON API",
"text": null,
"date": "2020-10-13"
},
{
"vote": 1,
"title": "Day 287 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 2 â€“ Information Extraction I",
"text": "Day 287.\n\n\nToday's post covers my learning towards the knowledge graph textbook on how to build domain-specific KGs, covering chapter 2! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/13/day-287-learn-nlp-with-me-domain-specific-kg-textbook-chapter-2-information-extraction-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-13"
},
{
"vote": 2,
"title": "[Q]-Using PCA with TF-IDF when classifying documents",
"text": "Hello,\n\n\nI wanted to ask if it is a good practice to do PCA to reduce the dimension of the TF-IDF  matrix when trying to classify documents?\n\n\nThanks",
"date": "2020-10-13"
},
{
"vote": 0,
"title": "Please share and follow",
"text": null,
"date": "2020-10-13"
},
{
"vote": 1,
"title": "NLP Work (1 hour)",
"text": "I need someone who is strong in NLP theory to work with me for 1 hour on a topic.  I will pay $100.  Please DM me",
"date": "2020-10-12"
},
{
"vote": 16,
"title": "Is the summary under some hashtag in \"What's happening\" in twitter generated manually or by some NLP technique?",
"text": "Is the summary under some hashtag in \"What's happening\"  in twitter generated manually or by some NLP technique? \nFor example\n, here is a summary for #Chomsky:\n\n\n> An interview with Noam Chomsky on ... Podcast is drawing strong reactions.",
"date": "2020-10-12"
},
{
"vote": 5,
"title": "Looking for German NLP Dataset",
"text": "Hello everyone,\n\n\nI am looking for a dataset in German, to train a categorization model. My model should categorize files based on their content, and I want it to process the files in their original format (whether it's PDF, DOCX, TXT etc.), so I'm looking for a dataset that contains such filetypes.\n\n\nDo you know of any dataset that may suit my needs?\n\n\nThanks!",
"date": "2020-10-12"
},
{
"vote": 6,
"title": "Need Ideas For NLP based Machine Learning or Deep Learning Projects",
"text": "Actually, I am an NLP enthusiast. I was looking for some project ideas regarding NLP Based on either Machine learning or deep learning. Can someone give any ideas regarding Such Projects, or and new NLP based Module Launched recently for making interesting projects or What websites or resources you would suggest that you personally prefer for making NLP based Projects?  \n\n\nInterested Developers and NLP Enthusiasts, we can even collab to discuss and work together on amazing NLP projects!!",
"date": "2020-10-12"
},
{
"vote": 14,
"title": "Day 285 of #NLP365 - Learn NLP With Me â€“ Domain-Specific KG Textbook â€“ Chapter 1 â€“ What Is A Knowledge Graph I",
"text": "Day 285.\n\n\nToday's post covers my learning towards the knowledge graph textbook on how to build domain-specific KGs! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/11/day-285-learn-nlp-with-me-domain-specific-kg-textbook-chapter-1-what-is-a-knowledge-graph-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-11"
},
{
"vote": 5,
"title": "GPT-3 can do word segmentation for English text with no spaces. Does this give any new insights into the inner workings of GPT-3?",
"text": null,
"date": "2020-10-11"
},
{
"vote": 2,
"title": "Does a dataset for pre-requisite knowledge exist?",
"text": "I'm trying to find a hierarchical data set of terms.\n\n\nFor example to understand \"BGP Routing\" you'd probably need to know what the terms \"computer networking\", \"IP address\" mean. And to understand those you'd need to know what a \"computer\" and the \"internet\" are. \n\n\nNot sure exactly the term I would be searching for",
"date": "2020-10-11"
},
{
"vote": 6,
"title": "How do you measure translation quality with a NUMBER?",
"text": "I've tried looking this up everywhere and nobody gives a satisfactory answer.\n\n\nMy company gets a lot of work for translation projects. We have to hire external contractors who are native speakers. Our client gives us thousands of words and phrases (mainly intended as dictionary entries) that they want translated and their definitions fully translated, so that every word, phrase and definition fully reflects the meaning of the source text. We send these thousands of peices of text to our external contractors and get them to translate.\n\n\nThere is NO WAY for us to check their work, or if they've actually done a good job. We don't speak these languages and even if we did, we cannot reasonably read all the text to make sure the translation accurately captures all the original meaning. They also need to annotate some finer points of it, like whether something is vulgar, or derogatory, or formal or informal, which they don't always do and that we have no way to check.\n\n\nSo what we end up doing is sending the translation to a second native speaker contractor, who just gives us a yes/no answer to \"is this a good translation, is the meaning fully captured, are all the extra annotations correct\" and if they say no it's re-done, if they say yes it's passed onto the big delivery for the client.\n\n\nBut this process doesn't work. The client still found a shit ton of errors, like a bunch of things not being marked as derogatory when they should've been, and a bunch of things being marked formal when they're not. This client expects less than 5% of everything to be marked \"formal\" and our translators were marking 25-30% oif the data as formal and our 2nd verifiers were saying this was ok. So this process doesn't work.\n\n\nWe have NO NUMBERS to quantify the quality of what we're doing, and everything I've looked up on this topic pretty much says to verify translation quality doing the exact thing we've been doing. It clearly doesn't work. The only \"statistic\" or number we get out of this is 100%, obviously, because we don't pass anything to client delivery if it received a \"no\" answer in the second step; we re-do that until it receives a \"yes\" answer. So all we can show them is \"our data was translated by a human and 100% verified by a 2nd human reviewer\".\n\n\nWell, that's not adequate. We clearly don't have 100% translation quality just because 2nd human reviewers said \"yes\" to every translation we delivered. So how do we actually get a NUMBER, a STAT, to actually measure the quality of all the translations, and also all the meta-annotations required like formal or derogatory (ie. what you'd see in dictionary entries)? I need a number to measure quality other than just the % of \";yes\" from our 2nd reviewers, which is always going to be 100% of what we deliver.\n\n\nHow can this be done? Does anyone know?",
"date": "2020-10-10"
},
{
"vote": 4,
"title": "Day 284 of #NLP365 - Learn NLP With Me â€“ Introduction To Flair For NLP",
"text": "Day 284.\n\n\nToday's post introduces FLAIR for NLP! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/10/day-284-learn-nlp-with-me-introduction-to-flair-for-nlp/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-10"
},
{
"vote": 7,
"title": "Can NLP Find / Predict Most Common Probable Mispelling of words",
"text": "Misspelling Government as Goverment is much more likely than misspelling Government as Gvrmnt, can NLP be used to *predict* a misspelling likelihood to a % chance and spit out the most common predicted misspell of a word with this % connected. E.G % chance you misspell Definitely as Definately is 10%. How could I go about making something that does this reasonably well? Happy to elaborate what I mean as I am a n00b to all this\n\n\n&#x200B;\n\n\n*Apologise if grammar/spelling is bad written on phone :]",
"date": "2020-10-10"
},
{
"vote": 6,
"title": "GPT-3 Vs Facebook's Blender",
"text": "Which among GPT-3 and Facebook Blender is a better at Conversational AI ? Given that GPT-3 has been trained on 175 billion parameters , is it fair to say it can perform better than Facebook's Parl AI ?",
"date": "2020-10-10"
},
{
"vote": 19,
"title": "Unsupervised Multi-Document Summarization using Neural Document Model | Research Paper Walkthrough",
"text": "Found this interesting paper early this week.  In this video, we will go through the work that proposes a Unsupervised Multi-document text summarisation method. This work claims to beat all existing unsupervised models in this task.\n\n\nAbstract -  In the age of information exploding, multi-document summarization is attracting particular attention for the ability to help people get the main ideas in a short time. Traditional extractive methods simply treat the document set as a group of sentences while ignoring the global semantics of the documents. Meanwhile, neural document model is effective on representing the semantic content of documents in low-dimensional vectors. In this paper, we propose a document-level reconstruction framework named DocRebuild, which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. We also apply two strategies, sentence filtering and beamsearch, to improve the performance of our method. Experimental results on the benchmark datasets DUC 2006 and DUC 2007 show that DocRebuild is effective and outperforms the state-of-the-art unsupervised algorithms.\n\n\nExplainer Video - \nhttps://youtu.be/qOoAlI5hpFk\n\nOriginal Paper - \nhttps://www.aclweb.org/anthology/C16-1143.pdf",
"date": "2020-10-09"
},
{
"vote": 1,
"title": "Unsupervised Multi-Document Summarization using Neural Document Model | Research Paper Walkthrough",
"text": "[deleted]",
"date": "2020-10-09"
},
{
"vote": 1,
"title": "NLP project to aggregate reports of covid outbreaks in public schools",
"text": "https://app.smartsheet.com/b/publish?EQBCT=00a2d3fbe4184e75b06f392fc66dca13\n\n\nHello,  the link above is to a 50 state database project that works by volunteers submitting media reports of COVID outbreaks in public schools.   As far as I know, this is one of the best tracking systems right now.\n\n\nIf you look at the state level data that is stored from each media report, this looks like an  NLP task that could be automated on a server using google serach results\n\nhttps://app.smartsheet.com/b/publish?EQBCT=dd5131656858466097b07ef98dd24dee\n\n\nI'm in comp sci, but not NLP, so came here to see if anyone can give suggestions on how to implement something like this.  thanks",
"date": "2020-10-09"
},
{
"vote": 4,
"title": "Day 283 of #NLP365 - Learn NLP With Me â€“ Hidden Markov Models (HMMs) III",
"text": "Day 283.\n\n\nToday's post continues on HMM Part 3! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/09/day-283-learn-nlp-with-me-hidden-markov-models-hmms-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-09"
},
{
"vote": 5,
"title": "Preprocessing before stemming but after lemmatization, right?",
"text": "Apologies in advance but I'm new to NLP, and I've not found any detailed examples of lemmatization.\n\n\n&#x200B;\n\n\nI am working on a project in which I would like to do some basic analysis of scholarly German texts. I would like to perform lemmatization rather than stemming on the corpus. It occurred to me that preprocessing destroys the contextual information that a lemmatizer needs to find the correct lemma. I'm looking for confirmation or correction of this assumption.",
"date": "2020-10-08"
},
{
"vote": 2,
"title": "Looking for well-researched dataset!",
"text": "Hello,\n\n\nI am looking for a well-researched and publicly available dataset for text classification. Would great if all the papers that used this dataset where gathered somewhere.",
"date": "2020-10-08"
},
{
"vote": 4,
"title": "The NLP Pandect update: new benchmarks and frameworks, latest blog posts and more",
"text": "[deleted]",
"date": "2020-10-08"
},
{
"vote": 5,
"title": "extract and label text from image documents",
"text": "I don't have PDF or word files but only IMG files of the original documents and am about to start a digitization process in order to perform nlp on the labeled text blocks. My original thought is to hand-copy each section from the documents to a JSON format and label each section, e.g., title, date, subtitle, main-section (the format of the documents is not uniform so some custom labels will need to be added). Will that be a good way to start?  Are there any tools that would help parse and label the data?",
"date": "2020-10-08"
},
{
"vote": 5,
"title": "GPT-2 Dataset Preparation:",
"text": "I wish to train GPT-2 the following way:\n\n\nMiddle:\nStart:\nEnd:\n\n\nSentence: At the core of the United Statesâ€™ mismanagement of the Coronavirus lies its distrust of science.\n\n\nMiddle: the United Statesâ€™ mismanagement of the Coronavirus \nStart: At the core of\nEnd: lies its distrust of science. \n\n\nIn other words, I input the middle part of a sentence and it generates the start and the end. Does anyone have a dataset for this or know how I can make one quickly?",
"date": "2020-10-07"
},
{
"vote": 61,
"title": "600 NLP Datasets and Glory",
"text": "Hey,\n\n\nQuantum Stat has curated some of the best public/academic datasets for various tasks in NLP. We have recently passed the 600th mark and we are continuing to add more. Check out the datasets here:\n\n\nhttps://datasets.quantumstat.com/",
"date": "2020-10-07"
},
{
"vote": 1,
"title": "Day 281 of #NLP365 - NLP Papers Summary â€“ Knowledge Reasoning Over Knowledge Graph I",
"text": "Day 281.\n\n\nToday's post is the first post that summarises an NLP paper on knowledge reasoning over knowledge graph. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/07/day-281-nlp-papers-summary-knowledge-reasoning-over-knowledge-graph-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-07"
},
{
"vote": 3,
"title": "[P] Multimodal Emotion Recognition Competition 2020 (MERC 2020)",
"text": "Object\n\n\nThis competition aims at developing the state-of-the-art models for emotion recognition from emotion-labelled facial videos with facial, speech, and text data. Participants are expected to put their efforts for obtaining the highest accuracy of recognizing one out of 7-classes, i.e., the neural and 6 emotions.\n\n\nEvaluation\n\n\n\n\nSubmissions are evaluated by the classification accuracy on the whole three test datasets.\n\n\n\n\nPrize\n\n\nThe rank of participating teams will be determined by the order of their final scores.\n\n\n\n\n1st ranked: 2000$\n\n\n2nd ranked: 1000$\n\n\n3rd ranked: 500$\n\n\n\n\nFor more information, please visit \nhttps://evalai.cloudcv.org/web/challenges/challenge-page/692/overview\n.",
"date": "2020-10-07"
},
{
"vote": 1,
"title": "Text Mining Project - gathering data",
"text": "[removed]",
"date": "2020-10-07"
},
{
"vote": 19,
"title": "Developing Chatbots with RASA From Developing to Deployment",
"text": "Hey There Folks! After interacting with certain Reddit developers, I have published a blog on Topic: Developing Chatbots with RASA Framework- From Intuition to Implementation-Which covers the understanding of the core concepts of a chatbot to developing and deploying a chatbot. This Blog also consists of the source code of a basic chatbot which I have implemented. I made this blog by keeping in mind For all NLP and Machine Learning Enthusiasts who were looking for an in-depth tutorial or guide on how to begin to develop or implement a chatbot.\n\n\nDo Have a read and let me know your thoughts about it!\n\n\nLink to Blog: \nhttps://medium.com/data-science-community-srm/developing-chatbots-with-rasa-intuition-to-implementation-39c1dd34274c\n \n\n\nLink to Project's Source Code:\nhttps://github.com/harshgeek4coder/Rasa-COVID-19-Tracker-FAQ-Chatbot\n\n\n#machinelearning\n \n#chatbots\n \n#deeplearning\n \n#ai\n \n#python\n \n#artificialintelligence\n \n#nlp\n \n#algorithms\n \n#ml\n \n#datascience\n \n#programming\n \n#learning\n \n#rasa",
"date": "2020-10-07"
},
{
"vote": 7,
"title": "Must all neural machine translation architectures include an encoder and a decoder?",
"text": null,
"date": "2020-10-06"
},
{
"vote": 5,
"title": "Day 280 of #NLP365 - NLP Discovery â€“ Lang.Aiâ€™s Unsupervised Intent Discovery (Whitepaper)",
"text": "Day 280.\n\n\nToday's post covers unsupervised intent discovery, specifically how \nlang.ai\n is tackling this problem. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/06/day-280-nlp-discovery-lang-ais-unsupervised-intent-discovery-whitepaper/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-06"
},
{
"vote": 3,
"title": "Punctuation in dependency parsing - some resources",
"text": "Hello everybody,\n\n\nI'm trying to understand how punctuation marks affect the dependency parsing task done through deep neural models.\n\n\nIn particular, I am trying to understand if there is any standard on the use of punctuation marks in the model training and evaluation process. I am using \nCoNLL-U format\n as a dataset, but looking through the documentation I could not find a standard on how to handle punctuation marks. In the various scripts for the evaluation of dependency parsing it seems to be left to the author of the model to decide whether or not to include punctuation marks.\n\n\nSearching for a bit on the web I found only two articles that talk about this theme, namely \"\nNightmare at test time: How punctuation prevents parsers from generalizing\n\" and \"\nPunctuation Processing for Projective Dependency Parsing\n\".\n\n\nHave any of you encountered this problem before? Do you have any other reliable articles or resources (in the sense that, eventually, I have to mention them within a thesis) that I can consult for this problem?\n\n\nThank you all!",
"date": "2020-10-06"
},
{
"vote": 13,
"title": "I'm a senior software engineer with no NLP experience. Is it possible to get a job without a masters or PHD and simply self study?",
"text": "[deleted]",
"date": "2020-10-05"
},
{
"vote": 3,
"title": "How can GPT-2 / GPT-3 possibly come up with new words?",
"text": "I'm lacking some understanding with regard to how projects like \nhttps://www.thisworddoesnotexist.com/\n are possible. Please help me understand were my conception is wrong:\n\n\n\n\nGPT-2 is trained on WebText, thereby containing (mostly) existing words, while learning long distance dependencies between these words. We therefore get nothing but a conditional probability distribution for each given sequence (?)\n\n\n\n\nWhen fine-tuning GPT-2, we simply over-emphasize certain things that GPT-2 has already learned, making some word sequences more probable than others, also pushing GPT-2 to \"forget\" previously known connections, that are not important to the fine-tuning task.\n\n\n\n\n\n\nThen how the hell does it come up with new words?",
"date": "2020-10-05"
},
{
"vote": 4,
"title": "Any open source AI assistant/ voice recognition technology with transfer learning capabilities?",
"text": "Hello,\n\n\nI would like to make my life easier with things like calendar management, todo list creation, task management in trello, journaling. However, existing technology I find to be very poor, the supposedly SOTA apps like google assistant don't work for me at all.\n\n\nSince I work in ASR/NLP myself, I can tell that it would be easier if two things were possible:\n\n\n\n\nAllow me to fine tune the model to my data e.g. by doing some speech recognition for me where I can correct words and gradually make the model better.\n\n\nAllow me to make custom extensions (AFAIK Alexa has this functionality).\n\n\nAllow me to make my own list of \"allowed\" commands - if there were only a handful of classes, the model would have an easier time classifying what I won't to do.\n\n\n\n\nAn ideal workflow for me would be the following:\n\n\n\n\nI choose a number of commands that I want to use and the initialising command for that.\n\n\nI specify the relevant integrations (google calendar, trello, journalling app).\n\n\nThe voice recognition continually learns from my corrections.\n\n\n\n\nGoogle assistant tries so hard to so many things that it ends up being useless. If I had a extensible, programmable app I could achieve much more with much higher accuracy.\n\n\nState of open-source AI assistants is quite miserable afaik, even the most popular project (Leon) has very few features and hasn't been worked on in over a year... Is there any hope?",
"date": "2020-10-05"
},
{
"vote": 2,
"title": "Day 279 of #NLP365 - Learn NLP With Me â€“ Trustworthy And Explainable AI Achieved Through Knowledge Graphs",
"text": "Day 279.\n\n\nToday's post covers the trustworthy and explainability aspect of knowledge graphs! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/05/day-279-learn-nlp-with-me-trustworthy-and-explainable-ai-achieved-through-knowledge-graphs/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-05"
},
{
"vote": 10,
"title": "Day 278 of #NLP365 - Learn NLP With Me â€“ Richer Sentence Embeddings Using Sentence-BERT",
"text": "Day 278.\n\n\nToday's post covers using sentence-bert to build richer sentence embeddings! Check it out :)\n\n\nhttps://ryanong.co.uk/2020/10/04/day-278-learn-nlp-with-me-richer-sentence-embeddings-using-sentence-bert/\n\n\nBest,\n\n\nRyan",
"date": "2020-10-04"
},
{
"vote": 4,
"title": "Request for Recommendation on Set of Technology for Project",
"text": "I am trying to identify a good set of technologies that will identify the trends in journal articles over time. I would like to start with the full-text articles of a journal publication, say the January 1990 edition that has a collection of articles, and identify the topics/keywords algorithmically. I would then like to continue to put new editions into the dataset to see how the topics/keywords change over time, whether it is month to month, year to year, or decade to decade. In the end, I want to work on visualizing the topics and trends.\n\n\nDoes anybody have a recommendation for a set of technology and/or good example of this kind of design?\n\n\nI'm solid on any programming and software setup. Mainly looking to see what the current trend is for this kind of project.",
"date": "2020-10-04"
},
{
"vote": 1,
"title": "Want to work with GPT-3?",
"text": "[deleted]",
"date": "2020-10-04"
},
{
"vote": 8,
"title": "NLP course recommendations (paid)",
"text": "[deleted]",
"date": "2020-10-02"
},
{
"vote": 3,
"title": "John Snow Labs Spark-NLP 2.6.2: New SentenceDetectorDL, improved BioBERT models, new Models Hub, and other improvements!",
"text": null,
"date": "2020-10-02"
},
{
"vote": 30,
"title": "Gnothi: personal journal that uses huggingface/transformers to provide insights &amp; resources",
"text": "Just launched \nGnothi\n, a personal journal with NLP features.\n\n\n\n\nResources\n. Embeds your journal entries via UKPLab/sentence-transformers to match you with self-help books and therapists\n\n\nSummaries\n. Summarize entries (per-entry, and global #days in #words reports): facebook/bart-large-cnn\n\n\nThemes\n. Shows common themes across entries (agglomorative clustering of embeddings)\n\n\nQuestion-answering\n. Ask questions of your entries: allenai/longformer-large-4096-finetuned-triviaqa\n\n\n\n\nOpen source\n - I'm a newb and could use help. Particularly from this community, \n#28\n about consideration of the models being used; and \n#10\n how to scale these things. Thanks huggingface / UKPLab for your incredible tools!",
"date": "2020-10-02"
},
{
"vote": 2,
"title": "Virtual Natural Language Processing Conference featuring Trevor Hastie",
"text": "The Chicago Chapter of the American Statistical Association is hosting a virtual conference on NLP and giving the annual Statistician of the Year Award to Trevor Hastie. We've put a ton of work into developing a great lineup of speakers focusing on many aspects of NLP.\n\n\nThe full agenda can be found here: \nhttps://community.amstat.org/chicagochapter/calendar/upcomingevents1/new-item2676284",
"date": "2020-10-01"
},
{
"vote": 2,
"title": "[Research] Utterance-level Dialogue Understanding: An Empirical Study",
"text": "[deleted]",
"date": "2020-10-01"
},
{
"vote": 3,
"title": "Sentiment Analysis specifics",
"text": "What are some more specific questions that typically get asked with sentiment analysis? I'm trying to brainstorm how I can approach sentiment on a deeper level\n\n\nThanks!",
"date": "2020-09-30"
},
{
"vote": 1,
"title": "Natural language processing applied to mobility",
"text": "Hello everybody!\n\n\nI am going to start a master in natural language processing, and I have the opportunity to do an internship in a company that analyses mobility data, but we are struggling to find a common topic. Do you know about some examples that applies natural language processing to mobility data?\n\n\nThanks in advance!",
"date": "2020-09-30"
},
{
"vote": 3,
"title": "Text Clustering Subcategory/Subtopic Assignment Question",
"text": "Hello, I am attempting to build a topic model for a large text dataset. In the end stage, I want to have a hierarchy of topics, where each topic contains more finegrained subtopics and so forth. Given new data, I also want to be able to send that data through the hierarchy of my topics and see what topics/subtopics this new data resides in. \n\n\nFor the purpose of topic modeling, I have tried many approaches so far, including Latent Dirichlet Allocation, Latent Semantic Analysis, and KMeans clustering. I have been dissatisfied with how LDA and LSI generate topics for my data, and I have also been dissapointed in KMean's ability to only generate one layer of clusters for my text data. \n\n\nSo far the best approach has been using KMeans, and accessing the words that appear in each of my cluster centroids. I have considered trying something where I generate separate KMeans models for each cluster created by my original KMeans model, but I feel like there is a better way to do this since Agglomerative clustering algorithms exist. I am wondering if I can leverage one of those algorithms (ward, etc) to accomplish my goal here. I'm not sure if this will work either though, as I have read online that Agglomerative clustering algorithms are not used for prediction on unseen data. \n\n\nHas anyone here experienced a problem like this, and if so, what methods did you use to solve it? \n\n\nThank you.",
"date": "2020-09-30"
},
{
"vote": 1,
"title": "Day 274 of #NLP365 - Learn NLP With Me â€“ Training The Named Entity Recognizer Using SpaCy III",
"text": "Day 274.\n\n\nToday's post continues on with training a custom NER model using spaCy. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/09/30/day-274-learn-nlp-with-me-training-the-named-entity-recognizer-using-spacy-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-30"
},
{
"vote": 2,
"title": "Training Domain Specific Semantic Role Labeling",
"text": "Hi everyone,\nI was wondering how difficult it is to train SLR specific to a domain from scratch. I have tried using pre-trained models from AllenNLP which works pretty well but I think I would benefit more by training on my own dataset. I think I am up for manually annotating dataset but definitely won't/can't label a lot of data points.\nI would really appreciate any papers, articles, or tools that can get me started on this. I have tried searching but everything seems far-fetched at this point.\n\n\nThank you very much!",
"date": "2020-09-30"
},
{
"vote": 2,
"title": "Topic Model for the web!",
"text": "This is quite a broad and optimistic question / discussion, but here goes!\n\n\nI am trying to build a single topic model to run across all types of web pages, so the dataset is naturally large, noisy and varied. My attempts so far have been using LDA & Amazon's NTM - and both seem to have pros and cons, but I am convinced that the way I am preparing the corpus is key to getting this to good quality!\n\n\nHas anyone tried to model a very varied corpus, what have you found out that works or does not; vocabulary size,  hyper-params, models, pre-processing etc",
"date": "2020-09-30"
},
{
"vote": 2,
"title": "What loss to use for question answering?",
"text": "I'm in the process of finetuning a BERT model to the long answer in the Natural Questions dataset. I'm training the model just like a SQuAD model with predicting start and end tokens. \n\n\nI use Huggingface and PyTorch.\n\n\nSo the targets and labels have a shape/size of ***[batch, 2]***. My problem is that I can't input \"multi-targets\" which I think is refering to the fact that the last shape is ***2***. \n\n\nShould I choose another loss function or is there another way to bypass this problem?\n\n\nThis code I'm using:\n\n\ndef loss_fn(preds, targets):\n    return nn.CrossEntropyLoss()(preds,labels)\n\n\n\n.\n\n\nclass DecoderModel(nn.Module):\n\n    def __init__(self, model_args, encoder_config, loss_fn):\n        super(DecoderModel, self).__init__()\n        # ...\n\n    def forward(self, pooled_output, labels):   \n        pooled_output = self.dropout(pooled_output)\n        logits = self.linear(pooled_output)\n\n        start_logits, end_logits = logits.split(1, dim = -1)\n        start_logit = torch.squeeze(start_logits, axis=-1)\n        end_logit = torch.squeeze(end_logits, axis=-1)\n\n        # Concatenate into a &quot;label&quot;\n        preds = torch.cat((start_logits, end_logits), -1)\n\n        # Calculate loss\n        loss = self.loss_fn(\n            preds = preds, \n            labels = labels)\n\n        return loss, preds\n\n\n\nThe targets properties are:\n***torch.int64*** & ***[3,2]*** \n\n\nThe predictions properties are:\n***torch.float32*** & ***[3,2]***",
"date": "2020-09-30"
},
{
"vote": 1,
"title": "Psychology and Natural Language Use (All are welcome)",
"text": "[removed]",
"date": "2020-09-30"
},
{
"vote": 2,
"title": "How to evaluate the factual consistency of generated questions?",
"text": "Hey guys. I'm currently working on a project to generate factually consistent questions. I got the idea after thinking that there's a lot of misinformation going around (especially with this COVID-19 thing) and that current QG methods don't exactly seem to take this problem into account.\n\n\nWould anyone know of any work that explores this idea? Any tips or feedback are appreciated. Thanks.",
"date": "2020-09-30"
},
{
"vote": 1,
"title": "[News] Introducing ML News (mln.dev)",
"text": "ML News\n (MLN for short) is a community for sharing and discussing all things related to machine learning, deep learning, AI, data science, and the like. (You can join now using the code \nconvergence\n).\n\n\nInspired by Hacker News, Lobste.rs, Reddit, and the original Slashdot, we decided to create a dedicated place for experts and enthusiasts to engage in open learning, discussion, and occasional whimsy.\n\n\nThere are many different channels for tech-related news. In our experience, these are often flooded with not-so-relevant posts which somehow fall under the \"tech\" umbrella. We've had a longstanding habit of sharing interesting ML-related articles and field-defining research with friends and peers, and know that we are by far not the only ones. That's why we decided to build a community dedicated to the field of ML. We hope that by narrowing the scope, we can build a space where ML people like ourselves can easily find interesting news, breakthroughs, and updates in the field, or essentially anything which could pique our interest.\n\n\nFrom ML experts to AI enthusiasts, this is a community that values honest debate, curiosity, positivity, and good humor. Please help us maintain these core values as the community grows.\n\n\nJoin the community\n using the code \nconvergence\n.",
"date": "2020-09-29"
},
{
"vote": 10,
"title": "GPT-3, AI Access Inequality Gap, and Advancing Technology question",
"text": "I am currently running the GPT-2 models locally/offline on my own hardware and learning a lot from these experiments, but I have been floored by the fantastic outputs I am seeing from GPT-3.\n\n\nUnfortunately, GPT-3, being just released and highly likely to find significant commercial application$$$ - not to mention probably occupying an estimated 700GB on disk for the model alone, and the need for an offline system with a similar amount of RAM at a minimum...  I doubt that any GPT-3 models will be released to the general public as GPT-2 has.  Yet, anyway...\n\n\nMy question here is:  In the estimation of those on here who are familiar with autoregressive models like GPT-3 and their enormous resource requirements, as well as how long it will take before something like GPT-3 becomes \"old tech\" and drops in market value to a point where it may be distributed freely for researchers/hobbyists - How many years hence would you estimate it might be before such an availability arrives?  In other words, how long will it take before reasonably priced computing power arrives that can easily support GPT-3, and simultaneously, the cost of GPT-3 becomes a non-issue as it is surpassed by... GPT-4, or 5, or something else that will undoubtedly arrive in a few years (assuming we don't destroy ourselves as a species before then lol).\n\n\nMy robots want to know :)",
"date": "2020-09-28"
},
{
"vote": 3,
"title": "Day 272 of #NLP365 - NLP Discovery â€“ Prodigy Annotation Tool",
"text": "Day 272.\n\n\nToday's post introduces a SpaCy annotation tool Prodigy that allows you to annotate your dataset to build custom NER models with ease! Unfortunately I later realised that you have to pay to use Prodigy but nonetheless, it looks pretty good. Check it out :)\n\n\nhttps://ryanong.co.uk/2020/09/28/day-272-nlp-discovery-prodigy-annotation-tool/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-28"
},
{
"vote": 2,
"title": "Sandwich Transformer: Improving Transformer Models by Reordering their Sublayers",
"text": null,
"date": "2020-09-27"
},
{
"vote": 28,
"title": "(Tutorial) Identifying the Gender of a Movie Character Based on the Lines They Deliver.",
"text": "Last summer, I worked on a project with some professors on identifying the gender of a movie character based on the lines they delivered. It was a great way for me to understand how to build a Text Classifier from scratch and I want to share what I learned with you guys. The dataset used is the Cornell Movie-Dialogs Corpus which contains dialogue from 600+ Hollywood films. This article will walk you through creating an LSTM to classify sentences that you can apply to any dataset you want. If you have any feedback or suggestions, I'd greatly appreciate it. Hopefully, this helps you out.\n\n\nhttps://www.siddcodes.com/identifying-the-gender-of-a-movie-character-with-deep-learning-nlp-and-pytorch/",
"date": "2020-09-27"
},
{
"vote": 3,
"title": "Day 271 of #NLP365 - Learn NLP With Me â€“ Hidden Markov Models (HMMs) I",
"text": "Day 271.\n\n\nToday's post covers the hidden markov models (HMMs). Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/27/day-271-learn-nlp-with-me-hidden-markov-models-hmms-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-27"
},
{
"vote": 1,
"title": "Why you should learn multiple languages?",
"text": null,
"date": "2020-09-27"
},
{
"vote": 15,
"title": "Computational Linguistics as a way of discovering the underlying mechanism of communication?",
"text": "This train of thought came from a Yoav Goldberg tweet that he thinks prediction using NLP Models is overrated and eventually we will start using NLP not as a tool for predicting language but as a way of determining some underlying mechanism of how language is used, created, or changes its form.   \n\n\nIs there any research in this direction? Also, what do you think about Computational Linguistics - is it a science useful in discovering inherent features of how humans communicate; or is it a way of replicating or mimicking human language?",
"date": "2020-09-27"
},
{
"vote": 7,
"title": "Good Tutorials for Custom Sentence Embedding",
"text": "Does anyone know of a good tutorial on custom sentence embedding techniques?  I am working on a project related to patent applications/patents and would like to create custom sentence embedding based on information contained in patent applications/patents.  \n\n\nThanks!",
"date": "2020-09-27"
},
{
"vote": 1,
"title": "Did Mary crush Charlie? Bert Word Embeddings",
"text": null,
"date": "2020-09-26"
},
{
"vote": 1,
"title": "Is there any difference in the way Spacy and NLTK tokenized the text?",
"text": "[deleted]",
"date": "2020-09-26"
},
{
"vote": 3,
"title": "BERT NLP: Using DistilBert To Build A Question Answering System",
"text": null,
"date": "2020-09-26"
},
{
"vote": 4,
"title": "Entity Detection with raw Ocr'd data",
"text": "Please excuse my long post. The bottom line of my post is the following: I want to know the best method for extracting entities from unstructured, poorly ocr'd data that sometimes does not resemble words. Should I use TensorFlow, spaCy, NLTK or another method? For more information, please read my project description below.\n\n\nHere is the long version of my quesiton\n\n\nI am a literature professor who works on 18th-century French theater. I am very new to coding. I started learning Python and R in December and have progressed to using some data analytic tools like Pandas DataFrames, machine learning algorithms like SVM or random forest as well as deeplearning tools like TensorFlowâ€™s Keras. Having said that, I may know how to write code that completes a task, but I donâ€™t have the depth of experience to know the best practice.\n\n\nI joined this forum because I would like to set up an entity detection on raw, ocr data of historical texts. As you can imagine the text is not very clean with a lot of spaces. Letters from a word can also be separated into lines.\n\n\nMy dataset includes extracted text from 2000+ newspapers from 1780s Bordeaux. (Iâ€™d like to scale up my project to include 20000+ newspapers from provincial cities around France) Through Google Drive's API, I managed to convert the pdfs into a Google Doc and then convert the Google Doc into JSONs. This enabled me to retain formatting information such as bold and italicized words. Finally, I used jmespath to pass the data into a pandas DataFrame. I can now search over the Dataset very quickly, but the extracted text is still pretty raw.\n\n\nI want to create a workflow to extract data from historical newspapers. For my project, I am interested in performance data. I decided to use my convoluted workflow so I could retain formatting information because the names of authors, theatrical works and performers are italicized. They also appear before or after certain key words (i.e. performance of, comedy, tragedy, followed by).\n\n\nI have a list of performers, theatrical works and venues that I created from reading through many newspapers. I started using Python's fuzzywuzzy library to find italicized strings that are closest to my list of performers, authors, theatrical works and venues. This is helpful but I was wondering if I could use deep learning methods to find entities not on my list of words. Would anyone recommend a library in Python that works best for this type of unstructured problem..such as spaCy, NLTK or TensorFlow.\n\n\nMy data can currently appear in two formats: as a string that I generate through str.cat() or as a Pandas.Series in a text column. The pandas series can either have extracted words/characters as a string, or a series of words as a string.\n\n\nHere are two examples of how my data can look: both are really best-case scenarios. There are quite a few examples that are not very legible. \n\n\n\n\nINSTRUCTION DES SOURDS ET MUETS. LeÃ§on publique & gratuite , par M. l'AbbÃ© SICARD , aujourdhui, depuis 3 heures aprÃ¨s midi jufqu'Ã  6h. du foir, en la falle du MufÃ©e. SPECTACLES. THÃ‰Ã‚TRE DE BORDEAUX. Aujourd. les Trois Coufines s ComÃ©die , en trois ailes, ornÃ©e de trois Ballets. VARIÃ‰TÃ‰S AMUSANTES , coursS'-Seurin, aujourd., Ã  7 h. du foir, une reprÃ©fentation des Petites Affiches, PiÃ¨ce Ã©pifodique , en & de Philips & Sara, ComÃ©die , en profe & en un aÃ©le , avec un Divertiffement. En attendant le Mariage, ComÃ©die, Â«n profe & en un aÃ©le. Lu & approuvÃ©, BARENNES.\n\n\n\n\nSPECTACLES. THÃ‰Ã‚TRE DE BORDEAUX. Aujourd. Richard Caur-de-Lion , Op.-bouff., avec un petit divertifT. ; prÃ©c. du MÃ©decin malgrÃ© lui. En attendant Renaud d Afts , Op.-bouff. nouveau. VARIÃ‰TÃ‰S AMUSANTES , allÃ©es de Tourny, aujourd. , Ã  6 h \\\\ du foir , une reprÃ©fentation de la Muficomanie, Com. en profe & en un afte ; luiv. des deux MarÃ»nes , & prÃ©cÃ©d. de Carmagnole & GuÃ¯Ã¯lot Gorju ,Trag.-burlefque. En attendant Blaife le Hargneux , ComÃ©- die , en profe & en un aÃ»e. Lu & approuvÃ©, BA RENNES.\n\n\n\n\n\n\nI need to extract the all caps words which are the venues/theatrical troupes (i.e. THÃ‰Ã‚TRE DE BORDEAUX), and then the plays (les Trois Cousines) which appears before the word ComÃ©die. There are other entities, and other issues that also follow predictable patterns. For instance, a play can be advertised for tomorrow but not performed on the day of the paperâ€™s publication. Or a play could have been advertised but then the following day a notice comes out about its cancellation. These situations also follow a predictable pattern when the ocr is good.\n\n\nAny suggestions would be helpful. Should I tokenize my data and set up a collocation protocol for entity detection? What works best on a dataset whose words are probably not in an NLP library because of the raw state of the ocr data? Would that be TensorFlow?",
"date": "2020-09-25"
},
{
"vote": 7,
"title": "Can I use ECB+ Corpus for Event and Timeline Extraction from news?",
"text": "Hello, I am currently doing Event and Timeline extraction from news as my project in uni, and I need corpus for it.  In numerous papers that i've read they use reuter or ontonotes and both of them are paid , I even requested LDC for corpus and they havent replied .Can people who have worked on this topic, help me find an alternative corpus? I came accross \nECB+ corpus\n, Is it any good and is it helpful for my project? I am a beginner in NLP so please help me out",
"date": "2020-09-25"
},
{
"vote": 3,
"title": "Day 269 of #NLP365 - Learn NLP With Me â€“ Training The Named Entity Recognizer Using SpaCy I",
"text": "Day 269.\n\n\nToday's post is a practical post on training a custom NER using SpaCy. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/25/day-269-learn-nlp-with-me-training-the-named-entity-recognizer-using-spacy-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-25"
},
{
"vote": 1,
"title": "Understanding the Pytorch Transformer Tutorial",
"text": "[deleted]",
"date": "2020-09-25"
},
{
"vote": 1,
"title": "PyData Pune: \"Intuitive Decision-making Using NLP\" (27th Sep 2020, free, online)",
"text": "https://www.meetup.com/PyData-Pune/events/273035234/\n\n\nDate:\n Sunday, September 27, 2020\n\n\nTime:\n 7:00 PM to 8:30 PM GMT+5:30\n\n\nTalk: \"Intuitive Decision-making Using NLP\"\n\n\nSpeaker: Bastin Robin\n\n\nAbstract: Data science becomes an essential component of decision making which makes great sense. Now apart from the generally supervised learning lets focus on using unsupervised learning methodologies to make sense from textual data. Let's explore more on Word2Vec and Doc2Vec to generate auto insights and understand how the NLP can even help us to build contextual recommendations.\n\n\nBio: Bastin Robin is CTO and, Cheif Data Scientist at CleverInsight. He has successfully built data products for the largest FMCG's and Retail, Banking, Telecom, Social, Government from scratch, and deployed it successfully with great feedbacks. He has experience of developing products which are a combination of Machine Learning, Deep Learning, Neural networks, Collective Intelligence & Web Intelligence.",
"date": "2020-09-25"
},
{
"vote": 2,
"title": "Identifying the important entity from list of named entities",
"text": "There are a lot of companies that do named entity recognition such as Amazon [ORG] , google [ORG] , microsoft [ORG] etc, I'm however interested in understanding how spacy [ORG] does named entity recognition\n\n\nwhat's the NLP task that I should look for if I want to filter out spacy as the only organization?.. because that's the main focus and everything else is a passing mention.",
"date": "2020-09-25"
},
{
"vote": 17,
"title": "Interactive Analysis of Sentence Embeddings",
"text": null,
"date": "2020-09-25"
},
{
"vote": 2,
"title": "Algebraic Linguistics Papers",
"text": null,
"date": "2020-09-24"
},
{
"vote": 46,
"title": "Facebook AI's benchmark release for NLP",
"text": "Facebook AI very recently launched a unifying model to perform knowledge intensive language tasks such as fact checking, entity linking, slot filling, question answering, chit chat dialogue using a unified knowledge source from Wikipedia page snapshots and a common interface. This uses a set of 11 corpora to achieve 5 tasks that will help the language processing for real world tasks and in the advancement of technology. \n\n\nDo check out the details at \nthis page",
"date": "2020-09-24"
},
{
"vote": 2,
"title": "Day 268 of #NLP365 - Learn NLP With Me â€“ Building A Conversational Interface III",
"text": "Day 268.\n\n\nToday's post is another post on building a conversational interface. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/24/day-268-learn-nlp-with-me-building-a-conversational-interface-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-24"
},
{
"vote": 5,
"title": "Internship at United Nations Development Programme",
"text": "[removed]",
"date": "2020-09-24"
},
{
"vote": 1,
"title": "Text Style Transfer?",
"text": "Is there some text style transfer implementation of a pretrained model (such as gpt2) which can be finetuned to accommodate different styles? I'm not looking for positive to negative style transfer, I am looking for factual to romantic style transfer.",
"date": "2020-09-23"
},
{
"vote": 1,
"title": "What would be a typical pre-processing and data normalization pipeline for time series data. (For Non-Linear Models i.e Neural Networks)",
"text": "I've started to working on time series and wondering, what would be the best data normalizing and pre-processing technique for Non Linear Models ie. Neural Networks.\n\n\n&#x200B;\n\n\nOne i can think of is minmax normalization\n\n\nz = (x - min(x))  /  (max(x) - min(x))",
"date": "2020-09-23"
},
{
"vote": 1,
"title": "Day 267 of #NLP365 - Learn NLP With Me â€“ Building A Conversational Interface II",
"text": "Day 267.\n\n\nToday's post is another post on buiding a conversational interface. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/23/day-267-learn-nlp-with-me-building-a-conversational-interface-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-23"
},
{
"vote": 1,
"title": "Stanford's GloVe with Python",
"text": "Word embedding is used to represent word into a vector format to be easily comprehended by machines. For this very reason, Stanford came up with a model called GloVe(Global Vectors) to use global level statistics and determine vector representation. \nThis model determines a word co-occurrence matrix to give the probability of any two words occurring together globally in order to predict what a word in any given maybe \nFor the \nPython implementation",
"date": "2020-09-22"
},
{
"vote": 1,
"title": "SotA on Discourse Parsing / Rhetorical Structure Theory",
"text": "[deleted]",
"date": "2020-09-22"
},
{
"vote": 1,
"title": "Day 266 of #NLP365 - Learn NLP With Me â€“ Building A Conversational Interface I",
"text": "Day 266.\n\n\nToday's post is a short post covering the 10 main steps of buiding a conversational interface. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/22/day-266-learn-nlp-with-me-building-a-conversational-interface-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-22"
},
{
"vote": 2,
"title": "Text classification for news theme retreival roadmap",
"text": "I have a task to write a model that will be able to retreive all the news with a \none\n defined theme, let it be \"football\". I built a model on Python scikit-learn with LinearSVM algo and tf-idf feature extracting method. Took about 5000 texts. Made preprocessing(remove punctuations and stop words, lowercase, lemmatize) But the results are not good enough, I have problems with new  texts with new keywords in them or very short or very long texts, despite the fact that confusion matrix shows me good results on the test set.\n\n\nActually I don't know where to find answers, so want to test my luck on Reddit. Here are the questions to NLP gurus:\n\n\n\n\nCan word2vec help me to improve my model instead of td-idf in order to catch the meaning of the text/words, not only their frequency or idf? I could not embed word2vec in my code by myself, so I wonder if it make sense...\n\n\nIf yes, does the size of a text matter?\n\n\n\n\n\n\nCan I somehow import my own keywords(that were not mentioned in the train set) to the classification model, in order to recognize them when they first appear in a text. \n\n\nI have read about LDA, and it sounds like it can help me to control my keywords - add relevant keywords and delete irrelevant; LDA is based on defining a text topic. Exactly what I need. But also people write that I can't control it enough. So may be there is a way to somehow build a supervised model on top of LDA to extract all news with a defined theme \"football\"? \n\n\nIf someone has encountered with similar task, could you please share the best set of steps to solve it?\n\n\n\n\nKindly, nlp youngling.",
"date": "2020-09-22"
},
{
"vote": 11,
"title": "Recommendations for someone starting out with NLP",
"text": "Iâ€™m just getting started on learning NLP and Iâ€™d really like some recommendations on books,courses, papers, etc. \n\n\n(This stuff should really be on the subâ€™s wiki or on a pinned post so that beginners like me arenâ€™t annoying lol)",
"date": "2020-09-22"
},
{
"vote": 58,
"title": "Facebook AI Releases KILT, A New Benchmark For Knowledge-Intensive NLP Tasks",
"text": "AI researchers have made significant advancements in building models that can generate text that mimic the natural language. State-of-the-art technology performs so well that it is sometimes hard to distinguish their output from the text written by a person. An essential next step is to make these models generate the fluent and grounded text in real-world knowledge.\n\n\nKILT (Knowledge Intensive Language Tasks)\nÂ helps AI researchers and enthusiasts build models that can better leverage real-world information to accomplish a broad range of tasks. UnitingÂ \n11 widely used public data sets\n, KILT represents five different tasks:Â \n\n\nSummary:\n \nhttps://www.marktechpost.com/2020/09/21/facebook-ai-releases-kilt-a-new-benchmark-for-knowledge-intensive-nlp-tasks/\n\n\nGithub:\n \nhttps://github.com/facebookresearch/KILT\n\n\nPaper:\n \nhttps://arxiv.org/abs/2009.02252\n\n\nGuide:\n \nhttps://ai.facebook.com/tools/kilt/",
"date": "2020-09-22"
},
{
"vote": 6,
"title": "Elasticsearch for NLP course(s)",
"text": "Hi Guys,\n\n\nDo you have any idea if there is a good course on ElasticSearch for the purpose of NLP. \n\n\nMOOCs or tutoirals would be great?",
"date": "2020-09-21"
},
{
"vote": 8,
"title": "I want to learn how to build a program that offers writing improvement suggestions to students writing. Where do I start?",
"text": null,
"date": "2020-09-21"
},
{
"vote": 2,
"title": "Day 265 of #NLP365 - Learn NLP With Me â€“ Intent Classification For Chatbots (Airbnbâ€™s Approach)",
"text": "Day 265.\n\n\nToday's post focuses on intent classification, taking Airbnb as an application example! Check it out below:\n\n\nhttps://ryanong.co.uk/2020/09/21/day-265-learn-nlp-with-me-intent-classification-for-chatbots-airbnbs-approach/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-21"
},
{
"vote": 9,
"title": "Partitioning news telecast transcript where topics change",
"text": "Hello everyone,\n\n\nFor my research work we are to classify news telecasts (bbc) into 21 political topics. We have the transcripts for these telecasts and we want to separate them into partition of texts that talk about same (roughly) topic. \n\n\nThe current approach that I am following is to check sentence by sentence and if the current sentence has a similarity beyond a threshold with the previous sentence, then we put them in same partition. While this approach works, we want to improve the quality of partitions.\n\n\nHow do I go about this? Is there a better algorithm to do so? More so something that works for news telecasts transcripts where topics can frequently change.\n\n\nThank you!!",
"date": "2020-09-21"
},
{
"vote": 7,
"title": "Day 264 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.26 â€“ Dialogue Systems And Chatbots VI",
"text": "Day 264.\n\n\nToday's post focuses on another chapter from the Speech and Language Processing textbook, specifically on chapter 26 - dialogue systems and chatbots.\n\n\nhttps://ryanong.co.uk/2020/09/20/day-264-learn-nlp-with-me-slp-textbook-ch-26-dialogue-systems-and-chatbots-vi/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-20"
},
{
"vote": 2,
"title": "Can we pretrain gpt-2 medium in colab or colab pro?",
"text": "Just wanted a quick suggestion for deciding my thesis topi.",
"date": "2020-09-20"
},
{
"vote": 13,
"title": "How good are Transformers in handling regression tasks, such as time series?",
"text": "Actually I've read somewhere that Transformers can only be efficient in finite token scenario. They can't handle regression tasks such as time series.\nHow correct is it?",
"date": "2020-09-20"
},
{
"vote": 1,
"title": "What is the WEIRDEST yet GENIUS problem statement you've seen NLP be applied to?",
"text": "[deleted]",
"date": "2020-09-19"
},
{
"vote": 18,
"title": "Short text classification models",
"text": "I'm relatively new to NLP but have been working on Computer Vision problems for a while. I'm looking for some advice on what papers/models I can look into for a text classification problem that I'm working on.\n\n\nThe input sentences are very short (about 3-10 words each.)\n\n\nIt would be great if you guys could suggest some word level or char level models that I could look into!\n\n\nThanks!",
"date": "2020-09-19"
},
{
"vote": 4,
"title": "Day 263 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.26 â€“ Dialogue Systems And Chatbots V",
"text": "Day 263.\n\n\nToday's post focuses on another chapter from the Speech and Language Processing textbook, specifically on chapter 26 - dialogue systems and chatbots.\n\n\nhttps://ryanong.co.uk/2020/09/19/day-263-learn-nlp-with-me-slp-textbook-ch-26-dialogue-systems-and-chatbots-v/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-19"
},
{
"vote": 1,
"title": "MVP for Text Generation to Initiate NLP Research in Current Firm",
"text": "[deleted]",
"date": "2020-09-19"
},
{
"vote": 4,
"title": "Noun -&gt; Noun, Verb -&gt; Verb after translating a sentence/text. Is this a solved problem?",
"text": "[deleted]",
"date": "2020-09-18"
},
{
"vote": 2,
"title": "MS CS programs in US for NLP research",
"text": "[deleted]",
"date": "2020-09-18"
},
{
"vote": 2,
"title": "Day 262 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.26 â€“ Dialogue Systems And Chatbots IV",
"text": "Day 262.\n\n\nToday's post focuses on another chapter from the Speech and Language Processing textbook, specifically on chapter 26 - dialogue systems and chatbots.\n\n\nhttps://ryanong.co.uk/2020/09/18/day-262-learn-nlp-with-me-slp-textbook-ch-26-dialogue-systems-and-chatbots-iv/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-18"
},
{
"vote": 18,
"title": "What's next after working as NLP Engineer for 2 years?",
"text": "After working in 2 NLP startups which built chatbots and search engine. I feel like I am doing same since past 2 years. Regex and 10% Deep Learning here and there. 2 questions.\n\n\n1: If I want to promote myself then shall I be Senior NLP Engineer or Data Scientist.\n2: If I move towards Software Engineer. Will it break my career.\n2: Seriously what's next.. \n\n\nAll opinions are welcomed as I believe you guys are best in knowledge interms of industry level NLP",
"date": "2020-09-18"
},
{
"vote": 2,
"title": "Recommend books on NLP?",
"text": null,
"date": "2020-09-17"
},
{
"vote": 1,
"title": "Loading saved NER transformers model causes AttributeError?",
"text": "I have trained and saved some NER models using\n\n\ntorch.save(model) \n\n\n\nI need to load these model files (extension .pt) for evaluation using\n\n\ntorch.load(&#039;PATH_TO_MODEL.pt&#039;) \n\n\n\nAnd I get the following error: 'BertConfig' object has no attribute 'return_dict'\n\n\nFor the same, I updated my transformer package to the latest one, but the error persists.\n\n\nThis is the stack trace:\n\n\nTraceback (most recent call last): File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 523, in &lt;module&gt; test_loss, test_cr, test_cr_fine = evaluate_i(test_model, optimizer, scheduler, validation_dataloader, args, device) File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 180, in evaluate_i e_loss_coarse, e_output, e_labels, e_loss_fine, e_f_output, e_f_labels, mask, e_cumulative_loss  = defModel(args, e_input_ids, attention_mask=e_input_mask, P_labels=e_labels, P_f_labels=e_f_labels) File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 150, in forward return self.module(*inputs[0], **kwargs[0]) File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &quot;/home/systematicReviews/models/mtl/model.py&quot;, line 122, in forward attention_mask = attention_mask File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/modeling_bert.py&quot;, line 784, in forward return_dict = return_dict if return_dict is not None else self.config.use_return_dict File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/configuration_utils.py&quot;, line 219, in use_return_dict return self.return_dict and not self.torchscript AttributeError: &#039;BertConfig&#039; object has no attribute &#039;return_dict&#039; \n\n\n\nHere is some more information about my system:\n\n\n- `transformers` version: 3.1.0 - Platform: Linux-4.4.0-186-generic-x86_64-with-debian-stretch-sid - Python version: 3.6.9 - PyTorch version (GPU?): 1.3.1 (True) - Tensorflow version (GPU?): not installed (NA) - Using GPU in script?: Yes - Using distributed or parallel set-up in script?: No \n\n\n\nIt worked pretty fine until now, but suddenly this bug appears. Any help or hint is appreciated.\n\n\n&#x200B;\n\n\nOriginal question \nhere\n.",
"date": "2020-09-17"
},
{
"vote": 2,
"title": "Day 261 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.26 â€“ Dialogue Systems And Chatbots III",
"text": "Day 261.\n\n\nToday's post focuses on another chapter from the Speech and Language Processing textbook, specifically on chapter 26 - dialogue systems and chatbots.\n\n\nhttps://ryanong.co.uk/2020/09/17/day-261-learn-nlp-with-me-slp-textbook-ch-26-dialogue-systems-and-chatbots-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-17"
},
{
"vote": 110,
"title": "Matching GPT-3's performance with just 0.1% of its parameters",
"text": "In our most recent paper, we show that language models are few-shot learners even if they have far less than 175B parameters. Our method (combining PET and ALBERT) performs similar to GPT-3 on SuperGLUE after training on 32 examples with just 0.1% of its parameter count: \nhttps://arxiv.org/abs/2009.07118\n - I would be happy about any feedback :)",
"date": "2020-09-17"
},
{
"vote": 3,
"title": "Tracking the Digital Yuan, in Chinese, with 50 lines of code",
"text": null,
"date": "2020-09-16"
},
{
"vote": 1,
"title": "Contextual Based Text Analysis Question",
"text": "[deleted]",
"date": "2020-09-16"
},
{
"vote": 20,
"title": "Is it worth to learn NLP in 2020 and beyond?",
"text": "[deleted]",
"date": "2020-09-16"
},
{
"vote": 4,
"title": "Day 260 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.26 â€“ Dialogue Systems And Chatbots II",
"text": "Day 260.\n\n\nToday's post focuses on another chapter from the Speech and Language Processing textbook, specifically on chapter 26 - dialogue systems and chatbots.\n\n\nhttps://ryanong.co.uk/2020/09/16/day-260-learn-nlp-with-me-slp-textbook-ch-26-dialogue-systems-and-chatbots-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-16"
},
{
"vote": 15,
"title": "Text Summarization of COVID-19 Medical Articles using BERT and GPT-2 | Research Paper Walkthrough",
"text": null,
"date": "2020-09-16"
},
{
"vote": 2,
"title": "Pooling word embeddings to create sentence embeddings?",
"text": "Hi everyone. I'm just curious if anyone's seen work w.r.t. \"pooling\" word embeddings (not sure if \"pooling\" is the right word) in order to create more effective sentence embeddings.\n\n\nI found a paper titled \nEfficient Sentence Embedding using Discrete Cosine Transform (Almarwani et al., 2019)\n that uses something called a discrete cosine transform (DCT) in order to do something similar, but I'm wondering if there are any other methods out there. Thanks.",
"date": "2020-09-16"
},
{
"vote": 1,
"title": "Looking for a corpus of geotagged news articles",
"text": "[deleted]",
"date": "2020-09-15"
},
{
"vote": 6,
"title": "Spanish dependency relation woes",
"text": "I have been using Stanza and Spacy a bit to explore NLP.  I am working on mining corpus for certain language phenomenon by reading a corpus and pulling out sentences that have certain dependency relations and other attributes.\n\n\nThe problem is I am finding the dependency relations to be inaccurate in some cases.  I am not sure how to move forward or what to ask, so let me relate a bit what I am seeing first.\n\n\n> Causa gran incomodidad que se corte el agua todos los dÃ­as. - It causes great discomfort if the water is cut every day.\n\n\nCausa       NOUN  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—â•â•— root\ngran        ADJ   &lt;â•â•â•â•â•â•â•â•â•â•â•â•â•— â•‘ â•‘ amod\nincomodidad NOUN  â•â•â•â•â•â•â•â•â•â•â•â•—â•â•&lt;â• â•‘ appos\nque         PRON  &lt;â•â•â•â•â•â•â•â•â•— â•‘     â•‘ nsubj\nse          PRON  &lt;â•â•â•â•â•â•â•— â•‘ â•‘     â•‘ obj\ncorte       VERB  â•â•â•â•—â•â•—â•â•â•â•&lt;â•     â•‘ acl\nel          DET   &lt;â•— â•‘ â•‘           â•‘ det\nagua        NOUN  â•â•&lt;â• â•‘           â•‘ nsubj\ntodos       DET   &lt;â•—   â•‘           â•‘ det\nlos         DET   â•â•&lt;â•— â•‘           â•‘ det\ndÃ­as        NOUN  â•â•â•â•&lt;â•           â•‘ obl\n.           PUNCT &lt;â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• punct\n\n\n\nThis has incorrect POS tagging as \"Causa\" is classified as a noun and not a verb.  I can get it to be considered verb by adding a pronoun...\n\n\n> Me causa gran incomodidad que se corte el agua todos los dÃ­as. - It causes me great discomfort if the water is cut every day.\n\n\nMe          PRON  &lt;â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   obj\ncausa       VERB  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—â•â•â•â•— root\ngran        ADJ   &lt;â•â•â•â•â•â•â•â•â•â•â•â•â•— â•‘   â•‘ amod\nincomodidad NOUN  â•â•â•â•â•â•â•â•â•â•â•â•—â•â•&lt;â•   â•‘ obj\nque         PRON  &lt;â•â•â•â•â•â•â•â•â•— â•‘       â•‘ nsubj\nse          PRON  &lt;â•â•â•â•â•â•â•— â•‘ â•‘       â•‘ obj\ncorte       VERB  â•â•â•â•—â•â•—â•â•â•â•&lt;â•       â•‘ acl\nel          DET   &lt;â•— â•‘ â•‘             â•‘ det\nagua        NOUN  â•â•&lt;â• â•‘             â•‘ nsubj\ntodos       DET   &lt;â•—   â•‘             â•‘ det\nlos         DET   â•â•&lt;â•— â•‘             â•‘ det\ndÃ­as        NOUN  â•â•â•â•&lt;â•             â•‘ obl\n.           PUNCT &lt;â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• punct\n\n\n\nNow Causa is marked as a verb, but the expression \"que se corte el agua todos los dÃ­as\" is considered and adjective clause modifying \"incomodidad\" when it should be an adverbial clause modifying the verb causa.  (an adjective clause would describe or select the affected noun, but it doesn't do that in this case it gives the subject of the verb).\n\n\nI'm not really sure where to go with this.  I suspect this is just a limitation of the training data being used.\n\n\nFirst, if anyone knows Spanish and can confirm the issues I see, I'd appreciate that.\n\n\nTalking to a maintainer of Stanza, I could submit tagged spanish sentences to extend the training data.  I don't know how effective that is going to be..  making the distinction between an adjective relative clause versus an adverb relative clause is not a syntactic detail but seems to depend heavily on the particular verb or noun being modified and their semantics.  So to make this work generally I think I would need to generate a lot of additional training data...\n\n\nSecond, maybe there is an issue in the original training data?  I could scan the training data and looked at all noun relative clauses to manually check if any of them were mis-classified and should be verb relative clauses.  I really don't know anything about the quality of the incoming training data, so can't assess this.\n\n\nSometimes when I sit down to write a question like this, the answer or an avenue of investigation comes to me.  I haven't really found a crisp question though to be honest.  Just have a bunch of doubt about the utility of what Stanza and Spacy are producing for Spanish.  Thoughts?",
"date": "2020-09-15"
},
{
"vote": 5,
"title": "Seeking advice on grad school. Low GPA but good NLP publications.",
"text": "I have a \n7.85/10 GPA\n from a Tier 2 college in India (2020 graduate). However I have put considerable amount of time in doing NLP projects and internships. I have \n2 ACL Papers, 1 NAACL Paper, 1 ACM HyperText Paper, and 1 Interspeech\n paper. I am the \nfirst author\n in all of these. Apart from this I also have \n2 US patents\n filed with the company I interned with. These patents are also in the domain of Natural Language Processing. My GRE score is \n158V 167Q.\n I was also a member of the only undergraduate student team in India to build a driverless car. I'll have strong LORs from professors who know me personally. Currently I'm working as a Data Scientist in a Fortune 10 company as part of their applied NLP Research Team. My current h-index is 3 with 18 citations.\n\n\nDo I stand a decent chance towards getting into CMU's LTI for a Masters Degree ? I feel like my GPA is going to be a burden for me and all this effort may go to waste.",
"date": "2020-09-15"
},
{
"vote": 3,
"title": "Automatically extract word onset times in audio file using audio and text file input?",
"text": "Any pointers? I'm sure this exists but can't find a working model.",
"date": "2020-09-15"
},
{
"vote": 2,
"title": "Vec2Doc? Reverse document embeddings? Does this exist?",
"text": "Today I was wondering about the following problem. Given an embedded document (e.g. a vector) return a text/sentence which when embedded, yields the original embedding or one which is very close.\n\n\nWhat I mean is different from just comparing the embedding against a known corpus and selecting the closest one. What I mean really is generating a fitting text from scratch. So something more like a conditional generation task.\n\n\nDoes anyone know if there is any research being conducted in this direction?",
"date": "2020-09-15"
},
{
"vote": 6,
"title": "Are abstractive summarization models supervised?",
"text": "So most modern abstractive summarization models are a variation of an encoder-decoder model, like T5, BART, Pegasus, etc. \n\n\nHowever, when I try to read these papers, the emphasis is often on the learning aspect. For instance, BART does both shuffling of the original sentences, and masks spans of text with a single token. Pegasus masks sentences entirely and forces the model to learn those sentences, while also nominating sentences it deems important. But what I'm unsure of is how these models use the human-made summaries, especially during fine-tuning.\n\n\nDo they use them in the same way we think of typical supervised learning? (use cross-entropy loss between the generated summary vs the human-made summary and use that to backpropagate through the model) Or is it mainly used to calculate the rogue score during evaluation?  \n\n\nOr if it's self-supervised, what role does the target summaries play during the fine-tuning process? \n\n\nI'm reading the github of Pegasus and it said that the \"fine-tuning dataset is expected to be supervised\". Does this mean that pre-training is a self-supervised task, while fine-tuning for generating summaries is a supervised task?\n\n\nIf this is so, (and perhaps it could be specific to pegasus), how does the model use the target summaries for fine-tuning?\n\n\nSorry if I'm totally wrong in some aspects, would really appreciate clarification on this regard.",
"date": "2020-09-15"
},
{
"vote": 2,
"title": "Multimodal Emotion Recognition Competition 2020 (MERC 2020) - Challenges(EvalAI)",
"text": null,
"date": "2020-09-14"
},
{
"vote": 4,
"title": "Well read Students Learn Better: On The Important Of Pre-training Compact Models",
"text": null,
"date": "2020-09-13"
},
{
"vote": 13,
"title": "Graduate course on computational historical linguistics?",
"text": "Good day! I'm currently following an NLP Master's program, which I really enjoy. However, I am looking for a course on computational historical linguistics, that unfortunately isn't offered by any of the universities in the country where I live. I looked around on websites of a few German universities, but couldn't find what I was looking for.\n\n\nCan anyone recommend me a university that teaches a course on computational historical linguistics? Preferably within the EU (since I also live in the EU, and arranging exchanges within the EU is easier).\n\n\nHope someone can help me out! Enjoy your day.",
"date": "2020-09-13"
},
{
"vote": 12,
"title": "How to find how much one sentence depends on previous sentence?",
"text": "I am doing a task where I have paragraph. And I want to find out how much a sentence depends on the previous sentence?\n\n\nFor example, if I have a paragraph like this:\n\n\n\"Sentence 1. Sentence 2. Sentence 3. Sentence 4. Sentence 5.\"\n\n\nWhat I want is to have a score of dependence of sentence 3 on sentence 2 and sentence 4 on sentence 3 and so on.\n\n\n(Dependence as in for eg. \"I hate Tom. He won't let me do my work.\" Here sentence 2 depends on sentence 1 but for another example, \"I hate Tom. I like bananas.\" there is no dependence of sentence number 2 on sentence 1.)\n\n\nIs this possible to be done easily? Are there any tools that can do this or what algorithm should I look for. I am looking for peice of code that does this. I am not an NLP expert.\n\n\nAny help would be appreciated. Thank you!",
"date": "2020-09-13"
},
{
"vote": 6,
"title": "Embedding dimensions",
"text": "I'm comparing different transformer architectures and I'm interested in understanding the embedding dimension a bit more better. It seems like 1024 is quite common choice for the embedding dimension according to the huggingface config files as seen below. Could someone point me to some good studies on selecting the dimension?\n\n\nI guess the main point of the embedding layer is to differentiate each token from other tokens and also to provide a working similarity measure between tokens. 1024 dimensions feels like an overkill for differentiating between 50k tokens (1024 bits could point to 2^(1024) different tokens). For token similarity a dimension between 100 and 1000 used to be a good value for w2v. Maybe the hidden layers tries to store some additional data to the token vectors so I guess there is a real need for more dimensions compared to the simple w2v model.\n\n\nAre the transformer parameters a result of some grid search or how is that chosen? How should I choose the values for training from scratch for a new language with smaller training dataset?\n\n\n\n\nGPT2 large\n: 1280\n\n\nGPT2 medium\n: 1024\n\n\nGPT2 small\n: 768\n\n\nbert\n: 768\n\n\nt5-11b\n: 1024\n\n\nt5\n: 768\n\n\nt5-small\n: 512\n\n\nroberta\n: 768\n\n\nalberta\n: 128\n\n\nxlnet\n: 768\n\n\nxlnet-large\n: 1024",
"date": "2020-09-13"
},
{
"vote": 9,
"title": "Learning resource suggestions for topic modeling?",
"text": "Iâ€™m looking to learn topic modeling: the theory, the math, and code implementations in python. Wouldnâ€™t mind also learning about where topic modeling research is headed as well.\n\n\nIâ€™m open to all kinds of topic models, but if I had to specify Iâ€™d say Iâ€™m interested in LSA, PLSA, LDA, and network-based topic models.\n\n\nDoes anyone have any recommendations for good resources on this? Maybe video lectures from a university course, a book chapter, or whatever else you think has a good presentation of the material. Unless amazing, Iâ€™m hoping to avoid short blog or medium posts on the topic.\n\n\nTopic modeling sadly isnâ€™t covered in the Stanford NLP courses that theyâ€™ve been putting out.",
"date": "2020-09-13"
},
{
"vote": 2,
"title": "Day 256 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.23 â€“ Discourse Coherence III",
"text": "Day 256.\n\n\nToday's post focuses on discourse coherence. I am new to it, trying to gain some awareness :)\n\n\nhttps://ryanong.co.uk/2020/09/12/day-256-learn-nlp-with-me-slp-textbook-ch-23-discourse-coherence-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-12"
},
{
"vote": 29,
"title": "Understanding Googleâ€™s BigBird â€” Is It Another Big Milestone In NLP?",
"text": null,
"date": "2020-09-12"
},
{
"vote": 13,
"title": "Identifying grammatically correct phrases or sentences",
"text": "I would like to classify phrases/sentences as grammatically correct or not, regardless of whether they make sense or how likely they are to be said. So I would like â€œtacos run helpfullyâ€ to be a yes, but â€œI like eat tacosâ€ to be a no. Any suggestions?",
"date": "2020-09-12"
},
{
"vote": 1,
"title": "data source for event extraction",
"text": "I'm working on event extraction on news articles and I have come across mentions of annotated corpora like the CoNLL and the OntoNotes project. while I am able to find documentations of these shared tasks , I am unable to find any corpora which I can use.\n\n\nplease help pointing me in the correct direction here. thanks in advance!",
"date": "2020-09-12"
},
{
"vote": 57,
"title": "NLP Engineers / Computational Linguists working in industry - what do you actually 'do'?",
"text": "Title is pretty self explanatory but i'm curious as to what an average day looks like.",
"date": "2020-09-11"
},
{
"vote": 4,
"title": "Day 255 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.23 â€“ Discourse Coherence II",
"text": "Day 255.\n\n\nToday's post focuses on discourse coherence. I am new to it, trying to gain some awareness :)\n\n\nhttps://ryanong.co.uk/2020/09/11/day-255-learn-nlp-with-me-slp-textbook-ch-23-discourse-coherence-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-11"
},
{
"vote": 11,
"title": "What is currently the best model for text-generation besides gtp-3?",
"text": "After testing gtp-3 it is clear that at this time it is the most robust model for generating text. I would like to ask what is the \"second\" one. I think that t5 has a model with 11 billion parameters, but I have not tried it.",
"date": "2020-09-11"
},
{
"vote": 1,
"title": "Named entity recognition with bert - Transformers",
"text": "Hi guys, im trying to build a custom token classifier (NER Model) using the transformers library, but i cant seem to find any good material, \n\n\nanyone have a good starting point ? a notebook or a link to any tutorial would be perfect , \n\n\nThanks i nadvance",
"date": "2020-09-10"
},
{
"vote": 9,
"title": "Day 254 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.23 â€“ Discourse Coherence I",
"text": "Day 254.\n\n\nToday's post focuses on discourse coherence. I am new to it, trying to gain some awareness :)\n\n\nhttps://ryanong.co.uk/2020/09/10/day-254-learn-nlp-with-me-slp-textbook-ch-23-discourse-coherence-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-10"
},
{
"vote": 7,
"title": "Bridging the Gap between Training and Inference for Neural Machine Translation [Paper summary]",
"text": "Summary Video: \nhttps://youtu.be/XEdFyFp06u0\n\n\nPaper: \nhttps://arxiv.org/pdf/1906.02448.pdf",
"date": "2020-09-10"
},
{
"vote": 2,
"title": "How can we predict the question types something like google forms do?",
"text": "I wanted to build a model for predicting the question types. Something like what google form does (below is the link for reference)\n\n\nhttps://www.bettercloud.com/monitor/the-academy/google-forms-can-now-predict-question-types-suggest-answers/\n\n\nIt is some kind of intent classification, right? How would you approach the similar problem, right from the data collection and serving the model.",
"date": "2020-09-10"
},
{
"vote": 7,
"title": "Day 253 of #NLP365 - Learn NLP With Me â€“ CS520 Knowledge Graphs â€“ Lecture 5 â€“ How To Evolve A Knowledge Graph?",
"text": "Day 253.\n\n\nToday's post focuses on lecture 5 of the CS520 Knowledge Graph lecture series! Check it out below.\n\n\nhttps://ryanong.co.uk/2020/09/09/day-253-learn-nlp-with-me-cs520-knowledge-graphs-lecture-5-how-to-evolve-a-knowledge-graph/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-09"
},
{
"vote": 3,
"title": "Google's Cloud Natural Language: what values to expect for news analysis?",
"text": "Hello, \n\n\nI want to do sentiment analysis on local newspapers using Google's API for this task. For a request on a text, the API returns two values: a score between -1 and +1 to identify a negative/positive emotion; and a magnitude value between 0 and +inf to identify how much emotional content is present. The last value is not normalized since a longer text can have more emotional content than a shorter one, but I assume that for news articles these values are somehow restricted since most of these texts have a similar length.\n\n\nDoes anyone here that has used this service for a similar project can tell me a bit about what magnitude values to expect?",
"date": "2020-09-09"
},
{
"vote": 22,
"title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans | Research Paper Walkthrough",
"text": null,
"date": "2020-09-09"
},
{
"vote": 2,
"title": "Biobert cosine similarity",
"text": "I thought Biobert was only trained on English biomedical corpus.  But I run some sample codes to calculate the semantic similarities.  I found that it works on Chinese, French words as well, why?\n\n\nBelow are the codes I run:\n\n\nimport torch\n\n\nimport argparse\n\n\nimport logging\n\n\n&#x200B;\n\n\nfrom transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n\n\nfrom transformers import BertTokenizer, BertModel\n\n\n&#x200B;\n\n\nmodel_version = 'biobert_v1.1_pubmed'\n\n\ndo_lower_case = True\n\n\nmodel = BertModel.from_pretrained(model_version)\n\n\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n\n\n&#x200B;\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndef embed_text(text, model):\n\n\ninput_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n\n\noutputs = model(input_ids)\n\n\nlast_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n\nreturn last_hidden_states\n\n\n&#x200B;\n\n\ndef get_similarity(em, em2):\n\n\nreturn cosine_similarity(em.detach().numpy(), em2.detach().numpy())\n\n\n&#x200B;\n\n\nvirus_em = embed_text(\"virus\", model).mean(1)\n\n\n# We will use a mean of all word embeddings.\n\n\nflu_em = embed_text(\"flu\", model).mean(1)\n\n\nvirus_Chinese_em = embed_text(\"ç—…æ¯’\", model).mean(1)\n\n\nflu_Chinese_em = embed_text(\"æµæ„Ÿ\", model).mean(1)\n\n\nprint(\"Similarity for virus and flu:\" + str(get_similarity(virus_em, flu_em)))\n\n\nprint(\"Similarity for ç—…æ¯’ and æµæ„Ÿ:\" + str(get_similarity(virus_Chinese_em, flu_Chinese_em)))\n\n\n&#x200B;\n\n\nprint(\"Similarity for virus and flu:\" + str(get_similarity(virus_em, flu_em)))\n\n\nprint(\"Similarity for ç—…æ¯’ and æµæ„Ÿ:\" + str(get_similarity(virus_Chinese_em, flu_Chinese_em)))\n\n\n&#x200B;\n\n\nThe results are:\n\n\n Similarity for virus and flu:[[0.9379361]] \n\n\nSimilarity for ç—…æ¯’ and æµæ„Ÿ:[[0.9999998]] \n\n\n&#x200B;\n\n\nSo the chinese words can be compared as well.  Why?",
"date": "2020-09-09"
},
{
"vote": 11,
"title": "What's the difference between an entity and a mention?",
"text": "Hi. I'm reading some papers on relation extraction and feel that the two words are used somewhat ambiguously. I'm having some trouble finding any resources that actually explain the difference between the two in a clear manner.\n\n\nMy understanding is that an \"entity\" is a larger concept than a \"mention\" in that it seems that mentions are usually used to refer to named entities. I'm not sure if this understanding is correct though.",
"date": "2020-09-09"
},
{
"vote": 0,
"title": "How can I extract texts from this image?",
"text": "How can I extract texts from this image? I tried an online ocr site \nhttps://ocr.space/\n but to no avail.\n\n\nhttps://i.imgur.com/8uUWXrJ.png",
"date": "2020-09-08"
},
{
"vote": 1,
"title": "What exactly is attention mask?",
"text": "I was fine tuning my BERT based model for a multilabel classification task.\n\n\nI thought of changing attention_mask which takes in bool value and changed it to some factor like 5 something which is not boolean for certain tokens which I want model to focus on.\n\n\nI want to know if this is a right approach, and why do we need attention mask exactly. \n\n\n&#x200B;\n\n\nI saw in \nattention.py\n of huggingface models, it is added with the attention_score.  \n\n\nIf someone could give me insight what exactly it is and whether what I am trying out is fine",
"date": "2020-09-08"
},
{
"vote": 7,
"title": "What are the essential Top 5 NLP research papers to read to get a great understanding of our progress in language technology, until now.",
"text": "Could be more than 5. Need not be recent. But the papers that you felt clarified the most things in terms of concepts, capabilities and deeper understanding of how to tackle natural language problems",
"date": "2020-09-08"
},
{
"vote": 1,
"title": "Day 251 of #NLP365 - Learn NLP With Me â€“ CS520 Knowledge Graphs â€“ Lecture 3 â€“ What Are Some Advanced Knowledge Graphs?",
"text": "Day 251.\n\n\nToday's post focuses on lecture 3 of the CS520 Knowledge Graph lecture series! Check it out below.\n\n\nhttps://ryanong.co.uk/2020/09/07/day-251-learn-nlp-with-me-cs520-knowledge-graphs-lecture-3-what-are-some-advanced-knowledge-graphs/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-07"
},
{
"vote": 31,
"title": "Transformer Architecture Explained",
"text": null,
"date": "2020-09-07"
},
{
"vote": 23,
"title": "Is there any approach to normalize words like helllllloooooo to hello?",
"text": "I am trying to treat and normalize text from social media.",
"date": "2020-09-07"
},
{
"vote": 2,
"title": "Which POS tags to determine a user is talking about themselves?",
"text": "Hi, I've tagged some textual data and I want to do a study on those sentences which a user is talking about themselves. \n\n\nAre there particular tags I should look out for?\n\n\nI was thinking, \"PRP (posessive pronoun)\" since it includes, \"I, my, she, ...\" - but this includes terms like \"you\". Would this be OK? \n\n\n(I'm using spaCy)",
"date": "2020-09-07"
},
{
"vote": 6,
"title": "Data Readiness for Natural Language Processing",
"text": "Hey there,\n\n\nWe've collected our experiences on Data Readiness in relation to innovation and research projects dealing with Natural Language Processing. Now, as this is work in progress, any input on how to make the document better is highly appreciated!\n\n\nGitHub: \nhttps://github.com/fredriko/nlp-data-readiness\n\n\narXiv: \nhttps://arxiv.org/abs/2009.02043\n\n\nAbstract: \"This document concerns data readiness in the context of machine learning and Natural Language Processing. It describes how an organization may proceed to identify, make available, validate, and prepare data to facilitate automated analysis methods. The contents of the document is based on the practical challenges and frequently asked questions we have encountered in our work as an applied research institute with helping organizations and companies, both in the public and private sectors, to use data in their business processes.\"",
"date": "2020-09-07"
},
{
"vote": 29,
"title": "Which parts of nltk are machine learning and which parts are rule based?",
"text": "I'd really appreciate a thorough and complete answer! I don't really know anyone that I can learn NLP with and there is a lot of fog on abstract information despite the resources online. So I'd also also appreciate making a friend who's educated on the subject here that I can ask some questions to. Maybe even be provided thorough abstract resources. I'm currently watching Sentdex's NLTK tutorial so until I finish that the only question I have for now is what functions are rule based or model based in nltk?",
"date": "2020-09-07"
},
{
"vote": 2,
"title": "Resources on neural network transfer learning?",
"text": "Hi, I am reading about neural network transfer learning for NLP.  Particularly RNNs, CNNs and attention models.\n\n\nIt looks like a lot of \"fine tuning\" transfer learning resources are heavily focused on computer vision and the area is not that well explored. Does anyone have any pointers as to what resources to look at?",
"date": "2020-09-06"
},
{
"vote": 8,
"title": "Day 250 of #NLP365 - Learn NLP With Me â€“ CS520 Knowledge Graphs â€“ Lecture 2 â€“ How To Create A Knowledge Graph?",
"text": "Day 250.\n\n\nToday's post focuses on lecture 2 of the CS520 Knowledge Graph lecture series! The lectures are good so far but not sure if I like the industrial splits in each lecture!\n\n\nhttps://ryanong.co.uk/2020/09/06/day-250-learn-nlp-with-me-cs520-knowledge-graphs-lecture-2-how-to-create-a-knowledge-graph/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-06"
},
{
"vote": 15,
"title": "Day 247 &amp; 248 &amp; 249 of #NLP365",
"text": "Day 247, 248, and 249.\n\n\nSorry all, I have been away on holiday so I didn't have time to share my blog posts on different social media platforms except posting it on my own website. Check out below the last two blog posts and today's one on knowledge graphs.\n\n\nhttps://ryanong.co.uk/2020/09/03/day-247-nlp-implementation-a-web-application-for-entity-tracking-react-frontend/\n\n\nhttps://ryanong.co.uk/2020/09/04/day-248-nlp-implementation-a-simple-knowledge-graph-walkthrough/\n\n\nhttps://ryanong.co.uk/2020/09/05/day-249-learn-nlp-with-me-cs520-knowledge-graphs-lecture-1-what-is-a-knowledge-graph/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-05"
},
{
"vote": 1,
"title": "Beta Testers Needed for NLP Annotation Tool",
"text": "Hi Everyone,\n\n\nWe are looking for beta testers to try out our new NLP annotation tool in the beta version \nhttps://ubiai.tools\n. Here is an \narticle\n that describes the tool features.\n\n\nWe will implement suggestions from members!\n\n\nThanks.",
"date": "2020-09-05"
},
{
"vote": 7,
"title": "How to create a domain specific knowledge graph using papers of a scientific domain?",
"text": "Hi.\n\n\nI have a set of academic texts, around 50k, all of which are linguistics papers/abstracts. Previously I had tried to do topic modelling or document clustering to find relationships between different entities, but having found out about Knowledge Graphs, I would like to give KGs a try to see how they will perform regarding my information extraction task. \n\n\nI'm a total beginner to KGs tho, and I'm confused which approach should I pursue. I have a couple of questions:\n\n\n1 -  Apparently I have to find the \"ontology\" of my corpus according to \nthis\n medium article. But I'm not sure if I'm qualified enough to do so. Plus, I think there is a high chance that this has previously been done for English written scientific text. \n\n\n2 - Which algorithms should I pursue considering the size of my dataset. (And my limited resources. A Laptop and Google Colab or Kaggle notebooks.)\n\n\n3 - Any good tutorial/article/video that helps me do this would be much appreciated.",
"date": "2020-09-05"
},
{
"vote": 13,
"title": "What kind of blogs posts on NLP do you enjoy reading?",
"text": "I am interested in what the community thinks on how a good NLP article should look like. Any favourite blogs?",
"date": "2020-09-05"
},
{
"vote": 1,
"title": "[Blog Post] Word Vectors Simplified",
"text": null,
"date": "2020-09-04"
},
{
"vote": 5,
"title": "What is the state of the art sentiment analysis model as of today?",
"text": "I am looking to find a state of the art sentiment analysis model that can also be used commercially - free, as a paid API/license. I would want to use the out of the box Google or Amazon models but I am sure something better exists out there. I have read that XLNet seems to be the best right now, I would like to verify that.",
"date": "2020-09-03"
},
{
"vote": 57,
"title": "100 Must-Read NLP Papers [Github]",
"text": null,
"date": "2020-09-03"
},
{
"vote": 17,
"title": "NLP Career Outlook",
"text": "Iâ€™ve been seeing a lot of comments both about NLP being saturated and about the field shrinking as more aspects of NLP get automated. \n\n\nOverall, how does the future of the NLP job market look? What steps or level of education is needed to be competitive in the job market?\n\n\nIâ€™m currently doing prereqs for a Data Science MS with the goal of working in NLP. My dream job would be something along the lines of building a chat bot for a language learning application, but Iâ€™m happy to accept anything. After seeing how hard it was for my gf to get hired with a PharmD and reading lots of content on DS being saturated, Iâ€™m a bit concerned. Iâ€™d appreciate any advice so I can enter the job market on good footing",
"date": "2020-09-02"
},
{
"vote": 1,
"title": "What to do after topic modeling?",
"text": "Iâ€™m kind of new to NLP and started with topic modeling after taking the DataCamp course. I found it really helpful and have been able to create some decent insights with it at work.  But Iâ€™m not really sure what to implement next beyond just tweaking my model. I work mostly in R if that matters.",
"date": "2020-09-02"
},
{
"vote": 4,
"title": "Hypothesis testing for lexical distributions?",
"text": "In any intro stats class, youâ€™ll learn about the normal dist and the frequentist hypothesis test to reject (or fail to reject) that a sample could have been generated by a distribution of known parameters. In other words, you accept/reject that the parameters are accurate.\n\n\nIâ€™ve developed text classifiers at jobs, internships, etc. And one thing that usually comes up is, ~â€The classifier is trained on some data, so how do we validate that todayâ€™s text data is still generated by the same distribution as the one that generated the training data?â€ \n\n\nTo put this in context, I once built a classifier that could take job descriptions and classify them as â€œsoftware engineerâ€, â€œproject managerâ€ etc etc. When trained the classifier was ~94% accurate. However, sometimes clients post job descriptions that are atypical based on changing needs, (one month a client hired 5-6 architects) and bringing new clients on also introduces change.\n\n\nSo Iâ€™m sure there is a metric that can summarize the nature of a lexical corpus and a test that can determine if todayâ€™s corpus and tomorrowâ€™s corpus were generated by the same distribution. \n\n\nI havenâ€™t heard of such a test before, but I think it could be super useful. Any thoughts?",
"date": "2020-09-02"
},
{
"vote": 2,
"title": "Day 246 of #NLP365 - NLP Implementation â€“ A Web Application For Entity Tracking â€“ Flask Backend",
"text": "Day 246.\n\n\nWe are on road to building a small web application that digest news articles through multiple different NLP techniques.\n\n\nToday's post focuses on creating our backend now that we have finished our frontend in the previous post.\n\n\nhttps://ryanong.co.uk/2020/09/02/day-246-nlp-implementation-a-web-application-for-entity-tracking-flask-backend/\n\n\nBest,\n\n\nRyan",
"date": "2020-09-02"
},
{
"vote": 3,
"title": "Aren't you afraid that GPT3 will trivialize the NLP field?",
"text": "I'm an NLP engineer. Seeing the capabilities of GPT3 I'm starting to worry about the future of the NLP field. It seems to me that we only barely scratched the field of what GPT3 is capable of. In my rough estimation I believe it can replace 80% of NLP jobs out there at the moment. And the thing is that you don't even need to know how to code to train it. You only need to image how to format the input / outputs, which even a high-school kid could easily figure out. So outside of really niche areas or cutting edge stuff you won't be needing an NLP scientist / engineer in the short term future. \n\n\nI'm kinda bumped tbh cause I joined this field because of the many exciting problems you could work on. And one big language model, that only one company has access to, comes and makes everything moot. Is anyone else sharing the same feeling?",
"date": "2020-09-02"
},
{
"vote": 1,
"title": "Identifying selected choice numbers from text.",
"text": "I am currently working on a problem where when a user is presented with a list of options, I need to detect what option/choice numbers from the list does  the user wants to select based on their text utterances.\n\n\nThe choices could be a single number based option(choice) or a range. The range could be specified by saying something like \"the first(or last)  2 choices\" or \"choices from 4 to 8\" etc.\n\n\nHow do I go about identifying the selected choices.\n\n\nCurrently I've tried a multi-label approach where I try predict the start and end of the choice range but haven't seen anything good.\n\n\n&#x200B;\n\n\nTIA.",
"date": "2020-09-02"
},
{
"vote": 1,
"title": "Applied Data Science with Python Specialization (University of Michigan)",
"text": "[removed]",
"date": "2020-09-02"
},
{
"vote": 1,
"title": "Is there any technique to suggest words to a language model for the task of text generation?",
"text": "Hello to all, \n\n\nI start by making it clear that I am new to this subject, so I apologize in advance if the question is silly.\n\n\nI would like to ask if there is any technique or model of nlp, that allows to suggest the words to the language model for text generation.\n\n\nAs I have seen in huggingface there is something called \"masked\" that allows the model to predict a missing token, however I think it is very rigid.\n\n\nThe scenario I'm thinking of is one where a) the suggested words are inserted into the output, regardless of their position. b) it allows you to do this with several terms.\n\n\nFor example, that the input can be \"reddit is\" and the suggested words are \"USA\", \"Web\", \"community\", and that the output includes all these words, no matter the position.\n\n\nIs this possible?",
"date": "2020-09-02"
},
{
"vote": 12,
"title": "Any good NLP Slack workspaces?",
"text": "curious if anyone is part of an active NLP community on Slack - found plenty of ML/data ones with the occasional (and usually inactive) nlp or text analytics channel, but has to be something good out there. Any recommendations?",
"date": "2020-09-01"
},
{
"vote": 4,
"title": "LSBert: A Simple Framework for Lexical Simplification | Research Paper Walkthrough",
"text": null,
"date": "2020-09-01"
},
{
"vote": 1,
"title": "Anybody have an non-kaggle site to download kaggle datasets",
"text": "[deleted]",
"date": "2020-09-01"
},
{
"vote": 3,
"title": "Using NLP to compare two Twitter accounts?",
"text": "Hey y'all, \n\n\nI've been looking to run some sort of NLP on web-scraped tweets from a bunch of different twitter accounts in my circle and try to match someone to an anonymous account that's popped up. Does anyone have any pointers or examples that they could shoot me? \n\n\nMore specifically, I want to analyze word use and texting habits between all of these accounts to see who speaks \"most similarly\" to the anonymous account. \n\n\nI'm not entirely sure how to get started with the NLP portion of this project. I have lots of experience in Python so that's what I'll be using. \n\n\nLet me know!\n\n\nEDIT: A sentence",
"date": "2020-09-01"
},
{
"vote": 5,
"title": "When you use Byte Pair Encoding (BPE) / SentencePiece are you still supposed to let the model learn embeddings?",
"text": "[deleted]",
"date": "2020-09-01"
},
{
"vote": 1,
"title": "Toolkit to compute linguistic features?",
"text": "[deleted]",
"date": "2020-09-01"
},
{
"vote": 16,
"title": "Does anybody know where I can get a list of stereotypically Caucasian or African American names?",
"text": "Hi. I'm currently working on a project regarding racial bias in NLP applications. I noticed that most of the work in this area is concerned with gender bias, but there's not a lot of work w.r.t. racial bias.\n\n\nMost work also use predefined lists of professions or names that are stereotypically associated with males or females. I've been looking for a resource that does the same for Caucasian or African American names, but am having trouble finding any.\n\n\nDoes anybody know if there exists such thing? Thanks.",
"date": "2020-09-01"
},
{
"vote": 7,
"title": "Are there any good modern POStaggers available for download?",
"text": "Hi guys! NLP newbie here..\n\n\nI need a POS tagger for a few languages. Preferably something Open Source, like Apache OpenNLP, which offers a few pre-trained models for download.\n\n\nI have some issues with Apache OpenNLP: It seems a bit dated, and I'm not sure how actively it is still used and developed. Given the huge progress in the field of Neural Networks during the last decade, it seems a bit strange that OpenNLP seems to not use anything from that progress. Or am I wrong?\n\n\nFor some languages like Italian, which I need, there are not much pre-trained models available. I found a 7-years-old model for Italian POStagging, haven't yet tried it out. Not sure if it still works with the latest OpenNLP version. For other languages, I could not find any models.\n\n\nSo my questions:\n\n\n\n\nIs OpenNLP still an actively used and developed platform?\n\n\nAre there newer projects (like something with a Keras/TF backend), where people offer pre-trained models for download?\n\n\nHow would you go about finding pre-trained models? Googling them? Or are there mailing lists where people share their models? Or do people train their models themselves, using some corpus they somehow obtained?\n\n\nIs there a free or paid POS tagging service, where I can post sentences to an API and get back the POS tags?\n\n\n\n\nThank you!",
"date": "2020-08-31"
},
{
"vote": 1,
"title": "Carding Business going on at cool rate iPhone X iPhone XS iPhone XS Max iPhone 11 iPhone 11 promax Samsung S20 MacBook Pro AirPod Dell and Sampsung I will deliver to your house before I receive payment Pm me on Telegram@Kranium20 TextNow @469 444 1907",
"text": "[removed]",
"date": "2020-08-31"
},
{
"vote": 3,
"title": "Day 244 of #NLP365 - NLP Implementation â€“ Entity Extraction And Linking â€“ Entity Linking Using DBPedia",
"text": "Day 244.\n\n\nWe are on road to building a small web application that digest news articles through multiple different NLP techniques.\n\n\nToday's post focuses on entity extraction and linking using DBPedia.\n\n\nhttps://ryanong.co.uk/2020/08/31/day-244-nlp-implementation-entity-extraction-and-linking-entity-linking-using-dbpedia/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-31"
},
{
"vote": 30,
"title": "Key Python Libraries for NLP",
"text": null,
"date": "2020-08-30"
},
{
"vote": 7,
"title": "Is it normal for fine-tuning on toy datasets to yield total gibberish results?",
"text": "[deleted]",
"date": "2020-08-30"
},
{
"vote": 10,
"title": "Text Data Augmentation with MarianMT",
"text": null,
"date": "2020-08-30"
},
{
"vote": 2,
"title": "Extracting the most relevant (popular?, valuable?, repeated?) sentences out of 500+ reports?",
"text": "Let's say I have 500 or more reports on a general topic like wind power.  Should I use something like NLTK: TF-IDF (\nhttps://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3\n) to extract the most relevant/popular/valuable sentences?  It seems too crude for a huge data set which would be more than 5,000 pages of text.  \n\n\nI don't think that PEGASUS would be a right fit for this, right?  It seems like the main purpose of that is to take a couple of paragraphs and create a 1 to 2 sentence abstract.\n\n\nIs there a NLP technique that tries to find the most \"repeated\" sentence? I'm sure that if you have 500+ documents on the same subject you'll see that some general sentence will be repeated dozens of times.  For instance some variation of this would probably be in many of the reports:\n\"Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists, but is not as significant.\"\n\n\nWould it be appropriate to take the PEGASUS abstract for each of the 500 reports and perform TF-IDF on that?\n\n\nI'm new to the whole NLP space if it isn't obvious so any documentation or reading you can point me too would be very valuable.\n\n\nThanks!",
"date": "2020-08-30"
},
{
"vote": 21,
"title": "Unsupervised Keyphrase Extraction",
"text": null,
"date": "2020-08-30"
},
{
"vote": 6,
"title": "Domain agnostic Aspect Based Sentiment Analysis",
"text": "Hi. I'm fairly new to targeted sentiment analysis and ABSA. Most of the implementation I see are on usual laptop/restaurant benchmark datasets and aspect term extraction is heavily tied to the domain of the training data. Has anyone here worked on a generic cross domain targeted sentiment analysis? If so how would one go about doing that? My uninformed take on this is that I'll have to classify my input message based on domains and then choose the corresponding trained model to do the Aspect term extraction and sentiment scoring. But I feel like I'm setting myself up to fail with this approach.",
"date": "2020-08-29"
},
{
"vote": 1,
"title": "Day 242 of #NLP365 - NLP Implementation â€“ Topic Modelling And Sentiment Analysis On News Articles (Sentence Level)",
"text": "Day 242.\n\n\nWe are on road to building a small web application that digest news articles through multiple different NLP techniques.\n\n\nToday's post focuses on topic modelling and sentiment analysis of news articles at the sentence level.\n\n\nhttps://ryanong.co.uk/2020/08/29/day-242-nlp-implementation-topic-modelling-and-sentiment-analysis-on-news-articles-sentence-level/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-29"
},
{
"vote": 3,
"title": "Anyone working on Spoiler Statement Detection in Text ?",
"text": "Hi,\nIs anyone working or has ideas to build â€œspoiler statements detectionâ€ in given text. Ex- potential usecase in movie/product reviews. \n\n\nOne simple way I can think of is to detect declarative statements and treat them as baseline. Although not sure how well will this behave",
"date": "2020-08-29"
},
{
"vote": 25,
"title": "Is no one working on document similarity these days?",
"text": "I have a peronal website that gives people similar artilcle recommendation. At the time of implementation, Jaccard similarity seems to be the quickest to write and the math was easy to follow for a software engineer like me.\n\n\nNow I wanted to uprade to something more modern. As I looked around the internet, I saw some say tf-id still works the best while others insist on the almithiness of BERT. I couldn't tell which one was right, so I ended up running an experiment myelf with 5 algorithms: jaccard, tf-idf, doc2vec, use, and bert, based on the article data I had (I made a formal \nblog post\n). It seemed tf-idf indeed did better than any other out of the box. I used the official pretrained models except doc2vec and I know they could be refined. But still I was quite disappointed. I was expecting the deep learning to deliver mindmblowing result.\n\n\nEvery time I look around for the latest, most discusions happen on q&a, tranlation, summarization, sentiment analysis and text generation. Is document similarity cconsidered like an already solved problem?",
"date": "2020-08-28"
},
{
"vote": 1,
"title": "Day 241 of #NLP365 - NLP Implementation â€“ Topic Modelling And Sentiment Analysis On News Articles (Document Level)",
"text": "Day 241.\n\n\nWe are on road to building a small web application that digest news articles through multiple different NLP techniques.\n\n\nToday's post focuses on topic modelling and sentiment analysis of news articles at the document level.\n\n\nhttps://ryanong.co.uk/2020/08/28/day-241-nlp-implementation-topic-modelling-and-sentiment-analysis-on-news-articles-document-level/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-28"
},
{
"vote": 2,
"title": "Binary classification of deletable documents",
"text": "I am trying to find out if it is feasible to delete huge amounts of documents. The task would be delete or not (archive or not) based on supervised learning. \n\n\nWhat would be the most SOTA solution for such a task? Would BERT be a good solution (performace/accuracy wise) or would baesian (spam or not kind of solution) perform better? \n\n\nWe are talking about TBytes and in the future maybe PBytes of documents.",
"date": "2020-08-28"
},
{
"vote": 14,
"title": "What should I learn to go about this project?",
"text": "I want to build a data extractor for images of medical reports. It should store field labels such as name, age, and labels in tables like blood pressure, glucose levels and the corresponding values of these labels in JSON output. I have tried PyTesseract for OCR but I don't how to proceed with the OCR output, as I don't know what to do to group the individual field label and it's field value. Please suggest techniques or existing packages to go about this problem.\n\n\nSample",
"date": "2020-08-28"
},
{
"vote": 2,
"title": "Can NLP used to find interest of users from a given text?",
"text": "Currently i am working on a project which requires that the machine automatically learns from a user's web history about various things that he/she might be interested in such as art , coding, or writting etc. the Data source for this [Not fixed yet] is to take the user's web history and then understand based on that. Content such as the full website link and whatever words are found from it. Now i am inexperienced in NLP and hence i want to understand whether this kind of problems can be solved using NLP? if so what algorithms should i study or things should i learn in order to do this? any help is highly appreciated.",
"date": "2020-08-28"
},
{
"vote": 19,
"title": "Topic modeling with Top2Vec",
"text": null,
"date": "2020-08-27"
},
{
"vote": 8,
"title": "Day 240 of #NLP365 - NLP Implementation â€“ Kaggleâ€™s Fake News Challenge â€“ BERT Classifier Using PyTorch And HuggingFace III",
"text": "Day 240.\n\n\nWe are on road to building a small web application that digest news articles through multiple different NLP techniques.\n\n\nToday's post is the last post on developing a simple fake news classifier baseline.\n\n\nhttps://ryanong.co.uk/2020/08/27/day-240-nlp-implementation-kaggles-fake-news-challenge-bert-classifier-using-pytorch-and-huggingface-iii/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-27"
},
{
"vote": 3,
"title": "A 2020 review of Handwritten Character Recognition",
"text": "https://nanonets.com/blog/handwritten-character-recognition/\n\n\nOCR is considered a solved problem in general but not in entirety ðŸŽ¯\n\n\nA key component of it, \nHTR\n is still a challenging problem.\n\n\nHandwriting Text Recognition(HTR) is the task of recognizing handwritten human text ðŸŽ«\n\n\nIt involves using both Computer Vision and NLP\n\n\nEvery person has a different style of handwriting ðŸ’ƒ, thus solving HTR is much more difficult than OCR\n\n\nIn this article I cover the progress of techniques in solving HTR and various SOTA models\n\n\nIn addition I have discussed the way to train your own HTR model on your own dataset\n\n\nHappy to discuss more if you interested more to learn more about handwritten text recognition",
"date": "2020-08-27"
},
{
"vote": 9,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2020-08-27"
},
{
"vote": 1,
"title": "[D] Synonym mapping from phrases to words.",
"text": "Hi,\nI have a huge corpus of text and my objective is to extract important phrases and for these phrases I want to identify similar words or phrases in the same corpus. \n\n\nMy approach as of now is as follows \n\n\n\n\nUse topic modeling to obtain \"important phrases\". (say 3-grams or 2-grams) \n\n\nFor above \"important phrases\" use word embeddings to retrieve similar words/phrases.\n\n\n\n\nThe question I have is about step 2. How would I use word embeddings to do this ? Does taking the average of all the words in the phrase and computing the similarity wrt to all the other words enough ?\n\n\n Also let me know if there's a better way to do the overall thing. \n\n\nThanks.",
"date": "2020-08-26"
},
{
"vote": 59,
"title": "Shift-Ctrl-F: Open-source chrome extension for searching a webpage using natural language instead of exact string match.",
"text": null,
"date": "2020-08-26"
},
{
"vote": 1,
"title": "GPT-2 Interactivity:",
"text": "I had an idea of how to increase the interactivity of GPT-2, but am unsure how to go about implementing it. The following is an example of my thinking:\n\n\nOriginal Text = â€œHello my name is Bucks, and Iâ€\nTop_N = 2\n\n\nOutput Options:\n\n\n\n\nenjoy\n\n\nlike\n\n\n\n\nYour choice?\nUser Input: 2\n\n\nProgress: Hello my name is Bucks, and I like\n\n\nOutput Options:\n\n\n\n\nsports\n\n\nschool\n\n\n\n\nYour choice?\nUser Input: 2\n\n\nProgress: Hello my name is Bucks, and I like school",
"date": "2020-08-26"
},
{
"vote": 1,
"title": "Suggestions Regarding a ML task.",
"text": "Hello all\nI have worked on a project \"Identity Anonymization\", for which I have made a program on Python. . The purpose of the program is to anonymize the names from the given text. I have built a database of around 200000 first and last names of different countries and whenever the user uploads a text file on the website, the algorithm runs and checks against all the names in the database, and when the name matches, it anonymizes like this. \"Hello my name is Mubashir and I am a student\" to \"Hello my name is ******** and I am a student\". \nNow I have been given a task to implement this using Machine Learning technique. The goal is if the name of a person is \"Mr. Winter\", now the algorithm doesn't know whether this \"winter\" is the name of a person or a season. If I put this in the database, every time the algorithm will anonymize this, even if it is supposed to be a season. So, can you please suggest me some technique on how should I approach this task?\nAny help in this regard would be great.\n\n\nThank you",
"date": "2020-08-26"
},
{
"vote": 13,
"title": "What are beams and how does their number affect a nlp model?",
"text": "Hello to all,\n\n\nI start by clarifying that I am new to the NLP and I'm beginning my learning path.\n\n\nI have been learning with HuggingFace and I have seen that there is commonly a parameter that is used which is num_beams.\n\n\nAlthough I've googled the subject it's still not clear to me what beams are, and especially how having, for example, num_beans = 4, affects a model in relation to having num_beans = 50.\n\n\nThank you for your help",
"date": "2020-08-26"
},
{
"vote": 1,
"title": "Help needed for research project.",
"text": "[deleted]",
"date": "2020-08-26"
},
{
"vote": 1,
"title": "Estimating the number of clusters in K-means without plotting for real time document similarity",
"text": null,
"date": "2020-08-26"
},
{
"vote": 2,
"title": "How to get correct answers using Huggingface transformers?",
"text": "I've used Hugginface transformers' question-answering pipeline for my question answering task.  But most of the answers were too short and some of them are irrelevant. Is there a way to handle this issue?",
"date": "2020-08-26"
},
{
"vote": 3,
"title": "splitting sentences from a large data stream in Python",
"text": "I have been playing with Stanza a bit and have some code I'd like to run on some large spanish corpora.  I have a tar file which contains Spanish text corpora, some individual files being a gig in size.  Understandably, Stanza runs out of memory if I try to process a whole file at once.  I figure it'd work if I could run Stanza against one sentence at a time.  Maybe if I split the file at any two newlines it would break things up to be small enough.\n\n\nWhat is a good way to break up the files?  I start with a byte stream of text.  I see NLTK has sent_tokenize(), but it takes a string and I'd rather not create a 1 gigatebyte string allocation as an intermediate step.  I thought I saw that NLTK's LineTokenizer would take a stream, but looking deeper into the docs it seems to only accept strings.\n\n\nGoogling for a solution, I feel like I'm chasing my tail as most everything seems geared for dealing with individual strings and not streams.  I'm pretty new to Python, maybe there is a standard solution out there and I'm just not seeing it.",
"date": "2020-08-25"
},
{
"vote": 3,
"title": "Reinforcement learning for WSD",
"text": "Hi ,\n\n\nI read on a research paper that The use of reinforcement learning is still under explored.  If so what will be the suitable approach?\n\n\nWill this gap be suitable to start a research project idea?\n\n\nAre there any papers related to NLP with reinforcement learning models?\n\n\nThank you.",
"date": "2020-08-25"
},
{
"vote": 1,
"title": "quick question on word frequency measures",
"text": "I'm looking for something like word frequency in a document, but you weight each word higher if it's less frequent and therefore more unique to the particular document. Is there a name for a measure like this, or an algorithm for achieving something similar, that I can Google?",
"date": "2020-08-25"
},
{
"vote": 17,
"title": "Using Natural Language Processing for Spam Detection in Emails",
"text": null,
"date": "2020-08-25"
},
{
"vote": 10,
"title": "Bootstrapping custom Spacy model for NER, Could you give me advice on my approach to label my data? Possible to automate annotation step?",
"text": "I'm trying to improve NER for parsing resumes, I'm currently building on a training set from a forked resume parsing repo. I have 2500 resumes that have been scraped from a student website, the data is well-formatted as it was accessed through an online API, in reality, the resumes I will be receiving will not be this well-formatted, most likely pdf. \n\n\nBecause the 2500 resumes are so well formatted I was contemplating converting the JSON data into plain text to simulate a real resume, and then mapping the entities from the JSON, finding the associated span of the entity. Finally resulting in a another JSON containing:\n\n\nresume_txt : plain txt\nentities: list of entities, span, text of entity\n\n\nIs this a logical approach?",
"date": "2020-08-25"
},
{
"vote": 4,
"title": "Natural Language Generation with Contentyze",
"text": "[removed]",
"date": "2020-08-24"
},
{
"vote": 2,
"title": "Software/method suggestions for analysing language attitude/folklinguistics maps",
"text": null,
"date": "2020-08-24"
},
{
"vote": 1,
"title": "Do researchers ever report bleu-1, bleu-2, bleu-3, and bleu-4 in their papers or just bleu-4?",
"text": "[deleted]",
"date": "2020-08-24"
},
{
"vote": 6,
"title": "Latest NLP Trends for this week",
"text": "[removed]",
"date": "2020-08-23"
},
{
"vote": 1,
"title": "Exploring Literature with Stanza",
"text": null,
"date": "2020-08-23"
},
{
"vote": 1,
"title": "[D] AI auto dubbing possiblty",
"text": "[removed]",
"date": "2020-08-23"
},
{
"vote": 0,
"title": "[]",
"text": "[removed]",
"date": "2020-08-23"
},
{
"vote": 3,
"title": "Finite state automata / transducers in NLP",
"text": "Hi all, Iâ€™m wondering if anyone who currently works in NLP or maybe more broad language technology uses FSTs in their work? I have a module in my linguistics MA which covers formal language and FSTs so Iâ€™m curious to see how/if anyone in the language technology industry actually uses them as part of their work. Thanks!",
"date": "2020-08-23"
},
{
"vote": 11,
"title": "Day 235 of #NLP365 - Learn NLP With Me â€“ Topic Modelling With LSA And LDA",
"text": "Day 235.\n\n\nToday's post covers me revisiting topic modelling using LSA and LDA.\n\n\nhttps://ryanong.co.uk/2020/08/22/day-235-learn-nlp-with-me-topic-modelling-with-lsa-and-lda/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-22"
},
{
"vote": 16,
"title": "Is there an NLP tool out there that creates a bunch of rephrasings of an input sentence? I struggle with writing and it would help to see different ways to structure a given phrase",
"text": null,
"date": "2020-08-21"
},
{
"vote": 0,
"title": "Better hottie than Alexa?",
"text": "[removed]",
"date": "2020-08-21"
},
{
"vote": 8,
"title": "BLEURT: Learning Robust Metrics for Text Generation | Research Paper Walkthrough",
"text": null,
"date": "2020-08-21"
},
{
"vote": 3,
"title": "Day 232 of #NLP365 - NLP Papers Summary â€“ Building And Exploring An EKG For Investment Analysis â€“ Deployment And Related Work",
"text": "Day 232.\n\n\nToday's post covers a paper summary on how to build an enterprise knowledge graph for investment analysis, focusing on deployment and related work.\n\n\nhttps://ryanong.co.uk/2020/08/19/day-232-nlp-papers-summary-building-and-exploring-an-ekg-for-investment-analysis-deployment-and-related-work/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-20"
},
{
"vote": 6,
"title": "Anyone know where I can get historic versions of text?",
"text": "Hi everyone. The title sounds a bit weird but I'm basically curious if there might be any corpora out there that has different versions of itself over time. For example, a 1990 version of Wikipedia scraped yearly until now. I'm trying to analyze how meanings of words change over time. Any tips or pointers are appreciated, thanks!",
"date": "2020-08-20"
},
{
"vote": 1,
"title": "16 Best Sentiment Analysis Tools and Services for Machine Learning",
"text": null,
"date": "2020-08-20"
},
{
"vote": 3,
"title": "Is there a multi-language out of the box syllable counter out there?",
"text": "I have been looking for a tool that can count syllables in multiple languages, not just English. What I mean by a tool is a library or an API that I can call through code. Haven't had too much luck with that, maybe I am missing something?  \n\n\nWhat would be a good proxy for counting syllables that could also be consistent across several European languages?",
"date": "2020-08-19"
},
{
"vote": 4,
"title": "looking for no-code tool or self service service that would allow building knowledge graph",
"text": "Hello fellow scholars! Could you point me in right direction - I am looking for no-code self service tool that would allow building knowledge graph from some human inthe loop feedback or even just explicitly defined rules as examples (snorkel approach ) ?",
"date": "2020-08-19"
},
{
"vote": 2,
"title": "Mapping Appstore Reviews to SO Questions?",
"text": "Hi All,\n\n\nI just wanted to ask if it is possible to somehow match software feedback (e.g. Appstore Reviews) to StackOverflow Questions which explain steps to solve the problem. End goal is then to infer the difficulty of the necessary change/implementation based on the SO question.\n\n\nI was thinking about either training a question generation model which questions then can gets matched to the SO questions.\n\n\nAnother approach I have in mind is just directly matching but I'm kinda afraid that both methods do not achieve the result I want.\n\n\nDoes anybody know a better way to do this? Thank you in advance!",
"date": "2020-08-19"
},
{
"vote": 0,
"title": "Building AGI Using Language Models -- Why GPT-X Could Become AGI",
"text": "[deleted]",
"date": "2020-08-19"
},
{
"vote": 1,
"title": "Use Permutation Languge Modeling to create a bidirectional text generator",
"text": null,
"date": "2020-08-19"
},
{
"vote": 8,
"title": "Cognate Finder",
"text": null,
"date": "2020-08-19"
},
{
"vote": 1,
"title": "Computational linguistics with a biological background",
"text": "Hi, I am currently an undergraduate in a biological field (BSc in Zoology). I would love to start a master's in computational linguistics once it's completed but, what are my options? Would I have to undertake a new bachelor's degree or are there supplementary courses I can take to get onto a master's course? Are there better options, outside of doing a master's, that I can look into to get into this field?",
"date": "2020-08-18"
},
{
"vote": 0,
"title": "New Discord Link",
"text": "Hey, I realized that the last discord link expired. \n\n\nNew link discord - \nhttps://discord.gg/NzumrWQ\n \n\n\nCome join us to discuss all NLP related topics!",
"date": "2020-08-18"
},
{
"vote": 12,
"title": "Is formal semantics useful for computational linguistics and NLP?",
"text": "I browsed the table of content of \nCann's Formal Semantics\n. Cann's book is for linguistics, and am I right that it is helpful for computational linguistics and natural language processing? \n\n\nBut it also seems to me that the approaches to NLP have been predominantly based on statistics and machine learning in applications (and maybe also research), which I think is completely unrelated to formal semantics? So is formal semantics still useful for computational linguistics and NLP in applications and in research? If not, what specific statistics and machine learning approaches have become successful replacement of formal semantics?\n\n\nThanks.",
"date": "2020-08-18"
},
{
"vote": 1,
"title": "Day 231 of #NLP365 - NLP Papers Summary â€“ Building And Exploring An EKG For Investment Analysis â€“ Building Knowledge Graphs",
"text": "Day 231.\n\n\nToday's post covers a paper summary on how to build an enterprise knowledge graph for investment analysis, focusing on building the knowledge graphs.\n\n\nhttps://ryanong.co.uk/2020/08/18/day-231-nlp-papers-summary-building-and-exploring-an-ekg-for-investment-analysis-building-knowledge-graphs/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-18"
},
{
"vote": 8,
"title": "How to apply for research internship in DL/NLP?",
"text": "I am a CSE Junior year undergrad from , Mumbai. I am very much interested in NLP/DL and want to try my hands around with a research based projects/research internship . I have done few projects which are over here : \nwww.github.com/talha1503\n  and I am currently working on a Nerual Question Generation project. I have worked previously as a Data Science intern at a risk management company and also have a paper in pre-print. Can anyone please suggest me where to apply and how to reach out to professors/labs?",
"date": "2020-08-17"
},
{
"vote": 7,
"title": "Are There Any Pre-Trained Models for Multi-Class Sentiment Analysis in Python?",
"text": "Almost all of pre-trained models for sentiment analysis  (e.g. NLTK's \"Vader\") only look at the \"positive-negative\" scale.\n\n\nAre  there any models that look at the problem as a multi class one,  aiming  to not only tag the input as \"negative\" or \"positive\", but also  find  the dominant sentiments it presents? Sentiments such as  \"happiness\",  \"disgust\", \"anger\", etc.?\n\n\nThis is a  common practice in NLP in general, but I can't find any  pre-trained  models for that task (meaning, ones that I could use on my  data without  further training needed).",
"date": "2020-08-17"
},
{
"vote": 2,
"title": "Day 230 of #NLP365 - NLP Papers Summary â€“ Building And Exploring An EKG For Investment Analysis â€“ Approach Overview",
"text": "Day 230.\n\n\nToday's post covers a paper summary on how to build an enterprise knowledge graph for investment analysis, focusing on the overall approach.\n\n\nhttps://ryanong.co.uk/2020/08/17/day-230-nlp-papers-summary-building-and-exploring-an-ekg-for-investment-analysis-approach-overview/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-17"
},
{
"vote": 1,
"title": "Need a tool to get vocabulary list from japanese text.",
"text": "Hi everyone. Can someone suggest some tool, better simple one, to extract a list of vocabulary from the given japanese text sorted by frequency? If there is no simple way available, I assume I need tools to split a sentence parts in words and to put them into infinitive form thereafter. Can you suggest me some?\n\n\nP. S. Sorry for possible mistakes, I'm not a native speaker.",
"date": "2020-08-17"
},
{
"vote": 1,
"title": "Supervised Extractive Summarisation | Research Paper Walkthrough",
"text": "[deleted]",
"date": "2020-08-16"
},
{
"vote": 4,
"title": "Day 229 of #NLP365 - NLP Papers Summary â€“ Building And Exploring An EKG For Investment Analysis â€“ Introduction And Challenges",
"text": "Day 229.\n\n\nToday's post covers a paper summary on how to build an enterprise knowledge graph for investment analysis.\n\n\nhttps://ryanong.co.uk/2020/08/16/day-229-nlp-papers-summary-building-and-exploring-an-ekg-for-investment-analysis-introduction-and-challenges/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-16"
},
{
"vote": 7,
"title": "Discord?",
"text": "I was wondering if anyone would like to join a discord server to discuss their work, share tools, ideas, ect.? I would really like to talk with other people that are getting into or experienced in the field. I'm currently finishing up a NLP related research project, but I want to continue working on NLP related projects in the future.\n\n\n&#x200B;\n\n\nEdit:  I went ahead and created a discord - \nhttps://discord.gg/NzumrWQ",
"date": "2020-08-16"
},
{
"vote": 49,
"title": "Google Open-Sources LIT: A Visual, Interactive Model-Understanding Tool For NLP Models",
"text": null,
"date": "2020-08-16"
},
{
"vote": 3,
"title": "Meet InvoiceNet, a software platform to train custom models and extract intelligent information from PDF invoice documents!",
"text": "https://github.com/naiveHobo/InvoiceNet\n\n\nhttps://i.redd.it/dj00xr9ux7h51.gif",
"date": "2020-08-15"
},
{
"vote": 4,
"title": "MARGE - paraphrasing as a task for learning language representation - from FAIR",
"text": null,
"date": "2020-08-15"
},
{
"vote": 23,
"title": "Art project: GPT3 for writing letters to world leaders from glaciers, islands and coral reefs under threat of climate change",
"text": null,
"date": "2020-08-15"
},
{
"vote": 8,
"title": "Google approved my translation app",
"text": "My translation app (\nTranslator++\n) was approved by google yesterday and wanted to share it with the community. You can download it on \nplay store here\n\n\nThe app is based on my python package: \ndeep_translator\n, where multiple translators are integrated so that the user can switch between translators and get different translations.\n\n\nAfter creating the package, I thought it would be a great idea to make an app with it so I made the Translator++ app. I just released the first version.\n\n\nI will definitely work on improving it. I just wanted to hear your feedbacks about the first version. Do you think it is a good idea/app?\n\n\nAll my projects are open source. You are welcome to contribute to the \ndeep_translator project\n or \nthe app\n. Feel free to fork the repo or make a pull request",
"date": "2020-08-13"
},
{
"vote": 10,
"title": "How to Tokenize Japanese in Python",
"text": null,
"date": "2020-08-13"
},
{
"vote": 5,
"title": "Facts in language models vs KBs",
"text": "Recetly, people have been probing BERT and family for commonsense/factual knowledge. GPT-3 does a very impressive job at knowing facts. How do you feel about this trend of getting a single language model to learn not only grammar, syntax and semantics but also acting as a kind of database? Should more work be done to effectively increase knowledge capacity of models via parameter increase? Or should the community focus on developing methods where a model focuses on learning syntax and semantics and can naturally interact with an external memory and/or logical system like a knowledge base?",
"date": "2020-08-13"
},
{
"vote": 9,
"title": "Data Augmentation using Pre-Trained Transformers (BERT, GPT, etc) | Research Paper Walkthrough",
"text": "Data augmentation is a widely used technique to increase the size of the training data. It helps in significatly increasing the diversity of data available for training models resulting in reducing over fitting and enhancing robustness of ML model, without actually collecting new data. In this video we will understand how we can use Transformers to do augmentation in NLP. ðŸ”¥ \n\n\nCheck out at - \nhttps://youtu.be/9O9scQb4sNo\n\nOriginal Paper - \nhttps://arxiv.org/abs/2003.02245\n\nAlso check out, \nEasy Data Augmentation in NLP - \nhttps://youtu.be/-1unNLkwImw\n\n\nFeel free to share your thoughts ðŸ‘",
"date": "2020-08-13"
},
{
"vote": 1,
"title": "Robust NLP Engine - I",
"text": null,
"date": "2020-08-13"
},
{
"vote": 7,
"title": "Domain Specific Language Models for Document Embeddings?",
"text": "Hi.\n\n\nI'm working on an academic corpus, consists of subfields of \"Linguistics\". Fields like Cognitive Linguistics, Generational Linguistics and etc. I'm trying to generate good document vectors, but so far after trying Doc2Vec, Word2Vec + TFIDF, even LDA vectors, I'm not really satisfied. I've also tried to use BERT and SciBERT, but their embedding is also not that good. I've also tried finetuning both, but still I'm not satisfied at all. Doc2Vec and Word2Vec outperforms both in any occasion.\n\n\nSo I had this idea to finetune or create my own domain specific language model. But I'm nearly clueless of how creating language models work. Is there any guide on how to create domain specific language models?",
"date": "2020-08-13"
},
{
"vote": 5,
"title": "Build custom chat bot",
"text": "I want to build a project to build a chat or using personal message data. Iâ€™m wondering if you have any suggestions on models to use?",
"date": "2020-08-13"
},
{
"vote": 17,
"title": "[Tutorial] Generating Text Summaries Using GPT-Style Models on PyTorch with Minimal Training",
"text": "This tutorial covers how to use GPT-style language models (GPT/GPT-2/GPT-3) on PyTorch to summarize text from the CNN/Daily Mail dataset, with minimal training.\n\n\nTutorial link: \nhttps://blog.paperspace.com/generating-text-summaries-gpt-2/",
"date": "2020-08-12"
},
{
"vote": 2,
"title": "What dictionary to use for OOV (out of vocabulary)?",
"text": "I'm in the pre-processing stage of a project and I think that the corpus will have a lot of OOV words. I'm reading a lot about using a dictionary to find the OOV words, but what dictionary do I use? Is it domain specific (generating it yourself) or is there a standard dictionary for every language?\n\n\nThe corpus consists of computer-mediated communication in English.",
"date": "2020-08-12"
},
{
"vote": 8,
"title": "Day 224 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.22 â€“ Coreference Resolution VI",
"text": "Day 224.\n\n\nToday's sixth post covering chapter 22: Coreference Resolution. As usual, my notes are in the form of questions. Hope these are helpful! Check it out below!\n\n\nhttps://ryanong.co.uk/2020/08/11/day-224-learn-nlp-with-me-slp-textbook-ch-22-coreference-resolution-vi/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-11"
},
{
"vote": 9,
"title": "deplacy: visualize dependency trees in the terminal",
"text": null,
"date": "2020-08-11"
},
{
"vote": 0,
"title": "I need something that will take in a short audio file of a person speaking, and output the exact phonemic sequence of the audio file",
"text": "[removed]",
"date": "2020-08-10"
},
{
"vote": 3,
"title": "good resources/ state-of-the-art for expert user recommendation ?",
"text": "Hi,\n\n\nI've started working on an expert recommendation problem for a given query/question. I'm very new to the field and would really appreciate any inputs on how to go about this problem or any helpful resources/links that you can offer. \n\n\nI do have experience with Neural Nets and ML but mostly with computer vision applications, if that helps. I'm starting out with a few review papers but it's very intimidating right now, I hope it's not as hard as it looks currently.\n\n\nThanks in advance !",
"date": "2020-08-10"
},
{
"vote": 9,
"title": "Day 223 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.22 â€“ Coreference Resolution V",
"text": "Day 223.\n\n\nToday's fifth post covering chapter 22: Coreference Resolution. As usual, my notes are in the form of questions. Hope these are helpful! Check it out below!\n\n\nhttps://ryanong.co.uk/2020/08/10/day-223-learn-nlp-with-me-slp-textbook-ch-22-coreference-resolution-v/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-10"
},
{
"vote": 10,
"title": "how to change the way a text is written without changing its meaning?",
"text": "Hello, everyone,\n\n\nWith my team we are developing a kind of chatbot, and I would like to ask your advice.\n\n\nI would like to ask if it is possible, starting from a text, to write a new one but keeping its meaning.\n\n\nFor example, if the input is \"I'm learning nlp since the beginning of this year\", the output could be \" since the beginning of this year I started to learn nlp\"\n\n\nIs there already a tool or algorithm that does this?\n\n\nIf not, what would be a good approach to train a model that could do this?",
"date": "2020-08-09"
},
{
"vote": 1,
"title": "Python SpellChecker",
"text": "[deleted]",
"date": "2020-08-09"
},
{
"vote": 0,
"title": "Extracting time zone information from MSG",
"text": "How can I extract TZ information of the sending time from MSG using python?",
"date": "2020-08-09"
},
{
"vote": 3,
"title": "Abstractive summarisation of non-fiction books",
"text": null,
"date": "2020-08-09"
},
{
"vote": 7,
"title": "Making a Plagiarism Checker",
"text": "I'm a newbie in the field of NLP. I wish to make a Plagiarism checker that can check the plagiarism percentage among a bundle of documents or like group the ones which are the most similar together. Can anyone help guide me on how shall I go about it?",
"date": "2020-08-08"
},
{
"vote": 4,
"title": "Why not translating Summarization Datasets e.g. CNN/Dailymail in other languages?",
"text": "Hi,\n\n\nCurrently the training and fine-tuning of many summarization models in other languages than English lack the provisation of large datasets. In my opinion it seems that translation is ahead of other NLP problems (such as summarization). \n\n\nI was asking myself why not using machine translation on existing datasets and translate those to specific languages in order to create more language diverse datasets? (besides of maybe manifesting already existing faults).\n\n\nI am keen to hear your thoughts.",
"date": "2020-08-08"
},
{
"vote": 10,
"title": "Idea for an NLP web app: Thought explorer",
"text": "Given an input sentence (thought), we obtain a sentence embedding of it in higher dimensions. The more thoughts in the database, the more data points to cluster, which enables us to see how our thoughts are associated with one another. Eventually this should help us vizualize our thoughts in 3D, by using a dimensionality reduction technique.\n\n\nApplications:\n\n\n\n\nhelps us understand how biased our thoughts are in certain dimensions\n\n\nsee how our thoughts align with another certain user\n\n\nif we improve and make modifications to the algorithm that generates embeddings by introducing relevant variables, we should be able to see, how our thoughts on certain topics evolve over time\n\n\n\n\nIncase someone needs a road map on how to approach this, let me know, I have a small colab notebook to vizualize sentences in a 3d projection.",
"date": "2020-08-08"
},
{
"vote": 1,
"title": "How do accents affect voice control?",
"text": "Not sure if this has been asked before! Also, do pitch and lisps count as well?",
"date": "2020-08-08"
},
{
"vote": 1,
"title": "ELMo Vocabulary Visualization.",
"text": "[deleted]",
"date": "2020-08-08"
},
{
"vote": 2,
"title": "Combining the train and test set in torchtext with ConcatDataset but vocabulary can't be obtained",
"text": "I want to use the examples in the test set of the IMDB Sentiment Analysis Dataset for training, as I have built my own benchmark with which I will compare the performance of various Models (my Matura Thesis). After trying with the normal torchtext.dataset function, I got the ConcatDataset function working with the train and test data as follows:\n\n\nfrom torchtext.experimental.datasets import IMDB\nfrom torchtext.data.utils import get_tokenizer\n\ntokenizer = get_tokenizer(&quot;spacy&quot;)\n\ntrain_data, test_data = IMDB(tokenizer = tokenizer)\n\ntrain_data, valid_data = torch.utils.data.random_split(train_data, [17500,7500])\n\nfrom torch.utils.data import ConcatDataset\ntrain_dataset = ConcatDataset([train_data, test_data])\nprint(f&#039;Number of training examples: {len(train_dataset)}&#039;)\nNumber of training examples: 42500\n\n\n\nBut when I try to get the vocabulary, even with the experimental version of vocab, it won't work. Any reason to circumvent this?\n\n\nvocab = train_dataset.get_vocab()\n\n-\nAttributeError                            Traceback (most recent call last)\n in \n      1 from torchtext.experimental.vocab import Vocab\n----&gt; 2 vocab = train_dataset.experimental.vocab()\n\nAttributeError: &#039;ConcatDataset&#039; object has no attribute &#039;experimental&#039;",
"date": "2020-08-08"
},
{
"vote": 4,
"title": "Day 220 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.22 â€“ Coreference Resolution II",
"text": "Day 220.\n\n\nToday's another post covering chapter 22: Coreference Resolution. As usual, my notes are in the form of questions. Hope these are helpful! Check it out below!\n\n\nhttps://ryanong.co.uk/2020/08/07/day-220-learn-nlp-with-me-slp-textbook-ch-22-coreference-resolution-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-07"
},
{
"vote": 2,
"title": "A question about the transformer model",
"text": "Hi,\nRecently I've been getting into the aiayn paper and I think I understand the idea behind multi headed self attention except for one thing.\n\n\nThe way I understand it, we have a batch of sequences of embeddings:  \n[batch, seq_len, emb_size]\n   and then we apply the multi headed self attention mechanism and we get back a tensor with the almowt the same exact dimensions:  \n[batch, seq_len, v_size * n_heads]\n . And then we apply self attention again on this resulting vector and on the next one etc,\n\n\nBut eventually we end up with the same: [b_s, seq, emb_size]. But the way I understand it a transformer looks at a sequence and outputs a \nsingle\n prediction. We need to somehow get rid of the \nseq_len\n middle dim. \n\n\n[b_s, seq_len, emb] > some op I don't understand > [b_s, emb] > [b_s, vocab]\n\n\nDo we just take the: [b_s, -1, emb] and put it through a linear level?",
"date": "2020-08-07"
},
{
"vote": 5,
"title": "Question about finding most important words for text classification.",
"text": "I've just started learning more about text classification with different models like RoBERTa. I have been following this simple tutorial (\nhttps://rsilveira79.github.io/fermenting_gradients/machine_learning/nlp/pytorch/text_classification_roberta/\n) that classifies phrases into 7 different classes.\n\n\nI was wondering if it was possible to find the  most \"important\" words in a phrase that led it being classified as a specific class.\n\n\nFor instance one class in the dataset is the \nGetWeather\n class and an example phrase is \"Is it windy in Boston, MA right now?\" I would think that the most important word(s) that would allow me to classify this as the \nGetWeather\n class would be \"windy\".\n\n\nWould I need to create a separate dataset of these most \"important\" words or it possible to get this from the model inference.",
"date": "2020-08-07"
},
{
"vote": 2,
"title": "Corpus tagging",
"text": "Hi folks,\n\n\nI have a set of ~20k textual documents fairly well written (almost typos-free, ~200 tokens/document, ~10 sentences/document) belonging to a specific domain. For each of them, I'd like to assign tags corresponding to linguistic concepts mentioned within the document. I don't have a set of expected tags. Ideally, those tags should take into account:\n\n\n\n\nthe presence/absence of negations (if the sentence 'We didn't found any trace of aggression' then the tag \nviolence\n shouldn't be flagged)\n\n\nthe morphological and lexical variations (\nviolence\n, \nassault\n, \nbrutality\n, \nattack\n, but also \naggression\n, \naggressions\n, \naggressive behaviour\n...).\n\n\n\n\nI've started implementing it with RegEx-based rules (plus embeddings at phrase level to improve the rules' coverage) but I was wondering whether there are more sophisticated techniques out there that I can use to achieve a more refined result.\n\n\nThanks!",
"date": "2020-08-07"
},
{
"vote": 0,
"title": "swiftsummarizer.com - Text summarizer and Analyzer",
"text": "[removed]",
"date": "2020-08-06"
},
{
"vote": 1,
"title": "metric design in word2vec",
"text": "Hi,\n\n\nI'm a bit curious when learning word2vec: it seems to maximize the inner product of accompanied words, is there any support for this choice? I mean, intuitively I could choose Euclidean distance to minimize. does any research compare these two?\n\n\nThanks!",
"date": "2020-08-06"
},
{
"vote": 27,
"title": "GAN BERT: Generative Adversarial Learning for Robust Text Classification (Paper Explained)",
"text": null,
"date": "2020-08-06"
},
{
"vote": 2,
"title": "Day 219 of #NLP365 - Learn NLP With Me â€“ SLP Textbook Ch.22 â€“ Coreference Resolution I",
"text": "Day 219.\n\n\nToday's post covers a new chapter of SLP - chapter 22: Coreference Resolution. As usual, my notes are in the form of questions. Hope these are helpful! Check it out below!\n\n\nhttps://ryanong.co.uk/2020/08/06/day-219-learn-nlp-with-me-slp-textbook-ch-22-coreference-resolution-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-08-06"
},
{
"vote": 2,
"title": "Text Classification with Meta Tags",
"text": "Can anyone let me know how can I build a classifier model that takes in sentences and few categorical tags and outputs a class?",
"date": "2020-08-06"
},
{
"vote": 3,
"title": "Weakly supervised seq2seq",
"text": "I have 1k annotated samples and 10mill unannotated samples in a seq2seq format task. What are the state-of-the-art methods for utilizing such vast amounts of unannotated data?",
"date": "2020-08-06"
},
{
"vote": 2,
"title": "Hiring NLP / Sentiment Analysis Expert",
"text": null,
"date": "2020-08-06"
},
{
"vote": 1,
"title": "What technology(data) do chatbots use to maintains states of all active conversations?",
"text": "So let's say there is a chatbot and let's say multiple people connect to it. What is the best way for the chatbot to maintain or store the state of each conversation?\n\n\nI am asking this from an engineering (production) point of view. What do all the big chatbot companies use to maintain the state of all the current ongoing conversations? Storing them in a database and querying every time the bot receives a new message in an ongoing conversation is going to be very expensive as far as I know.",
"date": "2020-08-06"
},
{
"vote": 1,
"title": "paragraph generation",
"text": "Does anyone know if there are NLP tech to generate a fluent paragraph based on a few (given) sentences?",
"date": "2020-08-06"
},
{
"vote": 1,
"title": "Using GPT3 to generate formal logical systems",
"text": null,
"date": "2020-08-05"
},
{
"vote": 16,
"title": "Evaluation Metrics For Information Retrieval",
"text": null,
"date": "2020-08-05"
},
{
"vote": 1,
"title": "Orwellian: Worldâ€™s Worst Word. â€” Jacob Kozhipatt",
"text": null,
"date": "2020-08-04"
},
{
"vote": 1,
"title": "Determining Semantic Equivalence",
"text": "I am new to NLP, and I am curious whether there is an efficient way, using NLP, to determine the semantic equivalence between words.  For instance, in news articles, we may look for articles about the 'United States'.  But 'USA', 'US', 'America', 'Trump', etc. may all be seen as relevant and semantically equivalent in the context of the search.\n\n\nIf a user was searching 'US', but we wanted to show them all articles pertaining to the United States, how would we take 'US' and then determine those semantically equivalent words using NLP?\n\n\nIs there a known application of NLP that accomplishes this?",
"date": "2020-08-04"
},
{
"vote": 24,
"title": "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning (Paper Explained)",
"text": null,
"date": "2020-08-04"
},
{
"vote": 8,
"title": "What should be the Query Q, Key K, and Value V vectors/matrics in torch.nn.MultiheadAttention?",
"text": "Following an amazing \nblog\n, I implemented my own self-attention module. However, I found PyTorch has already implemented a multi-head attention \nmodule\n. The input to the forward pass of the MultiheadAttention module includes Q (which is query vector), K (key vector), and V (value vector). It is strange that PyTorch wouldn't just take the input embedding and compute the Q, K, V vectors on the inside. In the self-attention module that I implemented, I compute this Q, K, V vectors from the input embeddings multiplied by the Q, K, V weights. At this point, I am not sure what the Q, K, and V vector inputs that the MultiheadAttention module requires. Should they be Q, K, and V weights or vectors and should these be normal vectors, or should these be Parameters?\n\n\n&#x200B;\n\n\nOriginal question \nhere\n.",
"date": "2020-08-04"
},
{
"vote": 5,
"title": "EasyExplain: Find similar ELI5 threads to your question",
"text": "Hello there!\n\n\nI just created my first python-based web-app.\n\n\nIt is a small web-service where users can enter questions like in ELI5 (e.g. \" Why do we feel nostalgia?\") and get the 4 most similar asked questions and their respective answers from ELI5 extracted with a link to the original thread. I used a data-dump from reddit between 2011 and 2018.\n\n\nHere\n is the site. The design is very minimalistic and not self-explaining yet. Feel free to try it out and give feedback :)",
"date": "2020-08-04"
},
{
"vote": 9,
"title": "NLP conferences calendar for 2020? Looking for a conference with a submission deadline after September 1st.",
"text": null,
"date": "2020-08-04"
},
{
"vote": 2,
"title": "Are there any word-level sentiment lexicons such as NRC Sentiment Lexicon?",
"text": "NRC sentiment lexicon is a great word-set with 14k+ words. It's categories are anger, sadness, fear, positive, negative etc. \nIs there a better one that you know off?\n\n\nhttps://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm",
"date": "2020-08-03"
},
{
"vote": 0,
"title": "Fuzzy offline-first search with leven-match",
"text": null,
"date": "2020-08-03"
},
{
"vote": 16,
"title": "Taggle: a new platform for your text annotation projects",
"text": null,
"date": "2020-08-03"
},
{
"vote": 36,
"title": "Fast inference for summarization, translation, text-generation, sentiment-analysis etc. with T5 on ONNX",
"text": "I started to work on the HuggingFace implementation of T5 and figured that it was possible to run it up to 4 times faster by moving it to ONNX. It was quite involved to port the model however and a few people showed interest, so I decided to make a small package which lets anyone easily use a pretrained T5 model, or port their own fine-tuned version, and run very fast inference on it in one line.\n\n\nhttps://github.com/abelriboulot/onnxt5\n\n\nThe package lets you run tasks including summarization, translation, sentiment-analysis, q&a, text-generation etc. in a fraction of the time, and with only a couple of lines!\n\n\nWould love to hear thoughts and comments so that I can make the package useful to the community.",
"date": "2020-08-02"
},
{
"vote": 12,
"title": "I created the deep_translator: a python library to translate between languages using different translators",
"text": "It's actually funny that there is a python package for almost everything :D. However, I was looking one day for a package to translate between languages and I found packages like google trans etc..\n\n\nBut I wondered why there is no tool, where I can switch between translators without installing a new package. Therefore, I created the \ndeep_translator\n . I worked on it for the last months, so I wanted to share it with you to get some feedback and also to tell the community that this project exists '--. Feel free to use it or make a pull request. \nhttps://github.com/nidhaloff/deep_translator\n\n\nI also made \nan app\n based on the package and using the kivy framework, which was surprisingly a solid framework and well documented. I published it today on google play store. \nhttps://github.com/nidhaloff/deep-translator-app\n\n\nI'm interested in any feedback and you can in touch with me anytime. Please consider liking the project on github if you find it useful ;)",
"date": "2020-08-02"
},
{
"vote": 1,
"title": "Need help in removing noise",
"text": "I'm trying to build a Question Answering system for large pdf documents, maybe around 1000 pages on top of BERT trained on SQUAD.  But I can't figure out how to remove noise from these documents, like every heading and irrelevant sentences that do not the answer the question can be noise. Any help would be appreciated.",
"date": "2020-08-02"
},
{
"vote": 4,
"title": "How to implement gender-inclusive language? (in French)",
"text": "Hello! I'm very new to studying NLP, I'm going to be starting a masters' degree in the fall, and I've been wanting to start a side project to learn even more and maybe add to my portfolio later.\n\n\nMy idea is to create a gender-inclusive \"converter\" in French. The user would give a sentence like :\n\n\n>Les chefs d'entreprises sont inquiÃ©tÃ©s par la crise.\n\n\nand it could turn it into :\n\n\n>Les chefÂ·fes d'entreprises sont inquiÃ©tÃ©Â·es par la crise.\n\n\nThe difficulty I'm having is that I need to figure out if the reference of a noun or a pronoun is a \ngeneric reference\n or, if it refers to a specific person or group, if the people referred to include women and/or men.\n\n\nI've been thinking about using coreference resolution to go back to the first instance and hopefully get information about the gender of people we're talking about.\n\n\nBut how can I determine if a reference is specific or generic? I've found some corpora that include this information, maybe I could use them....\n\n\nDo you think this can be achieved? What areas of NLP or tools should I look into?\n\n\n&#x200B;\n\n\nThanks for your help!",
"date": "2020-08-02"
},
{
"vote": 5,
"title": "Anyone working on Chinese LNP here, and ...",
"text": "... would be willing to answer rudimentary questions about Chinese LNP asked by this greenhorn programmer/linguist? \n\n\nI want to help contribute to the Chinese language learner's community by creating free to use tools to help people improve their Chinese. But, I have so many questions that are itching at the back of my mind with every step I take and I would like to ask them to someone familiar/expertised on the subject. \n\n\nFeel free to send me a PM/reply. \n\n\n(P.s. I can read Chinese technical documents, but my composition is probable at a 4-6th grade level.)",
"date": "2020-08-02"
},
{
"vote": 13,
"title": "Google Recent Semantic Search Implementation",
"text": "I noticed that recently Google has been not only finding related pages but also when I enter the page the answer is highlighted. For example, if I Googled \"Elon Musk's age\" his age will pop up, his birthdate will be bolded and as I enter biography.com the related sentence will be highlighted. \n\n\nI know this is a form of a Question-Answering bot algorithm but I'm wondering if anyone knows what was newly implemented to allow this, as well as the highlighting when I go into the page. I'm guessing it is probably related to recent advances with transformers-based neural nets. Also, is the highlighting related to training on Squad-type questions?\n\n\nI'm wondering this because I am trying to implement a Chrome Extension that accelerates research with semantic searching within websites that isn't too computationally heavy. A few guides recommended simply using word vectors with cosine similarity but I'm not too sure about the precision of such an algorithm. Any recommendations would be appreciated.",
"date": "2020-08-02"
},
{
"vote": 8,
"title": "Google search like capability on a dashboard for text data",
"text": "Planning to create a dashboard where I can have keyword search based filtering of text. For example I should be able to search multiple key words with and/ or  functionality . Is there any dashboard tech that already has this functionality I am even open to implement this in python as well",
"date": "2020-08-01"
},
{
"vote": 14,
"title": "How can I use NLP to extract the main food word from an ingredient?",
"text": "Let's say I have the following list of ingredients:\n\n\n\n\n1 fresh baguette, cut into sandwich parts \n\n\n1 tablespoon good quality unsalted butter, per sandwich (so 4 tbsp if making 4 sandwiches) \n\n\n1 apple or gala preferably (2 apples for 4 sandwiches) \n\n\n1 slice of english blue cheese, preferably a stilton (again, or 4 slices for 4 sandwiches)\n\n\n\n\nWhat I want to do is extract the main food word from the list of ingredients. So the result should be this:\n\n\n\n\nbaguette\n\n\nbutter\n\n\napple\n\n\ncheese\n\n\n\n\nHow can I use NLP to do this? Or is there something else I can use to do this?",
"date": "2020-08-01"
},
{
"vote": 1,
"title": "Unsupervised / Minimally Supervised methods for discriminating between antonyms and synonyms within word embeddings?",
"text": "Does anyone know of any unsupervised / minimally supervised methods for discriminating between antonyms and synonyms within word embeddings?",
"date": "2020-07-31"
},
{
"vote": 1,
"title": "What attention mechanism does this paper apply \"general\" or \"self-attention\"?",
"text": "This\n paper mentions that they apply the attention mechanism proposed by bahdanau et al., which I understand is general attention mechanism connecting encoder and decoder layers. However on their GitHub page, they seem to use the \"self-attention\" mechanism (with query, key and value information). Am I right that they are using self-attention and not general attention?",
"date": "2020-07-31"
},
{
"vote": 3,
"title": "Help needed in Node2Vec and Graphs",
"text": "Suppose I have a Directed Graph with data:\n\n\nAmazon -> Music\n\n\nAmazon -> Cloud\n\n\nAmazon -> Shopping\n\n\nApple -> Phones\n\n\nApple -> Music\n\n\nApple -> Laptops\n\n\nGoogle -> Cloud\n\n\nGoogle -> Phones\n\n\nGoogle -> Music\n\n\nHuwaie -> Devices\n\n\nHuwaie -> Phones\n\n\nI am embedding the nodes, edges using the Node2vec model and HadamardEmbedder.\n\n\nI need to find companies most similar to Amazon. But when I do I get nearby connected nodes that are Music, Cloud...\n\n\nIf  I embed the edge Amazon -> Music and find similar edges, I get :  \"('Phones', 'Service')\",  (\"('Modems', 'Music')\",  \"('Music', 'Search  Engine')\", \"('Music', 'Service')\",  \"('OS', 'Services')\"\n\n\nI get these kinds of results.\n\n\nAlthough what I want is most similar to come as Google as it shares 2 same traits.\n\n\nWhat can be done here, can you help?",
"date": "2020-07-31"
},
{
"vote": 1,
"title": "Seeking guidance on mining for particular grammatical forms",
"text": "I am proficient with C# but haven't touched Python.  Due to some goals though I want to try NTLK, SpaCy or something to try and collect example language uses.  I was hoping I could outline my goals and people could help point me in the right direction.  Maybe there is a library I could use in C# that is recommended.  (I'd rather learn some Python though and use mature tools than deal with a less mature NLP library in C#).\n\n\nMy goal is to find examples of various grammatical constructs in Spanish and English, to review how that construct is used in context.  By context, I'd like to be able to see the surrounding sentences.  I use linguee.com a lot for this, but it is limited  in a couple ways (1. not much context 2. search for individual words, not classes of words).  I use Google ngram search too, but it has similar limitations. \n\n\nAs an example, suppose I want to look for uses of Spanish verbs where the verb is using a present tense conjugation while the phrase is describing something that happened in the past.  In pseudocode, what I'd like to write is:\n\n\n\n\nFor each corpus available, enumerate sentences.\n\n\nRun the sentence through some POS processing.\n\n\nIf there isn't a present tense verb, skip the clause.\n\n\nIf there isn't an adjunct indicating a past time, skip the clause.\n\n\nOutput the location of the sentences that weren't skipped so I can go look at how they are used.\n\n\n\n\nFor step 1, it looks like NTLK has corpuses available.  Maybe spacey too.  I really am not sure how much corpus I would need, but maybe not that much.  I'm not trying to gather comprehensive stats, just find examples.  If a corpus has spanish and english translations that would be great but not necessary.  I suspect NTLK and Spacy also has something to break up the sentences.\n\n\nFor step 2 I presume NTLK or SpaCy have something that will do POS tagging, probably producing some sort of tree structure.  \n\n\nFor 3 and 4, I'd probably be writing some custom heuristics and am not too worried about completeness (since I just want some examples).  For 3 specifically I already have word lists of many verbs in particular forms I could string match against.  For 4 I could have a simple heuristic that looks for things like \"N years go\" or \"in year N\", and I could refine these heuristics over time.  Such heuristics are good targets for unit testing and I would want to do that.\n\n\nThoughts?  Maybe there is something that already does this out of the box and I don't need to reinvent the wheel?  I'm basically at the point of installing Python and NTLK or SpaCy.  Any tips / pointers to help shorten my learning curve?",
"date": "2020-07-31"
},
{
"vote": 19,
"title": "Write shorter, more effective email! GMail chrome extension using Facebook AI BART summarization model to turn a wordy email into an effective one.",
"text": null,
"date": "2020-07-31"
},
{
"vote": 11,
"title": "Leveraging BERT for Extractive Text Summarization on Lectures | Research Paper Walkthrough",
"text": null,
"date": "2020-07-31"
},
{
"vote": 1,
"title": "Philosophers On GPT-3 - Daily Nous",
"text": null,
"date": "2020-07-31"
},
{
"vote": 2,
"title": "Creating a valid dataset for obtaining results",
"text": "I have created a domain-specific dataset, lets say it is relating to python programming topic posts. I have taken data from various places specific to this topic to create positive examples in my dataset. For example, python related subreddits, stack exchange posts tagged with python, twitter posts hashtagged with python or python specific sites.   \n\n\nThe data points taken from these places are considered positive data points and then I have retrieved data points from the same sources but relating to general topics, searched if they contain the word python in them and if they do discard them to create the negative examples in my dataset.  \n\n\nI have been told that I can use the training set from the dataset as is, but that I need to manually annotate the test set for the results to be valid, otherwise they would be biased. Is this correct? How would they be biased? To be clear the test set contains different entries to the training set. \n\n\nThere are close to 200,000 entries in the test set which makes manual annotation difficult. I have seen similar methods been used in papers I have previously read without mention of manual annotation. Is this technique valid or do I have to take some extra steps to ensure the validity of the test sets?",
"date": "2020-07-31"
},
{
"vote": 9,
"title": "NLP for research papers",
"text": "are there any good NLP models that have been trained to analyze and understand scientific literature specifically? looking to analyze and do some word embedding models for research abstracts/biotech company descriptions.",
"date": "2020-07-30"
},
{
"vote": 0,
"title": "Understanding the state of natural language technology through a project-first approach",
"text": null,
"date": "2020-07-30"
},
{
"vote": 40,
"title": "What are the important blogs/magazine/subscription that I need to follow for NLP?",
"text": "Are there any GOOD magazine or blog that I can follow for latest tutorials and breakthroughs related to NLP & NLU? I already subscribed to â€œTowards Data Scienceâ€, which is under medium.com.",
"date": "2020-07-30"
},
{
"vote": 2,
"title": "Day 211 of #NLP365 - When To Use Which Clustering Algorithms?",
"text": "Day 211.\n\n\nToday's post covers when you should use which clustering algorithms. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/29/day-211-when-to-use-which-clustering-algorithms/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-29"
},
{
"vote": 9,
"title": "Chinese difficulty based word-hierarchy lists? Are there any?",
"text": "Is there a way or an already existing resource that approximates Chinese word complexity? Or even by grade level (e.g. preschool, primary school, junior high, senior high, secondary education, etc.). \n\n\nSomething like the Fog index wouldn't work in Chinese due to it being a logographic language. \n\n\n[While this article] (\nhttps://journals.plos.org/plosone/article/figures?id=10.1371/journal.pone.0163623\n) does dive into optimizing the order in which characters are learned, and expands on how the same method could be used for multi-character words I have yet to find any list produced with this method.\n\n\nEdit: Any place I could ask instead? I've been flinging search terms at google hoping any of it sticks for several hours now, but so far no luck.",
"date": "2020-07-29"
},
{
"vote": 7,
"title": "(Slow learner here unless learning through active application) How do I put SpaCy/NLTK to daily use for effective learning?",
"text": "Many thanks.",
"date": "2020-07-29"
},
{
"vote": 1,
"title": "Data Structures and Algorithms",
"text": "What data structures are seen across NLP/computational lingusitics vs natural languages? And what algorithms are preferred to translate it to language models?",
"date": "2020-07-28"
},
{
"vote": 0,
"title": "Using word classifiers to calculate sentiment",
"text": "Using word classifiers to calculate sentiment\n\n\nBy identifying and computing the word classifier â€“ noun, verb, adjective, pronoun, adverb, preposition, etc. â€“ for each word in a sentence, we can form some generalized assumptions about what is communicated by the written source. In fact, we can quantify classifiers of any sentence to create acceptable conditional statements that correspond with personal interpretations. But first, context of the written source must be known beforehand and that interpretations will only make sense for that particular topic.\n\n\n&nbsp;\n\n\nList of compliments, tamed or rude (some words may be classified wrong):\n\n\nExample 1\n\n\nSentence: Those are a nice pair of Mona Lisas. (pun intended)\nClasses: pron v n adj n n pron\nTotals: n = 3, v = 1, adj = 1, pron = 2, adv = 0, prep = 0\nInterpretation: There is at least one adjective to describe, most likely nouns. A couple pronouns are used, signifying some comparison with some well-known name. Because thereâ€™s at least 1 adjective, it might be a positive compliment.\n\n\n\nExample 2\n\n\nSentence: Your body is a work of art.\nClasses: pron n v n n v n\nTotals: n = 4, v = 2, adj = 0, pron = 1, adv = 0, prep = 0\nInterpretation: The sentence is talking about a lot of things, nouns specifically. Thereâ€™s no adjective, so this might not be a compliment easily. However, the sentence is a compliment with metaphor. Caution should be exercised if these kinds of sentences will be extracted.\n\n\n\nExample 3\n\n\nSentence: Annoying or not, this chick is way hot!\nClasses: adj prep adv n n v adj adj\nTotals: n = 2, v = 1, adj = 3, pron = 0, adv = 1, prep = 1\nInterpretation: Might be a compliment overall.\n\n\n\nExample 4\n\n\nIt is considered an archetypal masterpiece of the Italian Renaissance, and has been described as the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world. â€“ Wikipedia\nTotals of classes: n = 6, v = 14, adj = 5, pron = 2, adv = 0, prep = 11\nInterpretation: Verbs make 33% of the sentence. Multiple adjectives appear among the verbs that likely describe actions. This is backed by plenty of prepositions. Hence, this could likely be a critical review with some compliments.\n\n\n\nExample 5\n\n\nThe paintingâ€™s novel qualities include the subjectâ€™s expression, which is frequently described as enigmatic, the monumentality of the composition, the subtle modeling of forms, and the atmospheric illusionism. â€“ Wikipedia\nTotals of classes: n = 9, v = 7, adj = 4, pron = 0, adv = 1, prep = 7\nInterpretation: Nouns, verbs, and prepositions are almost balanced to each other; both combined have an average of 7.67. Adjectives are present too. The balancing of the totals mean the review was carefully articulated, so it may safely be labeled as a compliment.\n\n\n\n&nbsp;\n\n\nGiven a context, describing a set of class is fairly easy without having to look at the true sentence. You may cover the sentences and try analyzing the class sets again to see if you form your own interpretations of these compliments.\n\n\n&nbsp;\n\n\nPossibility for sentiment analysis on markets\n\n\nApplying this research to market predictions would need a new and extensive research. One must also be knowledgeable of investment vocabularies and lingo to understand market movements.\n\n\n&nbsp;\n\n\nWe analyze the responses of a thread that has a TA chart of Bitcoinâ€™s price increase from here \nhttps://www.reddit.com/r/Bitcoin/comments/hy90zp/bitcoin_is_moving_fast_now_smashed_through_two/\n\n\nSummary of the comments:\n\n\nExample 1\n\n\nThereâ€™s some rationale, but trade at your own risk.\nClasses: pron = 0, n = 5, v = 2, adj = 0, adv = 0, prep = 1\nConclusion: Critique.\n\n\n\nExample 2\n\n\nThe move is good but not impressive.\nClasses: pron = 0, n = 1, v = 2, adj = 2, adv = 0, prep = 2\nConclusion: Critique in movements more than nouns.\n\n\n\nExample 3\n\n\nNo. It hit the target. Both times.\nClasses: pron = 0, n = 5, v = 1, adj = 0, adv = 0, prep = 1\nConclusion: Lots of nouns. For or against chart analysis?\n\n\n\nExample 4\n\n\nThe faster the increase, the lower the MC will be.\nClasses: pron = 1, n = 2, v = 2, adj = 0, adv = 2, prep = 4\nConclusion: Objects described with adverbs and multiple uses of prepositions indicate action. \n\n\n\n&nbsp;\n\n\nThen we analyze a reputable news source that may influence price from this Reddit post \nhttps://www.reddit.com/r/Bitcoin/comments/hyfkuz/visa_and_btc_mass_adoption_is_getting_closer/\n\n\nResponses and replies:\n\n\nConversation 1\n\n\nType: Response\nTotals of classes: pron = 4, n = 11, v = 8, adj = 1, adv = 0, prep = 3\nConclusion: Lots of verbs. Expresses action. Up or down, unknown. Pronouns with action.\n\n\n\nReply 1\n\n\nType: Reply to 1.\nTotals of classes: pron = 2, n = 4, v = 2, adj = 0, adv = 1, prep = 0\nConclusion: Has pronouns. Maybe adding info.\n\n\n\nReply 2\n\n\nType: Reply to 1.\nTotals of classes: pron = 1, n = 6, v = 3, adj = 0, adv = 0, prep = 2\nConclusion: Plenty of nouns and 1 pronoun. Maybe adding info.\n\n\n\nConversation 2\n\n\nType: Response\nTotals of classes: pron = 0, n = 5, v = 1, adj = 0, adv = 0, prep = 1\nConclusion: No adjectives. Sentence is 71.4% nouns. Probably sarcasm.",
"date": "2020-07-28"
},
{
"vote": 1,
"title": "Day 210 of #NLP365 - Describing 4 Different Clustering Algorithms",
"text": "Day 210.\n\n\nToday's post describes the four types of clustering algorithms. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/28/day-210-describing-4-different-clustering-algorithms/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-28"
},
{
"vote": 1,
"title": "An overview of Unsupervised Machine Translation (MT)",
"text": null,
"date": "2020-07-28"
},
{
"vote": 1,
"title": "The Shocking Revelation of Natural Language Processing.",
"text": null,
"date": "2020-07-28"
},
{
"vote": 1,
"title": "The Shocking Revelation of Natural Language Processing.",
"text": "[deleted]",
"date": "2020-07-28"
},
{
"vote": 2,
"title": "Python and PDF: A Review of Existing Tools",
"text": null,
"date": "2020-07-28"
},
{
"vote": 0,
"title": "Brief overview of current word embedding methods. Code exaples are included in Google Colab Notebook so you can quickly test each method by yourself.",
"text": "[deleted]",
"date": "2020-07-28"
},
{
"vote": 18,
"title": "Behavioral Testing of NLP models with CheckList",
"text": null,
"date": "2020-07-28"
},
{
"vote": 11,
"title": "Language model licenses for spaCy and Stanza",
"text": "I want to better understand how licensing for these language models works as the overall license for these languages are open for commercial use. Stanza clearly offers a lot more language models than spaCy. I recently noticed that the language models have their own licenses from a commercial use perspective. While the majority of language models allow commercial use (CC, MIT) for both spaCy and Stanza out of the languages that I want, I see that Spanish and Polish (both GPL) cannot be used commercially.   \n\n\nI want to better understand how licensing for these language models works as the overall license for these languages are open for commercial use. Does spaCy offer its own Commercial License that can be bought in order to use these language models in production?",
"date": "2020-07-27"
},
{
"vote": 0,
"title": "Anyone Who Has Worked on Brazillian E-commerce Dataset for Recommendation system?",
"text": "I was thinking of making a recommendation system for \nthis Kaggle dataset\n. But since this will be my first recommendation project so I trying to find a kernel or a repo to guide me. this dataset is a pretty common NLP dataset but I can't seem to find a well-detailed recommendation project on it. So if any of you worked on it can you please share the code? TIA",
"date": "2020-07-27"
},
{
"vote": 1,
"title": "Day 209 of #NLP365 - Introduction To Clustering",
"text": "Day 209.\n\n\nToday's post introduces clustering as I believe it's important to familiar yourself with unsupervised techniques. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/27/day-209-introduction-to-clustering/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-27"
},
{
"vote": 21,
"title": "GPT-3 and A Typology of Hype",
"text": null,
"date": "2020-07-27"
},
{
"vote": 2,
"title": "Any Translator Application That Listens to Multiple Sentences Before Stopping Automatic Translation?",
"text": "Using the Google Translate App under Android, I have a problem when I turn on the \"Conversation\" feature when the speaker speaks many sentences. Google is capturing the first words that it can translate as a sentence - or sometimes just a fragment of a sentence - and then it stops listening to the conversation. I then I have to \"x\" to reset things and restart its listening.\n\n\nThe end result of this behavior is I miss about 70% of the conversation I am trying to translate. Google is so proud of itself for translating one sentence that it just drops all the subsequent sentences until I manually reset. Ideally what I would want would be for the application to store each translation it captures but keep listening to the conversation and continue to translate. Is there any way to get that more conversational behavior from Google's App?   Is there any other application that can listen to and continuously translate a longer monologue?",
"date": "2020-07-26"
},
{
"vote": 3,
"title": "[deleted by user]",
"text": "[removed]",
"date": "2020-07-26"
},
{
"vote": 2,
"title": "Content extraction libraries and approaches",
"text": "There are many projects whose aim is to extract meaningful information from webpages, such as:\n\n\n - \nhttps://pypi.org/project/readability-lxml/\n\n\n- \nhttps://github.com/postlight/mercury-parser\n\n\nAmong others, but all seem to be either abandoned or with small communities - is there a SOTA or leader in this space?",
"date": "2020-07-25"
},
{
"vote": 3,
"title": "Relative noob with a problem: searching poorly scanned documents for all potential matches for a given string",
"text": "This is kind of a shot in the dark, but I'm trying to do a fairly weird OCR task with no real programming background. I want to use a sort of captcha-reading fuzzy logic to search for places a given word might be, rather than force the computer to commit to a single transcription it considers most likely. I've got tens of thousands of pages of terribly scanned material and I just want to find all potential occurrences of a single word at a time. \n\n\n&#x200B;\n\n\nIs this something I could do with google vision? Or even with a standalone client I wouldn't need to program myself?",
"date": "2020-07-25"
},
{
"vote": 1,
"title": "What are the best Question Answer libraries, given a piece of text",
"text": "Title.\n\n\nMy domain is open-ended, the piece of text can be virtually any topic.",
"date": "2020-07-25"
},
{
"vote": 3,
"title": "OpenAI GPT3 - where to start?",
"text": "I have no prior idea about openAI. Want to learn some basics and play around a little to get some ideas about it. \nCanâ€™t seem to find a proper video tutorial. Any suggestion where can I start from?\nMy learning targets related to GPT3 include NLP+NLU based examples, predictions, data preprocess etc.",
"date": "2020-07-24"
},
{
"vote": 1,
"title": "Creating a name/address matching model",
"text": "I have a name/address dataset X and a reference name/address dataset Y. I want to build a model that takes an x in X and returns and valid matches in Y. The idea is that matches represent the same individual living at the same address. Not all individuals will have matches. Sometimes, there may be more than one decent match, and I'd like to see all such matches.\n\n\nIn both datasets, there is a great deal of variation in formatting, missing data and spelling errors. First names might be given as nicknames or initials in one but not the other. \"STREET\" may or may not be written as \"ST\" in either. Zip codes may be missing in some x in X. \n\n\nA simple string distance function might do an OK job, but ideally I would prefer a distance function that took context into account. For example, \"ALEX\" should be closer to \"ALEXANDER\" than \"ALAN\", \"MAIN STREET\" should be closer to \"MAIN ST\" than \"M STREET\". \n\n\nAny suggestions?",
"date": "2020-07-23"
},
{
"vote": 0,
"title": "Training an auto-regressive language model",
"text": "[deleted]",
"date": "2020-07-23"
},
{
"vote": 2,
"title": "Day 205 of #NLP365 - Learn NLP With Me â€“ Zero-Shot Learning For Text Classification (+ Idea Validation Survey)",
"text": "Day 205.\n\n\nZero-shot learning has been very popular and it has very strong practical implications. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/23/day-205-learn-nlp-with-me-zero-shot-learning-for-text-classification/\n\n\nOn a separate note, I am currently working on a personal project, and I am in the process of idea validation and building out the MVP. We are targeting people that consume a lot of books and that are highly driven and self-motivated to improve themselves every day. We are very excited to get your thoughts on this idea and have created a quick 2-minute MCQ survey. Let us know your thoughts! :) \n\n\nhttps://ryanmong.typeform.com/to/Fs4Fd7h2\n\n\nBest,\n\n\nRyan",
"date": "2020-07-23"
},
{
"vote": 13,
"title": "Algorithms for finding Key Phrases (2-3 words) in a given text",
"text": "I am looking to make a Key phrase extractor model given text, like a news article. I want to use a Machine Learning or preferably deep learning method but am not sure as to what architectures or datasets are available. If anyone can give any insight that would be greatly appreciated",
"date": "2020-07-23"
},
{
"vote": 2,
"title": "Workshop for NLP Open Source Software (NLP-OSS 2020)",
"text": "Are you using a lot of open source while developing machine learning applications, particularly Natural Language Processing (NLP) open source tools?\n\n\nIn November, we are organizing the second workshop for NLP Open Source Software (NLP-OSS) at EMNLP 2020. The vision of NLP-OSS is to democratize NLP, it's a bi-annual workshop where we share current state of NLP open sources and best practices.\n\n\nWe look forward to your submission of any insights to NLP open source tools and/or introduction of your open source tools! Here's some motivations as to why you should write something up and submit to NLP-OSS\n\n\n&#x200B;\n\n\n>I don't want to suffer anymore trying to install yet another open source NLP package! Document your pains and suggest good opens source practices, e.g. documentation / API design.\n\n\n&#x200B;\n\n\n>I couldn't find a tool to meet my NLP needs so I made one for myself and made it open source! Tell us more about it and make it known to the wider community. You can share/summarize the current status of your NLP open source in your submission.\n\n\n&#x200B;\n\n\n>I have contributed to a (perhaps famous) NLP open source, share your experience, tell us how you got your pull-request merged. How can we as a community improve NLP OSS contribution process?\n\n\nThe extended deadline for the paper submission is 19th August!! More details on \nhttps://nlposs.github.io/2020/index.html#call-for-papers\n\n\nLooking forward to your submission!!",
"date": "2020-07-22"
},
{
"vote": 13,
"title": "Are there any opensource options for using a neural machine translation with rule-based machine translation to correct errors in rare-resource domains?",
"text": "I'm new to this, so I hope this makes sense. Translators like GoogleTranslate do a great job of translating the text I am working with, but I have a large list of word-pairs that I would like to use to correct the initial translation. Is there a way I can use German-to-English word pairs (from a very large German to English dictionary specifically written for translating this particular author) as rules to compensate for neural machine translations errors? \n\n\nI have thousands of pages of untranslated text, and I want to speed up the process without having to manually correct every paragraph of every page.\n\n\nAgain total newbie here so I hope this all makes sense. Anyway, I would appreciate your input! Thx!",
"date": "2020-07-22"
},
{
"vote": 0,
"title": "AutoML-Zero",
"text": "[deleted]",
"date": "2020-07-22"
},
{
"vote": 5,
"title": "Text Annotation Tool Survey",
"text": "Hello Redditors,\n\n\nWe are working on a new text annotation tool for NER and we are conducting a survey to get feedback from NLP practitioners regarding the text annotation process , if you have a moment, can you please help us answering someÂ questions: \n\n\nhttps://docs.google.com/forms/d/e/1FAIpQLSekzkAzvh09XbUvdzDxMaQtJxIw4VcwPsm3s44lEE5Z92bdsA/viewform\n\n\nYour help will be greatly appreciated!\n\n\nUBIAI Team,\n\n\nhttps://ubiai.tools",
"date": "2020-07-22"
},
{
"vote": 3,
"title": "Does anyone have suggestions on Learning Rate Reduction on Plateau when training/finetuning Transformer type models?",
"text": "In CV, it is often the case that people reduce the learning rate when the evaluation does not improve, often with SGD, to get a noticeable performance boost. However, I do not think I see a lot of in NLP (but hey, what do I know). Prior to the popularization of the transformers, I often hear people say that it is not needed since Embedding + LSTM is easy to train (compared to ResNet for example). However, transformers are also very deep, therefore I have been wondering whether that trick helps. \n\n\nI personally have experimented finetuning BERT with something like that, the results are pretty mixed (I cannot really tell if the increase in performance is simply by luck or is consistent, and the increase is quite small). I have been using AdamW (instead of SGD) since I don't really find SGD to be working in transformers (but I could definitely be wrong). \n\n\n&#x200B;\n\n\nSo I would really like to know if someone has experimented with learning rate reduction on plateau when training on transformers. Other training tips are also welcomed. Thanks in advance.",
"date": "2020-07-22"
},
{
"vote": 11,
"title": "How does BERT go from pre-training to fine-tuning?",
"text": "I understand that the pretraining helps BERT understand language, context, and words. I also understand that fine-tuning uses very similar architecture as pre-training, and just an extra layer is added for the specific task.... but HOW does the pretrained knowledge get incorporated into fine-tuning stage? The fine-tuning learns to do the actual task, but how specifically does this â€œcontextual knowledge â€œ from the pretraining come into play? Something with weights for finetuning?",
"date": "2020-07-22"
},
{
"vote": 5,
"title": "BertForQuestionAnswering",
"text": "Hey everyone, i am new to using bert and i ve managed to make a model that returns specific sequence from a paragraph that answers a certain question,\n\n\nhowever, sometimes the answer is the whole paragraph, but yet Bert never picks this pattern up and return soem random quote from it, is there any way to make Bert do return the whole paragraph when necessary ?",
"date": "2020-07-21"
},
{
"vote": 12,
"title": "Talking with GPT-3 about COVID-19",
"text": null,
"date": "2020-07-20"
},
{
"vote": 8,
"title": "What algorithm to use for (legal) contract reviews",
"text": "I am trying to find some info about (legal) contract reviews with the aid of NLP.\nWhat would be the proper tech or right keywords to search on this. I checked also the site papers with code but could not find what to look for. Any idea under which category this falls under at paperswithcode.com?",
"date": "2020-07-20"
},
{
"vote": 3,
"title": "Day 202 of #NLP365 - Learn NLP With Me â€“ NLP And Transfer Learning Revisit",
"text": "Day 202.\n\n\nIf anyone of you is going through NLP interviews, today's post (and the last few days) would be useful for you :) It covers (IMO) some of the important concepts that one should know for a NLP data scientist role. Check it out below and give me some feedback!\n\n\nhttps://ryanong.co.uk/2020/07/20/day-202-learn-nlp-with-me-nlp-and-transfer-learning-revisit/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-20"
},
{
"vote": 20,
"title": "NLP equivalent to YOLO",
"text": "Does anyone know if there an existing approach, such as using transformers, to find sections of text that discuss a topic? Similar to how YOLO is used in computer vision to find a section of an image with an object within it.\n\n\nEdit: Thanks to the responses, I've done a little more research, and the following paper on topic segmentation satisfies my query: \nhttps://paperswithcode.com/paper/sector-a-neural-model-for-coherent-topic",
"date": "2020-07-20"
},
{
"vote": 1,
"title": "True or False and Explain Why. A poll of general opinion.",
"text": "[removed]",
"date": "2020-07-19"
},
{
"vote": 5,
"title": "are there any good models for mapping an entity to related words using a parse tree?",
"text": "I have created a parse tree using spacy and identified named entities and coreferences. For each entity I identify adjectives to describe them; verbs saying what they did; and words that flip them negative.......However sentences can be complex. Is there any standard set of relationships to look for to understand an entity?",
"date": "2020-07-19"
},
{
"vote": 1,
"title": "Is it mandatory that a corpus is separately annotated by two different annotators to measure IAA (Inter Annotator Agreement)?",
"text": "Medical text corpus annotation is a tough task because it requires expert annotations. Oftentimes, it is really expensive and time-consuming. It is also important that the corpus is annotated by more than one expert and Inter Annotator Agreement is measured (IAA). (IAA = inter-annotator agreement is a measure of how well two (or more) annotators can make the same annotation decision for a certain category.) IAA measures how trustworthy the annotations are and how easy was it to delineate the categories being annotated.\n\n\nSince annotation is really time-consuming, is it really necessary that an entire corpus is annotated by two separate annotators? Could a midway scenario like this be actually possible? The midway scenario - About 20% of the corpus documents are annotated by two experts and IAA is measured. If IAA is above a certain threshold, the rest of the corpus (80%) is only annotated by one expert annotator.\n\n\nSource: \nhttps://corpuslinguisticmethods.wordpress.com/2014/01/15/what-is-inter-annotator-agreement/",
"date": "2020-07-19"
},
{
"vote": 8,
"title": "How to include metadata information on sentence embeddings?",
"text": "So I have a bunch of sentence level embeddings after processing my sentences through sentence bert.\nNow the issue is I have tags like location , number of people as meta data corresponding to each sentence. Can you suggest me a way to embed these information also as embeddings and hence get a combined embedding.",
"date": "2020-07-19"
},
{
"vote": 0,
"title": "Russians Try To Speak English and Repeat Funny Tounge Twisters",
"text": null,
"date": "2020-07-17"
},
{
"vote": 1,
"title": "Detecting and extracting content from ordered/numbered list",
"text": "Hi, I was wondering whether there are ways to detect an ordered/numbered list from a piece of text. Then my goal is to have a way to either remove or ignore the number formatting.\n\n\nE.g., an ordered list in the form of:\n\n\n\n\nItem 1\n\n\nItem 2\n\n\nItem 3\n\n\n\n\nor:\n\n\n(1) Item 1\n\n\n(2) Item 2 etc\n\n\nor: \n\n\n1- Item 1\n\n\n2 - Item 2 \n\n\nI'm guessing regex may be useful but may be limited due to the fact that there are many different styles for ordered lists. Also, I can't simply use code to remove the numbers is because I still want \"Item 1 Item 2 item 3\" in the text. If I use regex to match patterns of numbers followed by dots or numbers inside () that may work but again there's a limitation if the text inside the list also contains floats (e.g. 3.5) or numbers inside parentheses.",
"date": "2020-07-17"
},
{
"vote": 4,
"title": "Is there any way to calculate scaled co-ocurrence matrix? (Python,pandas)",
"text": "I have a csv file. Now, I can generate word*word Co-occurrence matrix. However, I want to generate scaled Co-occurrence matrix. \nStack Question\n\nIs there any package / raw-code-sample to do it?",
"date": "2020-07-17"
},
{
"vote": 4,
"title": "What is SqueezeBERT in NLP?",
"text": null,
"date": "2020-07-17"
},
{
"vote": 1,
"title": "Analysis of statements in text",
"text": "Sorry, I'm not AT ALL affiliated with the technical side of things...\n\n\nIs there a way to analyse what percentage of statements overlap in a text with another piece of text? Or putting it another way, is NLP currently able to determine overlaps in intelligible statements in a body of text? \n\n\nEg. Could it tell for example, that the above two questions are more or less about the same thing?\n\n\nIf so, what technical requirements need to be met? \n\n\nAgain, sorry if this is not technical enough.",
"date": "2020-07-16"
},
{
"vote": 6,
"title": "Any good resources on Neural Question Generation?",
"text": "Hi there! I would like to implement Question Generation using Deep Learning. I have already tried it using NLP. Are there any research papers , which you could recommend me. I have seen some blogs using T5 transformer, but I would like to build the paper/implement it from scratch.",
"date": "2020-07-16"
},
{
"vote": 2,
"title": "Early TMS TECHNOLOGY providers",
"text": "Hi All! Do you by any chance have examples of other \nold enough\n \nTMS\n-type technology providers? Those that were here from 1980 to 2010? \n\n\nIt would be nice to expand and update this list: \nhttps://www.nimdzi.com/the-history-of-tms-technology-from-the-80s-to-2010/\n \n\n\nEspecially now, when a new \nTranslation Management System\n is born each month, it's important to remember and study the guru tech... I just feel like the History infographics can include more titles from the early days. \n\n\nThanks in advance!",
"date": "2020-07-15"
},
{
"vote": 3,
"title": "Need help for building a recommendation system",
"text": "I have this dataset/dataframe: \nhttps://imgur.com/rou62mu\n\n\nI'd like to take some movie/tv-series titles as input and recommend some similar items by taking in 'titleType', 'genres', 'averageRating' as features\n\n\nHow do I go about doing this? Can cosine similarity provide good results? If yes, how do I feed in both numerical and non-numerical data as input?\n\n\nAny help is appreciated!",
"date": "2020-07-15"
},
{
"vote": 4,
"title": "Day 197 of #NLP365 - Learn NLP With Me â€“ Filling The Gaps With NLP Interview Questions",
"text": "Day 197.\n\n\nToday's post is a new fun one. I googled around NLP interview questions and see what's missing in my knowledge. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/15/day-197-learn-nlp-with-me-what-is-coreference-resolution/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-15"
},
{
"vote": 3,
"title": "Webinar on Adversarial Testing is available on demand",
"text": "FYI, our webinar with Professor Chris Potts, \nImproving Natural Language Understanding through Adversarial Testing,\n is now available to stream on demand. Register to watch it \nhere\n.",
"date": "2020-07-15"
},
{
"vote": 1,
"title": "How to parse a paragraph sentence by sentence",
"text": "I have a relatively complicated paragraph which I am trying to parse sentence by sentence using a specific rule set. I've tried using the Stanford Parser but I haven't really been able to figure it out. I have a basic level of coding skill and could implement anything if I was told how but I really have no idea how to approach this one my own.\n\n\nAny chance anyone might be able to help out?\n\n\nThanks!",
"date": "2020-07-14"
},
{
"vote": 2,
"title": "How would I go about transforming a set of news articles into a fictional narrative?",
"text": "[deleted]",
"date": "2020-07-14"
},
{
"vote": 2,
"title": "Has anyone worked on pretraining BERT/GPT2/RoBERTa/any other model with more data?",
"text": "I was interested in discussing what are the common mistakes that you made while pretraining a language model and what you could have avoided. Anything that I should be careful about before carrying out the pretraining task?\n\n\n&#x200B;\n\n\nI will be really grateful to anyone participating in this discussion. :) Please feel free to write and discuss.",
"date": "2020-07-14"
},
{
"vote": 51,
"title": "New Hugging Face Transformers Notebooks",
"text": "You can find the pre-configured container and run the notebooks here (for free -- there's nothing that's not free here):\n \nhttps://ml-showcase.paperspace.com/projects/hugging-face\n\n\nWe're excited to be offering new resources from Hugging Face for state-of-the-art NLP.\n\n\nA previous roadblock to many users has been getting their environment set up to work with the library. The new Transformers container comes with all dependencies pre-installed, so you can immediately utilize the library's state-of-the-art models for training, fine-tuning and inference.\n\n\nThe new notebooks cover how to train a tokenizer from scratch, how to use popular pre-trained language models in a couple lines of code, and pipelines which embed the tokenizer and model in a single call for downstream tasks. This includes models like BERT, GPT-2, T5, Transformer-XL, XLM, and more.\n\n\nSome of the use cases covered include:\n\n\n\n\nSentence classification (sentiment analysis)\n\n\nToken classification (named entity recognition, part-of-speech tagging)\n\n\nFeature extraction\n\n\nQuestion answering\n\n\nSummarization\n\n\nMask filling\n\n\nTranslation\n\n\n\n\nFor a walkthrough of the code with Hugging Face's ML Engineer Morgan Funtowitz, \ncheck out the webinar\n.",
"date": "2020-07-14"
},
{
"vote": 2,
"title": "Day 196 of #NLP365 - Coreference Resolution With NeuralCoref (SpaCy)",
"text": "Day 196.\n\n\nToday's post covers the practical using neuralcoref spacy for coreference resolution in NLP. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/14/day-196-coreference-resolution-with-neuralcoref-spacy/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-14"
},
{
"vote": 3,
"title": "Translator learning NLP",
"text": "Hello!\n\n\nI recently found out about NLP and it seems really interesting.\n\n\nI have a bachelor's degree in translation and interpreting and I consider taking up a masters in NLP.\n\n\nFrom a quick research I did, it seems that I need to acquire certain technical skills in computer science, but my question is this: Can my career as a translator/interpreter be combined with NLP in any way? Not necessarily as a translator of course.\n\n\nThanks a lot",
"date": "2020-07-14"
},
{
"vote": 7,
"title": "Training a Shift-Reduce Parser (Stanford CoreNLP)",
"text": "I am trying to train a constituency parser for Portuguese (from the terminal), but the instructions are rather vague (or I am too inexperienced).\n\n\n> It is possible to train the Shift-Reduce Parser for languages other than English. An appropriate HeadFinder needs to be provided. This and other options are handled by specifying the \n-tlpp\n flag, which lets you choose the class for a \nTreebankLangParserParams\n.\n\n\nI am not sure how to \"provide the appropriate HeadFinder\". There are some example classes \nhere\n, but they unfortunately don't work out of the box.\n\n\nException in thread &quot;main&quot; java.lang.IllegalArgumentException: No head rule defined for N&#039; using class edu.stanford.nlp.trees.ModCollinsHeadFinder in (N&#039; (N vinho) (A branco))\n\n\nException in thread &quot;main&quot; java.lang.IllegalArgumentException: No head rule defined for NP using class edu.stanford.nlp.trees.international.spanish.SpanishHeadFinder in (NP (DEM Aquele) (N cliente))\n\n\nObviously I need to define the rules, but how/where am I supposed to go about this? Do I just need to create \nPortugueseHeadFinder.java\n? How would I incorporate this in place of, for example, the \nSpanishHeadFinder\n when I am using the \nSpanishTreebankParserParams\n (or will I need to define that entirely as well)?",
"date": "2020-07-13"
},
{
"vote": 5,
"title": "Morphological analyzers and other cool stuff (Questions)",
"text": "Hi I'm a developer and a language enthusiasts. recently I've been using a ton of Anki add-ons for learning languages. but some of the stuff I want are nowhere to be found so I want to make my own.\nBut I don't know where to start. Is there a tutorial out there?\nI want to make tools like morphological analyzers that can extract Arabic word roots and on top of that suggest the pitfalls, if you are interested a short example would be the templates ØªÙØ¹ÛŒÙ„ and ØªÙØ¹Ù„ which aren't consistent in the verbs they generate from a root. I also have a plan for a metric tool for Persian poetry which I'm already versed in.\nWhat should I do? I'm willing to go to any length. just give some direction.",
"date": "2020-07-13"
},
{
"vote": 0,
"title": "Grad project suggestion",
"text": "[deleted]",
"date": "2020-07-12"
},
{
"vote": 16,
"title": "Experiences with AllenNLP",
"text": "[deleted]",
"date": "2020-07-12"
},
{
"vote": 11,
"title": "Day 194 of #NLP365 - Learning PyTorch â€“ Tweets Sentiment Extraction (Part 2)",
"text": "Day 194.\n\n\nToday's post covers part 2 of tackling the tweets sentiment extraction challenge using PyTorch. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/12/day-194-learning-pytorch-tweets-sentiment-extraction-part-2/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-12"
},
{
"vote": 1,
"title": "Bilingual sense embeddings alignment evaluation",
"text": "I have (unaligned) sense embeddings in English and in French, and I want to evaluate the quality of the supervised alignment v unsupervised alignment (I am using \nMUSE\n).\n\n\nI know in theory the supervised alignment should yield better results, but I would like to use a metric to quantify the improvement (besides a TSNE plot).\n\n\nDo you have any ideas/suggestions?",
"date": "2020-07-12"
},
{
"vote": 4,
"title": "Using POS tags for text classification as the only feature?",
"text": "[deleted]",
"date": "2020-07-11"
},
{
"vote": 37,
"title": "Term Extraction Package in Python",
"text": "Hey guys, recently I was interested in automated term extraction but there aren't many implementations out there written in Python (though there are two implemented in Scala and Java) so I decided to implement my own (\nhttps://github.com/kevinlu1248/pyate\n). \n\n\nI currently have C-Value, Basic, Combo Basic, Weirdness and Term Extractor implemented using spaCy POS tagging though I am eager to implement more based on suggestions. Furthermore, the sources for these algorithms can be found at the bottom of the README file in the repo.\n\n\nThis project had taught me a lot regarding reading papers, implementing in practice, evaluating, and optimizing. I saw a few posts regarding term extraction so I decided to share this project here.",
"date": "2020-07-11"
},
{
"vote": 12,
"title": "Day 193 of #NLP365 - Learning PyTorch â€“ Tweets Sentiment Extraction (Part 1)",
"text": "Day 193.\n\n\nToday's post continues on with my journey to learn pytorch. What better way to learn than to use pytorch for a kaggle challenge. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/11/day-193-learning-pytorch-tweets-sentiment-extraction-part-1/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-11"
},
{
"vote": 2,
"title": "Are there hierachical pos tags models?",
"text": "[deleted]",
"date": "2020-07-11"
},
{
"vote": 6,
"title": "How are big neural networks deployed in production?",
"text": "For traditional ML models - they can be deployed as weights. I am curious how these large models like BERT are deployed in production while holding SLA promises. Any tips where I can find this info about deploying ML models? especially the large scale models.\n\n\nEdit (Gathering responses):\n\n\n\n\nDistillation is one way - but the model sees a slight drop in performance compared to original.\n\n\nDockerization.",
"date": "2020-07-10"
},
{
"vote": 1,
"title": "Negation Detection Algorithms in python",
"text": "I am writing a paper for a uni class (Fifth semester, CS undergraduate degree) about sentiment analysis, more precisely about negation detection. I have to write a survey about the different approaches to negation detection and hopefully compare different algorithms, by applying them on a couple newspaper articles.\n\n\nAfter researching for a while, I found multiple papers about negation detection algorithms, but most of them are in the field of medical reports. Nevertheless, it seems to me that \nNegEx\n, \nConText\n and \nNegBio\n are good candidates to test and write about. The problem is I am not sure if it makes sense to test them on \"normal\" text, i.e. documents not related to the medical field.\n\n\nI have the feeling I'm well over my head. I would really appreciate any guidance or resources about how negation detection algorithms work and how they differ from each other, preferably algorithms with python implementation that I can test and compare.\n\n\n&nbsp;\n\n\nPS: I posted this question on Stackoverflow, but it got closed there, so I hoped I will find more help here. Thanks in advance!",
"date": "2020-07-10"
},
{
"vote": 1,
"title": "Day 192 of #NLP365 - NLP Papers Summary â€“ Guiding Extractive Summarization With Question-Answering Rewards",
"text": "Day 192.\n\n\nToday's post covers a paper summary on \"Guiding Extractive Summarization With Question-Answering Rewards\". Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/10/day-192-nlp-papers-summary-guiding-extractive-summarization-with-question-answering-rewards/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-10"
},
{
"vote": 1,
"title": "Sample size for text classification",
"text": "Hello everyone. Do any of you have any interesting resources, articles or tips about choosing a sample size for document classification ? How would you vary this with average length and number of classes ? I need to manually label some documents, but I need to justify the number of labels I require with some kind of model.",
"date": "2020-07-10"
},
{
"vote": 6,
"title": "(Video) Transformer architecture explained",
"text": "Always wanted to know how Transformers work? \nHere is\n a concise and high-level video explaining the Transformer architecture.\nMore interesting videos on the \nAI Coffee Break - YouTube\n Channel!\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/2h7ffwv2k0a51.png?width=1008&format=png&auto=webp&s=1c763f339818c846e9a2ee4cc6259a08220e98ee",
"date": "2020-07-10"
},
{
"vote": 2,
"title": "Summarizing pre-recorded BioNLP talks from ACL2020.",
"text": "I tried to briefly summarize the pre-recorded talks from the ACL2020 bioNLP workshop in this Medium post.\n\n\nhttps://medium.com/@recurrent.pi/summarizing-papers-from-bionlp-workshop-in-acl2020-a6ba3d937705",
"date": "2020-07-09"
},
{
"vote": 1,
"title": "I need help with evaluating Bleu scores for this model",
"text": "[deleted]",
"date": "2020-07-09"
},
{
"vote": 3,
"title": "Day 191 of #NLP365 - Summarisation Of ArXiv Papers Using TextRank â€“ Does It Work?",
"text": "Day 191.\n\n\nToday's post covers my quick experiment playing with the ArXiv dataset and apply summarisation on it. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/09/day-191-summarisation-of-arxiv-papers-using-textrank-does-it-work/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-09"
},
{
"vote": 3,
"title": "Which keyword extraction algorithm would you recommend instead of TextRank?",
"text": null,
"date": "2020-07-09"
},
{
"vote": 1,
"title": "Humble Book Bundle: Data Science Essentials by Taylor &amp; Francis (pay what you want and help charity)",
"text": "[deleted]",
"date": "2020-07-08"
},
{
"vote": 3,
"title": "I created a small library to index (batches of) sentences, unindex and auto-pad them and more, all fully tested and compatible with PyTorch and Tensorflow",
"text": "Hey!  \n\n\nI always found the part of building a data structure that maps from tokens to index a pretty repetitive one when implementing a (deep) NLP model, which is why I wrote \nthis class here\n to make this process easier by requiring only one function call. I also included a bunch of other helpful features:\n\n\n\n\nTurning batches of indices back into sentences\n\n\nReading from vocab txt files\n\n\nFiltering by frequency\n\n\nAuto-padding sequences\n\n\nCompatibility with Tensorflow / PyTorch tensors\n\n\n... and more!\n\n\n\n\nI'd be happy to receive some feedback from you!\n\n\nhttps://github.com/Kaleidophon/token2index",
"date": "2020-07-08"
},
{
"vote": 1,
"title": "How to detect text normality* using Language model?",
"text": "Hi.\nPlease advise how to detect if a user typed normal comment or just a bag of words?\n\n\nFor ex:  \n\n\nnormal_comment = 'Was working on NLP ticket. Text readability detection'\n\n\n&#x200B;\n\n\nnot_normal = 'was working working working bla bla bla'\n\n\n&#x200B;\n\n\nTried to use Perplexity score \n\n\nfrom pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\nmodel = OpenAIGPTLMHeadModel.from_pretrained(&#039;openai-gpt&#039;)\nmodel.eval()\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = OpenAIGPTTokenizer.from_pretrained(&#039;openai-gpt&#039;)\ndef score(sentence):\ntokenize_input = tokenizer.tokenize(sentence)\ntensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\nloss=model(tensor_input, lm_labels=tensor_input)\n \n# return math.exp(loss)\n \nreturn math.exp(loss / len(tokenize_input))\n \n\n\nfor i in test_set:\nsc = score(i)\n \nprint(i, &#039;--&gt;&gt;&gt;Score:&#039;,sc)\n\n\n&#x200B;\n\n\nInteresrting result (first and last sentances should be Normal-positive):\n\n\nthere is a book on the desk --&gt;&gt;&gt;Score: 5.381236556374695\n\n\n \nbrbhr develepment brbhr develepment  brbhr develepment wdwd  --&gt;&gt;&gt;Score: 1.6756367975729165\n\n\n \nthis is very good is very good is very good cecece --&gt;&gt;&gt;Score: 1.850087075961654\n\n\n \nTo encapsulate uncertainty of the model, we can use a metric called perplexity. This is it --&gt;&gt;&gt;Score: 1.7560097914562294\n\n\n \nworking at development branch --&gt;&gt;&gt;Score: 37.99469869325517\n\n\n \nfffffffffffffffffffffff ffffffffffffffffffffff --&gt;&gt;&gt;Score: 1.2439040909256314\n\n\nI am working on nlp tasks to help to detect some not readable text --&gt;&gt;&gt;Score: 2.3496070619152047\n\n\n&#x200B;\n\n\nMaybe I should use some other approach ?\n\n\nThanks in anvance.",
"date": "2020-07-08"
},
{
"vote": 24,
"title": "John Snow Labs Spark-NLP 2.5.3: Detect Fake news, emotions, spams, and more classification models, enhancements, and bug fixes",
"text": null,
"date": "2020-07-08"
},
{
"vote": 11,
"title": "What technical questions do Amazon ask for Language Engineer position ?",
"text": "[removed]",
"date": "2020-07-08"
},
{
"vote": 21,
"title": "Implementing Bengioâ€™s Neural Probabilistic Language Model (NPLM) using Pytorch",
"text": "In 2003, Bengio and others proposed a novel way to solve the curse of dimensionality occurring in language models using neural networks. This marked the beginning of using deep learning models for solving natural language problems.\n\n\nWith the advent of advanced ML libraries like TensorFlow (Keras) and PyTorch, it is now quite easy to build and train custom models. In this post, I am trying to implement Bengio's NPLM using PyTorch with GPU acceleration for faster training and predictions.\n\n\nPost link: \nhttps://abhinavcreed13.github.io/blog/bengio-trigram-nplm-using-pytorch/\n\n\nThe key takeaway from this post is the neat implementation support of PyTorch for creating and training customised language models with the ease of GPU acceleration.\n\n\nLet me know your thoughts about this.\n\n\nP.S. Nowadays state-of-the-art models are using the concept of 'Transformers' such as BeRT, RoBERTa and several others.",
"date": "2020-07-08"
},
{
"vote": 1,
"title": "Are AI chatbots really intelligent? - AskSid Conversational AI",
"text": "[deleted]",
"date": "2020-07-07"
},
{
"vote": 6,
"title": "Can you achieve publishable results without strong hardware?",
"text": "[removed]",
"date": "2020-07-07"
},
{
"vote": 3,
"title": "Sentiment Analysis",
"text": "I wanted to construct a sentiment analysis model and I was looking for few state of the art text preprocessing methods and training models. \nAlso, is there a way to generate training data without manually annotating the data?",
"date": "2020-07-07"
},
{
"vote": 4,
"title": "I just realized that Mercer in Jelinek-Mercer smoothing is ...",
"text": "[removed]",
"date": "2020-07-07"
},
{
"vote": 13,
"title": "Context-Based Paragraph Splitting of Bigger Article",
"text": "[deleted]",
"date": "2020-07-07"
},
{
"vote": 1,
"title": "Free webinar: Improving Natural Language Understanding through Adversarial Testing",
"text": "Join Stanford Professor Christopher Potts for a discussion of the current state of natural language understanding. He will also describe how we can use adversarial testing to improve natural language systems. \nRegister now.",
"date": "2020-07-06"
},
{
"vote": 8,
"title": "Is it legal to use all the abstract text from PubMed?",
"text": "So PubMed is one of the biggest biomedical literature repositories. However, using full-texts from PubMed is not free. Only the full-texts with OA licenses could be used for any kind of machine learning or text mining tasks. However, could all abstracts be used for text mining tasks? We would like to use only the abstracts and not the full-texts. Has anyone used abstracts directly from PubMed and not PMC-OA subset?",
"date": "2020-07-06"
},
{
"vote": 10,
"title": "Feedback Request: Rules-based NER Python Library",
"text": "I am working on the documentation for a Python Library for \"rules-based\" Named Entity Recognition (NER) that I would appreciate any and all feedback and questions.\n\n\nYou can find a first draft of a README here:\n\n\nhttps://github.com/genomoncology/entitykb\n\n\nBy \"rules-based\", it means my focus is building out entity extractors using logic and patterns rather than using a model-based solution like SpaCy's core NER pipeline.\n\n\nEntityKB's core use case is where you can create an \"index\" of Entities and their terms (name + synonyms) and then are able to process text extracting out the entities.\n\n\nIt also supports \"grammar-based\" NER such as shown in the included Date Resolver:\n\n\nhttps://github.com/genomoncology/entitykb/tree/master/src/entitykb/date\n\n\nThe documentation will include information about the architecture (i.e. tokenizing, filtering), data structure/algorithms (Trie, Aho Corasick), modes of operation (Python Library, CLI, and API) and prior art (FlashText, SpaCy EntityRuler).\n\n\nThanks for reading,\nIan",
"date": "2020-07-06"
},
{
"vote": 2,
"title": "Using BERT to get certain kind of word/chunk similarities",
"text": "Hi\n\n\nBERT is \"a general-purpose language understanding model\" but I want to use it to get similarities according to my specific domain.\n\n\nThe way I'm approaching it is by searching for 'similar' text from a pre-defined set of sentences, \nD\n. This set is supposed to contain all of the vocabulary I want to map to (modelling assumption). I start by storing representations of words/chunks(e.g. avg of word emb) for every sentence in \nD\n. Then, for a given test sentence, I figure out what word/chunk I want replaced (with a \"better\" version) and find the closest word/chunk by doing a cosine sim between word vector from test sentence and all the ones I saved from \nD\n. As compared to w2v, the representations of 'bank' in water and money contexts should be different and hence using BERT is good IMHO.\n\n\nOne potentially useful detail is that the 'sentences' aren't always properly formed sentences. BERT base is fine-tuned on in-domain data (10s of thousands). The task for fine-tuning is sentence pair classification as that's the closest one to word similarity that I want to get out of the model. Example: bert base (without tuning) gives 'paracetamol' -> 'tylenol' but I want 'paracetamol' -> 'ibuprofen'. While the original answer also makes sense, what I want is a specific kind of replacement/similairty to be honoured by the model.\n\n\nProblem is, BERT still gives me general purpose similarities and not the domain-specific ones that I need. Another observation is that BERT isn't caring about context as much as I expect/need it to. What am I doing wrong here?\n\n\nI thought about starting from scratch but I don't have enough in-domain data to train 110M+ params from scratch. I'm completely open to alternate approaches to this problem formulation i.e. doing this contextual word/chunk mapping to a small pre-defined set of words/chunks, where I also have the whole sentences in both cases. Full sentence context seems to be important for my problem.\n\n\nThank you!",
"date": "2020-07-05"
},
{
"vote": 20,
"title": "Day 187 of #NLP365 - Learn NLP With Me â€“ Embeddings Of Language, Knowledge Representation, And Reasoning",
"text": "Day 187.\n\n\nToday's post is my notes on the KR2ML Keynote Talk by Andrew McCallum on Embeddings of Language, Knowledge Representation, and Reasoning. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/05/day-187-learn-nlp-with-me-embeddings-of-language-knowledge-representation-and-reasoning/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-05"
},
{
"vote": 2,
"title": "Day 186 of #NLP365 - NLP Papers Summary â€“ Contextualizing Citations For Scientific Summarization Using Word Embeddings And Domain Knowledge",
"text": "Day 186.\n\n\nToday's post is a NLP paper summary, covering Contextualizing Citations For Scientific Summarization Using Word Embeddings And Domain Knowledge. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/04/day-186-nlp-papers-summary-contextualizing-citations-for-scientific-summarization-using-word-embeddings-and-domain-knowledge/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-04"
},
{
"vote": 3,
"title": "Multilingual Speech Recognition",
"text": "[deleted]",
"date": "2020-07-04"
},
{
"vote": 13,
"title": "Has anyone ever tried any NLP techniques, NER, Topic Modelling, sentiment analysis on the index of wikileaks documents?",
"text": "I was thinking of trying some topic modelling just to see what the are the most prevalent topics in the wikileaks docs and was wondering if anyone else has tried it? what was your approach?",
"date": "2020-07-03"
},
{
"vote": 2,
"title": "Day 185 of #NLP365 - NLP Papers Summary â€“ A Discourse-Aware Attention Model For Abstractive Summarization Of Long Documents",
"text": "Day 185.\n\n\nToday's post is a NLP paper summary, covering Discourse-Aware Attention Model for Abstractive Summarisation of research papers. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/03/day-185-nlp-papers-summary-a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-03"
},
{
"vote": 1,
"title": "Category matching",
"text": "Hello everyone! \n\n\nI want to ask if someone can point to the right direction (suggest any algorithms, some links to read something) \n\n\nI need to match categories of some product that are the same or somewhat close by meaning\n\n\ne.g.\n\n\ncars -> wheels\n\n\nauto -> wheels\n\n\ncars->parts->wheels \n\n\nThese categories are kinda the same thing. So I want to do this:\n\n\nGiven a \"main\" category  tree convert all other category trees to match the \"main\" one. \n\n\nSo if I have \n\n\ncars -> wheels and auto -> wheels I want to say that these are the same thing\n\n\nThanks for any help in advance.",
"date": "2020-07-03"
},
{
"vote": 1,
"title": "Comparing English and Spanish Queries using NLP",
"text": "I am trying to see whether there are differences in sentences between the topics in 2 languages ie English and Spanish. Eg. (Face masks are mandatory,  la mascarilla es obligatoria) but this stretched out to n number of queries. The final goal is to find whether a document in English talks about 1 topic and the other corpus in Spanish talks about a different one.\n\n\nI tried using BiLDA for generating topics in both the languages simultaneously but my data is not super clean for the model to work properly and was giving me vague results.\n\n\nI then went on towards BERT and TRANSFORMERS by using this wrapper \nhttps://github.com/amaiya/ktrain\n  for applying the pre-trained models to classify text by giving my own topics to the model only realizing that it would not work for Spanish.\n\n\nI also tried to look at FACEBOOK'S LASER and thought about comparing the embedding space but again I am relying on their pre-trained space and would not get my intended goal.\n\n\nIf anybody has done anything like that or can help me direct this it would be really helpful.",
"date": "2020-07-02"
},
{
"vote": 0,
"title": "If you're working in Finance and want to start on NLP in a practical way",
"text": "NLP is becoming one of the most valuable skills in financial data science. Weâ€™ve released a new course to help you make your first steps to mastering this skill. In four simple steps youâ€™ll learn how to download, prepare and start analysing company documents automatically. \n\n\nIt's super practical so you can apply it straight away.\n\n\nAll finishers get a certificate too ðŸ¤™ \n\n\nCheck it out here: \nhttps://links.quant-quest.com/NLPboost",
"date": "2020-07-02"
},
{
"vote": 1,
"title": "Day 184 of #NLP365 - Learning PyTorch â€“ Machine Translation With TorchText",
"text": "Day 184.\n\n\nApplying PyTorch's torchtext to machine translation. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/07/02/day-184-learning-pytorch-machine-translation-with-torchtext/\n\n\nBest,\n\n\nRyan",
"date": "2020-07-02"
},
{
"vote": 26,
"title": "I created a small open source Python package for fast text search in corpus",
"text": "I  made a little package called TNT (Text Neighbor Tracker) to allow  searching inside list of text documents. Thought it could be useful to  you guys! It's still alpha version, so expect bugs. Tell me guys what  you think!\n\n\npip install tnt-learn\n\n\nIt  works by making an approximate k-nearest neighbor search on scikit's  Tf-Idf or CountVectorizer sparse features. TNT uses my own  implementation of the cluster pruning algorithm to achieve fast querying  (1ms for a query in 500k tweets). It's not meant to replace any real  solid search engine like elasticsearch, but only to test some ideas or  provide, for example, a simple and easy to build Flask micro-service. I  also use it in a more complicated environment where it serves as an  intermediate process, without resorting and paying yet for another  elasticsearch instance.\n\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom tnt import TNT\ndataset = fetch_20newsgroups(subset=&quot;all&quot;, shuffle=True, random_state=42)\ntnt = TNT(vectorizer=&quot;count&quot;)\ntnt.fit(dataset.data) # feed in a list of strings\nprint(tnt.search(&quot;computer science&quot;, k=5))\n\n\nHere's the link to github page if you find it interesting: \nhttps://github.com/kerighan/tnt\n\n\nIf you have any criticism or feature request, let me know !",
"date": "2020-07-02"
},
{
"vote": 5,
"title": "Best API / Library for web content extraction &amp; feature creation.",
"text": "I am shopping around for some 3rd party APIs (or libs) to make our web extraction pipeline easier to maintain & scale. There are two main parts we are concerned with:\n\n\n- extracting the content from a HTML page\n\n\n- appending  various features such as topics, keywords, categories\n\n\nRight now we have been using IBM NLU with some good success, but I was wondering if this community has some improved suggestions!",
"date": "2020-07-02"
},
{
"vote": 18,
"title": "What are the courses did you take in your grad school for PhD or MS in NLP or any field related to it ?",
"text": "I just wanted to see the varieties of course works that different people take in different schools",
"date": "2020-07-02"
},
{
"vote": 11,
"title": "Deploy your TensorFlow or Hugging Face model in a single line of code. Feedback welcome!",
"text": "Hey everyone! \n\n\nMy name is Yoav, I'm an engineer who's been working on ML + platforms for a few years at Google and startups. My latest project is \nhttps://modelzoo.dev\n. \n\n\nWhat?\n\n\nModel Zoo aims to significantly simplify your life if you need to deploy a TensorFlow model to an HTTP endpoint. You can deploy in a single line of code like this:  \n\n\nfrom modelzoo.tensorflow import deploy, predict\n\n# Train or load your TensorFlow here.\nmodel = train_model()\n\n# Deploy with one function.\nmodel_name = deploy(model)\n\n# Make predictions from Python.\npredictions = predict(model_name, image=&quot;test.jpg&quot;)\n\n\n\nAnd you still get all the bells and whistles you typically want from a production model endpoint: UI for debugging, metrics monitoring, documentation, and logging.\n\n\nWhy?\n\n\nMany existing deployment solutions I've used and seen in the past require interacting with infrastructure layers -- e.g. containerize an application with Docker, provisioning an EC2 instance, uploading the model code somewhere. Even AWS Sagemaker can be a time-suck if you need to set up IAM roles, lambdas, and API gateway yourself. Model Zoo is my attempt at distilling all of this setup into a single line of code.\n\n\nTry it\n\n\n\n\nLive Model Demo\n: \nhttps://app.modelzoo.dev/models/ssd-mobilenet-v2-coco\n\n\n5 minute quickstart on Google Colab\n: \nhttps://colab.research.google.com/github/model-zoo/examples/blob/master/transformers-quickstart/quickstart.ipynb\n\n\nEnd to end example on github for training + deploying a GPT2 model\n: \nhttps://github.com/model-zoo/twitter-turing-test\n\n\n\n\nYou can deploy up to 3 models on my free tier to play around with the product. If you want to use it more (hosted or on your AWS VPC), drop me a DM. I'm subsidizing some infrastructure costs (~same price or cheaper than Sagemaker) in exchange for close collaboration with a select group of private beta testers.",
"date": "2020-07-01"
},
{
"vote": 3,
"title": "What are currently the best parsers?",
"text": "I'm going to work with some annotated historical data and need the best parsers available (cost isn't an issue) that offer at least some adaptability for special cases and such. \n\n\nI used SpaCy last year and was delighted at the many options to adapt rules & models to better fit my data. Haven't done any parsing for over a year so I thought I'd ask if maybe spaCy got a competitor since then.",
"date": "2020-07-01"
},
{
"vote": 0,
"title": "The Best Beginners' Guide To Learning Chinese Language",
"text": null,
"date": "2020-07-01"
},
{
"vote": 6,
"title": "How to make a fully-grammatical predictive text system?",
"text": "I would like to find or make a system which provides a list of suggested words and allows you to select one by clicking on them. The system constructs grammatical sentences. It is ok if it is not as good as a native speaker, committing either an occasional error or being restricted in the sentences it can produce.\n\n\nHow would I make this? Is there a common library available now which can suggest common and grammatically correct next words, in a sentence?\n\n\nOr, does such a tool already exist, somewhere?",
"date": "2020-06-30"
},
{
"vote": 5,
"title": "Day 182 of #NLP365 - Learning PyTorch â€“ Custom Dataset And DataLoader",
"text": "Day 182.\n\n\nExploring PyTorch's custom dataset and dataloader for batching data. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/06/30/day-182-learning-pytorch-custom-dataset-and-dataloader/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-30"
},
{
"vote": 1,
"title": "Where do I start with NLP?",
"text": "I know basics of ML and deep learning. Could you please show me some pointers on how I can master NLP",
"date": "2020-06-29"
},
{
"vote": 6,
"title": "Bilingual Embeddings",
"text": "Hey everybody!\n\n\nHas any of you worked with bilingual or multilingual embeddings in a NN and any advice on the topic?\nIm struggling to find the best ones for German-English..\n\n\n Iâ€˜m pretty new to that area of NLP so Iâ€™m feeling a little confused!\nIâ€™d really appreciate it! Thank you!!!",
"date": "2020-06-29"
},
{
"vote": 25,
"title": "Revealing Dart Secrets of BERT (Analysis of BERT's Attention Heads) - Paper Explained",
"text": null,
"date": "2020-06-29"
},
{
"vote": 4,
"title": "Day 180 of #NLP365 - Learning PyTorch â€“ Language Model With Nn.Transformer And TorchText (Part 1)",
"text": "Day 180.\n\n\nIt's cool and interesting to read many different research papers but there are typically two stages in research (and life in general): consuming and implementation. It's time to develop the implementation part. Check it out below!\n\n\nhttps://ryanong.co.uk/2020/06/28/day-180-learning-pytorch-language-model-with-nn-transformer-and-torchtext-part-1/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-28"
},
{
"vote": 19,
"title": "Implementation of some QA papers with detailed explanations (annotations) using PyTorch",
"text": "I have been working on this as a side project for some time now. I have implemented 3 question-answering papers (DrQA, BiDAF, and QANet) with detailed explanations/intuitions of various components based on my understanding.  The implementations/tutorials are in the form of jupyter notebooks.  The repository link is:\n \nhttps://github.com/kushalj001/pytorch-question-answering\nI am not an expert. I just tried this out as I wanted to learn about QA in depth.  If you go through it, let me know how you find it. The tutorials would be more suited to those who are familiar with some basic deep learning, NLP, and PyTorch.",
"date": "2020-06-28"
},
{
"vote": 17,
"title": "What's the most widely used document summarization algorithm at the moment?",
"text": "I've been reading about CNNs being used for summarization but I don't think it's mainstream (unless I'm wrong).",
"date": "2020-06-26"
},
{
"vote": 1,
"title": "How do you get an aggregate of multiple transcriptions?",
"text": "[removed]",
"date": "2020-06-26"
},
{
"vote": 7,
"title": "Day 178 of #NLP365 - NLP Papers Summary â€“ GPT-3 : Broader Impacts",
"text": "Day 178.\n\n\nToday's post covers the broader impacts of GPT-3! Check it out below!\n\n\nhttps://ryanong.co.uk/2020/06/26/day-178-nlp-papers-summary-gpt-3-broader-impacts/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-26"
},
{
"vote": 0,
"title": "NATURAL LANGUAGE PROCESSING IN ARTIFICIAL INTELLIGENCE",
"text": "[removed]",
"date": "2020-06-26"
},
{
"vote": 3,
"title": "Compilation of preprocessing Functions",
"text": "Is there any notebook or any other resources where somebody has implemented all the basic text(English)  preprocessing functions in python or related libraries(nltk, spacy, etc.) ?",
"date": "2020-06-26"
},
{
"vote": 19,
"title": "Is Bayesian NLP a thing?",
"text": "I've recently transitioned from doing Bayesian machine learning (i.e. building statistical models of usually small datasets using PyMC3 and Stan) to focusing more on NLP (i.e. fine-tuning transformer models on large datasets for specific tasks).\n\n\nI'm curious if there's any overlap between these two fields. I've found some resources online [1] [2], but they seem to focus on Bayesian inference for computational linguistics (e.g. Bayesian inference on PCFGs or other grammar models), which is \nvery\n different from most transformer-based deep learning that's driven the success of NLP in recent years. Does anyone know of any meaningful marriage between these two fields?\n\n\n[1] \nhttp://homepages.inf.ed.ac.uk/scohen/bayesian/\n\n\n[2] \nhttp://www.morganclaypoolpublishers.com/catalog_Orig/samples/9781627054218_sample.pdf",
"date": "2020-06-26"
},
{
"vote": 40,
"title": "TextAttack: a Python NLP framework for adversarial attacks, data augmentation, and model training",
"text": "https://twitter.com/jxmorris12/status/1276161580271534081",
"date": "2020-06-25"
},
{
"vote": 13,
"title": "Fine Tuning BERT: Multilabel Text Classification",
"text": null,
"date": "2020-06-25"
},
{
"vote": 6,
"title": "Algorithm/model for detecting sentence similarity",
"text": "What would be the best way of computing the semantic similarity of a database of sentence. What I mean for that is say I have the list of sentences   \n\n\n[\"I am skilled at python\", \"user is a very proficient python use\", \"I can drive a car legally\"]   \n\n\nSentence 1 -> 2 should rank as more similar in meaning than 1 -> 3.   \n\n\nGoal would be to build a recommendation engine that allows the database to cluster multiple different phrases around the semantic meaning of the phrases",
"date": "2020-06-24"
},
{
"vote": 22,
"title": "Basic Roadmap which everybody interested in NLP should know?",
"text": "I am a newbie and have good grasp over Deep learning concepts and also a few concepts of nlp. I want to start nlp in a proper way. Could someone please provide a learning path for NLP.",
"date": "2020-06-23"
},
{
"vote": 0,
"title": "Hello I am new in this community and I hope to have a good time with you And now you can help me with this here I would be very grateful. In what language is it? And what does it mean? If you can help me I would appreciate it",
"text": null,
"date": "2020-06-23"
},
{
"vote": 11,
"title": "Day 174 of #NLP365 - NLP Papers Summary â€“ PEGASUS: Pre-Training With Extracted Gap-Sentences For Abstractive Summarization",
"text": "Day 174.\n\n\nThe SOTA model for 12 summarisation datasets: PEGASUS! Check out below my summary of the paper, this is a good read!\n\n\nhttps://ryanong.co.uk/2020/06/22/day-174-nlp-papers-summary-pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-22"
},
{
"vote": 3,
"title": "Token and Sentence statistics for 91 Languages",
"text": "[deleted]",
"date": "2020-06-22"
},
{
"vote": 1,
"title": "Is a language model capable of doing this?",
"text": "Hi, everybody,\n\n\nI have a general question that I hope will be interesting for you.\n\n\nSuppose I have a language model. Now, it's not enough for me to have a class (the most likely word) given the input. Instead, I have a number of possible labels (which may correspond to PoS tags, or syntactic or other relationships) and, given the input I want a class for EACH of these labels (for example, I want the most likely word given the direct object label or the subject label). The problem is that in my data I can only see the output of one class.\n\n\nHow would you solve a problem like that? Do you know if any techniques have been developed to tackle this problem? (I know that language models have implicit knowledge of syntax, but suppose I want to give them explicit knowledge)",
"date": "2020-06-22"
},
{
"vote": 1,
"title": "Enroll in our courses and reach to new heights of your passion DM FOR DETAILS",
"text": null,
"date": "2020-06-22"
},
{
"vote": 0,
"title": "Possibility of publishing undergraduate thesis?",
"text": "[deleted]",
"date": "2020-06-22"
},
{
"vote": 3,
"title": "Day 173 of #NLP365 - NLP Discovery â€“ Text-To-Text Transfer Transformer (T5)",
"text": "Day 173.\n\n\nThis isn't a new discovery (although it's still relatively new!) but it's something I always wanted to look into and learn about :) I really like the questions format that I recently adopted from covering the \nfast.ai\n NLP course where I categorised everything I learned into questions. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/21/day-173-nlp-discovery-text-to-text-transfer-transformer-t5/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-21"
},
{
"vote": 0,
"title": "Perfect artificial neuron, maybe perfect most ANNs",
"text": null,
"date": "2020-06-21"
},
{
"vote": 2,
"title": "What are the best sources to find labelled review data sets for multi labelled text classification?",
"text": "I have been searching for labelled review data sets for training for multi label classification. However it has been a bit surprising for me that I could find only sentiment classification data sets and nothing else.\n\n\nI am not sure if am doing the search right. While I understand that the engine should be trained for our data, trying with some ready made labelled review sets would make the algo be ready faster.\n\n\nAre you aware of any sources where I can obtain labelled review data sets for label classification? If a free one does not work, paid sources are also fine.",
"date": "2020-06-21"
},
{
"vote": 10,
"title": "ELI5: Google's Covid Twitter Bert's vocabulary file does not contain \"coronavirus\"",
"text": "Preface: I am a beginner in the field of NLP. So this could be obvious to some of y'all.\n\n\nDownloaded the \nCovid Twitter Bert model from TFhub\n. In the downloaded package, there's assets/bert-large-uncased-whole-word-masking-vocab.txt , which, to my surprise, does not contain \"covid19\" or \"coronavirus\". I thought the vocab file contains all the tokens known to the model. Chances are I'm missing/misunderstanding something here. Am I looking at the wrong file? Am I thinking about the model wrong? \n\n\nAll thoughts are appreciated.",
"date": "2020-06-21"
},
{
"vote": 1,
"title": "Extracting meaningful keywords",
"text": "[deleted]",
"date": "2020-06-21"
},
{
"vote": 2,
"title": "Generate Boolean query from labeled text?",
"text": "Iâ€™m building a model that classifies short text (company descriptions). \n\n\nTo find new companies that match a model I need to write queries to a company api and they support keyword Boolean search like â€œdescriptions contains (tech OR IT) AND (SaaS OR software) NOT (agency OR nonprofit)â€.\n\n\nAre there any strategies for taking the labeled examples to generate the keywords needed to find new examples? \n\n\nThe keyword searches are expected to be ~65-80% as accurate as the model but more meant to write as efficient as possible queries to the company api.",
"date": "2020-06-20"
},
{
"vote": 10,
"title": "X English WordNet (3) : applications",
"text": "&#x200B;\n\n\nXtended English WordNet\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nIn the wake of the recent English WordNet 2020, and following the recent announcement of XEnglish WordNet data resources and libraries, this is to announce the release of \napplication resources\n related to Xtended English WordNet.\n\n\n&#x200B;\n\n\nEach WordNet query produces a tree (with synsets the word is a member of near the trunk and distant synsets as branches connected to the trunk by relations, etc). These trees can be represented by graphic or collapsible trees. These apps are meant to represent such trees.\n\n\n&#x200B;\n\n\nGRAPHIC  RENDERING\n\n\nTreebolic WordNet\n\n\n\n\nTreebolic Wordnet (Swing framework for the desktop)\n\n\nTreebolic WordNet (Android for the mobile)\n\n\n\n\nTreebolic WordNet\n gives a hyperbolic rendering of WordNet query results in which display space is subject to a particular curvature : more space is allocated to the focus node while the parent and children, still in the immediate visual context, appear slightly smaller. The grandparents and grandchildren are still visible but come out even smaller. As we move away from the focus node, less display space is allotted to the nodes, which gradually disappear towards the disk's border, as though the whole hierarchy were seen through a fisheye lens. Dragging word nodes allows one to browse through hyperbolic space by moving the focus.\n\n\nScreencasts:\n \n\n\nhttps://www.youtube.com/watch?v=p_v0NZvZNm4\n\n\nhttps://www.youtube.com/watch?v=g2sPm2_nfoU\n\n\nFor the desktop:\n\n\nhttp://treebolicwordnet.sourceforge.net/\n\n\nFor the mobile:\n\n\nhttps://play.google.com/store/apps/details?id=org.treebolic.wordnet.browser.iab&hl=en\n  (free trial)\n\n\nhttps://play.google.com/store/apps/details?id=org.treebolic.wordnet.browser.premium&hl=en\n (premium)\n\n\nhttps://play.google.com/store/apps/details?id=org.treebolic.wordnet.browser.google&hl=en\n (with ads)\n\n\n&#x200B;\n\n\nCOLLAPSIBLE TEXT TREE\n\n\nSemantikos\n\n\n&#x200B;\n\n\n\n\nSemantikos (Android)\n\n\nSemantikos-EWN (Android)\n\n\nSemantikos-WN (Android)\n\n\n\n\n&#x200B;\n\n\nSemantikos\n explores senses and sense relations of English. It may be used like a dictionary but the app is geared towards the exploration of senses, semantic and lexical relations, as well as semantic roles and collocations in some versions. The full version includes (English) WordNet, VerbNet, PropBank, FrameNet and Predicate Matrix.\n\n\nThese apps are based either on Princeton WN or EWN.  In the latter case, an alternative dataset is downloadable that uses EWN as the core.\n\n\nhttps://play.google.com/store/apps/details?id=org.sqlunet.browser.ewn&hl=en\n\n\nhttps://play.google.com/store/apps/details?id=org.sqlunet.browser.wn&hl=en\n\n\nhttps://play.google.com/store/apps/details?id=org.sqlunet.browser&hl=en\n\n\n&#x200B;\n\n\n(This message has been/will be posted to multiple lists. Sorry for the inconvenience if you get the same message more than once.)",
"date": "2020-06-20"
},
{
"vote": 2,
"title": "How can I approach seperation of multiple documents within a single pdf with NLP?",
"text": "I have a bunch of labelled documents belonging to 20 different categories (different types of reports, forms etc). These documents can range anywhere from 5-100 pages. I am trying to train a supervised learning model that will seperate large pdf files (10k+ pages) into its composite documents(the files usually contain 500+ different documents) How could I go about doing this? Are there known methods for solving this problem? The challenge has been having the model cluster/group pages that belong to a single document.\n\n\nThanks in advance",
"date": "2020-06-20"
},
{
"vote": 1,
"title": "Does BookCorpus include all books published before 1900?",
"text": null,
"date": "2020-06-20"
},
{
"vote": 13,
"title": "NLP Theoretical",
"text": "I am looking for resources which covers topics of NLP in just theory, Like just the working behind and not the coding.\n\n\nAdditionally, also looking for current trends in this field.\n\n\nI am a beginner, with some basic python language with engineering background.",
"date": "2020-06-19"
},
{
"vote": 2,
"title": "Q: NLP techniques to extract meaning/context from quantification phrases",
"text": "[deleted]",
"date": "2020-06-19"
},
{
"vote": 7,
"title": "How can I leverage NER for topic modeling?",
"text": "[deleted]",
"date": "2020-06-19"
},
{
"vote": 1,
"title": "Clustering question",
"text": "Imagine a person had tags for themselves and associated likeliness/strength.\n\n\nTags are from a finite pool\n\n\nEx:\n\n\nPerson A - [&#039;Joyful&#039;: 54, &#039;Athletic&#039;: 91, &#039;Studios&#039;: 59] \nPerson B - [&#039;Skinny&#039;: 88, &#039;Bold&#039;: 41]\n .\n .\n . \nPerson n = [tag1: rate1, tag2: rate2, ...] \n\n\n\nSo goal is to classify each person into some category. For ex: \"Brave\", \"Intelligent\", \"Lazy\", etc..\n\n\nLet's say our finite number of tags are 10 and classification of groups is 4. This means that each person will only have tags that are part of the pool, and some combination of tags and strength make classify them into a category.\n\n\nThis isn't much of an NLP problem, but more of a clustering/classification problem.\n\n\nMy thoughts were to convert each person into a 1hot vector for tag with normalized strength. However, if the number of tags grow, the matrix becomes huge. The upper bound of tags are in 10000s. Even this approach doesn't always work for some reason. Would appreciate any thoughts on this or other potential ideas/solution you may have!\n\n\nHave been thinking about using hashing somehow so i make a fixed length vector to feed in a clustering algorithm, but I'm having a difficult time coming up with that. Any thoughts?\n\n\nI appreciate your help.",
"date": "2020-06-19"
},
{
"vote": 11,
"title": "Maximize topic modeling coherence score?",
"text": "I have been using LDA to model topics based on a corpus of short texts, slightly longer than tweets.  Unfortunately, the topic keywords returned aren't as coherent as I'd like them to be, even when adjusting the number of topics to maximize the coherence score.  Are there superior algorithms available to return more coherent topics?  Thanks!",
"date": "2020-06-18"
},
{
"vote": 2,
"title": "I Need Preprocessing Recommendations: Translator/Spellcheck (Python)",
"text": "I know nothing about translating text, especially text that's potentially misspelled. What's your go method of processing text from:\n\n\n\n\nBulgarian\n\n\nChinese (Simplified)\n\n\nChinese (Traditional)\n\n\nCzech\n\n\nDanish\n\n\nDutch\n\n\nFinnish\n\n\nFrench\n\n\nGerman\n\n\nGreek\n\n\nHungarian\n\n\nItalian\n\n\nJapanese\n\n\nKorean\n\n\nNorwegian\n\n\nPolish\n\n\nPortuguese\n\n\nPortuguese-Brazil\n\n\nRomanian\n\n\nRussian\n\n\nSpanish-Spain\n\n\nSpanish-Latin America\n\n\nSwedish\n\n\nThai\n\n\nTurkish\n\n\nUkrainian\n\n\nVietnamese\n\n\n\n\ninto English?",
"date": "2020-06-18"
},
{
"vote": 1,
"title": "relation of NLP with EE",
"text": "[deleted]",
"date": "2020-06-18"
},
{
"vote": 22,
"title": "Could someone please suggest project-based learning materials for NLP (online courses, video tutorials etc.)? I started studying the classic books like Natural Language Processing with Python and Speech and Language Processing, which are fine but mostly revolve around the theory.",
"text": null,
"date": "2020-06-18"
},
{
"vote": 5,
"title": "Convert sequence of word embeddings to fixed length sentence embeddings.",
"text": "I am working on a project that involves evaluation of word embeddings using semantic textual similarity task. I want to translate my word embeddings to sentence embeddings(fixed length) for giving it as input to model. Is there any tool or implementation that can do it in a  quick way?",
"date": "2020-06-18"
},
{
"vote": 1,
"title": "[OPEN-SOURCE] We are open sourcing NLP multitask learning toolkit",
"text": "[removed]",
"date": "2020-06-17"
},
{
"vote": 1,
"title": "Dataset of Annotated Political Headlines",
"text": "[removed]",
"date": "2020-06-17"
},
{
"vote": 2,
"title": "Topic Modelling using DNNs",
"text": "Hi. I was looking for unsupervised deep learning models for a Topic Modelling task I currently have. I am trying to draw comparison between different set of keywords each generate, and try to find what the difference usually mean. I have tried LDA, YAKE, the pke module in python but I could not find any deep learning models. Are there any ways to do topic modelling via DNNs?",
"date": "2020-06-17"
},
{
"vote": 3,
"title": "Are these projects worth it?",
"text": "[deleted]",
"date": "2020-06-17"
},
{
"vote": 2,
"title": "Twitter Flu Datasets",
"text": "Is anyone aware of large Twitter Flu datasets from the last decade or so?",
"date": "2020-06-17"
},
{
"vote": 9,
"title": "Why are some pickled NLP models so large?!",
"text": "I wrote a short blog post on why some simple text classifiers take so much disk space when pickled although the number of features used in them barely goes above 100k. The culprit is the CountVectorizer of sklearn which is usually used in the pipeline! Please checkout the description of the problem and proposed solution:\n\n\nhttps://hminooei.github.io/2020/06/16/pickle.html\n\n\nComments are appreciated.",
"date": "2020-06-16"
},
{
"vote": 1,
"title": "How to generate proper sentences from a running stream of text from a voice transcription?",
"text": "[deleted]",
"date": "2020-06-16"
},
{
"vote": 2,
"title": "Confused about HuggingFaces Transformers Library regarding SOTA dependency parsing",
"text": "Hello, \n\n\nI am a little confused about wheteher it is a feature of the Hugging Face Transformers library to be able to do dependency parsing and relation extraction. \n\n\n&#x200B;\n\n\nCurrently I have been working on a project to do dependency parsing and some rule based relation extraction in spacy and an well aware of its API and models. But it seems that I am a little bit behind on the state of the art for NER and dependency parsing. Is this information (dependency graph info and NER) not available through BERT and the huggingFaces Transformers library? It seems that so much more advanced functions are part of its feature set that I kind of assumed it would be there. \n\n\n&#x200B;\n\n\nIf this is not the case, what would currently be the best state of the art for NER and dependency parsing that is not Spacy?",
"date": "2020-06-15"
},
{
"vote": 1,
"title": "Abstractive text summarization with BERT embeddings and Bidirectional LSTM - model.fit()",
"text": "Hello,\n\n\nI am quite new to NLP and Keras. I am working on a Abstractive text summarisation tool, with BERT as such:\n\n\n1 - Text data\n\n\n2 - Tokenize using BERT tokenizer (hugging face)\n\n\n3 - Pad input text, input summary and respective attention masks\n\n\n4 - Create corresponding tensors, x, y, att_x and att_y\n\n\n5 - Feed data to a encoder - decoder model with BERT embedding layer (huggingface)\n\n\nI feed to my model the input data =x, y, att_x and att_y and as output the embedded y summaries  (last_hidden_states_y) as such:\n\n\nmodel.fit([input_ids_x, attention_mask_x, input_ids_y, attention_mask_y], last_hidden_states_y, batch_size = 10, epochs=50, verbose=1)\n\n\n\nIs that correct? Or should I use the tokenised summaries as output data Y? I would assume that the output of my decoder is in itself an array of embedded words, but I am not sure. Can someone help me or maybe explain to me what I should expect my decoder's output to be like? I am not sure what my X and Y in \nmodel.fit\n() .\n\n\n&#x200B;\n\n\nIn case it is needed:\n\n\nThis is the simplified model:\n\n\ndef bert_embedding(input_ids, attention_mask):\n  output_model = model(input_ids, attention_mask=attention_mask)\n  last_hidden_states = output_model[0]\n \n return last_hidden_states\n\n#Model\n\n#ENCODER\nencoder_inputs = Input(max_length_x, dtype = &#039;int32&#039; , name = &#039;input_ids_encoder&#039;) #input_ids text\nencoder_attention_mask = Input(max_length_x, dtype = &#039;int32&#039;, name = &#039;attention_ids_encoder&#039;) #mask text\n\n#embedding layer\nenc_emb =  bert_embedding(encoder_inputs, encoder_attention_mask)\n\n\nencoder_lstm = tf.keras.layers.Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\nencoder_outputs, state_h, state_c, *_= encoder_lstm(enc_emb)\n\n#DECODER\ndecoder_inputs = Input(shape = (max_length_y,), dtype = &#039;int32&#039;, name = &#039;input_ids_decoder&#039;)\ndecoder_attention_mask = Input(shape = (max_length_y,), dtype = &#039;int32&#039;, name = &#039;attention_ids_decoder&#039;) \n\ndec_emb = bert_embedding(decoder_inputs, decoder_attention_mask)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n\n# ATTENTION LAYER\nattn_layer = AttentionLayer(name=&#039;attention_layer&#039;)\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\ndecoder_concat_input = Concatenate(axis=-1, name=&#039;concat_layer&#039;)([decoder_outputs, attn_out])\n\n#DENSE LAYER\ndecoder_dense =  TimeDistributed(Dense(768, activation=&#039;sigmoid&#039;))\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\nmodel = Model(inputs = [encoder_inputs, encoder_attention_mask, decoder_inputs, decoder_attention_mask], outputs = decoder_outputs) \nmodel.summary()\n\nhistory = model.fit([input_ids_x, attention_mask_x, input_ids_y, attention_mask_y], last_hidden_states_y, batch_size = 10, epochs=50, verbose=1)\n\n\n\n&#x200B;\n\n\nThank you!",
"date": "2020-06-15"
},
{
"vote": 1,
"title": "X English WordNet Libraries",
"text": "&#x200B;\n\n\nXEWN libraries\n\n\nIn the wake of the recent \nEnglish WordNet 2020\n, and following the recent announcement of \nXEnglish WordNet\n data resources, here comes the release of \nlibrary resources\n for X-tended English WordNet.\n\n\n1 - TO BE USED WITH WNDB\n\n\nMIT's \nJava WordNet Interface\n has been extended to accept extensions to the WNDB format (roughly: accept 'is_caused_by' and 'is_entailed_by' relation extensions , ease the requirement that lexids be in the 0..15 range, and have extended configuration options to support multiple indexes and user-defined comparators). \n\n\nCode has been ported to Java8+. \n\n\nExtensive examples are to be found in the test sources. \n\n\nThe library is published on \nMaven Central\n under the '\nx-englishwordnet\n' group id.\n\n\nhttps://x-englishwordnet.github.io/jwi\n\n\n2 - NLTK\n\n\nNLTK python library has been tested to work with the WNDB files and can be used unmodified, but it does not bind to some areas of (X)EWN (or PWN for that matter).\n\n\n3 - TO BE USED WITH XEWN XML\n\n\nXSD schemas defined to validate data have also been put to good use to automatically generate Java objects (Pojos, Plain Old Java Objects) that interact with the XML directly, which means you have ready-made Synsets, Words, Senses and suchlike objects which represent an abstraction layer away from the XML.\n\n\nThese come in two flavours : \nApache's XML Beans\n and \nJakarta's JAXB\n (Java Architecture for XML Binding). Apache's XML Beans support XPath querying.\n\n\nExamples are to be found in the test sources. The libraries are published on \nMaven Central\n under the '\nx-englishwordnet\n' group id. \n\n\nFor compatibility reasons, sister libraries have been generated that deal with EWN data.\n\n\nhttps://x-englishwordnet.github.io/xewn-beans\n\n\nhttps://x-englishwordnet.github.io/xewn-jaxb\n\n\nhttps://x-englishwordnet.github.io/ewn-beans\n\n\nhttps://x-englishwordnet.github.io/ewn-jaxb\n\n\n&#x200B;\n\n\nAll Maven libraries:\n\n\nhttps://search.maven.org/search?q=x-englishwordnet",
"date": "2020-06-15"
},
{
"vote": 2,
"title": "Is English modeled as a deterministic CFL or a CFL?",
"text": "Books on linguistics and NLP often mention that English is modeled by context free grammars, but also is parsed by LR(k) parsers.\n\n\nLR(k) parsers are for parsing deterministic CFLs, while CFLs are nondeterministic.\n\n\nIs English largely a CFL or deterministic CFL?\n\n\nOr what aspects of English is modeled as a CFL, and what aspect of English is modeled as a deterministic CFL?\n\n\nThanks.",
"date": "2020-06-14"
},
{
"vote": 1,
"title": "NLP Paper recommendations",
"text": "Hey! I wanted some suggestions as to which papers to implement for NLP tasks . I have implemented Word2Vec and Get to The Point Summarization with Pointer Generator Networks. I have also tried my hand at Attention is all you need  ( transformer ) , but it was so huge that the training time was very lage . So can anyone suggest me any good papers , which are implementable through Google Colab since I dont have a GPU....",
"date": "2020-06-14"
},
{
"vote": 1,
"title": "Deployment of transformers",
"text": "I am trying to deploy a project based on keyword detection and summarization. For this, I've used transformer. I've only deployed models on binder before as it is a free service. Will binder be able to handle it? I've seen binder to be extremely slow even for resnet18 models, should I go ahead with the deployment? If not then is there any free service where I can deploy the model?",
"date": "2020-06-14"
},
{
"vote": 2,
"title": "A starting point for text-based analytics?",
"text": "I am a beginner in text-based analytics. Any resource where I can start?",
"date": "2020-06-14"
},
{
"vote": 1,
"title": "Discord group for Cross-lingual, Machine Translation, Word Embeddings discussions",
"text": "[removed]",
"date": "2020-06-14"
},
{
"vote": 1,
"title": "NLP Query",
"text": "I was going through online resources for NLP, found sentiment analysis as one of the use cases. \nBut not able to find a comprehensive guide of what all is covered by sentiment analysis? Is it just positive/negative, happy neutral and negative classification of data?",
"date": "2020-06-14"
},
{
"vote": 2,
"title": "How to split text into sentences per subject?",
"text": "I'm new to NLP and have been playing with TextBlob and NLTK. \n\n\nI'd like to split text into meaningful sentences. Been searching for hours on a way to do this but couldn't find anything. Only get articles on how to split based on dots or newlines :/\n\n\nI guess the best way is to give an example of what I'd like to do:\n\n\ntext = &quot;today was bad but it could have been worse&quot;\nres = some_powerful_magic(text)\n# res =&gt; [&quot;today was bad but it could have been worse&quot;]\n\ntext = &quot;today was bad but tomorrow will be good&quot;\nres = some_powerful_magic(text)\n# res =&gt; [&quot;today was bad&quot;, &quot;but tomorrow will be good&quot;]\n# today is one thing and tomorrow is another so it splits into two sentences\n\ntext = &quot;i like apples and oranges too&quot;\nres = some_powerful_magic(text)\n# res =&gt; [&quot;i like apples&quot;, &quot;i like oranges too&quot;]\n# this one is farfetched but something close to it would be nice\n\n\n\nIt would be simple to come up with a solution for this particular case but that would miss the point. I need something that is generic.",
"date": "2020-06-14"
},
{
"vote": 1,
"title": "Day 166 of #NLP365 â€“ NLP Papers Summary â€“ Publicly Available Clinical BERT Embeddings",
"text": "Day 166.\n\n\nAlthough I won't be doing NLP Papers summarisation everyday, I will still continue to cover some papers every now and then. Today's paper is \"Publicly Available Clinical BERT Embeddings\", covering ClinicalBert. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/14/day-166-nlp-papers-summary-publicly-available-clinical-bert-embeddings/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-14"
},
{
"vote": 3,
"title": "Natural language processing and discourse analysis",
"text": "I'm new to natural language processing and please understand me if my question is too naive. I am wondering if there is an algorithm for detecting whether an essay has specific points. For example, in the introduction, check if the essay mentions motivations. Or, check if each paragraph mentions the main sentence? Can this be only done with discourse analysis?",
"date": "2020-06-14"
},
{
"vote": 17,
"title": "Day 165 of #NLP365 â€“ Learn NLP With Me â€“ Fast.Ai NLP Course â€“ ULMFit For Non-English Languages",
"text": "Day 165.\n\n\nToday's post covers another lecture of the \nfast.ai\n course. It consists of how to train a language model from scratch, what is BPE and why is it useful etc. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/13/day-165-learn-nlp-with-me-fast-ai-nlp-course-ulmfit-for-non-english-languages/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-13"
},
{
"vote": 0,
"title": "Automating the process of identifying the preferred representational system in Neuro Linguistic Programming using Natural Language Processing",
"text": null,
"date": "2020-06-13"
},
{
"vote": 5,
"title": "Dravidian-CodeMix - FIRE 2020 Shared Task on Sentiment Analysis for Under-resourced Languages",
"text": "Hello there,\n\n\nI suppose many of you are multi-lingual speakers and tend to borrow words from other languages while communicating. This phenomenon of codemixing is commonplace but remains less explored in the NLP community.\n\n\nTo address this elephant in the room, we created a dataset of Tamil-English and Malayalam-English dataset for sentiment analysis. I am excited to share our research with you all and would love you guys to solve this problem with us by participating in \nDravidian-CodeMix Shared Task\n.  \n\n\nFor more information visit \nhttps://dravidian-codemix.github.io/2020/\n\n\nLooking forward to your participation!\n\n\nKey Dates:\n\n\n\n\nRelease of Trial data: 10 June\n\n\nRelease of Training data: 10 June\n\n\nRelease of Test data: 1 August\n\n\nRun submission deadline: 20 August\n\n\nResults declared: 31 August\n\n\nPaper submission: 20 September\n\n\nRevised paper: 30 October.\n\n\n10th-13th December - FIRE 2020",
"date": "2020-06-13"
},
{
"vote": 1,
"title": "How to test OCR quality?",
"text": "[deleted]",
"date": "2020-06-13"
},
{
"vote": 12,
"title": "Fantastic bootcamp/introduction to low-resource NLP by CMU Language Technologies Institute",
"text": null,
"date": "2020-06-13"
},
{
"vote": 2,
"title": "ULMFit for Machine Translaion",
"text": "[deleted]",
"date": "2020-06-13"
},
{
"vote": 36,
"title": "ðŸ¤–Mozilla Common Voice will release the biggest dataset under public domain in July and needs your help before June 22th!",
"text": null,
"date": "2020-06-12"
},
{
"vote": 0,
"title": "Recent advances in the field of NLP",
"text": "[deleted]",
"date": "2020-06-12"
},
{
"vote": 4,
"title": "Stuck at some weird problem in NMT",
"text": "I was trying to implement machine translation using LSTM seq2seq model but after trying all the tweaking, changing the datasets,I still haven't been able to successfully train a model\nIf I reduce the data,it is straight forward overfitting from 3rd -4th epoch\nIf I increase the data then the training is just too slow,I thought that maybe network isn't big enough to learn anything so I made it. A huge one with triple layers both in encoder and decoder but still the train accuracy just doesn't increase past 62-63%\nPlease can anybody who has worked on seq2seq or NMT help?\nI am using keras\nIf you want further details about my method,I can share my colab notebook",
"date": "2020-06-12"
},
{
"vote": 2,
"title": "John Snow Labs Spark-NLP 2.5.2: New Language Detection annotator, enhancements, and bug fixes",
"text": null,
"date": "2020-06-12"
},
{
"vote": 10,
"title": "NLP Methodology",
"text": "Hi all, hope everyone is doing well. First time posting here, so please let me know if something is off. \n\n\nI've tinkered with data science for quite some time and encountered methodologies such as \nSEMMA\n or \nCRISP-DM\n for data mining projects. However, I'm curious as to whether NLP-specific methodologies exist. My Google-Fu doesn't seem to reveal anything of the sort. I was hoping to pick your guys' brains on this. Do you know of anything relevant here?\n\n\nAnd if NLP-specific methodologies doesn't exist, why is that? Is the field simply to broad for it? \n\n\nSide note: Working on an NLP project, I was asked to describe the \"conceptual framework\", which apparently means \"concepts of relevance to research problem data analytics methods and techniques\". What the frick does that really mean?  \n\n\nThanks for reading!",
"date": "2020-06-12"
},
{
"vote": 4,
"title": "Best tech to understand social media conversations",
"text": "Hey all,\n\n\nI work in social media research  - of which a large part is text analysis to understand \nhow\n people are talking about brands, media, entertainment etc. There are a number of social media research tools out there (Brandwatch, Crimson, Netbase, Sysomos, Talkwalker etc...) that help guide this type of research but through my experience of working with them for 10yrs + they all fall very short when it comes to topic / theme analysis.\n\n\nCurious - what do you all believe to be the most advanced/ground-breaking technology & tools out there that does this well today?\n\n\nI'm thinking beyond \nsentiment\n (positive, negative) or \nentity\n (car, person, organization) detection. For example: if it's text about a car model - I'm trying to unpack if it's about the design, reliability, price, performance, nostalgia etc.... If it's text about a TV Show - is it about the plot, the actors, surprise twists, the 'world' etc....",
"date": "2020-06-11"
},
{
"vote": 1,
"title": "[R] A Look at Recent Advances in Google Translate",
"text": "[removed]",
"date": "2020-06-11"
},
{
"vote": 1,
"title": "Keras LSTM POS tagging script that uses GloVe embeddings?",
"text": "I found a really helpful Keras tutorial for POS tagging and it achieves pretty decent accuracy. Unfortunately, it does not use pre-trained word embeddings, which is something that I'd like it to do.\n\n\nhttps://nlpforhackers.io/lstm-pos-tagger-keras/\n\n\nDoes anyone have a boilerplate script that (A) uses an LSTM architecture, (B) performs a POS task, and (C) uses GloVe embeddings?",
"date": "2020-06-10"
},
{
"vote": 11,
"title": "Language technology map",
"text": "Hi all, there is a new language technology map already available on Nimdzi's website! Promos are coming tomorrow, but for the Redditers out there, you may be the ones to know before the rest ;)\n\n\n(higher res available on site)\n\n\n \nhttps://www.nimdzi.com/nimdzi-language-technology-atlas-2020/",
"date": "2020-06-09"
},
{
"vote": 2,
"title": "How realistic is publishing a corpus in the ACL conference?",
"text": "I do not specifically work in the field of NLP, but rather an interdisciplinary field of NLP and healthcare.\n\n\nMost of the conferences that we publish are healthcare-related.\n\n\nCurrently, we plan to develop a corpus that we would like to publish in a good conference. ACL came to my mind. However, I am not sure if publishing a corpus in ACL is a thing nowadays.\n\n\nSo I did a bit of research and found that ACL in 2018 published 2 papers about corpora, in 2017 published 2 papers about corpora again. The trend from 2015-2018 sees a reduction in corpora related papers. Is it still wise to submit a corpus-related paper to ACL2021?\n\n\nDoes ACL publish a corpus?\n\n\n\n\nA corpus of non-native written English annotated for Metaphor 2018\n\n\nAn annotated corpus for Machine reading of instructions in wet lab protocols 2018\n\n\nA Corpus of Compositional Language for Visual Reasoning. 2017\n\n\nA corpus of annotated revisions for studying argumentative writing 2017\n\n\nAbout 7 papers with corpus published 2016\n\n\nAbout 9 papers with corpus published 2015",
"date": "2020-06-09"
},
{
"vote": 2,
"title": "[R] DeepMind Introduces â€˜EATSâ€™ â€“ An Adversarial, End-to-End Approach to TTS",
"text": "[removed]",
"date": "2020-06-09"
},
{
"vote": 5,
"title": "How to apply for research internships for NLP?",
"text": "I am a CSE Sophomore from K.J.S.C.E , Mumbai. I am very much interested in NLP/DL and want to try my hands around with a research based projects/research internship . I have done few projects which are over here : \nwww.github.com/talha1503\n . Can anyone please suggest me where to apply and how to reach out to professors?",
"date": "2020-06-09"
},
{
"vote": 3,
"title": "Topic for Research paper (bachelor's)",
"text": "I am in my last semester of Linguistics bachelor's. I am an average student, who needs to start working on their bachelor's thesis.\n\n\nSome time ago I came to know about the field  Computational Linguistics, and it really fascinated me. It seemed like an interesting field. Watched some short YouTube videos and read 2/3 articles about it. Liked the field, so I plan on doing my master's in Computational Linguistics. \nBut before master's I want my research paper to be about something in Computational Linguistics.\n Unfortunately I don't know anything about the field. My bachelor's degree did not have any CL component. So I need your help with choosing a topic for my paper.\n\n\nThe topic should be for someone with zero knowledge about the field",
"date": "2020-06-09"
},
{
"vote": 9,
"title": "Day 161 of #NLP365 â€“ NLP Papers Summary â€“ BLEURT: Learning Robust Metrics For Text Generation",
"text": "Day 161.\n\n\nToday's post is a 6-minute summary of the NLP paper \"BLEURT: Learning Robust Metrics For Text Generation\".\n\n\nToday's paper is on the new generation metric BLEURT. This is the last papers summary (for now) as I am taking a step back to evaluate all the papers I have read as well as further developing my NLP implementations skill. From Day 102 - Day 161, we have been on this wild papers summary journey. I will do more papers here and there in the future whenever I come across interesting papers. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/09/day-161-nlp-papers-summary-bleurt-learning-robust-metrics-for-text-generation/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-09"
},
{
"vote": 18,
"title": "Is there some canonical textbook in nlp areas?i",
"text": "I am finding some Natural language processing textbooks.",
"date": "2020-06-09"
},
{
"vote": 1,
"title": "Whatâ€™s the best technique for next word prediction?",
"text": "Iâ€™m trying to educate myself in this area to build up or possible use a library to build a next word prediction for Arabic language. I feel lost because most of the work I see is either two years old or just a tutorial to show you how to build some basic algorithms.\n\n\nMy questions are, whatâ€™s the best technique for next word prediction? Any available libraries or open source projects that I could use? Any interesting reads?",
"date": "2020-06-09"
},
{
"vote": 1,
"title": "How does document context change accuracy of a personality insight APIs (like Watson)?",
"text": "I'm new to all this, so please pardon my uninformed question, my research isn't helping me with great answers.\n\n\nWatson's Personality Insight API delivers scores based on the provided text for a number of personality traits. The more text in the input, the more accurate the output. So here's a use case (theoretical?). We are a business and we have twitter accounts for some of our customers. For each customer, we aggregate the text of all of their tweets and then get personality insight scores for them (if minimum content requirements are met). We now use this data to better customize our communications with those customers.\n\n\nNow take those personality insight scores of all aggregated tweets. What does that mean?\n\n\nNow let's say I run a sentiment analysis on all tweets first and order them from saddest, to neural, to happiest. Now let's say I run only the happiest 20% of tweets and get personality insights scores for those.\n\n\nNow the same exercise for the saddest 20% of tweets.\n\n\nI'm trying to wrap my head around what the difference in these results mean and how they could be interpreted. Are my insights now segmented into personality trait scores for this person when happiest or saddest? How do I go about interpreting such results?\n\n\nThanks!!",
"date": "2020-06-08"
},
{
"vote": 1,
"title": "Training a classifier with text features",
"text": "Hello everyone,\n\n\nI would like to learn more about training a classifier using text features. For example, I would like to extract POS, and Named entities from documents, and use them in combination with other labeled data as training input to a classifier. \n\n\nCan anyone provide good articles, examples or similair information regarding this topic?\n\n\nThanks for the help!",
"date": "2020-06-08"
},
{
"vote": 1,
"title": "Day 160 of #NLP365 â€“ NLP Papers Summary â€“ Extractive Summarization As Text Matching",
"text": "Day 160.\n\n\nToday's post is a 6-minute summary of the NLP paper \"Extractive Summarization As Text Matching\".\n\n\nToday's paper tackles extractive summarisation from a new angle text matching. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/08/day-160-nlp-papers-summary-extractive-summarization-as-text-matching/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-08"
},
{
"vote": 12,
"title": "How to further pre-train BERT or RoBERTa model using own corpus?",
"text": "Recently I have seen many people further pre-training BERT or RoBERTa models with task or domain-specific corpus.  For example,  \nClinicalCovidBERT\n and \nBioCovidBERT (\n \nhttps://github.com/manueltonneau/covid-berts\n ). These models are initialized using ClinicalBERT & BioBERT respectively and further pre-trained on Kaggle  CORD-19 dataset. I would like to know the following details\n\n\na) Where can I find the code to further pre-train models like BERT or RoBERTa on own corpus?\n\n\nb) Can I use hugging face transformers library for this? If so, how can I?\n\n\nThanks in advance..................",
"date": "2020-06-08"
},
{
"vote": 4,
"title": "Zero-shot Text Classification With GPT-2",
"text": "Hi everyone,\n\n\nI've written a summary of a paper that trains GPT-2 on a new pre-training task for zero-shot learning. Please check it out and share your feedback.\n\n\nArticle: \nhttps://amitness.com/2020/06/zero-shot-classification-via-generation/",
"date": "2020-06-08"
},
{
"vote": 3,
"title": "Getting Triples from Lengthy Sentences",
"text": "Hi,\n\n\nI'm working on a project, a component of which is getting triples (subject->relation->object). I am using the \nStanford OpenIE library\n for this purpose, which works fine for short-medium length sentences, but does not give satisfactory result with lengthier sentences with multiple triples such as follows:\n\n\nSentence:\n\n\n\"It has been stated that item is mainly a part or a document but it is not necessary since items can have other subtypes as well other than just parts or documents as per new updates in agile plm.\"\n\n\nResults:\n\n\n\n\n{'subject': 'new updates', 'relation': 'is in', 'object': 'agile plm'}\n\n\n{'subject': 'items', 'relation': 'can have', 'object': 'subtypes'}\n\n\n{'subject': 'items', 'relation': 'can have', 'object': 'other subtypes'}\n\n\n\n\nExpected results:\n\n\n\n\nPrevious results\n\n\n{'subject': 'item', 'relation': 'is', 'object': 'mainly part'}\n\n\n{'subject': 'item', 'relation': 'is', 'object': 'part'}\n\n\n\n\n------------------------------------------------\n\n\nSimilarly,\n\n\nSentence:\n\n\n\"In this article will explain how to speed up your order management process to ensure orders are delivered to customers on time and that you never are faced with issues with low inventory ever again.\"\n\n\nResults:\n\n\n\n\n{'subject': 'issues', 'relation': 'is with', 'object': 'low inventory'}\n\n\n\n\nExpected results:\n\n\n\n\nPrevious results\n\n\n{'subject': 'orders', 'relation': 'are delivered on', 'object': 'time'}\n\n\n{'subject': 'orders', 'relation': 'are', 'object': 'delivered to customers on time'}\n\n\n{'subject': 'orders', 'relation': 'are', 'object': 'delivered'}\n\n\n{'subject': 'orders', 'relation': 'are delivered to', 'object': 'customers'}\n\n\n\n\n------------------------------------------------\n\n\nI have noticed that I can get the desired results if I break the sentence into chunks (\"item is mainly a part or a document\" and \"orders are delivered to customers on time.\") and send it to the library. I have tried checking out spaCy and other libraries but haven't found something that could break the sentences into suitable chunks for my desired results. Is anyone aware of any regex rules or a library that can do chunking of sentences moderately well for my use-case?",
"date": "2020-06-08"
},
{
"vote": 2,
"title": "Connect over virtual rooms with 3-4 like-minded buddies to build DL/NLP projects",
"text": "Not sure if anyone shares this pain point. Iâ€™ve always been keen to form a study group with like-minded peers to learn and bounce ideas, specifically in Machine Learning/ Deep Learning: Computer Vision or Natural Language Processing domain. Unfortunately, most of the study groups that Iâ€™ve been joining ended up drifted away because of the weak interaction. People drop out because they don't get the value they seek, they can't contribute, and they would never have that personalized experience from those generic discussions. On top of that, if there is little or no touchpoint during the meetup, you can just watch Youtube videos.\n\n\nI am organizing plenty of micro-scale study groups that capped at 4-5 members to heal my pains and help folks who might feel the same. I have been receiving 70+ interests from the community and successfully connect 20+ groups who share similar goals, proficiency, and commitment level.\n\n\nIf this sounds interesting to you, we welcome you to be a part of our 70+ active members community (\nhttps://www.boringppl.com/\n)",
"date": "2020-06-08"
},
{
"vote": 10,
"title": "Doc2vec SOTA?",
"text": "What is the state of the art in document embeddings (preferably with accompanying packages or code / implementation we can use but not necessary)",
"date": "2020-06-07"
},
{
"vote": 3,
"title": "Birthdate information extraction",
"text": "May I know how could I extract information from text / article / story as birth date birth location related to specific person",
"date": "2020-06-07"
},
{
"vote": 2,
"title": "Techniques for Cleaning Outliers/Anomalies in Multiclass data?",
"text": "My job has about 50k records of text (think comments) that have been labeled by hand by various individuals over a couple years (before my time).\n\n\nPrevious analysts made some tfidf/svm model that currently kicks out predictions. I was trying to make a w2v/d2v model coupled with xgboost but was getting subpar results. I got curious and attempted to recreate the SVM model which has awful metrics with the current training data. \n\n\nI went through a couple records manually and I figure 1/5 comments are mislabeled. I'm told there were some 'database problems' that caused this but... I have my doubts. Now, if this was traditional numerical data I'd be fine with outlier detection/removal but... I'm not really sure what the best practices are when it comes to NLP data like this.\n\n\nWhat I'm thinking is to go label by label and identity outliers/anomalies within the label. Any recommendations for techniques to implement? \n\n\nI tried some LDA and tfidf clustering but the results were sort of lackluster and I couldn't help but feel like there was a better way. I have some ideas around removing correlated document vectors, but again, really looking for best techniques. \n\n\nVideo/tutorial/paper/book/documentation recommendations are all welcome. \n\n\nThanks!",
"date": "2020-06-06"
},
{
"vote": 1,
"title": "Starting final year CS project in NLP with no experiece?",
"text": "Hello redditors of r/LanguageTechnology,\n\n\nI want to ask is it wise to start a CS student project or research in NLP with little prior experience in NLP? Keeping in mind that I have roughly 2-3 months to produce my project. If its wise and doable, is there any books or guides you recommend for a beginner? I am quite aware that NLP is broad topic but at this moment in time the specifics have not been given, as my project starts at a later date.",
"date": "2020-06-06"
},
{
"vote": 1,
"title": "Problems understanding the result of a NLP problem",
"text": "Hi there, this is my very first post here. My apologizes in advance if I do something wrong. I am reading the book \"machile learning for dummies\" and at some point in it is is thouhgt the concept of TF-IDF. Now I dn't like just believing results and I want to mathematically understand them. Here is the example of the book and how I tried to prove it mathematically. For some reason I can't get to the result and I am wondering what am I doing wrong:\n\n\n&#x200B;\n\n\n\n\nCorpus of 4 documents:\n\n\n\n\n['The quick brown fox jumps over the lazy dog',  'My dog is quick and can jump over fences',  'Your dog is so lazy that it sleeps all the day',  'A black dog just passed by but my dog is brown']\n\n\n&#x200B;\n\n\n\n\nThe books applies TF-IDF:\n\n\n\n\nTfidF = text.TfidfTransformer(norm='l1')\ntfidf = TfidF.fit_transform(vectorized_text)\nphrase = 0 # choose a number from 0 to 3\ntotal = 0\nfor word in vectorizer.vocabulary_:\n  pos = vectorizer.vocabulary_[word]\n  value = list(tfidf.toarray()[phrase])[pos]\n if value !=0:\n print('%10s: %0.3f' % (word, value))\ntotal += value\nprint('\\nSummed values of a phrase: %0.1f' % total)\n\n\n&#x200B;\n\n\n\n\nResult:\n\n\n\n\nthe: 0.217      quick: 0.109      brown: 0.109        fox: 0.138      jumps: 0.138       over: 0.109       lazy: 0.109        dog: 0.072\n\n\n&#x200B;\n\n\n\n\nNow I intend to mathematically get to that result but I just can't figure out what am I doing wrong:\n\n\n\n\nLet's analyse this. Term frequency TF (times a word/total words in sentence). This is normalizing and the sum must be 1:\n\n\n\n\nthe = 2/9 = 0,22\n\n\nquick = 1/9 = 0,11\n\n\nbrown = 1/9 = 0,11\n\n\nfox = 1/9 = 0,11\n\n\njumps = 1/9 = 0,11\n\n\nover = 1/9 = 0,11\n\n\nlazy = 1/9 = 0,11\n\n\ndog = 1/9 = 0,11\n\n\n\n\nNow we calculate DF (document frequency) which is the number of document where the word is present.\n\n\n\n\nthe = 2\n\n\nquick = 2\n\n\nbrown = 2\n\n\nfox = 1\n\n\njumps = 1\n\n\nover = 2\n\n\nlazy = 2\n\n\ndog = 4\n\n\n\n\n Inverse document frequency IDF measure the informativeness of the term.\n\n\n\n\nthe = log(4/2) = 0,301\n\n\nquick = log(4/2) = 0,301\n\n\nbrown = log(4/2) = 0,301\n\n\nfox = log(4/1) = 0,602\n\n\njumps = log(4/1) = 0,602\n\n\nover = log(4/2) = 0,301\n\n\nlazy = log(4/2) = 0,301\n\n\ndog = log(4/4+1) = 0,096\n\n\n\n\nFinally we multiply both values:\n\n\n\n\nthe = 0,22*0,301 = 0,066\n\n\nquick = 0,11*0,301 = 0,033\n\n\nbrown = 0,11*0,301 = 0,033\n\n\nfox = 0,11*0,602 = 0,066\n\n\njumps = 0,11*0,602 = 0,066\n\n\nover = 0,11*0,301 = 0,033\n\n\nlazy = 0,11*0,301 = 0,033\n\n\ndog = 0,11*0,096 = 0,011\n\n\n\n\nThank you my friends.",
"date": "2020-06-06"
},
{
"vote": 1,
"title": "Very expert in translating any languages",
"text": "[removed]",
"date": "2020-06-06"
},
{
"vote": 10,
"title": "Natural Language Processing Recipes: Best Practices and Examples - KDnuggets",
"text": null,
"date": "2020-06-06"
},
{
"vote": 7,
"title": "Determining the location of a cluster of news articles",
"text": "Hi,\n\n\nI have groups/clusters of news articles (between 3 and 10, typically) and  Iâ€™d need to figure out the location where the news took place (e.g. Los Angeles, US or London, England).\n\n\nIâ€™m not an expert but I think NER might be the key for this. Iâ€™d extract the named entities of type â€œlocationâ€ for the articles in each group and use some mechanism to get an approximation of the main â€œlocationâ€ entity.\n\n\nAny ideas would ne really appreciated, thanks in advance.",
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Does an adverb either modify verbs (not adjectives and adverbs), or modify adjectives and adverbs (not verbs)?",
"text": null,
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Similarity of underlying language between different classes",
"text": "[deleted]",
"date": "2020-06-05"
},
{
"vote": 1,
"title": "[Question] Trying to fit some theory in to implementation.",
"text": "All started by me trying to do dictation and actually feeling comfortable with the way the tools are working for me. So I took the problem seriously, and I started doing some readings and I think I got a rough idea of it needs to be done, on a high level.  \n\n\nMy questions lie in Voice to Text, where I seem to conclude to the following hypothesis, One can build a system consisting of two main components.\n\n\n\n\nA component for phoneme recognition, capable of classifying with enough accuracy all phonemes, that's spits out a reasonable probability map for the phoneme.\n\n\nA model that with a phoneme based dictionaries of the expected word (which of course is finite), to try to make sense out of the previous output in order to build the words making sense from a phoneme based point of view.\n\n\n\n\nSo the underling mechanism for the second model should be something that takes \nenough\n texts of things similar to what you would expect to detect.\n\n\nI've seen what \nlibraries\n like \nspaCy\n for python, can do, and it looks amazing, now, I think that If you took a similar model to make sense out of texts where the actual indexing occurs phoneme based. \n\n\nSo I want this \nthing\n to understand me, and I speak multiple languages, but it's still me it's the same phoneme, just the making sense of the words is different.\n\n\nSo from what I saw on documentation, I estimate that this could run a conventional computer (probably small too). \n\n\nAlso is it the Voice - Phoneme recognition actually a problem? I am really ignorant on implementations, but I think I have a solid grasp on the bases. What are the limitings here? I would guess that enough sampling exists, or is not that complicated to obtain.\n\n\nAnother Idea would be around selecting effectively sampling data for the models, that It's actually near to the expected \nuser\n.\n\n\nThere are some parts here and there where I have doubts, but I'll carry on, as I think by reading answers I'll better understand them.\n\n\nAny reference to literature or previous work would be useful.",
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Day 157 of #NLP365 â€“ NLP Papers Summary â€“ Explainable Prediction Of Medical Codes From Clinical Text",
"text": "Day 157.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Explainable Prediction Of Medical Codes From Clinical Text\".\n\n\nToday's paper uses modified CNN to encode clinical text to automate clinical coding with interpretability. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/05/day-157-nlp-papers-summary-explainable-prediction-of-medical-codes-from-clinical-text/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Need help for email datasets",
"text": "[deleted]",
"date": "2020-06-05"
},
{
"vote": 26,
"title": "The NLP Pandect - Most important NLP resources at a glance",
"text": null,
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Which languages use full-width numbers?",
"text": "Typically full-width numbers (\nï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™\n) are associated with CJK.\n\n\nHowever, there is Ainu in Japan, and many languages of greater China, not all Sinitic, some occasionally written in Perso-Arabic script.  Tibetan has its own numerals.\n\n\nIs there a complete list of languages for which full-width numbers are standard or frequent?\n\n\nAnd when if ever are they used in languages like English?",
"date": "2020-06-05"
},
{
"vote": 1,
"title": "Need help with classifying Indian addrrsses",
"text": "I have Indian addresses and their district labels and have to make a model which can identify the district based on address.\n\n\nI have earlier tried fasttext and fine tuning ELECTRA but they are not providing best of results which was obtained by vector embeddings. But vector embedding takes days to train and too heavy for the system.\n\n\nLet me give you an example of my data:-\n\n\nAddress :- \"I 32 mangol puri delhi\"\n\n\nLabel:-\"26-0\".\n\n\nHere \"delhi\" is the district name and \"mangol puri\" is the name of an area the input can also be \"mangolpuri\".\n\n\nI want to create a model which can be robust and predict the label even without the district name in address.\n\n\nThe problem here is the size of vocabulary with addresses there can be many unique words. I have been trying BERT, roBERTa, ELECTRA but my vocab size is huge 2800000. Because of large number of places, I selected frequency as 1 from the corpus. \n\n\nIs there any approach where the model can handle spelling mistakes, and out of vocabulary words with most effectiveness?",
"date": "2020-06-05"
},
{
"vote": 0,
"title": "Is there any package or model available to translate any language into English offline?",
"text": null,
"date": "2020-06-04"
},
{
"vote": 3,
"title": "Day 156 of #NLP365 â€“ NLP Papers Summary â€“ Asking And Answering Questions To Evaluate The Factual Consistency Of Summaries",
"text": "Day 156.\n\n\nToday's post is a 6-minute summary of the NLP paper \"Asking And Answering Questions To Evaluate The Factual Consistency Of Summaries\".\n\n\nI really like today's paper. It aims to evaluate factual consistency of summaries by generating questions, answering those questions using the source document and summary, and evaluate the answers similarity. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/04/day-156-nlp-papers-summary-asking-and-answering-questions-to-evaluate-the-factual-consistency-of-summaries/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-04"
},
{
"vote": 24,
"title": "Fine-tune Transformers on NLP Tasks: Colab Notebooks",
"text": "Hi,\n\n\nI have been working on Colab notebooks that illustrate the step by step guide to fine tune transformers on downstream NLP tasks. \nTransformer Tutorials\n\n\nIt's a work in progress and i plan to add more tasks to this repo. I hope the community finds it useful.",
"date": "2020-06-04"
},
{
"vote": 1,
"title": "Huggingface NLP, Uploading custom dataset",
"text": "Hello,\n\n\nDoes anyone know how we can call our custom dataset using the nlp.load command? Let's say that I have a dataset based on the same format as that of squad-v1.1, how am I supposed to load it using huggingface nlp.\n\n\n&#x200B;\n\n\nThank you!",
"date": "2020-06-04"
},
{
"vote": 6,
"title": "Classifying large pieces of text",
"text": "Hi everyone,\n\n\nI was wondering what the most effective approach is for classifying large pieces of text ~5000 words each. \n\n\nIs it worth using something like Doc2Vec or is extracting the most useful features from the text prior to applying standard Word2Vec/Glove?\n\n\nThanks",
"date": "2020-06-03"
},
{
"vote": 1,
"title": "Need Help with NER from a URL",
"text": "I am fairly new with making scripts primarily for NLP/NER/BERT and every other acronym out there. My background is in linked data engineering. \n\n\nI am trying to find a resource (at this point I don't care if its a Colab notebook or instructions written in crayon) on running NER for a URL and not a block of text. I had something going and its just not working. \n\n\nI have checked here  \nhttps://notebooks.quantumstat.com/\n  searched Medium, viewed research publications and anything I did find was not hitting the nail on the head. Any help here would be greatly appreciated.",
"date": "2020-06-03"
},
{
"vote": 4,
"title": "Day 155 of #NLP365 â€“ NLP Papers Summary â€“ TRAIN ONCE, TEST ANYWHERE: ZERO-SHOT LEARNING FOR TEXT CLASSIFICATION",
"text": "Day 155.\n\n\nToday's post is a 4-minute summary of the NLP paper \"TRAIN ONCE, TEST ANYWHERE: ZERO-SHOT LEARNING FOR TEXT CLASSIFICATION\".\n\n\nZero-shot learning is getting traction recently due to GPT-3 and so I got interested and read into it. This paper applied zero-shot learning for text classification. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/03/day-155-nlp-papers-summary-train-once-test-anywhere-zero-shot-learning-for-text-classification/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-03"
},
{
"vote": 36,
"title": "181 NLP Colab Notebooks Found Here!",
"text": "*UPDATE* Super Duper NLP Repo\n\n\nAdded 41 new NLP notebooks, bringing us to 181 total! Several interesting topics from information retrieval to knowledge graphs included in this update. Thank you to contributors David Talby and Manu Romero.\n\n\nhttps://notebooks.quantumstat.com/",
"date": "2020-06-03"
},
{
"vote": 0,
"title": "Sample decoder architecture for auto-regressive transformer.",
"text": "[deleted]",
"date": "2020-06-03"
},
{
"vote": 18,
"title": "Is it a good idea to do a master's in computational linguistics if I want to be a better NLP engineer?",
"text": "I have been a data scientist for a year now, in my mid 20s. I mostly do NLP at my job, but lot of it is regex, rules, and bit of ML thrown in between. I have been studying deep learning in NLP by myself and I'm aware of the classical techniques as well. I realised that I enjoy NLP much more than computer vision and structured data. So I want to improve myself and get into NLP role at bigger firms by getting a graduate degree ( eg. MS comp ling at University of Washington). However I have zero background in linguistics. I'm a math and coding person. My biggest concern is I'll be studying a lot of linguistics theory stuff and I'll get bored or worse regret my decision. Right now, if you ask me what an \"adverb\" is I wouldn't know. However I've seen a lot of graduates go on to work at Microsoft, Google etc as NLP engineers which is why I think should still do it.",
"date": "2020-06-03"
},
{
"vote": 5,
"title": "Help regarding parsing SEC filings",
"text": "Hello,\n\n\nI am carrying out an extensive parsing of SEC documents. Extracting information from tables seem straightforward, but extracting information from paragraphs like these seem tricky.\n\n\n\"After identifying the median employee, we calculated the annual total compensation for such employee using the same methodology we used for our CEO as set forth in the 2019 Summary Compensation Table found on page 46. For 2019, we estimate that our CEO to median employee pay ratio is: 234.2, the annual total compensation for the median employee was $43,368, and the annual total compensation for our CEO was $10,155,682.\n\"\n\n\nExtracting the pay info for different actors (CEO and employee) and also pay ratio seems tricky to me. Note that this info can be in different paragraphs or different points, but in the same page.\n\n\nAnother such parsing problem might be obtaining a executive's current and former positions.\n\n\nHow can information like these be extracted using NLP? I was looking at NER using spacy, from that I could get the \"MONEY\" tags. Another approach I was looking at was dependency parsing. Am I going in the right direction?\n\n\nThank you!",
"date": "2020-06-02"
},
{
"vote": 3,
"title": "Detecting antonymity in a sentence?",
"text": "[deleted]",
"date": "2020-06-01"
},
{
"vote": 1,
"title": "Need help in Automatic Categorization of Author Gender",
"text": "[deleted]",
"date": "2020-06-01"
},
{
"vote": 2,
"title": "Day 153 of #NLP365 â€“ NLP Papers Summary â€“ Span-ConveRT: Few-Shot Span Extraction For Dialog With Pretrained Conversational Representations",
"text": "Day 153.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Span-ConveRT: Few-Shot Span Extraction For Dialog With Pretrained Conversational Representations\".\n\n\nThis paper introduced Span-ConveRT, a model for dialog slot-filling task which frames the task as a turn-based span extraction task, allowing the model to leverage the representations of pre-trained language model. The paper also released RESTAURANT-8K, a new dataset of actual conversations in the restaurant booking domain. The dataset spans 5 different slots: date, time, people, first name, and last name. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/06/01/day-153-nlp-papers-summary-span-convert-few-shot-span-extraction-for-dialog-with-pretrained-conversational-representations/\n\n\nBest,\n\n\nRyan",
"date": "2020-06-01"
},
{
"vote": 1,
"title": "Glossary and Definition extraction from Text using Unsupervised Technique",
"text": "Recently wrapped up this semester project. This blog reflects my learning and approach on this. Read more at \nhttps://prakhartechviz.blogspot.com/2020/06/automatic-glossary-and-definition-extraction.html",
"date": "2020-06-01"
},
{
"vote": 1,
"title": "HAIMKE - an AI model that transform bullet points to full text",
"text": "[removed]",
"date": "2020-06-01"
},
{
"vote": 2,
"title": "The Grammar of Truth and Lies",
"text": null,
"date": "2020-06-01"
},
{
"vote": 43,
"title": "ðŸ”¥ Weak Supervision in the Age of Transfer Learning for NLP",
"text": null,
"date": "2020-06-01"
},
{
"vote": 4,
"title": "Need guidance for text summarization",
"text": "I've been asked by the company for which I'm interning to create a text summarizer since I've already completed all the tasks that I was supposed to do. I have experience with deep learning but new to NLP. I've been looking at transformers to get my job done. Can someone guide me or provide me links to some useful code which can be used to create summarizer as I've found many papers on summarization but not much blogs with code or simply code.",
"date": "2020-06-01"
},
{
"vote": 5,
"title": "Guidance Required: Need a better Langauge Model",
"text": "I am new to this machine learning and NLP domain. I am working on Indic Language.\n\n\nFor  a sentence of N words, I have M candidates for each i_th word. So there  would be total N*M sentences. Out of these sentences, I need to choose  the best possible sentences based on context.\n\n\nCurrently,  I am using n-gram model to choose the best candidate at each step (for  each word) and beam search method to maintain a beam of B size.  But I  face the loss of context here too.\n\n\nI  need some neural network that maintains a knowledge of context learned  from a big corpus. And It should be able to solve my issue to score  candidates and choose the best among them at each step. (step means one  word at a time: total N steps)\n\n\nPlease help me out. Suggest me some model or Some paper of similar work. I would be thankful to you.",
"date": "2020-05-31"
},
{
"vote": 4,
"title": "Noob question: How is my input interpreted in NLP?",
"text": "Hello all,\n\n\nMy main aim is to create a chatbot. So from what I've understood, I can \nnot\n understand the whole of NLP, and will have to identify the topics that are needed to be learnt. So, let's just say we have a user string input, we lemmatize it, parse it and do word jugglery, that I don't know specifically about. Now we have the input in a form, which our friendly computer can understand.\n\n\nNote: I can be wrong at any point. Please do point out any mistakes in my understanding, I'll be forever grateful.\n\n\nNow we need to \ninterpret\n the input. That is, we need to understand it. From what I've read here and there, there are two ways:\n\n\n\n\nNon Neural network, rule based interpretation, that isn't really flexible but doesn't require much effort to create.\n\n\nNeural network based interpretation. This requires data to create a model.\n\n\n\n\nSo, the output that comes out from both of these methods, is an indicator which tells what exactly is needed to be done (like if the input is \"Add 2 and 2\", then the output of interpretation stage would be a function \"add\" with args (2,2)). Along with the identifier, a confidence level is also returned which tells how much sure the interpreter is about the output.\n\n\n&#x200B;\n\n\nI am absolutely not sure if anything in NLP works as described. I would be more than glad if someone could tell if I am thinking in the right direction or not.",
"date": "2020-05-31"
},
{
"vote": 1,
"title": "Noob Queston 1: How is my input interpret in NLP?",
"text": "[deleted]",
"date": "2020-05-31"
},
{
"vote": 5,
"title": "Day 152 of #NLP365 â€“ NLP Papers Summary - OPINIONDIGEST: A Simple Framework for Opinion Summarization",
"text": "Day 152.\n\n\nToday's post is a 5-minute summary of the NLP paper \"OPINIONDIGEST: A Simple Framework for Opinion Summarization\".\n\n\nThis paper presented OPINIONDIGEST, an abstractive opinion summarisation framework that uses Aspect-based Sentiment Analysis (ABSA) to extract opinion phrases from reviews and trains a Transformer to reconstruct the original reviews from these extracted opinion phrases. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/31/day-152-nlp-papers-summary-opiniondigest-a-simple-framework-for-opinion-summarization/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-31"
},
{
"vote": 3,
"title": "What are your sources of getting a word list in different languages?",
"text": "What are some of the good word lists you have been using for different languages? While for english, it is quite straightforward as \nnltk.corpus.words.words()\n, or as of few other languages like German, French there is a good source from  \nhttps://www.sketchengine.eu/word-lists/\n, but it is not the case for other languages.",
"date": "2020-05-31"
},
{
"vote": 1,
"title": "Why does the BERT NSP head linear layer have two outputs? I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?",
"text": "Here's the code in question. \n\n\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score",
"date": "2020-05-30"
},
{
"vote": 0,
"title": "I need a text normalizer that can work for any language",
"text": "I need text normalization. As in \"123\" there can be multiple pronunciations by just reading that. Could be \"one two three\" \"one twenty three\" \"one hundred and twenty three\" \"a hundred and twenty three\" etc. I need an algorithm to normalize this so that there is only one possible way to pronounce it. But I need this for EVERYTHING. As in every possible abbreviation for anything needs to be expanded. \"Sgt\" needs to be fully spelled out as \"sergeant\". And for \"St.\" this could either be \"saint\" or \"street\" depending on the context, so I need my normalizer to make sure it normalizes to the right thing so it needs the context of all abbreviations. Also acronyms need to be recognized for how they would be pronounced an normalized accordingly. \"FIFA\" wouldn't need to change because that's pronounced as is, however \"NCAA\" needs to normalize to \"N C double A\" because that's how it's pronounced. I need my normalizer to know that and normalize it correctly.\n\n\nI also need this to work in any language. Like in French if \"tlm\" or \"mdr\" are written I need the normalizer to fully change that to \"tout le monde\" and \"mort de rire\". But if an abbreviation of letters is different depending on context, then I need the normalizer to know that and normalizer correctly according to context, and do it correctly every single time, for any possible language.\n\n\nWhat tools online are available for that. I really need something where I can just call a normaize() function on a string written in any language, and have it normalize correctly. Where can I find that, and if I can't, how can I write an algorithm to do it?",
"date": "2020-05-30"
},
{
"vote": 11,
"title": "GPT-3: A Brief Summary",
"text": null,
"date": "2020-05-30"
},
{
"vote": 11,
"title": "Day 151 of #NLP365 â€“ NLP Papers Summary â€“ A Large-Scale Multi-Document Summarization Dataset From The Wikipedia Current Events Portal",
"text": "Day 151.\n\n\nToday's post is a 4-minute summary of the NLP paper \"A Large-Scale Multi-Document Summarization Dataset From The Wikipedia Current Events Portal\".\n\n\nThis paper presented a large dataset for multi-document summarisation (MDS) built from Wikipedia Current Events Portal (WCEP) that contains 10200 document clusters and each document cluster has 235 articles on average. The dataset uses concise and neutral human-written summaries of news events, with links to external source articles. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/30/day-151-nlp-papers-summary-a-large-scale-multi-document-summarization-dataset-from-the-wikipedia-current-events-portal/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-30"
},
{
"vote": 6,
"title": "Choice of loss function for NLP tasks",
"text": "Hi, I'm new to NLP and I'm trying to understand the rationale behind using the cross-entropy as a common loss function in the training of most NLP tasks. \nWhat would it mean practically and theoretically if I try to use the MSE loss instead of the cross entropy loss for an NLP task, like conversational agents?",
"date": "2020-05-29"
},
{
"vote": 2,
"title": "What Is Inside the 20 Newsgroups Dataset? Human Readable Text? Yes, but Not Only (Also a Roman Gazebo)",
"text": null,
"date": "2020-05-29"
},
{
"vote": 9,
"title": "Semantic dissimilarity?",
"text": "Is there some model that can find dissimilarities like so:\n\n\n\"I like potatoes\"\n\n\nand \"I hate potatoes\"\n\n\nAnd sort of 'know' that it's talking 'about' the same thing, but it's kind of the inverse meaning?\n\n\nI'm familiar with semantic similarity but in my experience, I haven't found that it's actually good at something like this. If given 2 corpuses and asked to compare all sentences, it'd be like searching for a needle in a haystack to get something like \"I like potatoes\" and \"I hate potatoes\" because you'd have to look for sentences with a large cosine distance and if you're comparing millions of sentences it's hard for a human to find the actual sentences where a viewpoint about the same thing is expressed inversely.",
"date": "2020-05-29"
},
{
"vote": 2,
"title": "Day 150 of #NLP365 â€“ NLP Papers Summary â€“ Will-They-Wonâ€™t-They: A Very Large Dataset For Stance Detection On Twitter",
"text": "Day 150.\n\n\nToday's post is a 4-minute summary of the NLP paper \"Will-They-Wonâ€™t-They: A Very Large Dataset For Stance Detection On Twitter\".\n\n\nThis paper presented Will-They-Wonâ€™t-They (WT-WT), the largest stance detection dataset, which contains 51,284 tweets. All annotations are manually labelled by experts, ensuring high-quality evaluation of models. The dataset covers stance detection for rumours verification in finance, specifically around M&A. The reason for this is because an M&A process has many stages and the evolution of opinion from Twitter users about each stages is similar to that of rumour verification. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/29/day-150-nlp-papers-summary-will-they-wont-they-a-very-large-dataset-for-stance-detection-on-twitter/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-29"
},
{
"vote": 1,
"title": "Spell Check and Lemmatizing \"Ham Burger\"",
"text": "Are there any approaches that can handle scenarios where a word containing two words are separated by a space.  The example is \"ham burger\" and the goal is to achieve hamburger.",
"date": "2020-05-29"
},
{
"vote": 4,
"title": "Catching Attention with Automatic Pull Quote Selection",
"text": null,
"date": "2020-05-28"
},
{
"vote": 2,
"title": "A multi-reddit of all the top bot/ai communities on Reddit to stay up to date!",
"text": null,
"date": "2020-05-28"
},
{
"vote": 0,
"title": "Best way to identify terms related to medical diagnosis from a text?",
"text": "[deleted]",
"date": "2020-05-28"
},
{
"vote": 2,
"title": "Day 148 of #NLP365 â€“ NLP Papers Summary â€“ A Transformer-Based Approach For Source Code Summarization",
"text": "Day 148.\n\n\nToday's post is a 4-minute summary of the NLP paper \"A Transformer-Based Approach For Source Code Summarization\".\n\n\nThe objective of source code summarisation is to encode source code and generate a readable summary that describes the functionality of the program. This paper Utilised a simple transformer-based model with relative position representations and copy attention mechanism to generate SOTA results for source code summarisation. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-27"
},
{
"vote": 1,
"title": "Free News Datasets - Coronavirus News, Financial Crimes News, and Natural Disasters News",
"text": null,
"date": "2020-05-27"
},
{
"vote": 4,
"title": "hi r/LanguageTechnology, I work for TNW, and we're hosting an AMA on our site with James Vlaho, author of the book Talk to Me: How Voice Computing Will Transform the Way We Live, Work, and Think. Vlahos also created DadBot, an AI chatbot version of his late father. Come ask a question if you like!",
"text": null,
"date": "2020-05-27"
},
{
"vote": 3,
"title": "how to do inference for SQuAD style QA task using ALBERT?",
"text": "I am developing a question answering system using this. But it is quite hard to find code to do inference part. Can someone help me? What I need is a function that predict an answer to a question. function inputs are question and context.",
"date": "2020-05-27"
},
{
"vote": 0,
"title": "How to find the root of a word from its present participle or other variations?",
"text": "Hey all, I'm working on a NLP project, and right now, I'm stuck on detecting antonyms for certain phrases that aren't in their \"standard\" forms (like verbs, adjectives, nouns) instead of present-participles, past tense, or something to that effect. For instance, if I have the phrase \"arriving\" or \"arrived\", I need to convert it to \"arrive\". Similarly, \"came\" should be \"come\". Lastly, â€œdissatisfiedâ€ should be â€œdissatisfyâ€. Can anyone help me out with this? I have tried several stemmers and lemmanizers in NLTK with Python, to no avail; most of them donâ€™t produce the correct root. Iâ€™ve also thought about the ConceptNet semantic network and other dictionary APIs, but it seems far too complicated for what I need. Any advice is helpful. Thanks!",
"date": "2020-05-26"
},
{
"vote": 6,
"title": "John Snow Labs Spark-NLP 2.5.1: Adding support for 6 new BioBERT and ClinicalBERT models",
"text": null,
"date": "2020-05-26"
},
{
"vote": 16,
"title": "Day 147 of #NLP365 â€“ NLP Papers Summary â€“ Two Birds, One Stone: A Simple, Unified Model For Text Generation From Structured And Unstructured Data",
"text": "Day 147.\n\n\nToday's post is a 3-minute summary of the NLP paper \"Two Birds, One Stone: A Simple, Unified Model For Text Generation From Structured And Unstructured Data\".\n\n\nRather than continuously increasing the complexity of the neural network, this paper shows that a properly fine-tuned simpler model can also achieve SOTA results in table-to-text generation and neural question generation tasks, encouraging us to thoroughly explore simpler models before introducing complex ones. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/26/day-147-nlp-papers-summary-two-birds-one-stone-a-simple-unified-model-for-text-generation-from-structured-and-unstructured-data/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-26"
},
{
"vote": 5,
"title": "[D][R] Electra just toppled SciBert for round 2 of trec-covid",
"text": null,
"date": "2020-05-26"
},
{
"vote": 3,
"title": "Which conferences/workshops organize NLP shared tasks aside from SemEval?",
"text": null,
"date": "2020-05-25"
},
{
"vote": 9,
"title": "Job advice for a new grad",
"text": "[deleted]",
"date": "2020-05-25"
},
{
"vote": 0,
"title": "Build personal data narratives with AcceleratedText #NLG",
"text": "[deleted]",
"date": "2020-05-25"
},
{
"vote": 3,
"title": "Day 146 of #NLP365 â€“ NLP Papers Summary â€“ Exploring Content Selection In Summarization Of Novel Chapters",
"text": "Day 146.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Exploring Content Selection In Summarization Of Novel Chapters\".\n\n\nThis paper proposed a new summarisation task of summarising novel chapters from online study guides. This is much more challenging than the news summarisation due to the length of the source document and the higher level of paraphrasing. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/25/day-146-nlp-papers-summary-exploring-content-selection-in-summarization-of-novel-chapters/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-25"
},
{
"vote": 4,
"title": "X English WordNet",
"text": "&#x200B;\n\n\nX English WordNet\n\n\nIn the wake of the recent English WordNet 2020,  here comes the release of an \nXtended English WordNet\n.\n\n\nThe resource extends on EWN to provide the data that was missing (verb templates, tag count) and offer a lossless version of WordNet. However it is misleading to think of the set of core data as different from that of EWN.\n\n\nBut XEWN considers it essential to maintain compatibility with the WNDB standard, so that existing applications and libraries can continue to run with new data. Note that this does not exclude innovation (extended format, case-sensitive sensekeys, multiple indexes, new relations,...).\n\n\nFinally stress is laid on SQL because ... this is a standard for database apps, offering performance, versatility, reliability, scalability.\n\n\nXEWN comes in 4 formats:\n\n\n\n\nXML with XSLT-extended sources\n\n\nWNDB (seamless or lossless)\n\n\nSQL (MySQL or 'standard' SQL)\n\n\nSqlite3\n\n\n\n\nXEWN has been joined to (and is interoperable with):\n\n\n\n\nLegacy WordNet\n\n\nVerbNet\n\n\nPropBank + SemLink\n\n\nFrameNet\n\n\nSUMO\n\n\nIlfwn + Glf\n\n\nBNC\n\n\nPredicateMatrix\n\n\nSyntagNet (to be finalised)\n\n\n\n\nwithin the SQLUNet framework. The tables are joinable and interconnectable.\n\n\nXEWN comes complete with tools:\n\n\n\n\nXSLT transformation\n\n\nXSD validation\n\n\nXSLT merging\n\n\nXML to WNDB grinding\n\n\n\n\nMain entry point is here:\n\n\nhttps://github.com/x-englishwordnet/xewn\n\n\nSoon to be announced:\n\n\n\n\nXML Beans, JAXB and JWI libraries\n\n\nHyperbolic browser for the desktop.\n\n\nBrowsers (text trees and hyperbolic) for the mobile.",
"date": "2020-05-25"
},
{
"vote": 1,
"title": "Tokenization in seq2seq models for same language related tasks",
"text": "Should we have seperate tokenization for context and target words in seq2seq models (for the tasks like automatic headline generation /text summarization , chatbot, etc ) or we can tokenize by combining them.\n\n\nSuppose , I have list of articles (context) and corresponding headlines(target) ,\n\n\n1st _approach\n\n\nfrom keras.preprocessing.text import Tokenizer\n\n\nheadline_tokenizer = Tokenizer()\n\n\narticle_tokenizer = Tokenizer()\n\n\nheadline_tokenizer. fit_on_texts(list(headlines))\n\n\nheadline_dictionary = headline_tokenizer.word_index\n\n\nheadline_vocabs=len(headline_dictionary)+1\n\n\narticle_tokenizer. fit_on_texts(list(articles))\n\n\narticle_dictionary = article_tokenizer.word_index\n\n\narticle_vocabs=len(article_dictionary)+1\n\n\n2nd _approach\n\n\nheadline_article = headlines+articles\n\n\nheadline_article_tokenizer=Tokenizer()\n\n\nheadline_article_tokenizer. fit_on_texts(list(headline_article))\n\n\ncombined_dictionary = headline_article_tokenizer.word_index\n\n\ncombined_vocabs=len(headline_article_dictionary)+1\n\n\nMy question is which approach is better to follow and why?",
"date": "2020-05-25"
},
{
"vote": 9,
"title": "Call for Paper for Workshop for NLP-OSS",
"text": "(apologies for cross-posting)\n\n\n----------------------------------------------------------------\n\n\n*Workshop for NLP Open Source Software (NLP-OSS)*\n\n\n11 or 12 Nov 2020, Co-located with EMNLP 2020\n\n\nhttps://nlposs.github.io/2020\n\n\nDeadline for Long and Short Paper submission: 05 August 2020 (23:59, GMT-11)\n\n\n----------------------------------------------------------------\n\n\nYou have used NLP open source tools and bore grievances but found the solution after hours of coffee and computer staring. Share that at NLP-OSS and suggest how open source could change for the better (e.g. best practices, documentation, API design etc.)\n\n\nYou came across an awesome SOTA system on NLP task X that once ruled the F1 score. But now the code is stale and it takes an dinosaur to understand the code. Share your experience at NLP-OSS and propose how to \"replicate\" these forgotten systems.\n\n\nYou read an NMT paper with SOTA BLEU scores. But now the code is stale and it takes an dinosaur to understand the code. Share your experience at NLP-OSS and propose how to \"replicate\" these forgotten systems.\n\n\nYou see this shiny *BERT from a blogpost, tried it to reproduce similar results on a different task and it just doesn't work on your dataset. You did some magic to the code and now it works. Show us how you did it! Though they're small tweaks, well-motivated and empirically test are valid submissions to NLP-OSS.\n\n\nYou have tried 101 NLP tools and there's none that really do what you want. So you wrote your own shiny new package and made it open source. Tell us why your package better than the existing tools, how did you design the code? Is it going to be a one time thing? Or would you like to see thousands of people using it?\n\n\nAt last, you've found the avenue to air these issues in an academic platform at the NLP-OSS workshop!!!\n\n\nSharing your experiences, suggestions and analysis from/of NLP-OSS\n\n\n&#x200B;",
"date": "2020-05-25"
},
{
"vote": 1,
"title": "LIWC help?",
"text": "[deleted]",
"date": "2020-05-24"
},
{
"vote": 4,
"title": "This sub may be better positioned to answer this question. Any insights would be much appreciated, thanks!",
"text": "[deleted]",
"date": "2020-05-24"
},
{
"vote": 18,
"title": "Self Supervised Representation Learning in NLP",
"text": "Hi everyone,\n\n\nI am currently writing an overview on how current NLP methods formulate self-supervised learning for text data. I wanted to hear your feedback on the initial draft and your suggestions on any other problem formulations you're aware of.\n\n\nDraft: \nhttps://amitness.com/2020/05/self-supervised-learning-nlp/",
"date": "2020-05-24"
},
{
"vote": 4,
"title": "How to get data for a domain, where dataset doesn't exist.",
"text": "[deleted]",
"date": "2020-05-24"
},
{
"vote": 3,
"title": "Two questions about the architecture of Google Bert model (in particular about parameters)",
"text": "Hi everybody, I'm looking for someone who can help me clarifying a few details regarding the architecture of Bert model. Those details are necessary for me to come with a full understanding of Bert model, so your help would be really helpful. Here are the questions:\n\n\n\n\nDoes the self-attention layer of Bert model have parameters? Do the embeddings of words change ONLY according to the actual embeddings of other words when the sentence is passed through the self-attention layer?\n\n\nAre the parameters of the embedding layer of the model (the layer which transforms the sequence of indexes passed as input into a sequence of embeddings of size=size of the model) trainable or not?\n\n\n\n\nThank you in advance for your answers,\n\n\nP.",
"date": "2020-05-24"
},
{
"vote": 1,
"title": "Creating dataset from COVID19 recovery chat logs",
"text": "[removed]",
"date": "2020-05-24"
},
{
"vote": 8,
"title": "Day 145 of #NLP365 â€“ NLP Papers Summary â€“ SUPERT: Towards New Frontiers In Unsupervised Evaluation Metrics For Multi-Document Summarization",
"text": "Day 145.\n\n\nToday's post is a 5-minute summary of the NLP paper \"SUPERT: Towards New Frontiers In Unsupervised Evaluation Metrics For Multi-Document Summarization\".\n\n\nThis paper proposed SUPERT, an unsupervised evaluation metric for evaluating multi-document summary by measuring the semantic similarity between the summary and the pseudo reference summary. This could also be used to create reference summary for training supervised summarisation models. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/24/day-145-nlp-papers-summary-supert-towards-new-frontiers-in-unsupervised-evaluation-metrics-for-multi-document-summarization/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-24"
},
{
"vote": 2,
"title": "replacement rules definition and execution.",
"text": null,
"date": "2020-05-24"
},
{
"vote": 2,
"title": "Transform Text Files to Data Frame with Python",
"text": "Hi guys, I wrote a short guide to extract information from text files, combine them in a data frame and export the data with python. Since I usually work with java and this is my first article ever, I highly appreciate any feedback! Thanks!\n\n\nhttps://medium.com/@sebastian.guggisberg/transforming-text-files-to-data-tables-with-python-553def411855",
"date": "2020-05-23"
},
{
"vote": 48,
"title": "datasets for NLP",
"text": "this is a useful website for a lot of datasets for NLP\n\n\nhttps://datasets.quantumstat.com\n\n\nThey have descriptions and papers linked to each data set",
"date": "2020-05-23"
},
{
"vote": 1,
"title": "podcast machine learning",
"text": "[removed]",
"date": "2020-05-22"
},
{
"vote": 4,
"title": "Personal Events in Dialogue Corpus - new open source corpus :p",
"text": "http://www.artie.com/data/personaleventsindialogue/\n\n\n&#x200B;\n\n\nWhat is the Personal Events in Dialogue Corpus??\n\n\nThe PEDC is a corpus of 14 episodes of \nThis American Life\n podcast transcripts that have been annotated for events. The corpus contains excerpts from these episodes (listed in Tabe 1) that are dialogue. The granularity of annotation in this corpus is the token; each token is either annotated as an event, or a nonevent. For more information please \ndownload the corpus\n, and see the annotation guide for more specifics on how we define event, and the README for how the annotations are encoded. Also, much more information regarding the corpus, and its use is in the \nAutomatic extraction of personal events from dialogue\n paper.\n\n\nLicensing\n\n\nThis corpus is open sourced under a \nCC-BY license\n. This American Life holds the copyright of all transcripts, and has given us permission to distribute these transcripts. Here is the \ncopyright notice for the 14 episode transcripts\n.",
"date": "2020-05-22"
},
{
"vote": 2,
"title": "Translation Risk Prediction",
"text": null,
"date": "2020-05-22"
},
{
"vote": 4,
"title": "Is there something like vecs2vec? How to compare similarity of collections of vectors (e.g. 100K sentences from Person X and 100K sentences from Person Y)?",
"text": "Here's what I mean:\n\n\nLets say I have a corpus of all the speeches of person X, and a similar corpus for person Y.\n\n\nI want to 'measure' how similar the views of these 2 people are.\n\n\nAs someone with some behavioral science experience, I know that frequency is an important factor. For example, the more frequently you drink orange juice, that can lead me to greater confidence that you like orange juice.\n\n\nSame thing with 'frequently expressed thoughts' - the more frequently you talk about a certain topic or rephrase the same thought, the more I'm certain that you have a stronger attachment to that than other thoughts lets say (speaking loosely).\n\n\nNow, I had this idea: When you cluster sentences with say T-SNE and you see these clusters of sentences by a person, you begin to see that there are dense clusters and more sparse ones.\n\n\nCan I somehow compare the similarity of the densest clusters from person X's sentence embeddings to person Y's sentence embeddings? I don't want really to do a 1-to-1 comparison of every single sentence. Instead I want to compare 'general thought similarity' combined with the frequency of that thought.\n\n\nFor example if someone says \"I like potatoes\" then \"I love potatoes\" then \"I'm passionate about potatoes\" etc. etc. all of those cluster.\n\n\nThen person Y says \"I like sweet potatoes because they remind me of potatoes\", \"I love the skin of potatoes\" and \"I'm passionate about fried potatoes\", there's another cluster.\n\n\nThe question is: Is it possible to compare the similarity of all of person X's clusters and person Y's clusters as a whole? Or to compare simply the densest clusters (clusters containing the most similar sentences)?\n\n\nWould one approach to this be something like:\n\n\n\n\nReduce 100K sentence embeddings down to 1 vector which represents the densest clusters (since frequency is what matters). Or maybe instead of all 100K sentence embeddings, find the densest clusters (say 10K sentences), and take each cluster and create an embedding for it. Is there some vecs2vec approach?\n\n\nCompare that to person Y's vector?\n\n\n\n\nNote: I know this approach is likely to fail. Why? Because there seem to be too many edge cases I have in mind. Frequent sentences could be about something else entirely not related to that person's views. I understand that. However the point here is to start thinking about the problem, and those other issues I can try to work out as I go along.\n\n\nEdit: Another challenge is tokenization. How do you tokenize by 'thought expressed' or 'topic'? Sometimes views are expressed over 10 sentences, not just 1. If we split by sentences, we might lose essential elements of the view. Therefore tokenization that is based on shifts of a thought expressed or meaning would be preferable.",
"date": "2020-05-22"
},
{
"vote": 1,
"title": "Domain for Questionâ€“Answering NLP task?",
"text": "I have to build one web application with an NLP task. \nI described below one example so guys please help me to choose the \nbest Domain for this task.\n\n\n\nExample\n\n\n>User Input question:- \nwhat is COVID 19?\n>\n>user write answer:- \nCOVID 19 is  Coronavirus disease.\n>\n>Explanation filed:- \nanswer correctly\n>\n>\nIf answer wrong Explanation Field :  COVID-19 is an infectious disease\n\n\nIf answer wrong then the system generates an explanation \nof why my answer is wrong.\nsystem check the answer is correct or not.  \nif yes then everything good or if not then the system generates \nan explanation base on questions.\nI checked online but I did not find or I do not have enough knowledge \nhow to select the Domain for QA.",
"date": "2020-05-22"
},
{
"vote": 37,
"title": "ICLR 2020: NLP Highlights",
"text": "ICLR 2020: NLP Highlights: \nhttps://medium.com/@nguyenhc95/iclr-2020-nlp-highlights-511deb99b967\n \n\n\nThis is my first medium post, showing some highlights of NLP researches in ICLR 2020. Would love to hear your feedback and suggestions to better my writings and researches.",
"date": "2020-05-22"
},
{
"vote": 10,
"title": "Where can we try Turing NLG?",
"text": "I saw this broadcast from Microsoft: \nhttps://www.pscp.tv/w/1OyKAYWPRrWKb\nAt around 16 minutes they show a UI where you can paste in some text, and ask a question. Is this something available online that we can play with? Or is this probably something they have not made public? If not, are there other ways we can try Turing NLG?",
"date": "2020-05-21"
},
{
"vote": 3,
"title": "Day 142 of #NLP365 - NLP Papers Summary â€“ Measuring Emotions In The COVID-19 Real World Worry Dataset",
"text": "Day 142.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Measuring Emotions In The COVID-19 Real World Worry Dataset\". This paper proposed and made public a COVID-19 real world worry dataset that consists of 2500 short texts and 2500 long texts of people's worries of COVID-19. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/21/day-142-nlp-papers-summary-measuring-emotions-in-the-covid-19-real-world-worry-dataset/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-21"
},
{
"vote": 36,
"title": "BERTweet: The first large-scale pre-trained language model for English Tweets",
"text": "https://github.com/VinAIResearch/BERTweet\n\n\nBERTweet is the first public large-scale pre-trained language model for English Tweets.  The model was trained from scratch using the corpus of  850M general English Tweets ~ 16B words ~ 80GB of text.",
"date": "2020-05-21"
},
{
"vote": 1,
"title": "What do i need for a natural language calculator?",
"text": "Hi everybody,\n\n\nme and two friends want to make a calculator in python that uses text like \"Whats 5 plus 3 times 2\" or \"Add 1 to the square root of 9\" (and more complex calculations) as input. Libraries like StaCy seems to be to low-level for this (at least from what i found in their docs). Rasa or Snips NLU work fine for basic calculations, but dont work when chaining multiple calculations together. At what libraries should we look to solve this?",
"date": "2020-05-21"
},
{
"vote": 3,
"title": "Need help with question answering system",
"text": "Hi all,\n\n\nI'm working on extracting simple answers for queries on scientific research/literatures. \nCan anyone please suggest an advanced NLP book which has practical examples on question answering.\nAlso I couldn't find any tutorial from scratch which shows a question answering model working.\nCould anyone please help me on this?\nStuck since long.",
"date": "2020-05-21"
},
{
"vote": 1,
"title": "Gender Bias and Counterfactuals in Spanish NLP",
"text": "[deleted]",
"date": "2020-05-20"
},
{
"vote": 3,
"title": "Generating rhythmic text with NLP (e.g., poetry, rap)",
"text": "I want to develop a model that can generate text whose pronunciation is rhythmic (e.g., rap lyrics). Is there a way to express lyrics so that the rhythm is explicit? For example, is there a way to express this lyric in a way that shows where the bars are, so that I can train a model to generate it: \"His palms are sweaty, knees weak, arms are heavy; There's vomit on his sweater already, mom's spaghetti\"?",
"date": "2020-05-20"
},
{
"vote": 5,
"title": "[LREC paper] \"TopicNet: Making Additive Regularisation for Topic Modelling Accessible\"",
"text": "Our new LREC paper, \"TopicNet: Making Additive Regularisation for Topic Modelling Accessible\" is out.\n\n\nAbstract:\n This paper introduces TopicNet, a new Python module for topic modeling. This package, distributed under the MIT license, focuses on bringing additive regularization topic modelling (ARTM) to non-specialists using a general-purpose high-level language. The module features include powerful model visualization techniques, various training strategies, semi-automated model selection, support for user-defined goal metrics, and a modular approach to topic model training. Source code and documentation are available at \nhttps://github.com/machine-intelligence-laboratory/TopicNet\n\n\nPaper:\n \nhttp://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.833.pdf\n\n\nCode\n (and library documentation): \nhttps://github.com/machine-intelligence-laboratory/TopicNet/\n\n\nThe paper introduces a new library for topic modeling and outlines the design principles used. It is experimentally shown that, compared to GenSim, TopicNet trains models faster and topics in these models are less similar to each other.\n\n\nPS: This paper corresponds to an older version of TopicNet. Compared to the current release 0.7.1, many features are added. Most notably, we distribute several datasets in the required format (including the corpora described in the paper).",
"date": "2020-05-20"
},
{
"vote": 2,
"title": "Azure ML Workspaces vs. Cloud Functions vs. App Service on Python + Flask fo LDA?",
"text": "Hi guys!\n\n\nI'm trying to make sense of the different deployment options for my Topic Modeling algo on Azure, but I got a bit confused, hope some of you are more familiar with the ML related part of Azure than me and find some answers.\n\n\nI've used Azure ML Workspaces to successfully deploy my other Multiclass Classification model, however in the case of Topic Modeling the  LDA model needs to be trained whenever a new atrticle gets created (in our main news app).\n\n\nQuestion, is #1 Azure ML Workspaces still an option for LDA but I can neglect the registering of a model or should I rather go with #2 a Python LDA cloud function or #3 an App Service with Python environment + Flask as a backend to handle the routing of my tinny API internally?\n\n\nThanks!",
"date": "2020-05-20"
},
{
"vote": 3,
"title": "Topic Modelling for a Conversation",
"text": "I would like to begin with appreciating the community here and for all the information everyone keeps posting. It is amazingly useful for learners like us. \n\n\nI am recently looking at a problem where a call conversation has to be assigned a specific topic. The call conversation would be transcribed to text, but then needs to be assigned a particular topic in connection to the conversation. \n\n\nI have looked on traditional topic modelling methods and found that they are more suitable for long text with a coherent structure whereas a conversation can often go into any direction. \n\n\nI wanted to know what are the different ways I can approach this problem or if there is SOTA methods out there that I am not aware of. \n\n\nI think there are 2 ways - look at sentence-level and then look at the conversation level.\n\n\nI read somewhere that it is better to use word embeddings in place of simple words to maintain the contextual information of the conversation. \n\n\nI can always go for topic classification but that would require a large number of annotated labels.   \n\n\nPlease feel free to ask me more information if the question is not clear. \n\n\nThanks :)",
"date": "2020-05-20"
},
{
"vote": 2,
"title": "How to calculate the alignment between BERT and spaCy tokens effectively and robustly",
"text": "https://gist.github.com/tamuhey/af6cbb44a703423556c32798e1e1b704\n\n\n&#x200B;\n\n\nThis is a blog post that explains the underlying algorithm of my library tokenizations (\nhttps://github.com/tamuhey/tokenizations\n)",
"date": "2020-05-19"
},
{
"vote": 8,
"title": "Day 140 of #NLP365 - NLP Papers Summary â€“ Multimodal Machine Learning For Automated ICD Coding",
"text": "Day 140.\n\n\nToday's post is a 7-minute summary of the NLP paper \"Multimodal Machine Learning For Automated ICD Coding\". It covers different ML models used for unstructured, semi-structured, and structured datasets in healthcare and how the output of these ML models are combined for predicting ICD coding. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/19/day-140-nlp-papers-summary-multimodal-machine-learning-for-automated-icd-coding/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-19"
},
{
"vote": 1,
"title": "How to train a BERT model from scratch?",
"text": "[deleted]",
"date": "2020-05-19"
},
{
"vote": 2,
"title": "Internal Content Indexing NLU Service Now Available on the CityFALCON API",
"text": null,
"date": "2020-05-19"
},
{
"vote": 1,
"title": "Use of typeface as a feature",
"text": "Are there examples of research papers or projects where the typeface (bold, italic, underline, capitalization, etc) is also used as a feature for NLP tasks? For example, certain formatting styles like bold font or underlined words are used to give hints to the human reader (i.e. they add value to the meaning of the text), but I have not seen any work that utilizes these features to improve the accuracy of the task. Most of the preprocessing strips away these aspects, so I'm wondering if anyone has tried encoding this information in some way to improve accuracy on a particular task.",
"date": "2020-05-19"
},
{
"vote": 1,
"title": "Multi labeling with snorkel and good ways to write Labeling Function?",
"text": "Has anyone done multi labeling with snorkel? If so, please shed some light in that regard. Also, can someone tell me some general fair ways to write labeling functions for snorkel?",
"date": "2020-05-19"
},
{
"vote": 3,
"title": "Need guidance on semi-supervised NER",
"text": "I'm just starting out in NLP and ML in general, so I'm sure I probably didn't phrase the title correctly. My task is extracting attribute-value pairs from e-commerce product titles. I have a set of attributes to extract, and dictionaries (data sets) for each of them.\n\n\nExample:\nProduct title:\n \"MSI Prestige 14-inch \nCore\n Comet Lake 10th gen. \ni7 7700U\n \nGTX 1650 (max-q)\n gaming laptop 1920x1080\"\nAttributes to extract:\n [\"CPU\", \"GPU\"]\nDictionary for \"CPU\"\n (exhaustive list of all CPU model names): [\"AMD Ryzen 3600x\", \"Intel Core i3-3500H\", \n\"Intel Core i7-7700U\"\n, \"Intel Xeon E550\", ...]\nDictionary for \"GPU\"\n (exhaustive list of all CPU model names): [\n\"NVidia GeForce GTX 1650 Max-Q\"\n, \"AMD Radeon Pro Vega III\", \"NVidia GeForce GTX 1080\", ...]\n\n\nI need to find a (fuzzy, obviously, as there are word omissions, insertions, char. deletions, etc.) match from each dictionary in the string.\n\n\nHow would I go about this? What topics should I look into, how to search for them? All of the papers I've found cover more advanced topics of unsupervised attribute extraction.\n\n\nThanks!",
"date": "2020-05-19"
},
{
"vote": 14,
"title": "Making Sense of Unstructured Text Data",
"text": "I recently came across the company Quid after its merger with Netbase. Their core specialization is in creating platforms that make sense of any unstructured text corpus with intuitive visualizations.\n\n\nhttps://technology.quid.com/2018/05/how-does-quid-create-reliable-business-intelligence/\n\n\nI have worked with NLP for quite some time now. I've tried out several Topic Modeling approaches for this as well. But what still amazes me is their claim of a framework that is capable of deciphering the number of topics and labeling them in a completely unsupervised manner. They claim to work with online review data, community threads, company descriptions, patents, news data - basically a wide range of data sources. Is it possible to have such a framework that works with a decent accuracy on such a general basis?\n\n\nI have these specific questions.\n\n\n\n\nHow do they come up with the number of topics in an unsupervised manner? Approaches like HDP which infer the number of topics on their own have not been up to the mark in my experience. Also doing a grid search on model perplexity doesn't seem neat to me. Would that yieod good results though?\n\n\n\n\nHow do they assign labels in an automated way? For this, the topics have to be very coherent and their has to be some supervised learning involved for quality topic labels.\n\n\n\n\nWhat is their overall approach with topic modeling? LDA is by far the most popular of all algorithms and I'm assuming it's most probably a variation of this that they're using. Could it be that they have some ingenious, proprietary approach towards topic modeling?\n\n\n\n\n\n\nI'm not really interested in the visualization though. I'd be more into finding if such great results are possible with current methods.\n\n\nI'm also aware of lda2vec, Stochastic Block Models, and Guided LDA (to go the semi-supervised way). Are there any other innovations in this space that I should be aware of?\n\n\nAny help with this is appreciated! Thanks!",
"date": "2020-05-19"
},
{
"vote": 1,
"title": "Internal Content Indexing NLU Service Now Available on the CityFALCON API",
"text": "[deleted]",
"date": "2020-05-18"
},
{
"vote": 3,
"title": "Sentiment Analysis in Python with NLTK. 10 Videos ~ 1hour",
"text": null,
"date": "2020-05-17"
},
{
"vote": 0,
"title": "Good and Fast Sentiment Analysis model",
"text": "Does anyone know any model which is small and fast (easy to run on cpu , without gpu) and still performs better? I guess transformers' sentiment analysis models will take long and will require gpu. But if there is a workaround for tranformers' models then please let me know.\nBy transformers model I mean any transformer model (Bert, xlnet etc)",
"date": "2020-05-17"
},
{
"vote": 38,
"title": "A Visual Survey of Data Augmentation in NLP",
"text": "Hi everyone,\n\n\nI have written a blog post visualizing current data augmentation techniques for NLP. Please read it and share your feedback.\n\n\nArticle: \nhttps://amitness.com/2020/05/data-augmentation-for-nlp/\n\n\nIf you're aware of other augmentation techniques, please let me know in the comments.",
"date": "2020-05-17"
},
{
"vote": 2,
"title": "[Project] Self host-able production ready Inference pipelines, free to use APIs for some NLP use-cases.",
"text": null,
"date": "2020-05-17"
},
{
"vote": 3,
"title": "Day 138 of #NLP365 - NLP Papers Summary â€“ Neural Approaches To Conversational AI â€“ Conversational AI In Industry",
"text": "Day 138.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Neural Approaches To Conversational AI â€“ Conversational AI In Industry\".\n\n\nToday's post continues on summarising the survey paper on neural methods for conversational AI. In today's post, we will cover the different conversational AI that's currently being used in the industry. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/17/day-138-nlp-papers-summary-neural-approaches-to-conversational-ai-conversational-ai-in-industry/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-17"
},
{
"vote": 6,
"title": "Help understanding LSTM",
"text": "[removed]",
"date": "2020-05-17"
},
{
"vote": 0,
"title": "Need a co-author for EMNLP2020 paper.",
"text": "Hi, I am writing an article for EMNLP 2020. It is due on July 1st. Would you like to co-author it with me?",
"date": "2020-05-17"
},
{
"vote": 3,
"title": "NER Phrase Level Precision/Recall Partial Annotations Question",
"text": "Hey guys,  \n\n\nI have a question regarding scoring phrase level annotations using the MUC standard. The MUC standard uses Correct, Incorrect, Partial, Missing, and Spurious to calculate Precision, Recall, and eventually F1.  \n\n\nMy question is, if an annotation is long enough that you have two partial annotations for a single full annotation, how do you count that? Do you merge them and just count them as a single annotation? Do you count them as two? \n\n\nAlso, if you have a partial annotation that extends between two annotations, IE, end of one and beginning of another, how do you score that? \n\n\nThank you for your time!",
"date": "2020-05-17"
},
{
"vote": 1,
"title": "New python library for variable-size data serialization",
"text": null,
"date": "2020-05-16"
},
{
"vote": 7,
"title": "How do I do Lemmatization using machine learning?",
"text": "Am working on lemmatization for Afaan Oromo language it's a resource poor African language. \nI've built a rule based lemmatizer but the accuracy is only 67%\n\n\nI was wondering if I could use machine learning algorithms to do the Lemmatization.\nThe good thing about the language is that it's not morphologically complex and I've already prepared a lemma-inflected_derivation pair file containing 100,000 instances. \nHow do I approach the Lemmatization?",
"date": "2020-05-16"
},
{
"vote": 1,
"title": "Day 137 of #NLP365 - NLP Papers Summary â€“ Neural Approaches To Conversational AI â€“ Social Botâ€™s Landscape",
"text": "Day 137.\n\n\nToday's post is a 4-minute summary of the NLP paper \"Neural Approaches To Conversational AI â€“ Social Botâ€™s Landscape\".\n\n\nToday's post continues on summarising the survey paper on neural methods for conversational AI. In today's post, we will cover the overall social bot's landscape which includes datasets, evaluation metrics, and open benchmarks. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/16/day-137-nlp-papers-summary-neural-approaches-to-conversational-ai-social-bots-landscape/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-16"
},
{
"vote": 4,
"title": "Auto Labelling with known labels on unsupervised data",
"text": "I am quite new to NLP. I have a dataset of customer queries related to a certain application. How do I proceed to do auto-labelling on the texts in line with labels I already have. I tried out LDA but the topics I've gotten are not in accordance with the labels I have.\nI wanted to try Guided LDA but I don't know how to obtain the seed words for it. Are there any other Variants of LDA or non-LDA approaches that can solve the auto labelling problem?\n\n\nEDIT 1: The topics I've gotten with LDA are distinguished based on the features of the application which are quite commonly used in all the labels I have.",
"date": "2020-05-15"
},
{
"vote": 1,
"title": "Are there any cases where a classical ML algorithm outperforms BERT?",
"text": "[removed]",
"date": "2020-05-15"
},
{
"vote": 1,
"title": "I know how to get WER of transcribed text based off of audio, but how do you get WER of audio based off of text?",
"text": "[removed]",
"date": "2020-05-14"
},
{
"vote": 14,
"title": "[D][R] Insight from AllenAI on a possible advantage of Bert over RoBertA. \"The NSP pre-training task seems to be particularly important for effective transfer to relevance ranking\"",
"text": null,
"date": "2020-05-14"
},
{
"vote": 1,
"title": "CRIM at SemEval-18 - Hypernym Discovery ( Paper Summary )",
"text": "I have summarised the paper titled as CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery. The authors were winner to 3 of 5 sub tasks. Read at - \n\nhttps://prakhartechviz.blogspot.com/2020/05/crim-at-semeval-2018-task-9-hybrid.html",
"date": "2020-05-14"
},
{
"vote": 1,
"title": "Prepare data for NER model with BIO annotation on domain-specific task",
"text": "Note: Training a custom NER model for domain-specific need not looking for pre-trained models.\n\n\nAssume the task is extracting important facts for resume like a candidate skills and his education. First, I'll obtain skills and education data from various online websites, job portals etc..,\n\n\nNow I have two text files and in each file every row represent a skills or a university name like below,\n\n\nskills.txt \n___________\nc \npython \njava\nnode js\n\n\n\n&#x200B;\n\n\neducation.txt \n___________\nmassachusetts institute of Technology \nharvard university \n\n\n\nHow can I annotate this data with BIO annotation? I want to know if these be enough to be able to train a named entity recognition model to recognize skills and education for raw resume text. I've read somewhere that we require some context along with the word for NER model to learn better. Like this example below, \n\n\nskills.txt \n___________\nc is used at facebook\npython is my favorite programming language\n\n\n\nBut I don't know how to access such data for my resume-extraction domain. How do I proceed further? What should I do? How to build an effective NER model for my resume facts identification domain-specific task? Any inputs/suggestions would really help. Thank you",
"date": "2020-05-14"
},
{
"vote": 1,
"title": "How great AI can make the mapping of Big Data a much simpler task",
"text": "In today's world we are experiencing an explosion of information (both real and fake) while our attention span remains the same.\n\n\nInLoop industry expert Yasha Neiman, will explain how we can harness machine learning algorithms to bring order to the chaos - How can we crawl the internet and bring only the relevant content using Artificial Intelligence.\n\n\nIn his last role Yasha Neiman was the CTO of InLoop, a company that automatically creates news feeds using Natural Language Processing (NLP) algorithms on large scale.\n\n\nYasha has more than 13 years of industry experience - from data engineering to leading data science teams. Yasha graduated from the Technion with a B.Sc in industrial engineering and management, information systems.\n\n\nThe LIVE webinar will start at 19:00 IDT (12pm EST) - \nRSVP FOR THE EVENT NOW!\n\n\n&#x200B;\n\n\nThe secret to mapping big data with Artificial Intelligence",
"date": "2020-05-14"
},
{
"vote": 1,
"title": "Sentiment analysis data preparation",
"text": "I have scraped the required data which is basically corpus of text from reddit. Now I need to make a sentiment analysis classifier from this data but the problem is there is no labelled data(because I haven't annotated the corpus as positive/negative/neutral). How do I label the data through a script?",
"date": "2020-05-14"
},
{
"vote": 2,
"title": "[D] Anyone interested in using NLP for making search engines for Covid-19 papers? We have a casual group working on this, looking for others who have an interest in IR / scientific texts.",
"text": null,
"date": "2020-05-13"
},
{
"vote": 2,
"title": "Which model captures the best word representations for small to medium texts when pre-training?",
"text": "I don't know if there is a benchmark or a way to evaluate how well the model learns from the pretraining dataset.  All scores are based on finetuning tasks but what about the NLU part?\n\n\nIf I want to pre-train on a small dataset, which currently available model has shown to have the best language understanding capability? Especially if I'm not pre-training on wikipedia dumps but sentences and paragraphs from conversations & comments? (I'm working on low resource languages where online data is VERY sparse)",
"date": "2020-05-13"
},
{
"vote": 8,
"title": "Website AMA Chrome Extension",
"text": "&#x200B;\n\n\nhttps://reddit.com/link/gj4r7p/video/mhdgfhk7tky41/player\n\n\nAsk Natural Language Questions on any Website. \n\n\nMade using the Huggingface Transformers Library and TensorflowJS.\n\n\nCheck out here :\n\n\nhttps://www.producthunt.com/posts/website-ama",
"date": "2020-05-13"
},
{
"vote": 1,
"title": "Day 134 of #NLP365 - NLP Papers Summary â€“ Neural Approaches To Conversational AI â€“ NLG And E2E",
"text": "Day 134.\n\n\nToday's post is a 4-minute summary of the NLP paper \"Neural Approaches To Conversational AI â€“ NLG And E2E\".\n\n\nToday's post continues on summarising the survey paper on neural methods for conversational AI. Specifically, we are focusing on the natural language generation and end-to-end dialogue systems. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/13/day-134-nlp-papers-summary-neural-approaches-to-conversational-ai-nlg-and-e2e/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-13"
},
{
"vote": 6,
"title": "Fine-tuning BERT for Binary Classification",
"text": "Hello guys, I wanted some advice while fine-tuning BERT on the task of Binary Classification.\n\n\nmodel = BERTForSequenceClassification (BERT-base-uncased)\n\n\ninput_max_size = 256, batch_size = 16, epochs = 6, lr = 2e-5\n\n\nI have training data of 32k documents and spilt 10% of it into dev set.\n\n\nHowever, I am getting pretty bad results: On the dev set I am getting 59% accuracy which is not even improving after further epochs. I think the model is unable to learn anything from fine-tuning.\n\n\nCan you suggest me what might be the problem?",
"date": "2020-05-13"
},
{
"vote": 1,
"title": "High impact adjectives (dataset?)",
"text": "I'm look for a resource, namely, a list of high impact adjectives. For example: Great, terrible, fresh, dirty, broken, tasty, etc.\n\n\nI wouldn't be surprised if such a thing already existed. But if not, what are some good strategies for identifying these words? (perhaps training a sentiment analysis model and examining the variables that are associated with highest and lowest sentiment scores.)",
"date": "2020-05-12"
},
{
"vote": 8,
"title": "Looking for a post - Using GPT-2 to generate all possible responses to a prompt",
"text": "I think it was in this subreddit in the last week. I'm looking for a post that had a GitHub repository where they had used GPT-2 to take a prompt, and then generate all possible responses with X likelihood. Anyone know what I'm talking about?",
"date": "2020-05-11"
},
{
"vote": 47,
"title": "Spark NLP 2.5.0: ALBERT &amp; XLNet transformers, state-of-the-art spell checker, multi-class sentiment detector, 80+ new models &amp; pipelines in 14 new languages &amp; more",
"text": null,
"date": "2020-05-11"
},
{
"vote": 3,
"title": "Insights on the industry right now?",
"text": "I'm starting to write personal statements to study NLP at different universities. I would like to include what the state of the industry is right now so as to find a way to tie in what I want out of the master.\n\n\nThis is hard to find out if you are not currently working in the industry. Could any of you give me your opinion on where the market is going and what is most valued right now? Any insights will help.\n\n\nThank you!",
"date": "2020-05-11"
},
{
"vote": 2,
"title": "Comparing skipgram and cbow performance",
"text": "[deleted]",
"date": "2020-05-10"
},
{
"vote": 1,
"title": "Are there any pre-trained cross-lingual LM available?",
"text": "Recently I have trained a classifier with USE (multi-lingual) using the news category dataset in English, then tested it on  a few Portuguese articles and it worked surprisingly well.\n\n\nSince I can't find many annotated datasets in PT, I wanted to explore using training cross-lingual language models on English data, but this time at token level.\n\n\nLike USE (XL) I assume the vectors must be aligned for words in different languages for this to work, right?\n\n\nAny recommendation on what to use?\n\n\n&#x200B;\n\n\nAnother question, I have been exploring Hugging Face's transformers, and I can't seem to find information (paper or blog) on this one here:\n\n\nhttps://huggingface.co/transformers/multilingual.html#xlm-without-language-embeddings\n\n\nLike BERT, I get the tokens and embeddings, but I'm not sure how to use them. In the case of BERT, I have been googling and sometimes I hear the [CLS] should be used for classification (of the whole sentence), or use the the embedding of the starting token (for word level), or the average of the word's tokens. And then re-average word-level embeddings for sentence representation.\n\n\nThis XLM model seem to yield </s> tokens at the start and end of text inputs, which I assume they're the analogous to the [CLS] and [SEP] tokens. Also noticed the tokens use trailing \"</w>\" instead of prefixing \"##\" on the next token. So I wonder if I should still use the first token of a word or the last now.\n\n\nAnyways, without wasting time experimenting by trial and error, I'd like to know how am I supposed to use the embeddings, what the authors recommend.",
"date": "2020-05-09"
},
{
"vote": 29,
"title": "A Commit History of BERT and its variants",
"text": null,
"date": "2020-05-09"
},
{
"vote": 4,
"title": "How to detect if the user is talking about the same topic in his/her posts?",
"text": "[Actual problem statement]\n\n\nFor example and account posts about the same topic for at least 10 posts, shifts to a new topic for at least 10 posts.\n\n\n&#x200B;\n\n\nHow can I deal with the statement above?",
"date": "2020-05-09"
},
{
"vote": 1,
"title": "Looking for resources for an overview of NLP",
"text": "Sorry if this isn't the right place for this question! \n\n\nI'm doing a report about NLP and I was wondering if anyone here could point me towards any good books or websites to draw from. I'm only in my first semester so it's nothing too in depth, I'm hoping to do a sort of overview of the topic and talk about some of its importance/influence.",
"date": "2020-05-09"
},
{
"vote": 1,
"title": "Best available n-gram model to implement in python?",
"text": "[deleted]",
"date": "2020-05-09"
},
{
"vote": 4,
"title": "Relying on NLTK, Stanza, SpaCy, Gensim too much?",
"text": "Iâ€™m starting to think that between NLTK, stanza, SpaCy and Gensim, which are all very user friendly, tensorflow and PyTorch are just a pain to use. \n\n\nSure, if you need to train a NER tagger to find custom entities and you have a substantial volume of data, TF and PT will work great. But if you need to make use of existing models, SpaCy and the like might be more than sufficient.\n\n\nIâ€™m a little skeptical tho. Thereâ€™s so much hype around TF and PT that I feel guilty not spending hours/week leveling up there. \n\n\nYet most of the time, I really donâ€™t need that kind of firepower. Seems like I can do more and faster by using the subj line alternatives.\n\n\nThoughts?",
"date": "2020-05-08"
},
{
"vote": 1,
"title": "How to reduce BERT attention span?",
"text": "I'm using BERT for a specific task where I need only the first three layers and a smaller attention span. Facebook released an \nadaptive attention span\n paper but it seems that this is needed only for the higher layers, and I won't be dealing with them. In my case, I believe I could limit the attention span to a fixed value, which would be more simple.\n\n\n However, I'm still quite clueless about how to actually implement that either on the original BERT repository or the PyTorch alternatives ( such as the HuggingFace library ). I have read the source code for BERT but I still haven't understood how to do that.",
"date": "2020-05-08"
},
{
"vote": 15,
"title": "NLP for Argument Analysis?",
"text": "Hi All,\nI'm going to start a data science graduate program in the fall. I'm interested in doing research in NLP.\n\n\nI was wondering if anyone has any thoughts on using NLP to classify an argument style to point out logical fallacy.\n\n\nI'm motivated to build a tool that will provide a critical thinking crutch, that is arguably necessary in apost truth world.  \n\n\nIf anyone has research papers to point out please do!\n\n\nThanks.",
"date": "2020-05-08"
},
{
"vote": 1,
"title": "[R] AI2 &amp; UW Researchers Propose â€˜TL;DRâ€™ Paper Summarization Task",
"text": "[removed]",
"date": "2020-05-08"
},
{
"vote": 1,
"title": "Day 129 of #NLP365 - NLP Papers Summary â€“ Neural Approaches To Conversational AI â€“ KB-QA (Neural Methods)",
"text": "Day 129.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Neural Approaches To Conversational AI â€“ KB-QA (Neural Methods)\".\n\n\nToday's post continues on summarising the survey paper on neural methods for conversational AI. Today's post covers the neural methods for KB-QA. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/08/day-129-nlp-papers-summary-neural-approaches-to-conversational-ai-kb-qa-neural-methods/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-08"
},
{
"vote": 3,
"title": "How to use machine learning to improve performance of dictionary-based analysis?",
"text": "I have a dictionary of words rated for an attribute, concreteness. Right now, I analyze language by just averaging the concreteness scores for all the words in the text. The issue is that just averaging the scores for all the words doesn't capture well the perceived concreteness, because some words matter more than others. \n\n\nI have a second dataset of several thousands of paragraphs rated as either concrete or abstract. How could I train a model to detect what words matter the most to predict concreteness? Any recommendation on how to do that in Python?",
"date": "2020-05-08"
},
{
"vote": 1,
"title": "QA, NER, or other means to extract complaints from reviews?",
"text": "Recently, I've seen a lot of tutorials floating around, like predicting what rating an amazon product review should receive given the text supplied. While this is a great way to build some familiarity with ML, I'm not certain that's profitable in and of itself. \n\n\nSomething that might be actually profitable is - being able to summarize the common complaints from a series of review. In the context of Yelp reviews, this might take the form of ~ {servive: slow 50%, food: cold 20%}. Where the actual language used across reviews will vary, but the core \"complaints\" can be extracted.\n\n\nIs there a state of the art solution already out there? Is this a NER task, QA, summarization, etc? (Or maybe a combination of these tools coupled with some hard-coded logic using dynamic programming.)",
"date": "2020-05-07"
},
{
"vote": 0,
"title": "Free Webinar on Introduction to Natural Language Processing",
"text": "[deleted]",
"date": "2020-05-07"
},
{
"vote": 6,
"title": "News Topics Modeling",
"text": "Hi All,\n\n\nI wanted suggestions for the problem I am working on. I am not very experienced in this so please hear me out.\n\n\nOk. I have been working on classifying topics to news articles. I understand that LDA is normally used for unsupervised topic modeling, but for my case I already have predefined topics and bigrams for these topics which come from parliament debates. Now using these topics and topic bigrams, I want to classify another set of data, which is news articles into topics.\n\n\nHow do I go about doing this?\n\n\nMy initial idea was to run LDA on the news articles with fixed topic numbers and then map these topics to the predefined topics that I already have but it didn't work out very well.\n\n\nWould be very helpful if someone has experience with similar problem and can help me out.\n\n\nThanks.",
"date": "2020-05-06"
},
{
"vote": 1,
"title": "Identical results with Cased and Uncased BERT?",
"text": "So I am using cased and uncased versions of BERT for generating features from text and the classification results are identical.\n\n\nI think it makes sense but does anyone else also has similar observations.",
"date": "2020-05-06"
},
{
"vote": 3,
"title": "Anyone worked with ELECTRA? code is driving me crazy",
"text": "I've pre-trained ELECTRA on my own dataset, and I can't find a way to:\n\n\n- use the trained discriminator to test if a new sentence I give passes as 'original' or 'replaced' \n\n\n- get embedding of the vocabulary \n\n\nThe code is so dense and entangled and i've been at this for a week now, anybody managed to do it?",
"date": "2020-05-06"
},
{
"vote": 5,
"title": "Detect if a person is sharing personal information (self).",
"text": "I'm working on a problem that has a module to detect if a person is sharing any personal information in a twitter post. I tried regular expressions etc.\n\n\nWhat are the possible ways to tackle this problem?",
"date": "2020-05-06"
},
{
"vote": 14,
"title": "Are there any situations where Bert is preferred over Roberta? Is Bert obsolete?",
"text": "They're the same architecture, and the most notable difference was that Roberta was trained on a ton more data. \n\n\nI am wondering if Bert is obsolete, or if there are some circumstances where original Bert is preferred.",
"date": "2020-05-06"
},
{
"vote": 0,
"title": "Generate full-length articles with AI",
"text": null,
"date": "2020-05-06"
},
{
"vote": 1,
"title": "Building gold corpus manually: hypothesis test",
"text": "I am trying to build a gold corpus manually on twitter tweets data to tag named entities. I have a question about how to test my hypothesis. My hypothesis is that manually tagging named entities will create a gold corpus with perfect inter-rater agreement using kappa value.\n\n\nAny help will be great.",
"date": "2020-05-05"
},
{
"vote": 2,
"title": "Is it possible to use a language model like GPT-2 to generate word, sentence, paragraph, or document vectors?",
"text": "Maybe this is a bit of a weird question but it seems to me that language models like GPT-2 seem to have a surprisingly good grasp of language and I was wondering if that could be harnessed to generate more precise vectors somehow",
"date": "2020-05-05"
},
{
"vote": 3,
"title": "[Question] How to generate text using n-grams",
"text": "[deleted]",
"date": "2020-05-04"
},
{
"vote": 1,
"title": "ERNIE",
"text": "I am new to NLP, and found BERT to be extremely useful. In my search of replacing [MASK] tokens, however, I came across ERNIE (phrase-level masking). There is little on GitHub relating to it, and I was curious as to why.",
"date": "2020-05-04"
},
{
"vote": 62,
"title": "April 2020 NLP Papers: Reading List",
"text": "I have compiled a list of NLP papers published on arXiv last month that I enjoyed reading: \nApril 2020 NLP Papers: Reading List\n\n\nHope some of you find this helpful ðŸ˜Š \nLet me know if you think an important paper is missing!",
"date": "2020-05-04"
},
{
"vote": 2,
"title": "Day 125 of #NLP365 - NLP Papers Summary â€“ A2N: Attending To Neighbors For Knowledge Graph Inference",
"text": "Day 125.\n\n\nToday's post is a 4-minute summary of the NLP paper \"A2N: Attending To Neighbors For Knowledge Graph Inference\".\n\n\nToday's NLP paper summary covers a new NLP area - knowledge graph. It uses neighbouring nodes and relations to build a query-dependent entity embeddings for better tackle the knowledge graph completion task. It achieves SOTA results in one of the evaluation datasets. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/04/day-125-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-04"
},
{
"vote": 18,
"title": "Texthero for text preprocessing and representation",
"text": "Given a tabular data, it's easy to understand the underline data. Just open Pandas, read the csv and with some basic commands such as \ncount_values\n, \nagg\n, \nplot.bar()\n, get some good understanding of the dataset.\nIn contrast, given a text-based data, it's harder to quickly \"grasp the data\". Right? \n\n\nSince a while, I'm working on a tool to quickly preprocess text-based data, represent it (with TF-IDF for instance) and visualize it (scatterplot for instance). The idea is to simplify and accelerate the development of the default pipeline that almost any NLP task requires.\n\n\nSuch a tool is called \nTexthero\n. The strength of it is that it's super easy to understand it and it comes with great documentation: \nGetting Started\n. Clearly, everything is under constructions, but you get the idea if you read the README in the Github repo and the documentation.\n\n\nNow I need your feedback to understand if it may be useful for some of you and, if yes, how I can improve it. Is there anyone with willing to have a look at it and report back any advice and comment? Thank you in advance, your opinion is crucial!",
"date": "2020-05-03"
},
{
"vote": 4,
"title": "Day 124 of #NLP365 - NLP Papers Summary â€“ TLDR: Extreme Summarization Of Scientific Documents",
"text": "Day 124.\n\n\nToday's post is a 6-minute summary of the NLP paper \"TLDR: Extreme Summarization Of Scientific Documents\".\n\n\nToday's paper introduced a new summarisation task, the TLDR generation task for scientific documents. It released a new dataset SCITLDR for the TLDR generation task and has fine-tuned BART to provide baseline results for this dataset. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/03/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-03"
},
{
"vote": 8,
"title": "Size of Wikipedia",
"text": "I tried downloading Wikipedia's JSON dump, which is a 50Gb bz2 file but it claimed that 220Gb is not enough space to decompress it. I'm wondering if anyone knows how big it is.",
"date": "2020-05-03"
},
{
"vote": 42,
"title": "Solving challenging NLP tasks from just 10 examples",
"text": "I recently wrote a paper showing how pretrained LMs can learn from as little as 10 labeled examples: \nhttps://arxiv.org/abs/2001.07676\n - I would be happy about any feedback :)\nIn case you are interested, the source code is available at \nhttps://github.com/timoschick/pet",
"date": "2020-05-01"
},
{
"vote": 10,
"title": "Day 122 of #NLP365 - NLP Papers Summary â€“ Applying BERT To Document Retrieval With Birch",
"text": "Day 122.\n\n\nToday's post is a 3-minute summary of the NLP paper \"Applying BERT To Document Retrieval With Birch\".\n\n\nToday's summary is a short one (and in a new area), applying BERT to document retrieval. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/05/01/day-122-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch/\n\n\nBest,\n\n\nRyan",
"date": "2020-05-01"
},
{
"vote": 4,
"title": "Tokenization for words not in the original vocabulary?",
"text": "[deleted]",
"date": "2020-05-01"
},
{
"vote": 7,
"title": "Implement transformers simply using simpleTransformers",
"text": "Modern deep nlp doesn't have to hard. The first step in democratizing AI is making sure that barrier of entry isn't high both in terms of cost and technical knowledge. Modern NLP models are both computationally demanding and complex in terms of operations. But great open source work by various teams are simplifying the process of training and deploying these models at scale. \n\n\nI wrote this report using that explores training transformers on common NLP tasks. It can be viewed as a repo containing most common NLP tasks and visualisations. Here's the link to the report - \nhttps://app.wandb.ai/cayush/simpletransformers/reports/Using-simpleTransformer-on-common-NLP-applications---Vmlldzo4Njk2NA",
"date": "2020-05-01"
},
{
"vote": 1,
"title": "Is sentiment analysis across translations possible?",
"text": "As an amateur data scientist, I thought it would be fun as a personal project to try to use sentiment analysis to compare a game's (Final Fantasy VI) original, Japanese script against its official translation and a fan's translation. I thought it would be interesting to use quantitative data to discuss how and why a fan might decide to take this task upon themself, and to see how the sentiment (or other interesting, quantifiable data) in both translations compare with the original. However, I don't know Japanese, and am not sure if it's even theoretically sound to compare sentiment across languages. Is this something worth taking on? Or would I be better off just comparing the sentiment across the English translations.",
"date": "2020-04-30"
},
{
"vote": 1,
"title": "Day 121 of #NLP365 - NLP Papers Summary â€“ Concept Pointer Network For Abstractive Summarization",
"text": "Day 121.\n\n\nToday's post is a 6-minute summary of the NLP paper \"Concept Pointer Network For Abstractive Summarization\".\n\n\nPointer-generator network was one of the breakthroughs in summarisation that allows models to effectively deal with OOV, by allowing them to copy from source documents. Today's paper extend this architecture and introduced concept pointer network. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-30"
},
{
"vote": 7,
"title": "How to bring diversity in extractive summarization?",
"text": "I'm currently using textrank to get top 10 summary sentences from a document. But the sentences selected have some duplicates in them. Is there a way to get more diverse results from textrank?\n\n\nNaively I've thought of using cosine similarity to filter out duplicates after getting the rankings. Are there better ways to approach this problem?",
"date": "2020-04-30"
},
{
"vote": 0,
"title": "Winds of Change: Robot Writers Take the Web",
"text": "[removed]",
"date": "2020-04-30"
},
{
"vote": 2,
"title": "Question about Apertium- let me know if there's a more suitable sub to ask this",
"text": "So from what I've been able to grasp of Apertium, it works in terms of words, and a dictionary contains an alphabet that specifies what characters can appear in its words. But how would one go about handling languages like Chinese and Japanese where the set of characters that can in principle be used in a document has no clear limit other than what's in Unicode? That is, there are tens of thousands of Chinese characters that could in principle appear in a document. Would you just have to paste in the whole Unicode Han section? And how would it handle languages with no word spacing like Japanese, Chinese, and Thai?",
"date": "2020-04-30"
},
{
"vote": 2,
"title": "Free Online Resources for Data Scientists During COVID-19",
"text": null,
"date": "2020-04-30"
},
{
"vote": 5,
"title": "Run my calls through sentiment analysis! How?",
"text": "I am a newbie to natural language processing. I would like to run a bunch of calls that I get from my customer through a NLP and perform sentiment analysis on these calls and submit the results back to the client. I've seen a few services that do that like observe.ai.\n\n\nCan someone outline the high-level steps+ services/packages that would be required to build an in house solution?\n\n\nThanks!",
"date": "2020-04-29"
},
{
"vote": 38,
"title": "Facebook releases its 'Blender' chatbot as an open-source project",
"text": null,
"date": "2020-04-29"
},
{
"vote": 3,
"title": "Is it possible to convert regular text to a .story file?",
"text": "The \nDMQA Dataset\n contains news articles from CNN and Daily Mail in .story format. How can I convert a regular text data to .story format? Thank you.",
"date": "2020-04-29"
},
{
"vote": 3,
"title": "How do I measure redundancy of a text",
"text": "Dear All\n\n\nHow do I measure the redundancy of of sentences in text?\n\n\n&#x200B;\n\n\n#1 sentence about Apple\n\n\n#2 sentence about Orange\n\n\n#3 sentence about Apple",
"date": "2020-04-29"
},
{
"vote": 4,
"title": "Seeking your thoughts on building a personalized AI tutor that makes heavy use of various NLP techniques",
"text": "Hi all, \n\n\nI'm writing a piece on \"Digital Aristotle\", which is basically speculation about how to create an AI tutor capable of giving personalized learning. It's definitely not an Advanced General Intelligence or anything like that. Rather, it's an ensemble of functions, so I'll list the core ones below and will hopefully get your feedback on them. \nDo you think they're realistic? Do you know ways to implement them? What other functions do think would be included in this ensemble?\n\n\nSo, here they are:\nFirst, make a model that can keep track of a large set of attributes for a given student. These attributes are then used based off their ability to contribute towards a reward function, let's say a test score. Two examples: an attribute that shows student learns well from analogies and an attribute that shows the student learns well by hearing rhyming mnemonics. So, using a predictive model that uses these attributes as input, the AI tutor would have the ability to curate content from a wide variety of online sources. Given that there's so much content out there to learn from, this curation could be precise and rapid, taking snippets and samples from numerous courses, lectures and quizzes, and giving the student the material that is most suited to them based off the predictive model discussed earlier. The biggest problem I see with this approach is the difficulty of meta-tagging content. And the solutions I can envisage are things like a\n Snorkel approach\n, using technical methods that offer automated but suboptimal labelling. Alternatively, there could be a joint effort where people attached to the course are invited or even paid to do the tagging. This could also work as a Mechanical Turk approach, where people are paid nominal sums to hand-label the data for all of these courses.\n\nDo you think it's too unrealistic to have this curation algorithm and if not, how do you suggest one could create it?\n\n\nThe next function is the ability to reduce the complexity of a text. It could take in say, a University level Physics chapter and successfully reword the material to a level of say, a high-schooler. Of course, there will be things like equations which can't be simplified, but all natural text in jargon and complicated language\ncan be reworded and simplified. The aim of this is to help students understand concepts at a level more suited to them as individuals. So, I think the biggest problem with this is contextual modelling. You can't just use Sysnet to do a synonym lookup, as you'll end up turning the content into absolute nonsense. You require context and a consistent thread between individual words, sentences and paragraphs. I can think of models like GPT-2 and BERT, which have some okay results with contextual modelling. Are there other ways of doing this text complexity reducer? An amalgamation of n-grams, similarity lookups, word embedding, attention mechanisms, etc without necessarily relying on one particular model such as GPT2?\n\nHow can we make software that can reduce the complexity of a text, to make it suited to a particular individual, without losing too much of the meaning of the original text?\n\n\nIn a similar vein, a question answering system. Since this AI tutor is supposed to be highly-personalized, a student could in theory be learning about say, an essay on an obscure poem from the 16th century. Then, if they have a question about the topic, that requires a descriptive answer, the AI tutor should be able to answer it. Again, models like GPT2 have some success in question-answering systems, but not enough. \n\nWhat are methods could be used to achieve this question-answering system?\n\n\nGiven all that has been discussed, do you have any further ideas, suggestions and criticisms? Do you know of any other research going on in this field of interest? Let me know!",
"date": "2020-04-28"
},
{
"vote": 0,
"title": "Launching Scale Document, ML-augmented document processing.",
"text": "[removed]",
"date": "2020-04-28"
},
{
"vote": 2,
"title": "[R] New Study Tracks NLP Development and Direction Through a â€˜World Scopeâ€™",
"text": "[removed]",
"date": "2020-04-28"
},
{
"vote": 23,
"title": "Animal Crossing AI workshop -- Call for Abstracts ACAI 2020",
"text": "Animal Crossing Artificial Intelligence Workshop\n\n\nhttp://acaiworkshop.com/\n\n\nWe are announcing the first AI workshop hosted in Animal Crossing New Horizons. This is an experiment to see what it feels like to experience a workshop located in Animal Crossing. We would like to build a space for AI researchers to have meaningful interactions, and share their work.Â \n\n\nThis workshop is partially in response to the world in quarantine for Corona Virus. All academic conferences are now remote. One of the most valuable parts of conferences are the conversations and random interactions shared with colleagues. This is missing from most remote conferences. We hope to fill that void, by hosting a workshop in the virtual space of Animal Crossing, while having Zoom rooms where attendees can network and have conversations. The talks will be presented in a workshop area on an Animal Crossing Island. The actual audio, slide shows, and the virtual conference space will be live streamed to all attendees over Zoom.Â \n\n\n&#x200B;\n\n\nCall for Abstracts\n \n\n\nWe welcome abstract submissions from any domain of AI, however we highly encourage presentations in the following fields:\nâ€‹\n\n\n\n\nComputational models of narrative\n\n\nAutomatic speech recognition\n\n\nImage generationÂ \n\n\nNatural language understanding\n\n\nConversational AI\n\n\nComputer vision\n\n\nComputational creativity\n\n\nMusic information retrieval\n\n\nAutomatic musical understanding\n\n\nVideo game AI\n\n\n\n\nWe are highlighting these topics due to their relationship to Animal Crossing and interacting with virtual characters. These fields have the potential to affect the depth of the interactions between people and virtual characters in any context, be they Animal Crossing villagers, virtual companions, or even virtual teachers.Â   \n\n\nIf you are interested in submitting, please head over to the \nSubmit an Abstract\n page.\n\n\nhttp://acaiworkshop.com/submit-an-abstract.html\n\n\n&#x200B;\n\n\nPresentation Logistics\n \n\n\nEach presentation will be 15 minutes long, followed by 5 minutes of questions from the audience. There are two components to each presentation: 1) Your Animal Crossing character willÂ \ngive\nÂ the presentation in a workshop area on our workshop island. There will be workshop attendees on the island toÂ \nlisten\nÂ to your talk. 2) You will call into a Zoom room, and give your talk over video call. You can also share your screen if you wish to use slides or whatever visual materials you desire.Â   \n\n\nCoffee Breaks + Chance Interactions\nÂ Â â˜•â˜•â˜•â˜•  \n\n\nSince our desire is to replicate the social interactions of a real workshop, we will schedule coffee breaks into the workshop. We will have many different Zoom rooms so that smaller conversations can happen simultaneously. We want to provide a virtual space for you (the participant) to meet other researchers, and make meaningful connections.Â   \n\n\nOrganizers\n \n\n\nThis workshop is being organized by me, \nJosh Eisenberg\nÂ PhD. I am an NLU researcher who focuses on teaching computers to understand narrative and dialogue. I am currently the lead scientist in NLU at \nArtie Inc\n. I am putting this workshop together to build meaningful connections with other like-minded AI researchers, who also just happen to enjoy Animal Crossing.  \n\n\nIf you have any questions or feedback please contact me at: \njoshuadeisenberg@gmail.com\n \n\n\nDates\n \n\n\nDeadline for abstract submission: Friday June 12, 2020\nNotification of acceptance: Friday June 26, 2020\nWorkshop: Thursday July 24, 2020",
"date": "2020-04-28"
},
{
"vote": 6,
"title": "Day 118 of #NLP365 - NLP Papers Summary â€“ Extractive Summarization Of Long Documents By Combining Global And Local Context",
"text": "Day 118.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Extractive Summarization Of Long Documents By Combining Global And Local Context\".\n\n\nCombining sentence representations with global document representation and local topic segment representations to perform extractive summarisation. The architecture consists of 3 components. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/04/27/day-118-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-global-and-local-context/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-27"
},
{
"vote": 6,
"title": "Are there modules for unsupervised and semi-supervised part of speech tagging?",
"text": "Hey guys, I am working on part of speech tagging for Amharic (a language spoken in Ethiopia). I've used the modules in NLTK for supervised tagging(Brill and TNT) and I was wondering if there were other modules for unsupervised and semi-supervised tagging?",
"date": "2020-04-27"
},
{
"vote": 0,
"title": "NLP News Cypher | 04.26.20",
"text": "[removed]",
"date": "2020-04-26"
},
{
"vote": 2,
"title": "Are there any pre-trained character embeddings available to download?",
"text": "Hi everyone. I am a little new to this so sorry in advance if this is a silly question. I currently trying to build a text generation model based on my downloaded facebook data.  I know about pre-trained word embeddings like word2vec and glove but as I'm working with message data that is likely to contain a lot of typos, emoji and made up words I though it would be better to use a character-based encoding.\n\n\nI've found a few papers discussing the use of character embeddings and their advantages but I can't seem to find anywhere to download these embeddings. Do these exist? Would it be worth it for me to train my own? given that I don't have access to any high-performance computing for this task as I'm doing it on my free time.\n\n\nThanks in advance for the help",
"date": "2020-04-26"
},
{
"vote": 1,
"title": "Cloud Hosting recommendation request",
"text": "I am building a text analytics web analysis and graph tool that will be based on text that my users will introduce everyday.\nI don't expect my users to add more that 250 words per day, but I expect around 100.000 users per day for input (Approximately 2mb of new data per day) and maybe more to visualize and do graphical analysis on that text.\nWhat kind of hosting should I be looking for?",
"date": "2020-04-26"
},
{
"vote": 1,
"title": "Day 117 of #NLP365 - NLP Papers Summary â€“ Abstract Text Summarization: A Low Resource Challenge",
"text": "Day 117.\n\n\nToday's post is a 3-minute summary of the NLP paper \"Abstract Text Summarization: A Low Resource Challenge\".\n\n\nToday's paper proposed a data augmentation technique to generate synthetic data for abstractive summarisation in low resource languages such as German. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/04/26/day-117-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-26"
},
{
"vote": 1,
"title": "Papers/case studies where dependency relations are used as input for training a supervised learning classifier.",
"text": "Hello everybody,\n\n\nI would like to extract specific relationship dependencies and pos within a sentence that describe cause and effects and use them as inputs along side other labeled data within a dataset to predict a target class.\n\n\nFor example, one training instance from a sentence describing cause and effects would look like (nmod:agent,NN) for cause and (nsubjpass,NN) for effect.\n\n\nCan anyone recommend papers/cases that use these types of instance along with other labeled inputs to train a predictive model? Or possible something similiar? \n\n\nI'd like to know if it is even possible before going down this rabbit hole.\n\n\nI am new to Machine learning and NLP so I apologize if my ability to communicate what I would like to accomplish is weird. Hopefully I can clear it up.\n\n\nThanks",
"date": "2020-04-25"
},
{
"vote": 8,
"title": "Submit to EMNLP or NeurIPS?",
"text": "Hello\n\n\nI am doing NLP application (applied RL onto Q&A task) and got some good empirical results.\n\n\nI am looking to submit to either one of the two conferences: EMNLP, or NeurIPS. Which is more suitable and more easier to get accepted?",
"date": "2020-04-25"
},
{
"vote": 2,
"title": "[P] Article virality prediction",
"text": "Hello there folks, I was recently shortlisted for an internship and was given an assignment where I have to crawl news and information websites and predict the likelihood of virality of its articles. How do I go about executing this project? I have prior experience in Selenium, BeautifulSoup, Pandas, scikit-learn, etc., if that helps.",
"date": "2020-04-25"
},
{
"vote": 15,
"title": "Day 116 of #NLP365 - NLP Papers Summary â€“ Data-Driven Summarization Of Scientific Articles",
"text": "Day 116.\n\n\nToday's post is a 4-minute summary of the NLP paper \"Data-Driven Summarization Of Scientific Articles\".\n\n\nThis paper might be a good starting point for those who are interested in summarisation for scientific articles. It introduces two datasets, title-gen and abstract-gen. Title-gen uses abstract as the input to generate title of the research paper whereas abstract-gen uses the full body text of the paper as input to generate the abstract. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/04/25/day-116-nlp-papers-summary-data-driven-summarization-of-scientific-articles/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-25"
},
{
"vote": 4,
"title": "Looking for free annotation tool for you project? We need your feedback and suggestions",
"text": "Hello Redditors,\n\n\nWe are looking for beta testers to try out our new NLP annotation tool in the beta version \nhttps://ubiai.tools\n. The tool has the following features:\n\n\n\n\nEasy to use UI for NER annotation\n\n\nMulti-format document upload: TXT, CSV (each row corresponds to a doc), JSON (allows you to import and modify already annotated JSON files), PDF, DOC, HTML\n\n\nDictionary/Regex auto-annotation: input a list of words or regex patterns along with their associated entities. The tool will automatically scan the documents and auto-annotate\n\n\nML auto-annotation: Train an NER model to auto-annotate your documents\n\n\nBias detection: visualize entity and word distribution across your documents to detect skewed annotation toward specific entities.\n\n\nCollaboration: Share annotation tasks among team members and monitor progress\n\n\nAnnotation format export: JSON, IOB, Amazon Comprehend, Stanford CoreNLP\n\n\n\n\nIf you're interested in joining our beta tester community, please email us at \nadmin@ubiai.tools\n to send you your username the testing plan. We are offering free premium membership for 3 months once the final version is launched.\n\n\nFor those of you who are not interested in beta testing, please feel free to use our app for your projects (limited to 1000 document and 1000 word/doc per user), documentation can be found \nhere\n.\n\n\nWe will implement suggestions from our fellow Redditors.",
"date": "2020-04-25"
},
{
"vote": 4,
"title": "Suggestions on multimodal topic modeling",
"text": "Hi guys,\nI hope you are safe from virus madness.  I am currently working on a video indexing application and was researching relevant existing literature in multimodal topic modeling. Can anyone suggest any relevant content they've come across?",
"date": "2020-04-24"
},
{
"vote": 1,
"title": "Pytorch Paraphrase Generator Recommendations",
"text": "Hello,\n\n\nI was wondering if anyone could point me to resources on paraphrase generators implemented in Pytorch?  More specifically, I would like to create a generator that takes something like \"A widget comprising: element A, element B, and element C\" and generate one or more sentences \"A widget includes element A.  The widget also includes  element B.  The widget also includes element C.\"\n\n\nThanks!",
"date": "2020-04-24"
},
{
"vote": 1,
"title": "Difference between detach and retain_graph = True (Torch)",
"text": "So, this question has been bugging me since quite a lot of time. Whenever I use LSTMs (or any recurrent neural network) I generally save the hidden states. For example, I do something like this  \n\n\nhid = init_hidden_state()\n#Loop for some iterations\nyhat, hid = lstm_model.forward(inp, hid)\nlosses = loss(yhat, out)\nlosses.backward(retain_graph = True)\noptimizer.step()  \n\n\nThis code is inside a loop of batches. Can someone explain where the graph remains and explain which to use when and why?  \n\n\nP.S. If I am in the wrong subreddit please tell me.",
"date": "2020-04-24"
},
{
"vote": 13,
"title": "Day 115 of #NLP365 - NLP Papers Summary â€“ SCIBERT: A Pretrained Language Model For Scientific Text",
"text": "Day 115.\n\n\nToday's post is a 4-minute summary of the NLP paper \"SCIBERT: A Pretrained Language Model For Scientific Text\".\n\n\nToday's paper released SCIBERT, a pretrained language model trained on multiple scientific corpuses to perform different downstream scientific NLP tasks. These tasks include sequence tagging, sentence classification, dependency parsing, and many more. SCIBERT has achieved new SOTA results on few of these downstream tasks. We also performed extensive experimentation on the performance of fine-tuning vs task-specific architectures, the effect of frozen embeddings, and the effect of in-domain vocabulary.\n\n\nhttps://ryanong.co.uk/2020/04/24/day-115-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-24"
},
{
"vote": 2,
"title": "Education requirements to become a computational linguist?",
"text": "Hi everyone :) I'm Italian and I recently got my BA in Foreign Languages and Cultures. I've been studying English, French and German for almost 10 years now, since I also attended a linguistic high school. My BA in Foreign Languages was mainly about Linguistics (General linguistics, Sociolinguistics, Semantics, Pragmatics etc.). \n\n\nI'm now looking for a job and I've found some job offers both as Computational Linguist/ Knowledge Engineer. I didn't think to be qualified for the job but then I had a look at the employees Lindedin profiles  The most common education backgrounds seem to be: \n\n\n- Foreign Languages (BA) + Linguistics/ Translation/ Italian as a Foreign Language (MA) \n\n\n- Italian literature (BA) + Philology/ Linguistics (MA)\n\n\nIn few cases, just a BA in Foreign Languages. \n\n\nNow the thing is: I feel like I've been studying tons of theoretical linguistics and I love it but I wouldn't have a clue about how to use anything related to Artificial Intelligence. Moreover, I think a MA in Linguistics would be a plus on my CV but it wouldn't be useful to acquire new PRACTICAL skills. \n\n\nAre the theoretical knowledges enough to be a \"computational linguist\"? Maybe the kind of work they want me to do is doable without extensive knowledge about AI. They are looking for someone who knows about  \"NLP\" and who speaks Italian, English and another language between French/Spanish and German. \n\n\nThanks everyone :)",
"date": "2020-04-23"
},
{
"vote": 16,
"title": "Stanza vs SpaCy vs SparkNLP",
"text": "Currently starting a research project in digital humanities that requires accurate anaphora resolution, NER, and a good pos-tagger\n\n\nWhat are your thoughts on these three libraries?",
"date": "2020-04-23"
},
{
"vote": 3,
"title": "What are some fast and solid clustering algorithms for text?",
"text": "Iâ€™m looking for more recent and close to state-of-the-art, non-supervised clustering approaches for text, even better if using vectors to represent the text. Any tips?",
"date": "2020-04-23"
},
{
"vote": 1,
"title": "Free NLP Annotation Tool for Beta Testing",
"text": "[removed]",
"date": "2020-04-23"
},
{
"vote": 1,
"title": "AMA with the HuggingFace team tomorrow",
"text": "[removed]",
"date": "2020-04-22"
},
{
"vote": 1,
"title": "More than 100 Colab Notebooks Found Here!",
"text": "[removed]",
"date": "2020-04-22"
},
{
"vote": 1,
"title": "[Project] Need some help for Question Generation",
"text": null,
"date": "2020-04-22"
},
{
"vote": 1,
"title": "DUC-04 Dataset",
"text": "Hello\n\n\nI am currently working on summarization with the DUC-04 dataset. I don't know how exactly do I evaluate my model output. \n\n\nSpecifically, for each input example, I generated a system output. \n\n\nNow, the dataset provides 4 human summaries for each input example. Do I calculate ROUGE score against each 4 of these \"ground-truth\" summary and average them, or I just compute the best  one?",
"date": "2020-04-21"
},
{
"vote": 1,
"title": "VOICE Talks Begins on April 28th!",
"text": "[removed]",
"date": "2020-04-21"
},
{
"vote": 1,
"title": "[Paper] StereoSet: Measuring stereotypical bias in pretrained language models",
"text": "Hey all, excited to announce â€œStereoSet: Measuring stereotypical bias in pretrained language modelsâ€. I'm also happy to answer any questions.\n\n\n>\nAbstract:\n A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present StereoSet, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and XLNet on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at \nhttps://stereoset.mit.edu/\n\n\nPaper:\n \nhttps://arxiv.org/abs/2004.09456\n\n\nWebsite:\n \nhttps://stereoset.mit.edu\n\n\nCode:\n \nhttps://github.com/moinnadeem/stereoset",
"date": "2020-04-21"
},
{
"vote": 7,
"title": "Using contextualized word embeddings (ELMo) for predicting words that are semantically related",
"text": "I am looking for an easy-to-read implementation of ELMo or any contextualized word embeddings to predict semantically-related words on the basis of the linguistic context. Do you know by any chance where I could find such implementation? Thanks in advance.",
"date": "2020-04-20"
},
{
"vote": 1,
"title": "COVID-19 and CEO Language: 91% of CEOs discussed COVID-19 last week in US earnings calls. CEO positive sentiment down 20 points compared to the same week the past five years.",
"text": null,
"date": "2020-04-20"
},
{
"vote": 12,
"title": "Day 111 of #NLP365 - NLP Papers Summary â€“ The Risk Of Racial Bias In Hate Speech Detection",
"text": "Day 111.\n\n\nToday's post is a 4-minute summary of the NLP paper \"The Risk Of Racial Bias In Hate Speech Detection\".\n\n\nBiases in AI has been a key research area. In this paper, it explores the impact of human's unconscious biases (annotators) when it comes to annotating datasets and how that could propagate to our AI models. Check it out below :)\n\n\nhttps://ryanong.co.uk/2020/04/20/day-111-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-20"
},
{
"vote": 3,
"title": "What's the simplest way to generate word vectors for out of vocabulary words using FastText or something similar?",
"text": null,
"date": "2020-04-20"
},
{
"vote": 1,
"title": "Difference between Artificial Intelligence Machine Learning and Neural Networks",
"text": null,
"date": "2020-04-20"
},
{
"vote": 1,
"title": "Pre=trained weights for machine translation?",
"text": "Hi,\n\n\nWas interested in translating some text in German, Chinese and Japenese back to English. Was looking for pre-trained models that I could plugin and get the translation. I have seen that fairseq has pretrained models for German but not any other languages. Does someone know if I can get something similar for Chinese and Japense?\n\n\nIt's just a one time task and did not want to train a model on my own.",
"date": "2020-04-20"
},
{
"vote": 31,
"title": "What are the best universities in Europe for NLP?",
"text": "Edit: thank you all for the useful information!",
"date": "2020-04-19"
},
{
"vote": 3,
"title": "How does the alignment of two sets of monolingual embeddings in two languages work?",
"text": "I have two sets of monolingual embeddings in English and French, with a different size of vocabulary and embedding dimension, so that they have shape (n, emb_dim):\nemb_en\n = (140000,  2048) \n, emb_fr\n = (40000 , 1536)\n\n\nI want to end up with a bilingual matrix where every embedding has dimension 200:\n(140000+40000, 200) , so I did a linear transformation of both monolingual embedding matrices and applied dropout. How do I verify if the resulting embeddings are aligned? As far as I understand, projecting both matrices to the same dimension doesn't necessarily mean that the embeddings are aligned, but then how can I make sure of that?\n\n\nI do have a dictionary that maps part of the two matrices, but it doesn't cover them all since I have over 3 times more embeddings for English than I do for French, so I don't think it could be useful.",
"date": "2020-04-19"
},
{
"vote": 3,
"title": "Data Augmentation for End-to-End Speech Translation",
"text": null,
"date": "2020-04-19"
},
{
"vote": 19,
"title": "EMNLP adding new acceptance category \"Findings\"",
"text": null,
"date": "2020-04-18"
},
{
"vote": 0,
"title": "You can drive 136.000 kilometers with the energy needed to train one NLP model",
"text": "[deleted]",
"date": "2020-04-18"
},
{
"vote": 2,
"title": "[R] Information-Theoretic Probing with Minimum Description Length: a paper and a blog post",
"text": null,
"date": "2020-04-18"
},
{
"vote": 1,
"title": "Named Entity Recognition For Product Names Of Clothes With SpaCy",
"text": "I am trying to extract product names from a plain text, the problem with product names is that they don't have a specific pattern and I don't want to give the algorithm a set of data that has fixed names I want it to be generic.\n\n\nI am using SpaCy and I'm looking for a way to make it detect the product names as an Entity.\n\n\nAny help please?\n\n\nHere's an example of the text\n\n\n>Order dispatched Your new clothes are on their way. Track your\n>\n>delivery with Royal Mail: VB 9593 7366 0GB\n>\n>Order Details\n>\n>Men's Dark Navy Jersey Cotton Lounge Shorts Size: XL\n>\n>Â£45.00\n>\n>Men's Navy Cotton Jersey Lounge Pants Size: XL\n>\n>Â£60.00\n>\n>Delivery Â£0.00\n>\n>Total Â£95.00\n\n\nI want to extract\n\n\n>Men's Navy Cotton Jersey Lounge\n\n\nand\n\n\n>Men's Dark Navy Jersey Cotton Lounge Shorts\n\n\nAnother example\n\n\n>Your order summary\n>\n>Delivery between 18/11/2019 and 19/11/2019\n>\n>Shipping from\n>\n>O'\n>\n>adidas\n>\n>Lxcon sneakers\n>\n>Â£80.96\n>\n>Delivery between 18/11/2019 and 19/11/2019\n>\n>Shipping from\n>\n>BOUTIQUE ANTONIA\n>\n>MARCELO BURLON COUNTY OF MILAN\n>\n>Confidencial striped swimsuit\n>\n>Â£97.58\n>\n>Shipping\n>\n>Total\n>\n>Payment method\n>\n>Â£20.00 Â£153.90 VISA\n\n\nI want to extract\n\n\n>adidas\n>\n>Lxcon sneakers\n\n\nAnd\n\n\n>MARCELO BURLON COUNTY OF MILAN\n\n\nFor your information this text is an email of orders and I have a lot of different patterns of emails.",
"date": "2020-04-18"
},
{
"vote": 3,
"title": "Reverse POS-Tagger",
"text": "Hi! I am searching for a python library that applies (in my case german) inflection on a given word with the inflection-information I gets from a POS tagger (CoNLL-U Format) (Demo:  \nhttps://pub.cl.uzh.ch/demo/parzu/\n).\n\n\nBasically, I 'just' need a dictionary: Input: haben & VVAFIN & 2|Sg|Pres|Ind Output: hast\n\n\nIs there a word for this type of task, so I can get googling? Do you know any library that does that?\n\n\nI could obv parse a whole wikitionary dump but I thought it might be worth a little investigation first.\n\n\nFYI: I checked pattern_de but these heuristics aren't that convincing as far as I understood.\n\n\nThanks for your help <3",
"date": "2020-04-18"
},
{
"vote": 0,
"title": "What Databases do you use with your NLP technologies?",
"text": "[deleted]",
"date": "2020-04-18"
},
{
"vote": 13,
"title": "Personal NLP/Speech projects that made you stand out",
"text": "Dev, research, interdisciplinary, or anything else you've found to be pretty neat.",
"date": "2020-04-17"
},
{
"vote": 2,
"title": "Transfer Learning for NLP - Podcast",
"text": null,
"date": "2020-04-17"
},
{
"vote": 5,
"title": "Highlight the most influential words for the results of the sentiment analysis [R][D]",
"text": "I have some results of the sentiment analysis on the sentence level. I'd like to check which of words have the strongest influence on the final result, i.e. why the particular sentence is considered as the negative or positive. I am wondering whether the some kind of modified TfIdf should be useful, but not sure whether there are some Python based tools or solutions that can meet this objective.",
"date": "2020-04-17"
},
{
"vote": 6,
"title": "Day 108 of #NLP365 - NLP Papers Summary â€“ Simple BERT Models For Relation Extraction And Semantic Role Labelling",
"text": "Day 108.\n\n\nToday's post is a 5-minute summary of the NLP paper \"Simple BERT Models For Relation Extraction And Semantic Role Labelling\". \n\n\nThe key takeaways here show you how you can use a simple BERT-based LSTM architecture to achieve strong / SOTA results in relation extraction and semantic role labelling. This paper was written in April 2019 so this might no longer be the SOTA results.\n\n\nhttps://ryanong.co.uk/2020/04/17/day-108-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-role-labelling/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-17"
},
{
"vote": 1,
"title": "Differentiating adjectives",
"text": "Hello, does anyone here knows how to differentiate adjective that is part of the object name (e.g.  \"digital camera\") with adjective that states an opinion to an object (e.g. \"cheap camera\")? Any help would be appreciated. Thanks!",
"date": "2020-04-17"
},
{
"vote": 3,
"title": "GPT-2 models in other languages - what's the holdup?",
"text": "It has almost been two years since GPT-2 got released. I excepted to see models of different sizes for different languages popping up left and right in regular intervals, but thus I'm not aware of any >774M models in other languages than English. Shouldn't universities around the world be working on training these?",
"date": "2020-04-17"
},
{
"vote": 3,
"title": "Fuzzy checking if a sentence is grammatical",
"text": "I'm building a scraper to extract sentences (~5-50 tokens long).\n\n\n\n\nSome of which are relevant for my task - grammatical sentences/have some meaning in English\n\n\nMost are clearly not - don't mean much for the task, e.g. just a street address or a factory code.\n\n\n\n\nWhat is the simplest tool to differentiate between the two? Perhaps NLTK/SpaCy offer something to solve it? It would tremendously help me to find even an imperfect method just to filter out irrelevant signals from my pipeline.\n\n\nMany thanks!",
"date": "2020-04-17"
},
{
"vote": 5,
"title": "Need resources on multi-task training",
"text": "Hello all, I need examples/resources on multi task learning especially training loops (preferably NLP tasks). I have no clarity on how to setup the tasks;\n\n\n\n\nif I should just add the loss functions in cases where the inputs are the same for both the tasks but labels are different\n\n\nHow to train if the losses are separate? Should I alternate between tasks for each epoch?\n\n\nWhat if the tasks are hierarchical i.e one task outputs feed into the other task as inputs.\n\n\n\n\nAnything would help.",
"date": "2020-04-17"
},
{
"vote": 0,
"title": "Need to get 100K conversational sentences in 4 different domains for 10 different languages, where can I get all of it?",
"text": "I need 400K sentences, 100K each in 4 domains, or topics of conversation. The domains are: Healthcare, Entertainment, Hospitality, and Automotive. The sentences need to be in natural human conversation and they need to be from web sources, like discussion boards and stuff like that, but the discussion boards or other sources need to be specifically within these domains. For example, if I go to a website and see user comments, I need to be sure that all of these comments are within the healthcare domain, or the entertainment domain, etc.\n\n\nI need all of this in: English (UK), English (Australia), French (France), French (Canada), Spanish (Spain), Spanish (Mexico), Portuguese (Brazil), German (Germany), Dutch (Netherlands), and Italian (Italy). The sentences need to be from sources  from the respective countries in order to capture the colloquialisms and specific grammatical differences. For example, I can't use a Spanish website to get my dataset for Mexican Spanish, it needs to be a website specifically based in Mexico and used by Mexicans. Same with British vs. Australian, French vs. French Canadian.\n\n\n100k sentences x 4 domains x 10 languages = 4 MILLION SENTENCES. I need this whole dataset in about a week, where do I go to get it? Oh and I need all the data sources to be clean and have a regular way of typing everything, no spelling mistakes or typos or abbreviations, and all numbers typed out properly as words and not digits, things like that.",
"date": "2020-04-16"
},
{
"vote": 2,
"title": "Beginner project",
"text": "[deleted]",
"date": "2020-04-16"
},
{
"vote": 1,
"title": "CMU, DeepMind &amp; Googleâ€™s XTREME Benchmarks Multilingual Model Generalization Across 40 Languages",
"text": "[removed]",
"date": "2020-04-15"
},
{
"vote": 2,
"title": "Ideas for Uni Project",
"text": "Iâ€™m taking a course in NLP at my university, and we have to propose a final project of our own choosing to deliver. \n\n\nAlthough I now feel somewhat experienced in the basics of NLP + deep learning for text data, itâ€™s a bit hard to come up with an interesting topic. The project lasts about a month and could maybe be considered as a mini masters thesis. So ideally I would like to do something â€œmoreâ€ than just: download data -> build model, train model-> predict and evaluate on a test set.\nBut Iâ€™m not quite sure what would be realistic given the limited time. \n\n\nAny suggestions for interesting project ideas, or places I can look for inspiration, would be greatly appreciated.",
"date": "2020-04-15"
},
{
"vote": 6,
"title": "Need a tool to handle lots of use cases of spoken numbers, looking at spaCy",
"text": "I am working on a project that involves movie titles, TV show titles, and such. The title is captured from a user's speech (Alexa) and a database is searched. I am experiencing several issues with titles with numbers in them. Some titles use digits, some have the word spelled out, roman numerals, the list goes on and on. I have some ideas about handling this but am looking for tools that either already do or provide a good framework to continue from. I have very little experience with NLTK and I just heard of spaCy. spaCy seems like it might be a better fit but I'm not sure. I'm familiar with NLP, but I am relatively new with it. It seem like spaCy has some capabilities I can build off of but I'm not sure. Looking at the docs, there's tagger, text categorizer, entities, and I can't tell which would be useful. They all sound too similar.\n\n\nEverything I receive as input will presumably be written out in text. My goal is to identify numbers in the text and convert them to all possible, but correct interpretations. I can then search the database for the best match to determine what they are looking for. My thinking is I tokenized the string, identify numbers and number-like words, and expand the number tokens to include all considered possibilities. Maybe there's a better way. An example of just one issue/use case to consider:\n\n\none hundred one dalmatians\n\n\none hundred and one dalmatians\n\n\na hundred one dalmatians\n\n\na hundred and one dalmatians\n\n\nhundred one dalmatians\n\n\nhundred and one dalmatians\n\n\nThere are also issues with people saying 'Oh' for zero, 'sixteen hundred' vs 'one thousand six hundred', knowing 'twenty fifteen' means concatenating '20' and '15' but 'two thousand fifteen' means adding 2000 and 15, knowing when 'point' is a decimal or not, fractions, ordinal indicators (6th/sixth) etc. I'm sure some of the simpler cases are already handled in some things. Can spaCy do any of these? Or at least which tools does it have to do it myself? Some other library? It doesn't have to by Python, it just seems like Python would have the best libraries for it. I'd appreciate any advice.",
"date": "2020-04-15"
},
{
"vote": 0,
"title": "[Update] Latent Semantic Analysis using nltk gives strange output",
"text": "An update on the \nprevious thread\n. I decided to build a standard nltk algorithm to test against the \"homemade\" one and check for errors. They give almost the same results but I see that it only classifies the first semantic words and repeats the same terms over and over on top of the other documents.\n\n\nI tried feeding it the documents found in \nHERE\n too and it does exactly the same. Repeating the values of the same topic several times in the other ones.\n\n\nCould anyone help explain what is happening? I've been searching all over and everything seems exactly like in the tutorials. And are there any resources to do unit tests for LSA and word2vec?\n\n\nThanks again!\n\n\ntestDocs = [\n                &quot;The Neatest Little Guide to Stock Market Investing&quot;,\n                &quot;Investing For Dummies, 4th Edition&quot;,\n                &quot;The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns&quot;,\n                &quot;The Little Book of Value Investing&quot;,\n                &quot;Value Investing: From Graham to Buffett and Beyond&quot;,\n                &quot;Rich Dad&#039;s Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!&quot;,\n                &quot;Investing in Real Estate, 5th Edition&quot;,\n                &quot;Stock Investing For Dummies&quot;,\n                &quot;Rich Dad&#039;s Advisors: The ABC&#039;s of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss&quot;,\n                ]\n    stopwords = [&#039;and&#039;,&#039;edition&#039;,&#039;for&#039;,&#039;in&#039;,&#039;little&#039;,&#039;of&#039;,&#039;the&#039;,&#039;to&#039;]\n    ignorechars = &#039;&#039;&#039;,:&#039;!&#039;&#039;&#039;\n\n    #First we apply the standard SKLearn algorithm to compare with.\n    for element in testDocs:\n        #tokens.append(tokenizer.tokenize(element.lower()))\n        element = element.lower()\n\n        print(testDocs)\n\n    #Vectorize the features.\n    vectorizer = tfdv(max_df=0.5, min_df=2, max_features=8, stop_words=&#039;english&#039;, use_idf=True)#, ngram_range=(1,3))\n    #Store the values in matrix X.\n    X = vectorizer.fit_transform(testDocs)\n#Apply LSA.\n    lsa = TruncatedSVD(n_components=3, n_iter=100)\n    lsa.fit(X)\n\n    #Get a list of the terms in the order it was decomposed.\n    terms = vectorizer.get_feature_names()\n    print(&quot;Terms decomposed from the document: &quot; + str(terms))\n    print()\n\n    #Prints the matrix of concepts. Each number represents how important the term is to the concept and the position relates to the position of the term.\n    print(&quot;Number of components in element 0 of matrix of components:&quot;)\n    print(lsa.components_[0])\n    print(&quot;Shape: &quot; + str(lsa.components_.shape))\n    print()\n    for i, comp in enumerate(lsa.components_):\n        #Stick each of the terms to the respective components. Zip command creates a tuple from 2 components.\n        termsInComp = zip(terms, comp)\n        #Sort the terms according to...\n        sortedTerms = sorted(termsInComp, key=lambda x: x[1], reverse=True)\n        print(&quot;Concept %d&quot;, i)\n        for term in sortedTerms:\n            print(term[0], end=&quot;\\t&quot;)\n        print()",
"date": "2020-04-14"
},
{
"vote": 10,
"title": "What does downstream tasks mean in NLP?",
"text": "[deleted]",
"date": "2020-04-14"
},
{
"vote": 12,
"title": "Semania - Falling Words NLP Game",
"text": "I created an NLP word game at \nhttps://www.boredhumans.com/semania.php\n .  Try to stop words from falling by typing other related words. It uses a word embedding model to measure how similar the word you type is to the word that is falling. Also, just to make sure it responds well, I added a 2nd word embedding model, along with synonyms and antonyms from WordNet (via TextBlob).",
"date": "2020-04-14"
},
{
"vote": 0,
"title": "[Q] Multi-class text classifier for tweets | Realtime Twitter Data",
"text": "I am trying to build a \nmulti class text-classifier\n that classifies whether the tweet belongs to one of the categories ( \nAdvise\n or \nScience\n or \nothers\n )\n\n\nlet the input be any tweet like \nthis\n ,\n\n\nInput :\n\n\nThe goal of teaching should not be to help the students learn how to memorize and spit out information under academic pressure. Brain\n\nThe purpose of teaching is to inspire the desire for learning in them and make them able to think, understand, and question.\n\n#maths #richardfeyman\n\n\n\nOutput :\n\n\n( Advise\n : 98% , \nothers\n : 1 % , \nScience\n : 1 % )\n\n\nfor now i came up with this\n\n\nIdea 1 :\n\n\nMaybe i can use LDA to get the topics of the tweet and perform semantic matching between these labels ( advise , science , others ) so one with the highest score can be choosen as right one (\nargMax(...)\n ).\n\n\nWhat do you think about the above idea ?\n\n\nCan anyone please enlighten me or point me in the right direction .",
"date": "2020-04-14"
},
{
"vote": 1,
"title": "Clustering dependency trees?",
"text": "Given a large collection of sentences and their dependency trees, is there a good way to cluster the trees to determine different types of utterances? \n\n\nI've had limited success on arxiv-sanity etc., so I'd be happy to hear more takes on how to do this. Current approaches seem to be based on various graph embeddings, e.g. \n\"Structural Embedding of Syntactic Trees for Machine Comprehension\"\n, where a window around each word defines a neighborhood on a graph. \n\n\nAny other approaches?",
"date": "2020-04-14"
},
{
"vote": 7,
"title": "Can I build a language model with my own vocabulary on top of pre-trained word embeddings?",
"text": "The best way to achieve high accuracy in NLP tasks is to use SOTA pre-trained word embeddings.\nIs there any value in building my own language model using my vocabulary on top of the word embeddings? Would it achieve better accuracy in downstream tasks like text classification or summarization?",
"date": "2020-04-14"
},
{
"vote": 8,
"title": "Any help interpreting Latent Semantic Analysis?",
"text": "So long story short, I'm starting to learn about the field of NLP and trying to implement Latent Semantic Analysis so I followed this tutorial: \nhttps://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/\n\n\nSo far I've managed to understand most of it, but on the part of dimension reduction I got stuck. I see a bunch of terms which I assume are the U, Sigma and V^t vectors.\n\n\nFrom here I have no idea how to make dimension reduction and return the matrix back to query topics from it. I read some more tutorials such as this \nhttp://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html\n and cooked the following code:\n\n\nhttps://pastebin.com/WYdQkdzd\n\n\nNonetheless it gives me a bunch of numbers and I have no idea how to interpret the output. I understand that each element in the array is one of the terms and the frequency associated but I don't get how to turn it back from a matrix to terms or how to enact a query on them. Also I'm fairly certain this is meant to output 9 rows since that's the number of documents its being fed, yet it gives me 10.\n\n\nI know programming and basic stuff like regex and tokenization but this is the first time I've ever done any serious nlp and tried to start here because it seemed easier than doc2vec. What is the best way to start?\n\n\nAnd is there any notebook or dataset I could use to test whether the algorithm is doing what I intend it to?",
"date": "2020-04-14"
},
{
"vote": 10,
"title": "I need a full list of profanity for Portuguese and German",
"text": "I need to do profanity filtering of datasets in multiple languages, and I'm doing it with list-based profanity filtering. \nThis\n is the source I found for full profanity lists. Great, it's given me English, French, Italian, and Spanish. But that's it. I need this same thing for Portuguese and German. Possibly a few other languages as well. Where can I get it? Portuguese first preferably. Thanks.",
"date": "2020-04-14"
},
{
"vote": 1,
"title": "[D]:Is there any repository only papers related to text classification using deep learning or non-deep learning papers?",
"text": "Please suggest any latest papers including tweet classification.",
"date": "2020-04-13"
},
{
"vote": 5,
"title": "Viterbi Forced alignment in speech recognition",
"text": "Hi all, I am trying to understand GMM-HMM parameter training with respect to speech recognition. \n\n\nHow does viterbi force alignment works during training?\n\n\nMy current assumption is that during training since phones and observation is known so the state path is known. Is this called viterbi force alignment ? Once we know the state path, the parameter can be estimated using Baum-Welch. Is it so ?\n\n\nMoreover, for one state can be associated with multiple frames because the utterance of a phone can extend over multiple frames. How this is  trained?",
"date": "2020-04-13"
},
{
"vote": 0,
"title": "Multiple correspondence analysis by prince in Python: TypeError: object() takes no parameters",
"text": null,
"date": "2020-04-13"
},
{
"vote": 3,
"title": "Does (or will) Abstract Meaning Representation have practical applications",
"text": "Lately I've been looking at a number of papers on \nAMR\n text parsing and generation.  With the 2017 SemEval Task 9, it seems like there has recently been a fair bit of work on the topic but I have yet to see anyone applying this technology to a practical application.  It looks like, at least for now, this is simply an academic pursuit.\n\n\nAre there currently practical applications that I'm missing?  If not, do you think we'll get there eventually and what is going to be needed (ie.. if one were going to spend time on this, what are the areas that need improvement.)",
"date": "2020-04-13"
},
{
"vote": 0,
"title": "Make the PC Speak with Python (Windows)",
"text": "[deleted]",
"date": "2020-04-13"
},
{
"vote": 2,
"title": "Open Source Data Set for Audio Files of Numbers",
"text": "Does anyone know of a public data set of Audio Files for Numbers being read aloud. I'd like to train my own model to be really good at recognizing numbers and translate them to text. \n\n\nThis model needs to be offline. I've already looked into the python module Speech Recognition and set it up with Sphinx...but it is having one heck of a time identifying the word \"Eight\". After saying about 300 numbers aloud yesterday I realized that there's probably a better way to do this...",
"date": "2020-04-12"
},
{
"vote": 4,
"title": "Position embedding in neural networks",
"text": "I am trying to understand the position embedding in neural networks.  I came across this resource (attached) but still did not understand it. Why minimal and maximal distance?  what is the difference between (position 1 and position-indices1)?\n\n\nhttps://preview.redd.it/doak1esl99s41.png?width=1838&format=png&auto=webp&s=a97ec31b6adf5923c33fdee966aee0c0ae0e0265\n\n\nThanks",
"date": "2020-04-11"
},
{
"vote": 1,
"title": "position embedding in a neural network",
"text": "[deleted]",
"date": "2020-04-11"
},
{
"vote": 2,
"title": "NLP and Marketing Campaign Performance Insights",
"text": "I am looking for resources and strategies on how to use NLP to find insights on marketing campaigns. I have social media data for my company and want to see if there are certain n-gram terms in the message of the post (generic ex. \"Our brand is great. Check out this celebrity that is using it at this link...\") that have high correlation to certain metrics, like actions, retweets, video views, on twitter, facebook, youtube, and others.   \n\n\nWe are currently testing out some things, like NMF/LDA then aggregating metrics and comparing, random forest to gauge feature importance, but I wonder what else could be done. Personally, I don't find sentiment analysis that compelling, but I know that is done a lot. Any other thoughts? The corpus is about",
"date": "2020-04-11"
},
{
"vote": 1,
"title": "User generated content collection for research",
"text": "[removed]",
"date": "2020-04-10"
},
{
"vote": 1,
"title": "Is it possible to train BERT with custom word embeddings?",
"text": "I am working on project to generate word embeddings using graph. I am curious to known the performance of my embeddings on pretrained network like BERT for various NLP tasks.\nIs there any other way to evaluate the quality of word embeddings?",
"date": "2020-04-10"
},
{
"vote": 2,
"title": "[Help] NLP book recommendations",
"text": "[deleted]",
"date": "2020-04-10"
},
{
"vote": 2,
"title": "Day 101 of #NLP365 - In-Depth Study Of RASAâ€™s DIET Architecture",
"text": "Day 101.\n\n\nToday's post is an in-depth study of the RASA's DIET architecture! DIET stands for Dual Intent and Entity Transformer. If you want to build a chatbot or perform intent and/or entity extractions, then consider experimenting with DIET, the new SOTA for intent and entity extractions.\n\n\nhttps://ryanong.co.uk/2020/04/10/day-101-in-depth-study-of-rasas-diet-architecture/\n\n\nHope you are all safe and healthy :)\n\n\nRyan",
"date": "2020-04-10"
},
{
"vote": 1,
"title": "Is there a language dataset out there with multiple label choices from more than 10 annotators?",
"text": "We are planning to work on research to study of the annotation sample. I'm looking for suggestions on a dataset, simply put closer to extracting facebook posts with their reactions. Any ideas? I went through the Entire Big bad NLP datasets page as well, with no luck.\n\n\nEdit: I'm looking for a dataset, 1 document -> multiple label choices (more than 2) -> more than 10 annotators.",
"date": "2020-04-10"
},
{
"vote": 2,
"title": "Biology Graduate Student Interested In an NLP Job After Graduation",
"text": "[deleted]",
"date": "2020-04-09"
},
{
"vote": 0,
"title": "Where can I find a full pronunciation dictionary of every word in the English language for the IBM phonetic alphabet?",
"text": "I know that there's the CMU pronunciation dictionary. But this one is only in ARPABET. What I need is the CMU pronunciation dictionary, but in the IBM phonetic alphabet, the one used by IBM Watson. Is there one that exists? Thanks.",
"date": "2020-04-09"
},
{
"vote": 4,
"title": "[HELP] Need for cyberbullying text/tweets/posts dataset (offensive, sexism and racism etc.)",
"text": "I'm currently building a model to predict the offensiveness of a tweet or a text but I'm unable to find quality datasets. Is there a dataset with a large volume of tuples, if not is there a way I combat this issue?",
"date": "2020-04-09"
},
{
"vote": 2,
"title": "Online Training - Spark NLP",
"text": null,
"date": "2020-04-09"
},
{
"vote": 1,
"title": "Reversing the order of sentences during training and testing in skip-thought?",
"text": "Was going through some skip-thought explanations, and was curious whether sentence embeddings would be as effective if the sentences were reversed during training and testing. I think they might be more effective, since it'd be sort of similar to a bi-directional approach where you have information from left-to-right AND right-to-left, but I'm not too sure. Thoughts?",
"date": "2020-04-09"
},
{
"vote": 8,
"title": "Anyone worked on huggingface transformers in tensorflow?",
"text": "Hey,\n\n\nIf anyone worked on hugging face transformers in Tensorflow, kindly share your work. As I'm finding it difficult to fine tune.\nOr any link or information will be useful.\n\n\nThanks.",
"date": "2020-04-09"
},
{
"vote": 3,
"title": "Data Augmentation in NLP using GPT2",
"text": "Data Augmentation is a technique that is heavily used by Deep Learning practitioners to add diversity and size in their training dataset for designing robust machine learning systems. Did you know you could very well use the GPT2 model to do the same for Natural Langauge?  Read more at \nhttps://prakhartechviz.blogspot.com/2020/04/data-autmentation-in-nlp-gpt2.html",
"date": "2020-04-09"
},
{
"vote": 3,
"title": "Rescoring output pairs of visual relationship neural network",
"text": "I'm currently trying to rescore the outputs of my visual relationship neural network by making use of some sort of language model. The neural net looks at objects within an image and outputs a relationship between objects within the image of form subject, predicate and object (i.e. man holding sword)\n\n\nI am not sure on which lm to use. Looking for a lm that is trained on a large text corpus such as Wikipedia that could suppress output scores of outputs that do not make semantically sense (i.e man holding building). Should I be looking at the perplexity score of the language model?",
"date": "2020-04-09"
},
{
"vote": 3,
"title": "How do you extract article/blog text?",
"text": "What's your process for extracting text from articles/blogs/company sites for a dataset? \n\n\nDo you use an HTML scraper like Scraper API and then a free text extraction library like newspaper3k? Paid solution like Diffbot?\n\n\nWhat was your experience with these?",
"date": "2020-04-08"
},
{
"vote": 2,
"title": "Long document (&gt;10 paragraphs) analysis datasets",
"text": "Are there good baselines datasets or benchmarks for similarities/retrieval of long texts (extremely long documents, books, etc.)? Although significant both academically and applicational, I cannot get a hold of any significant dataset nor remarkable papers from the last AAA conferences.\n\n\nDaily Mail, CNN or IMDB reviews are too short for me.\n\n\nPairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles(2020)\nOr\nGraph Convolutional Networks for Text Classification(2018)\nFor example, state to work on long text, but only use the first paragraph of the article.\n\n\nMy direction would probably be Wikipedia or Arvix datasets, but I would be happy to hear your experience or to find a relevant baseline to compete against.",
"date": "2020-04-07"
},
{
"vote": 1,
"title": "Ideas for NLP exam project regarding Covid-19 ig social/cultural dynamics",
"text": "[deleted]",
"date": "2020-04-07"
},
{
"vote": 15,
"title": "NLP groups in UK",
"text": "As the title suggests, I am looking for NLP research groups in the UK. Does MSR Cambridge not have any active NLP research going on?",
"date": "2020-04-07"
},
{
"vote": 1,
"title": "DISCO by Linguatools (Help)",
"text": "Has anyone worked before on DISCO offered by linguatools? basically it gives the power to create word spaces and extract features using several methods some of which use machine learning. I want to try and use it on any english corpus so that I can later use it on a project.\n\n\nMy problem is that there are barely any tutorials or guides and I don't know why im finding it hard to use. \n\n\nIf anyone who used it can dm me for a little chat or send me a guide or tutorial that I haven't been fortunate enough to find on google then that would be great.\n\n\n&#x200B;\n\n\nThanks in advance",
"date": "2020-04-07"
},
{
"vote": 1,
"title": "Day 97 of #NLP365 - Learn PGM With Me â€“ Representation â€“ Dependencies Of A Bayesâ€™ Network",
"text": "Day 97.\n\n\nOverall, Bayesian networks uses products of smaller, local conditional probability distributions to represent probability distributions. This is only possible through assuming that some variables are independent.\n\n\nThese independencies between variables can be described by directed graphs by assessing three types of structures:\n\n\n\n\nCommon parent\n\n\nCascade\n\n\nV-structure\n\n\n\n\nCheck out today's post for more details :)\n\n\nhttps://ryanong.co.uk/2020/04/06/day-97-learn-pgm-with-me-representation-dependencies-of-a-bayes-network/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-06"
},
{
"vote": 5,
"title": "[Project] If gpt-2 read erotica, what would be its take on the Holy scriptures?",
"text": null,
"date": "2020-04-06"
},
{
"vote": 1,
"title": "[Q] Creating a raw dataset for custom task - best practices?",
"text": "For a custom IE task at work that requires token-level tagging I'm looking for references - both papers and code - with best practices for raw-dataset creation and processing. \n\n\nBecause our task shares similarities with NER, I looked specifically at how common NER datasets were created but without success. Ideally something that points:\n\n\n\n\nWhich corpuses better be scraped? Can sentences be extracted by matching terms or only at random?\n\n\nIn which contexts/cases is it better to enable multiple tokens under the same entity, as opposed to only allowing for one or two at most?\n\n\nHow to measure and increase variance in the raw dataset (in terms of representing multiple cases)?",
"date": "2020-04-06"
},
{
"vote": 1,
"title": "GoLang implementation of Neural Machine Translation of Rare Words with Subword Units",
"text": "Hi, I am currently maintaining the GoLang implementation of Neural Machine Translation of Rare Words with Subword Units. \nhttps://github.com/khaibin/go-subword-nmt/\n\n\nPull requests are welcome :)",
"date": "2020-04-06"
},
{
"vote": 1,
"title": "GoLang implementation of Neural Machine Translation of Rare Words with Subword Units",
"text": "[deleted]",
"date": "2020-04-06"
},
{
"vote": 18,
"title": "[N] Launching a competition for more energy-efficient NLP models",
"text": null,
"date": "2020-04-05"
},
{
"vote": 3,
"title": "Method for metaphor analysis in corpus",
"text": "Hi everyone! I'm working on a project that requires extracting and analyzing metaphors in a corpus. I don't have time to develop my own model as I need to submit the paper before my qualifying exam. I've read many articles such as papers submitted to metaphor workshop, but now I'm lost as to what method to choose. Does anyone have any suggestions? any help is appreciated!",
"date": "2020-04-05"
},
{
"vote": 1,
"title": "Starting out",
"text": "Hi, I am starting out trying to make a program that can \"understand\" what objects in a given piece of text are. \n\n\nFor example, the sentence \"the object named bill runs across the floor\"\n\n\nFollowed by the question \"what is the object?\" should return a description along the lines that the object is a physical system capable of performing the behaviour of running. When asked if the object is capable of walking, the system should respond yes. \n\n\nThe connection between running and walking would be in a behavioural library. For example, if an object is capable of performing a physical activity of walking, running is merely another class of running. \n\n\nGot any good starting points?",
"date": "2020-04-05"
},
{
"vote": 5,
"title": "Can't figure out the meaning of (purple text). Is every word being used for calculation the topic itself?",
"text": null,
"date": "2020-04-05"
},
{
"vote": 5,
"title": "Day 96 of #NLP365 - Learn PGM With Me â€“ Representation â€“ Introduction To Bayesian Networks",
"text": "Day 96.\n\n\nNow that we have completed a good review of the probability theory required to get started, we are ready to move to the first area of using graphical models to model the world and that is the Representation area. How can we express a probability distributions that models some real-world phenomenon? Well, there are two methods:\n\n\n\n\nBayesian networks (Directed graphs)\n\n\nMarkov random fields (Undirected graphs)\n\n\n\n\nCheck out today's post on introduction to bayesian networks.\n\n\nhttps://ryanong.co.uk/2020/04/05/day-96-learn-pgm-with-me-representation-introduction-to-bayesian-networks/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-05"
},
{
"vote": 0,
"title": "The working method of Machine Translation expert Franz Josef Och",
"text": null,
"date": "2020-04-03"
},
{
"vote": 12,
"title": "[Q] TF-IDF vectorization of n-grams?",
"text": "As the title suggests, is this done in practice? My experience with n-gram models is that they're often use in \"language modeling\" in text generation algorithms. Conversely, my experience with TF-IDF vectorization is that it's normally used in a unigram setting.\n\n\nBut I'm curious if these have been used together for some task, such as text classification. Naively, I would that as the n grows, the sparsity of the word-document matrix will grow exponentially. So some preprocessing such as stemming would be extremely valuable as to limit this sparsity. \n\n\nFor example (there, is) and (he, was) are probably very common bigrams in virtually any dataset. But (horrible, because) is probably quite infrequent. So my suspicion is that this isn't really done in practice, though it's theoretically useful with a big enough dataset.\n\n\nFurter, with LSTMs, attention models, and even text-based CNNs, this approach probably is just redundant but less effective.\n\n\nThoughts?",
"date": "2020-04-03"
},
{
"vote": 0,
"title": "Day 93 of #NLP365 - Learn PGM With Me â€“ Probability Review For Graphical Models â€“ Elements Of Probability",
"text": "Day 93.\n\n\nToday's blog post is a review of the first section of our probability checklist for graphical models. Below is my notes. I have finally figured out how to write mathematical equations on WordPress although it's not easy to do so...\n\n\nhttps://ryanong.co.uk/2020/04/02/day-93-learn-pgm-with-me-probability-review-for-graphical-models-elements-of-probability/\n\n\nBest,\n\n\nRyan",
"date": "2020-04-02"
},
{
"vote": 4,
"title": "Anyone have any tips for improving QnA for emails?",
"text": "I noticed QnA models are having some trouble with email text. Anyone have experience with this?",
"date": "2020-04-02"
},
{
"vote": 0,
"title": "Can a subjective statement be a fact ?",
"text": null,
"date": "2020-04-02"
},
{
"vote": 7,
"title": "How to identify and extract phrases from a document that are similar to a set of other phrases?",
"text": "[deleted]",
"date": "2020-04-02"
},
{
"vote": 3,
"title": "First time working with Word2Vec, try to cluster users bases on skillset",
"text": "For my thesis I have to make some analysis about the skillsets of potential candidates. I already made a post on Stackoverflow but a friend of mine also advised me to put the question on reddit. So added the link of my question on stackoverflow. Some tips would really  be appreciated.\n\n\n \nhttps://stackoverflow.com/questions/60990292/first-time-working-with-word2vec-try-to-cluster-users",
"date": "2020-04-02"
},
{
"vote": 0,
"title": "Breaking Down Reddit's Imposter",
"text": null,
"date": "2020-04-02"
},
{
"vote": 9,
"title": "Any research scientists able to share their experience?",
"text": "I've noticed that there seem to be a lot more research scientist positions available in industry for PhDs in Computer Science (and related fields) in NLP than there are computational linguist roles. \n\n\nDoes this suggest that a PhD in computational linguistics won't be enough to be competitive for these types of roles after graduation? Is the difference too small to matter? For example, I don't see much difference in conferences that some computational linguists and some NLP/CS faculty publish in: ACL, EMNLP, ICML. My hunch is that for these roles, your publication record is more important than necessarily what you did your degree in.",
"date": "2020-04-02"
},
{
"vote": 15,
"title": "Sentiment Analysis in Python with NLTK. 10 Videos ~ 1hour",
"text": null,
"date": "2020-04-01"
},
{
"vote": 2,
"title": "[D] Health Checks for Machine Learning - A Guide to Model Retraining and Evaluation",
"text": "Machine Learning in production is exponentially more complex than offline experiments. The main reasons being - changing environment, increasing infrastructure, and evolving business needs. In this blog, we talk about model evaluation, maintenance, and retraining.\n\n\n&#x200B;\n\n\nLink\n\n\n&#x200B;",
"date": "2020-04-01"
},
{
"vote": 1,
"title": "Simple Italian-to-English dictionary-based translation in Python?",
"text": "Hi,\n\n\nI've been looking for ready-to-use code where the program translates Italian to English purely based on dictionary, so for each Italian word, it checks if it finds it in the italian-english dictionary, if yes, it translates it. So the result of each translation is expected to be a list of words (the ones that were found in the dictionary). I've been looking for it but I can't find any.\n\n\nAnyone has any suggestion about this?",
"date": "2020-04-01"
},
{
"vote": 18,
"title": "brain2text with seq2seq: \"Machine translation of cortical activity to text with an encoderâ€“decoder framework\"",
"text": null,
"date": "2020-03-31"
},
{
"vote": 4,
"title": "Extractive and abstractive summarization recipes",
"text": null,
"date": "2020-03-30"
},
{
"vote": 1,
"title": "Day 90 of #NLP365 - Learn PGM With Me â€“ What Is Probabilistic Graphical Modelling?",
"text": "Day 90.\n\n\nI am starting a new learning series today, learn probabilistic graphical modelling (PGM) with me! My last few posts was about generative models and I am aware that variational autoencoders are used in NLP; topic modelling being one of the applications. During my master's degree, I took probabilistic inference module and although it was the most challenging module, it was also the most interesting! Variational autoencoders was covered in the probabilistic module which involve the combination of probability and graph theory. From today onwards, part of my time would be devoting to learning this with the aim to both better understand it at a theoretical level and also to be able to practically apply it to the field of natural language processing! Below is my notes on introduction to PGM.\n\n\nhttps://ryanong.co.uk/2020/03/30/day-90-learn-pgm-with-me-what-is-probabilistic-graphical-modelling/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-30"
},
{
"vote": 2,
"title": "MFCC vs PLP",
"text": "Is there are reason why MFCC is preferred over PLP in ASR ? Even though the \npaper\n suggest that PLP performs slightly better than MFCC.",
"date": "2020-03-30"
},
{
"vote": 3,
"title": "How to compute Entity Similarities",
"text": "Hello to everyone, I'm doing a research project and I'm new in this field. I would know if it is possibile to have some sort of scores that indicates the similarity between two entities. My actual project uses tagme API to find entities from texts, then, I would know if it is possibile to reach the following thing:\n\n\nGiven the entity \"Crime\" and the entity \"Death\", I would know if it is possbile to give them some score to indicate their relateness.\nWhat I should look?\n\n\nThank you in advance",
"date": "2020-03-30"
},
{
"vote": 3,
"title": "Sort very large set of unlabeled sentences by its relevance",
"text": "given a very large set of unlabeled sentences . What can be a good method to sort those sentence by its relevance ?   The texts are documentation of physician response to medical cases . some examples:\n\n\n- i have changed the head bandages (--relevant --)\n\n\n- 3/02/2018 - ID 3021034 no answer (-- not relevant--)\n\n\n-  patient complained that he have blood in urine (--not relevant , no response in it --) \n\n\n- wound cleaning , no  blood found  (--relevant--)\n\n\n- 21/02/2015 - ID 9291 removal of object from lungs  (--relevant--)\n\n\nyou got the idea.. \n\n\nat the moment i want to find a unsupervised method , i.e not using a model . Also , i don't have a close list of things that physician are doing (and i won't have it . its not an option) \n\n\nAny idea on how to sort that large list (its about  500k sentences and many of them are just trash)\n\n\nThank you",
"date": "2020-03-30"
},
{
"vote": 1,
"title": "I need to train a BERT model to classify Covid-19 stories as either local, regional, national, or international",
"text": "I'm building an open-source Coronavirus news aggregator with some fellow engineers. We want to be able to discriminate between stories that are happening at the local, community level vs. state capitol vs. Washington D.C. vs., ultimately, stories from around the globe. I have fine-tuned BERT models before using the excellent transformers library from Huggingface, but only for masked language modeling tasks, not question answering.\n\n\nThere is an abundant pool of volunteers who have offered to help build a training set of tagged articles, which makes me optimistic that we can build a comprehensive enough corpus to adequately teach a BERT model to discriminate between 5-6 tags for each of the different article radii. But there's no way that we can build a corpus of examples as large as the SWAG/RACE/ARC datasets, which involve hundreds of thousands of examples. \n\n\n\n\nFor this specific problem, what is a reasonable number of examples to start with? I expect it to be more than a hundred, but would it possible to achieve >90-95% accuracy with fewer than a thousand?   \n\n\nWould it be helpful to fine-tune the model to an acceptably low perplexity on generic multiple choice questions (e.g., SWAG), then fine-tune it again using the domain-specific classification examples generated by other volunteers?",
"date": "2020-03-30"
},
{
"vote": 6,
"title": "Solving old captcha images",
"text": "Recently I've been trying to solve these old style captcha images (see link).\n\n\nMost sites have moved on to using more modern approaches such as ReCaptcha V3. This makes me believe that the older style captchas weren't secure enough and solvable using moder day OCR.\n\n\n&#x200B;\n\n\nHowever, I haven't been able to find any articles or write-ups on how to solve these.\n\n\nWithout succes, I've been trying to solve these using PyTesseract en OpenCV in Python.\n\n\n&#x200B;\n\n\nSo my questions: \n\n\nAre the images of this kind easy solveable nowadays using free software?\n\n\nAre there any known good write-ups or articles on how to solve this problem?\n\n\nIs tesseract a suitable ocr solution for this kind of problem?\n\n\n example image: \nhttps://i.stack.imgur.com/Pismk.jpg",
"date": "2020-03-29"
},
{
"vote": 0,
"title": "BERT Models for Every Language.",
"text": "Why we do not collect all BERT Models for all languages.",
"date": "2020-03-29"
},
{
"vote": 6,
"title": "Swedish BERT model - BERTsson",
"text": "BERTSSON Models\n\n\nhttps://preview.redd.it/75nidwvtsfp41.jpg?width=1020&format=pjpg&auto=webp&s=4e19b079f7c32068ffaf471ab103ef7fc8648320\n\n\nThe models are trained on:\n\n\n\n\nGovernment Text\n\n\nSwedish Literature\n\n\nSwedish News\n\n\n\n\nCorpus size: Roughly 6B tokens.\n\n\nThe following models are currently available:\n\n\n\n\nbertsson\n - A BERT base model trained with the same hyperparameters as first published by Google.\n\n\n\n\nAll models are cased and trained with whole word masking.\n\n\nStay tuned for evaluations and larger models.\n\n\nFeel free to try it out at:  \nhttps://huggingface.co/jannesg/bertsson",
"date": "2020-03-28"
},
{
"vote": 8,
"title": "Searching for an annotated financial dataset",
"text": "Hello! I am trying to find an annotated financial dataset with which to train machine learning classifiers. I know for a lot of NLP research I have read for my thesis, a lot of people work with giant, open-source NLP datasets. I wanted to ask around to check if there might be one suitable to what I need. \n\n\nI will be using this dataset to make predictions on sentences from the Management Discussion & Analysis subsection of U.S. 10-K filings. Thus, ideally the training data would be financial in nature. After a lot of research (checking papers with code, published research, semeval, etc.) I did find a few financial papers that shared their datasets (although most don't) but they are often small and don't seem to generalise well. When I try predicting with models trained on them, they don't seem to work all that well.\n\n\nIdeally, I'd like a dataset that's annotated at the sentence-level (or I guess longer tweets would work, anything around 30 or 40 characters), and with three labels instead of two (the third being neutral). However, datasets with binary classification or annotated at another level (like document-level) might work too, so really anything you know of would be helpful! I guess I'm wondering if there are typical/standard annotated financial datasets that many other NLP/ML researchers use that I don't know about. And if not, are there any massive annotated datasets that might still generalise well to financial text (in this case 10-K text), maybe something like wikipedia based? Thank you :)",
"date": "2020-03-28"
},
{
"vote": 6,
"title": "Transfer Learning in NLP with Tensorflow Hub and Keras",
"text": null,
"date": "2020-03-27"
},
{
"vote": 1,
"title": "No more news datasets! Nina Lopatina explains how she designed the upcoming WMT QE dataset for Russian â†’ English proverbs and Reddit comments, using the scoring method from FAIR and human labels from ModelFront.",
"text": null,
"date": "2020-03-27"
},
{
"vote": 17,
"title": "43 NLP Interview Questions and Answers!",
"text": "[deleted]",
"date": "2020-03-27"
},
{
"vote": 2,
"title": "Getting Started with End-to-End Speech Translation",
"text": null,
"date": "2020-03-27"
},
{
"vote": 1,
"title": "What is the state of the art for the SemEval2013 task for French?",
"text": null,
"date": "2020-03-26"
},
{
"vote": 2,
"title": "Day 86 of #NLP365 - Mini NLP Data Science Project â€“ Implementation VII â€“ Text Similarity",
"text": "Day 86.\n\n\nIn today's post, we used TFIDF to measure the similarity between sentences and clustered them using the K-Means algorithm. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/03/26/day-86-mini-nlp-data-science-project-implementation-vii-text-similarity/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-26"
},
{
"vote": 85,
"title": "Stanfords new Python NLP Library: Stanza",
"text": "[deleted]",
"date": "2020-03-26"
},
{
"vote": 3,
"title": "Inferring sentiment during sentiment analysis annotation?",
"text": "Hi! Wondering what other practictioners of NLP think of this small question of mine.\n\n\nMy team and I are labeling data for sentiment analysis now, and I was wondering if it's ok to infer sentiment based on the text.\n\n\nLike, it's easy to annotate or tag the text as positive when there's language there that signifies happines (\"yey I'm going to disneyland\") or sth. But what if the text doesn't explicitly say any sentiment, but one can infer it?  \n\n\nFor instance, in the context of the covid19, here's a sample tweet:\n\n\n\"Where can we get tested for ncov?\"\n\n\n&#x200B;\n\n\nor\n\n\n\"Call for donations for peasant families! ...\"\n\n\n&#x200B;\n\n\nFirst one can either be just simple fact finding tweet, but can also be out of fear. Second one can also be simply a call out for donations, but can also be inferred as having hope. \n\n\nI understand this will likely be a judgement call, but can I infer the emotions behind the tweet? Or is it safer to declare both of those tweets as neutral (in the pos/neg/neutral scheme)?",
"date": "2020-03-26"
},
{
"vote": 1,
"title": "What is the most effective way to analyze the trend in a given sentence.",
"text": "For example, I have a paragraph/sentence describing the trend of a product sold in the past few days.\nI would like to know whether the trend of selling is going upwards or going downwards or stay average.\n\n\neg:\nOur company is mainly responsible for selling French fries. The product sold in the past few months are soaring rapidly. \n\n\nThank you",
"date": "2020-03-26"
},
{
"vote": 0,
"title": "How to use my own pre-trained Bert model?",
"text": "Hey guys, I am wondering how do I \nuse\n the \nBert\n model I \npre-trained from scratch\n?\n\n\nSo I was using the \nsentence-transformer\n (great repo btw) and had quiet bad results due to the fact that I was working with domain specific terms and vocabs. So I wanted to try different approaches: pre-training and fine-tuning.\n\n\nI followed the pre-training on the official \nBert repo\n and it trained from scratch. Now how do I actually use the output model?\n\n\nHow do I simply feed example sentences into my new model and get the sentence embeddings out? The files I got now are the 'model.ckpt-50' checkpoints, but how do I use it? - Is there a way to use that new model with the simple-transformers library?\n\n\nI am working on a STS task by the way. So what I want is to compare the resulting embeddings/vectors and see if my new results will be better then the old ones.\n\n\nAny ideas? I cant manage to solve it.",
"date": "2020-03-25"
},
{
"vote": 3,
"title": "Using Gensim for tf-idf",
"text": "Hey. I'm quite new to python and NLP (polisci student trying to analyze reddit comment data from pushshift.io) I'm trying to use Gensim python module for tf-idf on a large amount of data. When i process my data, it spits out 3 lists into python, and i'm having trouble figuring out what type of number each list shows. \n\n\n&#x200B;\n\n\nMy python file looks like this: \n\n\n documents = [\"List with strings\", \"...\", \"...\"]  \n\n\n# Create the Dictionary and Corpus \n\n\nmydict = corpora.Dictionary([simple_preprocess(line) for line in documents]) \n\n\ncorpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]  \n\n\n# Create the TF-IDF model \n\n\ntfidf = models.TfidfModel(corpus, smartirs='ntc')  \n\n\ndef tfidf_sorter(tfidf_processed_document):\n\n\nfor doc in tfidf_processed_document:\n\n\ntfidf_list = [[mydict[id], np.around(freq, decimals=2)] for id, freq in doc]     \n\n\nprint (tfidf_list) \n\n\n&#x200B;\n\n\nThen it prints 3 separate lists with words and associated numbers, and i can't figure out what list shows what number.\n\n\n&#x200B;\n\n\nThanks a lot",
"date": "2020-03-25"
},
{
"vote": 3,
"title": "Information Extraction with NER",
"text": "Hi everyone,\n\n\ni have been working on a NLP project with the goal of extracting information of hazardous events from tweets. For example, I would like to extract out the Location, Dates, Time, Number of Casualties etc.\n\n\nMy implementation would be to first do binary classification with BERT to determine if the tweet is hazardous or not hazardous. And then i follow up with BERT NER on the truly hazardous tweets to extract out the previously mentioned entities.\n\n\nThe NER model requires the input data to be in Conll 2003 format.  I have been labeling tweets in my conll-formatted dataset such as B-EVT (hazardous event) or B-LOC (Location) on every word. While labeling, a thought came to my mind.\n\n\nIs it possible that using the NER dataset, for tweets that are truly hazardous, i label the individual words as how i intend to, and for the non-hazardous tweets i label the individual words as '0', skipping the entire need of the classification model in the first place? I would be able to tag entities and at the same time filter hazardous from non-hazardous?\n\n\nLet me know about your thoughts if it is possible or there might be other more effective ways to go about this.",
"date": "2020-03-25"
},
{
"vote": 1,
"title": "Integrating tensorflow's official implementation of nmt with fasttext",
"text": "Hii everyone. I am trying to tackle out of vocab words in my machine translation model build using \nTensorflow's official implementation of NMT\n . I am very much confused how to do it since I am new to NLP. It turns out there is no tutorial or stackoverflow answer available for this. Any other guides/resources about how to integrate word embeddings into this implementation of nmt also will be a great help.\n\n\nThanks a ton !!",
"date": "2020-03-25"
},
{
"vote": 2,
"title": "Is there any Python lib for fast fuzzy searches over documents?",
"text": "I know about fuzzywuzzy and simstring, but I want to search a small string (query) in a longer string (text/paragraph), which have length difference and screw the similarity.",
"date": "2020-03-24"
},
{
"vote": 2,
"title": "Entity Linking Query",
"text": "I have a query regarding a project.\n\n\nI want to detect mentions of a document into a given text and if the document is present in our knowledge base (our own data storage not public URL like  Wikipedia) . Unfortunately, all I could find is an entity linking to Wikipedia. Kindly guide me through this that, Is it even possible to do that kind of stuff I have vague knowledge about NLP so your suggestions are more than welcome.\n\n\nThanks in advance.",
"date": "2020-03-24"
},
{
"vote": 0,
"title": "Advice on Undergrad Study for a Career in NLP",
"text": "Iâ€™m an incoming freshman and Iâ€™m looking for some advice.\n\n\nObviously, itâ€™s far too early for me to be 100% certain about my future, but I would like to build a plan to get to a career in NLP.\n\n\nThe college I am choosing to go to has both a CS degree and a Linguistics degree. \n\n\nI am wondering if it would be alright to major in CS with a minor in Linguistics. I am worried that I wonâ€™t develop a strong enough background in Linguistics, however, if I donâ€™t double major. The reason I would like to minor is because of some external time constraints.\n\n\nWhat would I lose if I donâ€™t double major and instead do a major/minor path?\n\n\nThank you so much in advance.",
"date": "2020-03-24"
},
{
"vote": 0,
"title": "Day 83 of #NLP365 - Mini NLP Data Science Project â€“ Implementation IV â€“ Text Clustering II",
"text": "Day 83.\n\n\nIn today's post, we implemented part of the TopicModelling class. We set up the constructor, the method to create the dictionary (vocab), and the method to turn all our text sentences into its respective numerical representation. We will finish the TopicModelling class tomorrow. Check out today's progress below:\n\n\nhttps://ryanong.co.uk/2020/03/23/day-83-mini-nlp-data-science-project-implementation-iv-text-clustering-ii/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-23"
},
{
"vote": 2,
"title": "Which corpus to use for calculating the probabilities in PMI and NPMI for topic coherence?",
"text": "I'm working on topic modelling, and reading through various papers some say the reference corpus used to calculate the probabilities of two words co-occuring (used in PMI) is 300k documents from wikipedia, some say they use the test set of the corpus they trained on, and in other papers, it is not mentioned at all which reference corpus they use. What are the best practices when it comes to calculating PMI and NPMI? What is the generally accepted approach when it comes to the reference corpus?",
"date": "2020-03-23"
},
{
"vote": 4,
"title": "[Dataset] \"due date of a document\" dataset",
"text": "Hi, \n\n\nI am wondering if anyone knows about a dataset on detecting the \"due date of a document\" . It could be contracts / legal documents / financial documents. \n\n\nIt'll be great if you could link me to something! \n\n\nThanks!",
"date": "2020-03-22"
},
{
"vote": 0,
"title": "Day 82 of #NLP365 - Mini NLP Data Science Project â€“ Implementation II â€“ Text Clustering I",
"text": "Day 82.\n\n\nIf you have read my previous implementation series on summarisation using TFIDF, where I have built a TFIDF summariser class, you would know that I personally like to build up the skeleton of the class before diving into actual code implementation. In today's post, I would introduce the skeleton of my TopicModelling class that I am planning to build and use to cluster the Spotify text messages. I have also included a small section of using VaderSentiment library for sentiment analysis. Check out more details in the blog post below:\n\n\nhttps://ryanong.co.uk/2020/03/22/day-82-mini-nlp-data-science-project-implementation-ii-text-clustering-i/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-22"
},
{
"vote": 17,
"title": "How can I do next word prediction using my own corpus? (E.g. what would person x say after this set of words, or in this context)",
"text": "I have a corpus (around 15K sentences) from an author and Iâ€™d like to apply or play with (preferably) an already built tool or plug n play tool where I give it those sentences and have it predict what THAT person would most likely say after a set of words I type or what word Iâ€™d say in a particular context (something like masked word prediction) \n\n\nIs this possible and if so are there already tools out there I can use to do this? Iâ€™m most comfortable with python and not really an NLP expert, but have played around with existing NLP tools in the past",
"date": "2020-03-22"
},
{
"vote": 1,
"title": "Professional Translation Services",
"text": "[removed]",
"date": "2020-03-21"
},
{
"vote": 7,
"title": "Day 81 of #NLP365 - Mini NLP Data Science Project â€“ Implementation I â€“ Text Processing",
"text": "Day 81.\n\n\nThe next step in the process is text processing. This is where we analyse the support messages to derive methods to process / clean the data. Our text processing function will transform the support messages as follows:\n\n\n\n\nRemove all the missing message body from our dataset.\n\n\nRemove Spotify handles, URLs, punctuations, numbers, and any special symbols. We will also lowercase all the texts.\n\n\nRemove any duplicated messages (might be spams generated from bots).\n\n\nRemove stopwords.\n\n\nLemmatisation.\n\n\nRemove messages that are too short (less than 5 tokens) or too long (more than 25 tokens).\n\n\nJoin all the tokens into a single string.\n\n\n\n\nhttps://ryanong.co.uk/2020/03/21/day-81-mini-nlp-data-science-project-implementation-i-text-processing/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-21"
},
{
"vote": 23,
"title": "Mongolia abandons Soviet past by restoring alphabet",
"text": null,
"date": "2020-03-21"
},
{
"vote": 6,
"title": "Expert in the loop - or how to instruct models to correct its errors ?",
"text": "A question that might sound trivial but in the days black-box deep learning i'm not sure i know how to tackle:\n\n\nLet's say i have a good text classification model, 95% F1 ,No more available data (or atleast small amount of it ).  Now a domain expert is testing my model and he found errors and even can point me out to those errors.  \n\n\nWhat are my available methods to incorporate the expert tested samples (or guidelines regarding my error) into my model? The expert might find. 10-20 erros  , not something that re-training the model with it will change dramatically the dataset . \n\n\nerrors can be from different types but they can be of local overfitting (i.e unrelated words that had correlation to a label in point of time ) or out of vocabulary words ,things like that",
"date": "2020-03-21"
},
{
"vote": 6,
"title": "How to detect a reversed pattern on the opposite side of a string in Python?",
"text": "[deleted]",
"date": "2020-03-20"
},
{
"vote": 1,
"title": "Applied NLP/Lang Tech for the legal field",
"text": "[deleted]",
"date": "2020-03-20"
},
{
"vote": 3,
"title": "word2vec derivative w.r.t. center and context word",
"text": null,
"date": "2020-03-20"
},
{
"vote": 2,
"title": "Question answering through the context",
"text": "Given a set of questions, I need something that can answer those questions (or try) by analysis a bunch of comments that are related to the questions.\n\n\nMy dataset contains questions and comments related to each question. There is no guaranty that the answer can be found just by analysing those comments.\n\n\nAny suggestions on how I should go about solving this problem?",
"date": "2020-03-20"
},
{
"vote": 1,
"title": "I got into the top 24% on a Kaggle Challenge without writing a line of code!",
"text": "[removed]",
"date": "2020-03-19"
},
{
"vote": 1,
"title": "Day 79 of #NLP365 - Mini NLP Data Science Project â€“ Implementation Series â€“ Introduction",
"text": "Day 79.\n\n\nAs mentioned in previous post, I want to shift my NLP focus towards more implementation rather than solely theoretical. I am still in the learning process of striving a balance between the two. Here's a small step towards that balance.\n\n\nIn today's post, I am introducing a multi-series implementation of a mini NLP data science project, where we are dealing with the Spotify customer support messages dataset. As with any data science project, it is very important to have a good understanding of the business context and problem statement. You can find out more information about that and my action plan in the blog post below :)\n\n\nhttps://ryanong.co.uk/2020/03/19/day-79-mini-nlp-data-science-project-implementation-series-introduction/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-19"
},
{
"vote": 26,
"title": "Sentiment and Emotion Analysis using NLP in Python | YouTube Playlist",
"text": "Hey guys, I have been recently working on a series of videos that help in finding out the emotion in text (\njealousy,happy,sad\n etc.) using Python. We will be doing this with and without NLTK. Here is the curriculum  \n\n\nYouTube Playlist\n - \nhttps://www.youtube.com/playlist?list=PLhTjy8cBISEoOtB5\\_nwykvB9wfEDscuEo\n \n\n\n\n\nIntroduction\n- Demo of the project\n- How we will go about the project - First without nltk\n\n\n\n\nInstallation Python and Pycharm\nCreating our first project\n\n\n\n\nCleaning text for Natural Language Processing\n\n\n\n\nTokenization and Stop words\n\n\n\n\nIdentifying Emotions and Sentiments from emotions.txt \n\n\n\n\nComparing Emotions and Counting different emotions\n\n\n\n\nPlotting them with Matplotlib\n\n\n\n\nPositive or negative sentiment\n\n\n\n\nAnalyzing Tweets for Sentiments and Emotions\n\n\n\n\nUsing NLTK for sentiment/mood analysis\n\n\n\n\n\n\nYouTube Playlist\n - \nhttps://www.youtube.com/playlist?list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo",
"date": "2020-03-19"
},
{
"vote": 3,
"title": "Saving TorchText fields for use in production",
"text": "So i feel a bit confused regarding serving pytorch nlp models in production:\n\n\ni have trained a text classification model using pytorch and all the data is preprocessed using the torch-text. Now , i have that field \"TEXT\" where my vocab and tokenizer is stored. when i'm receiving new raw text in production i need to preprocess and transform words into index according to the vocab i've generated during train time\n\n\n&#x200B;\n\n\nso it means that beside the model itself , i also need load that TEXT field right ? how do i have it ?",
"date": "2020-03-19"
},
{
"vote": 5,
"title": "continuous dynamic topic modeling code?",
"text": "I see a few implementations of discrete Dynamic Topic Modeling in python and C++, but I'm much more interested in continuous dynamic topic modeling, as outlined in \nBlei et al (2008)\n.\n\n\nIt doesn't seem like the authors published their code. Is it possible to apply cDTM using gensim?",
"date": "2020-03-19"
},
{
"vote": 8,
"title": "Can someone give constructive feedback on a proposed path from a ling BA --&gt; working in industry compling? (w/o a masters)",
"text": "I've searched far and wide and haven't found anything exactly like this yet, so here goes. (feel free to prove me wrong; I would love to see somebody else working through this specific problem)\n\n\nA little background: I got a ling BA (w/ some CS coursework, little bit of python knowledge) from Cal a little under a year ago, and after doing a lot of research, I am strongly considering pursuing a career in compling. I haven't found much in the way of well-trodden paths from a BA to real CL work, but I'm hoping they exist, because I am not in a position to pay for grad school rn. Thanks in advance to anyone reading. The following guide is based on helpful redditors' DMs, other reddit self posts articles, and countless linkedin work histories.\n\n\nThis is my road map:\n\n\n- do essential readings.\n Jurafsky and Martin, NLTK book, \nrecent\n \npapers\n, \nthis primer\n, \nFoundations of Statistical Natural Language Processing\n, \nInformation Retrieval\n. (I'm sure there are more, so throw 'em at me if you have some)\n\n\n- make a few basic projects\n\n\n- make something original\n, or original-ish, put it under the projects tab on my resume\n\n\n- apply to a billion internships, eventually get one.\n as a data analyst, language analyst, linguist, etc.\n\n\n- do a good job as an intern, make useful things, prove myself\n\n\n- get CL job either (at company I interned for or otherwise)\n\n\nLike I said, this is based on a patchwork of things I've read various places, from the perspective of somebody on the outside, so it could be misinformed or lacking in nuance in all sorts of ways. I expect constructive criticism and I'm all ears--notebook ready. Cheers!",
"date": "2020-03-19"
},
{
"vote": 3,
"title": "How do the Embeddings and Word2Vec actually generalize on unseen vocab words ?",
"text": "Consider an example. I fed these examples into the model as training data (y = 1 means happy)\n\n\n I'm feeling awesome today!\t1\n\n\nI'm sad my cat is ill.\t0\n\n\nReally enjoying this!\t1 \n\n\n&#x200B;\n\n\nNow if I try the sentence \"I'm Great\" on the model, how does it find the cosine similarity of the word, given the fact that Great is not in the vocab plus we cant neglect *Great* to find the similarity",
"date": "2020-03-18"
},
{
"vote": 5,
"title": "Generating text by generating syntax trees?",
"text": "It came to me recently that it should be possible to generate text by generating syntax trees. Is this approach plausible and if so, what can I read about its SOTA?",
"date": "2020-03-18"
},
{
"vote": 2,
"title": "NLP applications for retail environments",
"text": "Hello,\n\n\nI've been asked by work to look into possible NLP solutions that could be applied in our retail stores. Does anyone have any resources they can recommend for me to research? I have some basic understanding of how we can use this but any additional info I can gather would really help.",
"date": "2020-03-18"
},
{
"vote": 3,
"title": "John Snow Labs Spark-NLP 2.4.4: The very first native multi-class text classifier and pre-trained models and pipelines in Russian",
"text": null,
"date": "2020-03-18"
},
{
"vote": 1,
"title": "Natural language processing for market insights and trends in the market for a particular brand",
"text": "[deleted]",
"date": "2020-03-18"
},
{
"vote": 2,
"title": "How to make a BiLSTM do language modeling aggressively ?",
"text": "Coreection: agrressively->autoregressively\n\n\nIs this achievable?",
"date": "2020-03-18"
},
{
"vote": 2,
"title": "Day 77 of #NLP365 - Learn NLP With Me â€“ FLT â€“ Context-Free Languages â€“ Context-Free Grammars",
"text": "Day 77.\n\n\nContext-free languages are specified by context-free grammars. But how are they being represented formally? Check out today's blog post with an example on this :) \n\n\nhttps://ryanong.co.uk/2020/03/17/day-77-learn-nlp-with-me-flt-context-free-languages-context-free-grammars/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-17"
},
{
"vote": 1,
"title": "Are You Going to Touch This? Voice Technology Offers an Option",
"text": null,
"date": "2020-03-17"
},
{
"vote": 10,
"title": "Semantic parsing for Question-Answering clustering",
"text": "Hi NLP community. I was just wondering how crazy it is to apply semantic parsers to find similarities between sentences in a QA setting. The goal is to find clusters of similar answers for a given question. I haven't heard of these methods lately, not even if these have ever been applied to question-answering problems. Seems like Deep Learning and more specifically LSTM architectures are taking over now (Gmail Smart Reply) but I would happy to learn about alternatives. Any suggestion about computationally cheaper solutions will be appreciated, too.",
"date": "2020-03-16"
},
{
"vote": 22,
"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (paper explained)",
"text": null,
"date": "2020-03-16"
},
{
"vote": 2,
"title": "NLP News Cypher | 03.15.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n>\nTensorFlow Quantum\nHaste\nElectra Feel\nHugging Papers\nDataset of the Week: Jeopardy Questions\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-03-15-20-734849e613af",
"date": "2020-03-16"
},
{
"vote": 10,
"title": "Day 74 of #NLP365 - Learn NLP with Me - I.E. - Hedges, Denials, and Hypotheticals - Introduction",
"text": "Day 74.\n\n\nThere are many ways to describe events and relations using natural language. You can describe events in terms of likelihood - is the event likely to happen or not? This brings us to the concept of modality and negation. But what is modality? Check out the blog posts below to find out:\n\n\nhttps://ryanong.co.uk/2020/03/14/day-74-learn-nlp-with-me-i-e-hedges-denials-and-hypotheticals-introduction/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-14"
},
{
"vote": 1,
"title": "Does anyone have any experience using AntPConc?",
"text": null,
"date": "2020-03-13"
},
{
"vote": 4,
"title": "LSTM many-to-one regression architecture?",
"text": "For a research project, I'm trying to build a model that takes a textual job descriptions as inputs and predicts an hourly wage in USD as the output.\n\n\nI've trained a Random Forest Regressor on TF-IDF tokenized word vectors, which worked decently well. Most wages vary between $10/hr to $50/hr with a few outliers >$80+. The mean absolute error is $6. Which means that on average, predictions are ~$6.00 off from what the actual hourly wage was.\n\n\nAs noted at the begging, I'd like to make a better model using LSTM (or some RNN variant) which can take text as inputs and predict the hourly salary. Most many-to-one architectures I've seen use softmax as the final layer for classification purposes. But because my goal is to predict a continuous variable, this is not ideal.\n\n\nA few questions:\n\n\n- Can anyone link a many-2-one LSTM architecture with a regressor output?\n\n\n- Is LSTM a good choice? Job descriptions are often >100 words, which can test the limits of LSTM's ability to \"remember\" the first x words. I want to improve upon TF-IDF vectorization, but I'm not certain that this approach will be fruitful.\n\n\n- Any alternative recommendations you'd like to offer? (Preferably in PyTorch; I wouldn't mind learning Keras and/or TF, but it would be starting from square one for me.)",
"date": "2020-03-12"
},
{
"vote": 5,
"title": "[Help] Actionable diologue phrase classification",
"text": "So im quite new to the NLP field, and have been reading papers on what i think might be the right direction to solving the problem i have but im not sure.\n\n\nThe task is classifying multi-party dialogue utterances/phrases into action-related items. \n\n\ni.e. \"Hey john, can you be sure to get that report to me for Friday ?\" \"yeah absolutely, ill get started on that asap\"\n\n\n&#x200B;\n\n\nIn this example, the model should recognize that John has a new task to do, which is creating a report for friday.  Again im super new to this field so i apologize if im misusing terminology.\n\n\n&#x200B;\n\n\nWhat kind of research papers would you all be reading for accomplishing such a task ? or maybe how would you go about doing so ? I thought intent classification with BERT might be the route but im not sure anymore. Any thoughts or suggestions would be great, cheers!",
"date": "2020-03-12"
},
{
"vote": 2,
"title": "Transfer Learning for NLP",
"text": "Hi guys, \n\n\nDoes anyone know how to finetune a pretrained (on SQUAD dataset) Question Answering model on my unlabeled text data so that it understands the context of my data and give relevant answers ?",
"date": "2020-03-12"
},
{
"vote": 1,
"title": "How to master nlp",
"text": "I have been working in this field for about an year and have an intermediate knowledge of computer vision and NLP. In my organization I am working on very trivial stuff bringing together APIs (NLU, STT) to create a chatbot i.e. very non data science kind of work.\nI just read the transformers paper (attention is all you need , bert) but what next I don't know what to do next .\nHow to practice ,most importantly what to practice?\nAll the advice available is for beginners or how to find a job.\nI want to excel in this field. To be more specific NLP I don't want to be a researcher but an applied scientist kind of guy keeping up with the latest research but not creating models from scratch .\nCan someone please guide me.",
"date": "2020-03-11"
},
{
"vote": 3,
"title": "Pretraining or Finetuning BERT for STS",
"text": "Hi All, I got a question I can not get my head around. I have a special \nSTS\n task at hand and cant solve it. Also I \ndon't find any similar examples\n that could help me with it. Maybe you guys can?\n\n\nMy task is to \npair sentences\n written in a domain \nspecific language\n (Requirements in engineering and product development to be precise).\n\n\nMy problem is that the pre-trained BERT out of the box only has a success rate of about 30% in pairing the right ones based on STS. - So I want to train or finetune the model.\n\n\nOnly that all the STS finetuning examples that I find use training-data similar to this:\n\n\nindex filename year old_index source1 source2 sentence1 sentence2 \nscore\n\n\n0 MSRvid 2012test 0001 none none \nA plane is taking off. An air plane is taking off\n. \n5.000\n\n\n1 MSRvid 2012test 0004 none none \nA man is playing a large flute. A man is playing a flute.\n \n3.800\n\n\n...\n\n\nMy issue with this is that I don't have a matching score to my existing data. All I have is already manually paired sentences/requirements and no negative matches or bad matches. - no scores in any sense.\n\n\nHow can I improve my STS score? Finetuning somehow? Continue Pre-Training in any way?\n\n\nVery thankfull for all tips!",
"date": "2020-03-11"
},
{
"vote": 5,
"title": "Vanilla Transformer as Strong Baseline in Abstractive Summarization.",
"text": "I optimized the \nTensorflow official model\n for Abstractive Summarization. \n\n\nTensor2Tensor\n code was not readable for me so I made it. \n\n\nYou can check it out \nHERE\n.",
"date": "2020-03-11"
},
{
"vote": 1,
"title": "Top five document summarization tools",
"text": null,
"date": "2020-03-10"
},
{
"vote": 18,
"title": "Stretch your NLP legs and build some semantic capturing text features (DS comp, 200k news article dataset)",
"text": "QuantQuest is a data science competition platform similar to Kaggle but for time series and financial problems.\n\n\nWe're currently hosting a competition where you have to use news articles to predict a price feature of 5 stock indexes.\n\n\nUnlike most data science competitions, where the aim is to xgboost your light gbmboost, this competition is looking at the real-world applicable skill of feature engineering with text data. \n\n\nIt's a great dataset for people of all skills to get a little workout and brush up on their NLP skills, so I thought I would post it here in case anyone was interested. Feel free to ask any questions :)",
"date": "2020-03-10"
},
{
"vote": 3,
"title": "Near-lossless Binarization of Word Embeddings",
"text": "\"Experimental results on semantic similarity, text classification and sentiment analysis tasks show that the binarization of word embeddings only leads to a loss of ~2% in accuracy while vector size is reduced by 97%. Furthermore, a top-k benchmark demonstrates that using these binary vectors is 30 times faster than using real-valued vectors.\"\n\n\nNot sure if I'm late to this party, but this is a potential game-changer for real-time NLP. \n\n\nhttps://arxiv.org/abs/1803.09065",
"date": "2020-03-10"
},
{
"vote": 1,
"title": "Day 70 of #NLP365 - Learn NLP With Me â€“ I.E. â€“ Knowledge Base Population â€“ Information Fusion",
"text": "Day 70.\n\n\nThere are often multiple evidence for a single relation type. The knowledge base population needs to populate these evidence and predict the relation that are most likely to fit the situation. Today's blog post is about information fusion and how to aggregate these multiple evidence to predict the most likely relation.\n\n\nhttps://ryanong.co.uk/2020/03/10/day-70-learn-nlp-with-me-i-e-knowledge-base-population-information-fusion/\n\n\nAlthough it's fun to keep on learning new things everyday, I realised that a lot of them is under the hood theoretical stuff. That's why concurrently, I am trying to do online courses such as DataCamp, which focuses more on practical implementations! Maybe I will do a blog post on my thoughts on all the online courses I have completed (there are more data science related but has some NLP components to it).\n\n\nBest,\n\n\nRyan",
"date": "2020-03-10"
},
{
"vote": 35,
"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
"text": null,
"date": "2020-03-10"
},
{
"vote": 0,
"title": "speech recognition model (speech-to-text)",
"text": "I need help to build a deep learning model for speech recognition (speech to text), can some one help me to find a data base and choise the best algorithme??",
"date": "2020-03-10"
},
{
"vote": 18,
"title": "Day 69 of #NLP365 - Learn NLP With Me â€“ Information Extraction â€“ Relations â€“ Knowledge Base Population",
"text": "Day 69.\n\n\nWas feeling lazy to post on reddit today but nonetheless I made it! ðŸ’ªðŸ¼\n\n\nToday's on introducing knowledge base population in the context of relation extraction. Knowledge base population is broken down into two subtasks:\n\n\n\n\nEntity linking (check out my previous posts on them)\n\n\nSlot filling\n\n\n\n\nCheck out the blog post below to learn about slot filling and its difference to sentence relation extraction:\n\n\nhttps://ryanong.co.uk/2020/03/09/day-69-learn-nlp-with-me-information-extraction-relations-knowledge-base-population/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-09"
},
{
"vote": 2,
"title": "NLP News Cypher | 03.08.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n-\nWalking Wikipedia\n\n\n-\nLearning From Unlabeled Data\n\n\n-\nA Jiant Among Us\n\n\n-\nHugging Faceâ€™s Notebooks\n\n\n-\nComposing Semantics\n\n\n-\nTransformer Size, Training and Compression\n\n\n-\nGraphing Knowledge\n\n\n-\nDataset of the Week: DVQA\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-03-08-20-64918ffd1f",
"date": "2020-03-09"
},
{
"vote": 15,
"title": "Amazon Textract... Azure Text Analytics... Watson Natural Language Understanding... Google Cloud Natural Language API - Which NLP APIs have you used?",
"text": "Amazon Textract, Amazon Comprehend, Azure Language Understanding, Azure Text Analytics, Azure Cognitive Search, Watson Natural Language Understanding, Watson Discovery, IBM Watson Knowledge Studio, Google Cloud Natural Language API...\n\n\n&#x200B;\n\n\nWhich of these services have you used (or other APIs)? What was the use case? If you tried multiple services, which worked best for your use case?",
"date": "2020-03-09"
},
{
"vote": 1,
"title": "/r/languagetechnology hit 20k subscribers yesterday",
"text": null,
"date": "2020-03-09"
},
{
"vote": 2,
"title": "[Question] how would you detect all similar sections in multiple documents?",
"text": "[deleted]",
"date": "2020-03-07"
},
{
"vote": 0,
"title": "gpu oc and cooling options",
"text": "hello guys, \n\n\nI'm new to NLP/deep learning area and just now looking to buy an RTX 2080ti. There are so many 2080ti models available and price ranges that I would like to know your opinion and experience about these two issues i'm in doubt:\n\n\n\n\nthe cooling system: 2 or 3 fans? Are heat levels a really critical issue when training deep learning models for long periods (> 2h)?\n\n\noverclock feature: some models offer OC options. Is this feature a big gain in deep learning tasks?  Or using the standard base clock good enough?\n\n\n\n\nthanks a lot",
"date": "2020-03-07"
},
{
"vote": 2,
"title": "Day 67 of #NLP365 - Learn NLP With Me â€“ Fast.Ai NLP Course â€“ Derivation of NaÃ¯ve Bayes &amp; Numerical Stability",
"text": "Day 67.\n\n\nMy notes on \nfast.ai\n NLP course Lecture 6, derivation of Naive Bayes (in NLP context) and the reason behind using log-probabilities for Naive Bayes.\n\n\nhttps://ryanong.co.uk/2020/03/07/day-67-learn-nlp-with-me-fast-ai-nlp-course-derivation-of-naive-bayes-numerical-stability/\n\n\n====================================================================================\n\n\nBelow are all my notes on previous \nfast.ai\n lectures:\n\n\nLecture 5 - \nhttps://ryanong.co.uk/2020/03/06/day-66-learn-nlp-with-me-fast-ai-nlp-course-sentiment-classification-with-naive-bayes-logistic-regression/\n\n\nLecture 4 - \nhttps://ryanong.co.uk/2020/02/26/day-57-learn-nlp-with-me-fast-ai-nlp-course-sentiment-classification-with-naive-bayes/\n\n\nLecture 3 - \nhttps://ryanong.co.uk/2020/02/11/day-42-learn-nlp-with-me-fast-ai-nlp-course-topic-modelling-svd-revisited/\n\n\nLecture 2 - \nhttps://ryanong.co.uk/2020/02/10/day-41-learn-nlp-with-me-fast-ai-nlp-course-topic-modelling-with-svd-nmf/\n\n\nLecture 1 - \nhttps://ryanong.co.uk/2020/02/06/day-37-learn-nlp-with-me-fast-ai-nlp-course-what-is-nlp/\n\n\nComment down below any NLP courses that you would recommend to look at!!\n\n\nBest,\n\n\nRyan",
"date": "2020-03-07"
},
{
"vote": 3,
"title": "Identifying things of significance",
"text": "Preface: I'm not an NLP guy.  I just have an idea about creating a wiki that helps you identify topics in that might not have been marked up, but maybe should.\n\n\nSo, let's say you are rolling your own LOTR fanfic wiki.  You've just finished up the Sauron article and publish it.  Within that article you mentioned the One Ring, but you didn't write up a separate wiki article for it.  You didn't even mark it up as a stub.  You just typed, \"One Ring\" and left it at that.\n\n\nNLP has named entity recognition (NER). Is that the type of tool I would use to flag \"One Ring\" as a thing that maybe-might-should have its own article?  Is there any NER resources smart enough to correlate \" the one ring\" to [[One Ring]] and not \"it was just one ring out of manage\" to [[One Ring]]?\n\n\nI also would want to avoid generic things from being cited by this wiki daemon (e.g. the word \"river\").  \n\n\nI'd appreciate some ideas about where to research.",
"date": "2020-03-07"
},
{
"vote": 16,
"title": "Day 66 of #NLP365 - Learn NLP With Me â€“ Fast.Ai NLP Course â€“ Sentiment Classification With NaÃ¯ve Bayes &amp; Logistic Regression",
"text": "Day 66.\n\n\nMy notes on \nfast.ai\n NLP course Lecture 5, continuing sentiment classification using Naive Bayes and Logistic Regression.\n\n\nhttps://ryanong.co.uk/2020/03/06/day-66-learn-nlp-with-me-fast-ai-nlp-course-sentiment-classification-with-naive-bayes-logistic-regression/\n\n\nHave a good weekend all,\n\n\nRyan",
"date": "2020-03-06"
},
{
"vote": 0,
"title": "No Code Text Classification using Deep Learning",
"text": "This blog post shows you how you can develop text classifiers without writing a single line of code or worrying about GPUs or their setup.\n\n\nMedium Post\n\n\n&#x200B;\n\n\nIf you are interested to learn more head on to :\n\n\nhttp://aasaanai.com/",
"date": "2020-03-06"
},
{
"vote": 1,
"title": "Stance Detection with Dependencies Tree and POS Tagging",
"text": "Hi guys, I'm working on a thing at the moment and I'm trying to do what's basically written on the title of this post. \n\n\nDisclaimer: I'm not trying to get my job done by others. Just share ideas and gathering material (if any). \n\n\nDo you know if there's some study/example/paper on something like that? I'm thinking about dep trees and POS tagging to try not to focus on the train set, so that my model is not too \"word\" dependent. I mean, if I train it on some text, it won't work with something different while, maybe, the structure of agreement and disagreement may show similar patterns event with different actors.\n\n\nLet me know and thank you in advance.",
"date": "2020-03-06"
},
{
"vote": 5,
"title": "Character Embedding Training",
"text": "Can someone explain for me training of character embedding?\nThere's a lot of material about the general idea, behaviour and applications but not so much about really training specific network to do that.",
"date": "2020-03-06"
},
{
"vote": 3,
"title": "Reliable text classification services for test data?",
"text": "I have 10k survey responses (1-3 sentences) that need to be labeled on about 6-8 categories. The goal is to build a text classification model from it.\n\n\nWhat are some reliable services to do this with?\n\n\nI've tried Sagemaker Groundtruth (awful) and ScaleAI (better, but not accurate enough). Any other options?",
"date": "2020-03-06"
},
{
"vote": 3,
"title": "Request for submissions to Workshop on Cross-lingual Event-centric Open Analytics",
"text": "CLEOPATRA â€“ 1st International Workshop onÂ Cross-lingual Event-centric Open Analytics\n\n\nCo-located with The 17th Extended Semantic Web Conference (\nESWC 2020\n).\n\n\nhttp://cleopatra-workshop.l3s.uni-hannover.de/",
"date": "2020-03-06"
},
{
"vote": 2,
"title": "Day 65 of #NLP365 - Learn NLP With Me â€“ Information Extraction â€“ R.E. As Classification Task â€“ Kernel Method",
"text": "Day 65.\n\n\nUsing kernel function to tackle relation extraction classification task. The similarity function can be used as a kernel to tackle the binary classification problem for relation extraction. Check it out below:\n\n\nhttps://ryanong.co.uk/2020/03/05/day-65-learn-nlp-with-me-information-extraction-relations-relation-extraction-as-classification-task-kernel-method/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-05"
},
{
"vote": 2,
"title": "Handling Glove embedding for unknown words?",
"text": "[deleted]",
"date": "2020-03-05"
},
{
"vote": 45,
"title": "\"AI for user delight and safety, Reddit NLP\" - Charles Curry, Head of ML at Reddit, talks about NLP at Reddit at last week's Bay Area NLP meetup [video]",
"text": null,
"date": "2020-03-05"
},
{
"vote": 20,
"title": "The Most Influential NLP Research of 2019",
"text": null,
"date": "2020-03-04"
},
{
"vote": 1,
"title": "Requesting Guidance for Designing a ChatBot Project Architecture",
"text": "Hi All,\n\n\nI am willing to learn advanced text analysis thus planning to build a simple interactive chatbot.\n\n\nThe overall idea is: the chatbot will greet the user first and then ask the user about a topic he/she wants to discuss about..be it game, movie, politics and then the bot and the human will start interaction.\n\n\nMy basic plan is to collect data on various topics and then build a attention model which would be trained with the collected data.\n\n\nI am kinda novice in this field and in learning stage who has never built an end-to-end project.\n\n\nNow, this is where I require help:\n\n\n\n\nShall I create train three different models for say three different discussion topics: game, movie and politics?\n\n\nAnd also for the greeting part..shall all the three above mentioned training data sets have some common greeting data points?\n\n\nAnd finally, if the above stated approach of training three different models for three diff. topics are correct, how to club them all(as the user can choose any topic to discuss)?\n\n\n\n\nMy queries might sound very funny and naive but I would really appreciate if you could guide me to clear my perseption and help coming up with a proper project architecture.\n\n\nThanks!",
"date": "2020-03-04"
},
{
"vote": 2,
"title": "A framework to create interactive scatterplots to visualize differences in phrase usage",
"text": null,
"date": "2020-03-03"
},
{
"vote": 3,
"title": "Question: Are perception verbs events?",
"text": "Hi!  \n\n\nIn the sentence:  \n\n\nI saw Laura plant a bomb.  \n\n\nPLANT is an event.\nIs saw (SEE) an event as well?  \n\n\nThanks!",
"date": "2020-03-03"
},
{
"vote": 1,
"title": "Day 63 of #NLP365 - Learn NLP With Me â€“ FLT â€“ Regular Languages â€“ Finite State Composition",
"text": "Day 63.\n\n\nToday is the last blog post on finite state and regular language. It will cover the concept of modularisation when it comes to the heavy task of building/designing finite state automata.\n\n\nhttps://ryanong.co.uk/2020/03/03/day-63-learn-nlp-with-me-flt-regular-languages-finite-state-composition/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-03"
},
{
"vote": 1,
"title": "Poem",
"text": null,
"date": "2020-03-03"
},
{
"vote": 1,
"title": "[D]: Is there any tutorial for text classification with pretrained word embeddings for our own dataset using pytorch?",
"text": "I am planning to use pytorch for text classification. But I didn't get any proper material for text classification using pretrained embeddings for custom datasets. How to the pass the pass the dataset to the embedding layer with the use of pretrained embeddings. Can any help?",
"date": "2020-03-03"
},
{
"vote": 6,
"title": "AI still doesnâ€™t have the common sense to understand human language",
"text": null,
"date": "2020-03-03"
},
{
"vote": 2,
"title": "Graph database recommendation for NLP-oriented stack",
"text": null,
"date": "2020-03-03"
},
{
"vote": 5,
"title": "[D] : Error analysis of Different deep learning architectures of different word embeddings?",
"text": "Hello, I experimented with different deep learning architectures with different word embeddings for text classification.  From the results, how a good error analysis can be done. What does it mean? Can you anyone suggest some papers related to error analysis of the machine learning models or give some idea of how to explain? I am new to this work please help me.",
"date": "2020-03-03"
},
{
"vote": 7,
"title": "[arXiv:2002.11402] Detecting Potential Topics In News Using BERT, CRF and Wikipedia",
"text": "I am not the author or otherwise affiliated, but I thought this was interesting.  I don't see any links to their code, however.\n\n\n\n\nDetecting Potential Topics In News Using BERT, CRF and Wikipedia\n | Swapnil Ashok Jadhav | \nhttps://arxiv.org/abs/2002.11402\n\n\n> For a news content distribution platform like Dailyhunt, Named Entity Recognition is a pivotal task for building better user recommendation and notification algorithms. Apart from identifying names, locations, organisations from the news for 13+ Indian languages and use them in algorithms, we also need to identify n-grams which do not necessarily fit in the definition of Named-Entity, yet they are important. For example, \"me too movement\", \"beef ban\", \"alwar mob lynching\". In this exercise, given an English language text, we are trying to detect case-less n-grams which convey important information and can be used as topics and/or hashtags for a news. Model is built using Wikipedia titles data, private English news corpus and BERT-Multilingual pre-trained model, Bi-GRU and CRF architecture. It shows promising results when compared with industry best Flair, Spacy and Stanford-caseless-NER in terms of F1 and especially Recall.\n\n\n\n\nOut of interest (I am developing a Stanford CoreNLP Python script) I reformatted (cased; ...) their \"Example 1\" text,\n\n\n> text = 'Around $1-1.5 trillion or around two percent of global GDP,  are lost to corruption every year, president of the Natural Resource Governance Institute NRGI has said. Speaking at a panel on integrity in public governance during the World Bankgroup and International Monetary Fund annual meeting on Sunday, Daniel Kaufmann, President of NRGI, presented the statistic, result of a study by the NRGI, an independent, non-profit organisation based in New York. However, according to Kaufmann, the figure is only the direct costs of corruption as it does not factor in the opportunities lost on innovation and productivity, Xinhua news agency reported. A country that addresses corruption and significantly improves rule of law can expect a huge increase in per capita income in the long run, the study showed. It will also see similar gains in reducing infant mortality and improving education, said Kaufmann.'\n\n\n... and ran it through CoreNLP.\n\n\nSent\tindex\tWORD\tLEMMA\tPOS\tNER\n0\t1\tAround\taround\tIN\tO\n0\t2\t$\t$\t$\tMONEY\n0\t3\t1-1.5\t1-1.5\tCD\tMONEY\n0\t4\ttrillion\ttrillion\tCD\tMONEY\n0\t5\tor\tor\tCC\tO\n0\t6\taround\taround\tIN\tO\n0\t7\ttwo\ttwo\tCD\tPERCENT\n0\t8\tpercent\tpercent\tNN\tPERCENT\n0\t9\tof\tof\tIN\tO\n0\t10\tglobal\tglobal\tJJ\tO\n0\t11\tGDP\tgdp\tNN\tO\n0\t12\t,\t,\t,\tO\n0\t13\tare\tbe\tVBP\tO\n0\t14\tlost\tlose\tVBN\tO\n0\t15\tto\tto\tTO\tO\n0\t16\tcorruption\tcorruption\tNN\tO\n0\t17\tevery\tevery\tDT\tSET\n0\t18\tyear\tyear\tNN\tSET\n0\t19\t,\t,\t,\tO\n0\t20\tpresident\tpresident\tNN\tTITLE\n0\t21\tof\tof\tIN\tO\n0\t22\tthe\tthe\tDT\tO\n0\t23\tNatural\tnatural\tJJ\tORGANIZATION\n0\t24\tResource\tResource\tNNP\tORGANIZATION\n0\t25\tGovernance\tGovernance\tNNP\tORGANIZATION\n0\t26\tInstitute\tInstitute\tNNP\tORGANIZATION\n0\t27\tNRGI\tNRGI\tNNP\tORGANIZATION\n0\t28\thas\thave\tVBZ\tO\n0\t29\tsaid\tsay\tVBN\tO\n0\t30\t.\t.\t.\tO\n1\t1\tSpeaking\tspeak\tVBG\tO\n1\t2\tat\tat\tIN\tO\n1\t3\ta\ta\tDT\tO\n1\t4\tpanel\tpanel\tNN\tO\n1\t5\ton\ton\tIN\tO\n1\t6\tintegrity\tintegrity\tNN\tO\n1\t7\tin\tin\tIN\tO\n1\t8\tpublic\tpublic\tJJ\tO\n1\t9\tgovernance\tgovernance\tNN\tO\n1\t10\tduring\tduring\tIN\tO\n1\t11\tthe\tthe\tDT\tO\n1\t12\tWorld\tWorld\tNNP\tORGANIZATION\n1\t13\tBankgroup\tBankgroup\tNNP\tORGANIZATION\n1\t14\tand\tand\tCC\tORGANIZATION\n1\t15\tInternational\tInternational\tNNP\tORGANIZATION\n1\t16\tMonetary\tMonetary\tNNP\tORGANIZATION\n1\t17\tFund\tFund\tNNP\tORGANIZATION\n1\t18\tannual\tannual\tJJ\tO\n1\t19\tmeeting\tmeeting\tNN\tO\n1\t20\ton\ton\tIN\tO\n1\t21\tSunday\tSunday\tNNP\tDATE\n1\t22\t,\t,\t,\tO\n1\t23\tDaniel\tDaniel\tNNP\tPERSON\n1\t24\tKaufmann\tKaufmann\tNNP\tPERSON\n1\t25\t,\t,\t,\tO\n1\t26\tPresident\tPresident\tNNP\tTITLE\n1\t27\tof\tof\tIN\tO\n1\t28\tNRGI\tNRGI\tNNP\tO\n1\t29\t,\t,\t,\tO\n1\t30\tpresented\tpresent\tVBD\tO\n1\t31\tthe\tthe\tDT\tO\n1\t32\tstatistic\tstatistic\tNN\tO\n1\t33\t,\t,\t,\tO\n1\t34\tresult\tresult\tNN\tO\n1\t35\tof\tof\tIN\tO\n1\t36\ta\ta\tDT\tO\n1\t37\tstudy\tstudy\tNN\tO\n1\t38\tby\tby\tIN\tO\n1\t39\tthe\tthe\tDT\tO\n1\t40\tNRGI\tNRGI\tNNP\tMISC\n1\t41\t,\t,\t,\tO\n1\t42\tan\ta\tDT\tO\n1\t43\tindependent\tindependent\tJJ\tRELIGION\n1\t44\t,\t,\t,\tO\n1\t45\tnon-profit\tnon-profit\tJJ\tO\n1\t46\torganisation\torganisation\tNN\tO\n1\t47\tbased\tbase\tVBN\tO\n1\t48\tin\tin\tIN\tO\n1\t49\tNew\tNew\tNNP\tSTATE_OR_PROVINCE\n1\t50\tYork\tYork\tNNP\tSTATE_OR_PROVINCE\n1\t51\t.\t.\t.\tO\n2\t1\tHowever\thowever\tRB\tO\n2\t2\t,\t,\t,\tO\n2\t3\taccording\taccord\tVBG\tO\n2\t4\tto\tto\tTO\tO\n2\t5\tKaufmann\tKaufmann\tNNP\tPERSON\n2\t6\t,\t,\t,\tO\n2\t7\tthe\tthe\tDT\tO\n2\t8\tfigure\tfigure\tNN\tO\n2\t9\tis\tbe\tVBZ\tO\n2\t10\tonly\tonly\tRB\tO\n2\t11\tthe\tthe\tDT\tO\n2\t12\tdirect\tdirect\tJJ\tO\n2\t13\tcosts\tcost\tNNS\tO\n2\t14\tof\tof\tIN\tO\n2\t15\tcorruption\tcorruption\tNN\tO\n2\t16\tas\tas\tIN\tO\n2\t17\tit\tit\tPRP\tO\n2\t18\tdoes\tdo\tVBZ\tO\n2\t19\tnot\tnot\tRB\tO\n2\t20\tfactor\tfactor\tNN\tO\n2\t21\tin\tin\tIN\tO\n2\t22\tthe\tthe\tDT\tO\n2\t23\topportunities\topportunity\tNNS\tO\n2\t24\tlost\tlose\tVBN\tO\n2\t25\ton\ton\tIN\tO\n2\t26\tinnovation\tinnovation\tNN\tO\n2\t27\tand\tand\tCC\tO\n2\t28\tproductivity\tproductivity\tNN\tO\n2\t29\t,\t,\t,\tO\n2\t30\tXinhua\tXinhua\tNNP\tORGANIZATION\n2\t31\tnews\tnews\tNN\tO\n2\t32\tagency\tagency\tNN\tO\n2\t33\treported\treport\tVBD\tO\n2\t34\t.\t.\t.\tO\n3\t1\tA\ta\tDT\tO\n3\t2\tcountry\tcountry\tNN\tO\n3\t3\tthat\tthat\tWDT\tO\n3\t4\taddresses\taddress\tVBZ\tO\n3\t5\tcorruption\tcorruption\tNN\tO\n3\t6\tand\tand\tCC\tO\n3\t7\tsignificantly\tsignificantly\tRB\tO\n3\t8\timproves\timprove\tVBZ\tO\n3\t9\trule\trule\tNN\tO\n3\t10\tof\tof\tIN\tO\n3\t11\tlaw\tlaw\tNN\tO\n3\t12\tcan\tcan\tMD\tO\n3\t13\texpect\texpect\tVB\tO\n3\t14\ta\ta\tDT\tO\n3\t15\thuge\thuge\tJJ\tO\n3\t16\tincrease\tincrease\tNN\tO\n3\t17\tin\tin\tIN\tO\n3\t18\tper\tper\tIN\tO\n3\t19\tcapita\tcapita\tNN\tO\n3\t20\tincome\tincome\tNN\tO\n3\t21\tin\tin\tIN\tO\n3\t22\tthe\tthe\tDT\tO\n3\t23\tlong\tlong\tJJ\tO\n3\t24\trun\trun\tNN\tO\n3\t25\t,\t,\t,\tO\n3\t26\tthe\tthe\tDT\tO\n3\t27\tstudy\tstudy\tNN\tO\n3\t28\tshowed\tshow\tVBD\tO\n3\t29\t.\t.\t.\tO\n4\t1\tIt\tit\tPRP\tO\n4\t2\twill\twill\tMD\tO\n4\t3\talso\talso\tRB\tO\n4\t4\tsee\tsee\tVB\tO\n4\t5\tsimilar\tsimilar\tJJ\tO\n4\t6\tgains\tgain\tNNS\tO\n4\t7\tin\tin\tIN\tO\n4\t8\treducing\treduce\tVBG\tO\n4\t9\tinfant\tinfant\tNN\tO\n4\t10\tmortality\tmortality\tNN\tO\n4\t11\tand\tand\tCC\tO\n4\t12\timproving\timprove\tVBG\tO\n4\t13\teducation\teducation\tNN\tO\n4\t14\t,\t,\t,\tO\n4\t15\tsaid\tsay\tVBD\tO\n4\t16\tKaufmann\tKaufmann\tNNP\tPERSON\n4\t17\t.\t.\t.\tO\n\n\n\n[\nSET\n is a CoreNLP temporal named entity: \nhttps://stanfordnlp.github.io/CoreNLP/ner.html#description]\n\n\n\n\nThat [CoreNLP, above] is actually not too bad, and compares somewhat more favorably\n\n\n...\n[Set of] Named entities in all sentences: {&#039;DATE&#039;, &#039;SET&#039;, &#039;MONEY&#039;, &#039;TITLE&#039;, &#039;PERCENT&#039;, \n                                           &#039;STATE_OR_PROVINCE&#039;, &#039;PERSON&#039;, &#039;RELIGION&#039;}\n\n{token: ner} dictionary:\n\n{\n  &quot;$1-1.5 trillion&quot;: &quot;MONEY&quot;,\n  &quot;daniel kaufmann&quot;: &quot;PERSON&quot;,\n  &quot;every year&quot;: &quot;SET&quot;,\n  &quot;independent&quot;: &quot;RELIGION&quot;,\n  &quot;new york&quot;: &quot;STATE_OR_PROVINCE&quot;,\n  &quot;president&quot;: &quot;TITLE&quot;,\n  &quot;sunday&quot;: &quot;DATE&quot;,\n  &quot;two percent&quot;: &quot;PERCENT&quot;\n}\n...\n\n\n\n... than the author's comparison in their Table 4,\n\n\n[Stanford]\ndaniel\ninternational monetary fund\nnew york.\nnrgi\nxinhua\n\n\n\nNevertheless, their system is also able to capture named entities such as\n\n\ncapita income\ninfant mortality\ninternational monetary fund            ## also captured by CoreNLP when cased\nannual meeting\nnatural resource governance institute  ## captured also by CoreNLP when cased\npublic governance\n\n\n\n\n\n\n\n[Aside: authors (or arXiv algorithms?) are mucking up the year 2020 in arXiv URLs.]",
"date": "2020-03-02"
},
{
"vote": 1,
"title": "What should be the first approach to classify 1 million tweets?",
"text": "i have 1 million tweets, and i have to classify it. problem is that my ram and gpu cant handle such a big data. i am using google colab. so what should be my approach",
"date": "2020-03-02"
},
{
"vote": 3,
"title": "Day 62 of #NLP365 - Learn NLP With Me â€“ FLT â€“ Regular Languages â€“ Inflectional Morphology",
"text": "Day 62.\n\n\nIn previous post, we discovered that there are two types of morphology: derivational and inflectional. We saw how FSAs was used for derivational morphology. In today's post, we are going to learn about using finite state transducers to do inflectional morphology analysis. A quick recap, inflectional morphology is the addition of details such as gender, number, person, and tense to lemmas. Check out today's blog post below:\n\n\nhttps://ryanong.co.uk/2020/03/02/day-62-learn-nlp-with-me-flt-regular-languages-inflectional-morphology/\n\n\nBest,\n\n\nRyan",
"date": "2020-03-02"
},
{
"vote": 2,
"title": "Email Model Suggestions?",
"text": "[deleted]",
"date": "2020-03-02"
},
{
"vote": 3,
"title": "[Python Library] Accessible BERT-based Sentence Classification with TensorFlow 2 / Keras (NLP)",
"text": "I'm sharing this new Python library for sentence classification with TensorFlow 2.x / Keras that is built on top of HuggingFace Transformers. It's very straightforward.\n\n\nhttps://github.com/brunneis/ernie\n\n\nBERT's best friend.",
"date": "2020-03-01"
},
{
"vote": 17,
"title": "Day 61 of #NLP365 - What Is Semantic Textual Similarity?",
"text": "Day 61.\n\n\nToday's on semantic textual similarity (STS). Ever wonder how a chatbot can suggest questions if they find your original question vague? Well STS makes that possible. The basic method for computing STS between two texts is to compute the cosine similarity between two sentence representations.\n\n\nHowever, Google AI research introduced (old news) few innovative methods to tackle STS. Check out the short blog post below for more details:\n\n\nhttps://ryanong.co.uk/2020/03/01/day-61-what-is-semantic-textual-similarity/\n\n\nHappy Sunday everyone.\n\n\nBest,\n\n\nRyan",
"date": "2020-03-01"
},
{
"vote": 3,
"title": "ConceptNET with Language Models",
"text": "I'm in the process of fine-tuning GPT to generate sentences for a specific task. I want to use Common sense for this by using ConceptNet but I have no idea how to use it. I'm not finding many resources for this. Does anyone have ideas on how to get started, or relevant papers that have used ConceptNet or another Common sense Knowledge Base?",
"date": "2020-03-01"
},
{
"vote": 14,
"title": "Day 58 of #NLP365 - Learn NLP With Me â€“ Formal Language Theory â€“ Regular Languages â€“ Morphology Analysis",
"text": "Day 58.\n\n\nIn this post, we will use finite state acceptor (FSA) to do derivational morphological analysis. We will use FSA to represents morphological rules explicitly so that the FSA can apply existing morphological rules to new words and names.\n\n\nhttps://ryanong.co.uk/2020/02/27/day-58-learn-nlp-with-me-formal-language-theory-regular-languages-morphology-analysis/\n\n\nBest,\n\n\nRyan",
"date": "2020-02-27"
},
{
"vote": 1,
"title": "BETTER LANGUAGE LEARNING â€” Learn languages by speaking",
"text": null,
"date": "2020-02-27"
},
{
"vote": 2,
"title": "Any existing methods to find if the answer to a question is relevant or not?",
"text": "Here, the answer given by \"B\" is not relevant to what was asked by \"A\".\nA: \"What is your favorite brand?\"\n\n\nB: \"My name is XYZ\"  \n\n\nWhat are some techniques we could use to find if answer given to a question is relevant or not? \n\n\nI'm familiar with Natural Language Inference where a text entails/contradicts some other text. I was wondering if there has been some work applying such techniques to the question-answer pairs.",
"date": "2020-02-27"
},
{
"vote": 63,
"title": "I built a tool for naming your website with a pun! (Using word embeddings and phonetic vectors.)",
"text": "Iâ€™ve been working on \nan interactive pun searching tool\n (read: given two input categories, searches among them for words that share a lot of sounds). Iâ€™ve added a feature that \nconstrains the search just among top level domains\n so that people can find fun website names that benefit from the rhyme-as-reason effect.\n\n\neels.deals\ncalzone.zone\npastry.page\npuns.plus\nLeo.CEO\nforearms.forum\ntulle.tools\nlace.place\nnoun.town\nlingo.bingo\n\n\n\nHereâ€™s \na blog post\n explaining more about the whole project. Basically, it finds category synonyms using word2vec and then finds \"puns\" using phonetic nearest neighbor vectors. Iâ€™m interested in feedback on this and what fun domain names people are able to find, and I'm happy to answer any questions!\n\n\nThe UI has a long way to go! Some notes: you can select to search for words that match via alliteration, rhyme, or flexible phonetic match. And you can click on words in the blue columns to do a more specific sub-search, which can often be more helpful than just looking at the top overall results in the center column.\n\n\n(Please please let me know if you actually register a domain you found with this.)",
"date": "2020-02-26"
},
{
"vote": 2,
"title": "A Python package for reading word embedding files (various formats) and easily build an embedding matrix for a vocabulary",
"text": "I wrote this a couple of years ago but I published it only recently. Hope it can be of help to some people.\n\n\nhttps://github.com/janLuke/embfile\n\n\nWritten for:\n\n\n\n\nproviding a common interface for different file formats (including a custom format);\n\n\nproviding a flexible function for building â€œembedding matricesâ€ that you can use for initializing the Embedding layer of your deep learning model;\n\n\ntaking as less RAM as possible: no need to load 3M vectors like with gensim.load_word2vec_format when you only need 20K vectors;\n\n\nsatisfying my (inexplicable) urge of writing a Python package.\n\n\n\n\nThe package is quite well documented, type annotated and (automatically) tested.",
"date": "2020-02-26"
},
{
"vote": 1,
"title": "Learn Spanish",
"text": "[removed]",
"date": "2020-02-26"
},
{
"vote": 2,
"title": "Day 57 of #NLP365 - Learn NLP With Me â€“ Fast.Ai NLP Course â€“ Sentiment Classification With NaÃ¯ve Bayes",
"text": "Day 57.\n\n\nI am constantly doing online courses, both in NLP and Data Science. Currently I am doing the \nfast.ai\n NLP course. Here's my notes on Lecture 4 (or video 4):\n\n\nhttps://ryanong.co.uk/2020/02/26/day-57-learn-nlp-with-me-fast-ai-nlp-course-sentiment-classification-with-naive-bayes/\n\n\nBelow are all my notes on previous lectures:\n\n\nLecture 3 - \nhttps://ryanong.co.uk/2020/02/11/day-42-learn-nlp-with-me-fast-ai-nlp-course-topic-modelling-svd-revisited/\n\n\nLecture 2 - \nhttps://ryanong.co.uk/2020/02/10/day-41-learn-nlp-with-me-fast-ai-nlp-course-topic-modelling-with-svd-nmf/\n\n\nLecture 1 - \nhttps://ryanong.co.uk/2020/02/06/day-37-learn-nlp-with-me-fast-ai-nlp-course-what-is-nlp/\n\n\nI have completed the Stanford CS224N, Coursera NLP (HUS), Datacamp NLP courses, and others. I am considering redoing the CS224N once I finished the \nfast.ai\n course as I completed them 2 years ago but I didn't make solid notes on them.\n\n\nComment down below any NLP courses that you would recommend me to look at!!\n\n\nBest,\n\n\nRyan",
"date": "2020-02-26"
},
{
"vote": 1,
"title": "Text classification of Dutch bank transactions (with multilingual distilBERT)",
"text": "[deleted]",
"date": "2020-02-26"
},
{
"vote": 0,
"title": "Are there any tools from NLP to simplify or ELI5 any complex text filled with jargon?",
"text": "Perhaps similar to text summarizers, are there any NLP tools that can, say, take an input of a wikipedia page, and sort of simplify the language (reduce jargon) so that it's much clearer to someone who is not in the field?",
"date": "2020-02-26"
},
{
"vote": 0,
"title": "10 Best Text Annotation Tools and Services for Machine Learning",
"text": "[deleted]",
"date": "2020-02-26"
},
{
"vote": 1,
"title": "Discussion on Percy Liang's SEMPRE",
"text": "Hello,   \n\n\nI'm fairly new to NLP, and I have been focusing on Percy Liang's SEMPRE as a way to build a semantic parser for robot applications/tasks e.g. (navigation, manipulation, etc.)  \n\n\nI want to pursue a grammar/rules based approach to building my semantic parser, but I have a couple of questions regarding SEMPRE.  \n\n\n\n\nIs SEMPRE widely used? So far in their publications page on github and through google scholar, I have only seen Liang's lab use SEMPRE. Other citations are commonly attributed to the Freebase resources he provides in his SEMPRE papers.  \n\n\n\n\nIf SEMPRE isn't widely used, what is? I'm partially looking to move away from SEMPRE because I'm not having much luck with complicated NL input, and because I plan on using ROS which only has support for Python and CPP.  \n\n\n\n\nI have already experimented with SEMPRE by manually providing canonical phrases for each predicate-action I plan on using for a task, but I didn't have much luck in handling complex, compositional phrases.\n\n\n\n\n\n\nExample Navigational Phrase:\nâ€œGo straight down the hallway past a bunch of rooms until you reach an intersection with a hallway on your left;turn left there.â€\n\n\n Are there ways to provide better grammar rules based on a pre-collected data set of NL input? Or is it standard practice to manually write these rules as well?  \n\n\n\n\nIs SEMPRE considered a Combinatorial Categorical Grammar (CCG)? In his 2013 freebase-emnlp paper, Liang draws a distinction between his approach and CCG's by saying\n\n\n\n\n\"At the compositional level, a semantic parser must combine the predicates into a coherent logical form. Previous  work  based  on  CCG  requires  manually specifying  combination  rules  (Krishnamurthy  andMitchell,  2012)  or  inducing  the  rules  from  annotated logical forms (Kwiatkowski et al.,  2010;  Caiand Yates,  2013).   We instead define a few simple composition rules which over-generate and then use model features to simulate soft rules and categories. In particular, we use POS tag features and features on the denotations of the predicted logical forms.\"  \n\n\nBut it seems really nuanced, especially since I've exclusively used SEMPRE to manually annotated rules in their weird pseudo-lisp.  \n\n\n&#x200B;\n\n\nThanks for reading this very long post! I really appreciate any commentary or input. I've invested quite a bit of time in SEMPRE, but I'm concerned I'm not using it to the fullest or using a very outdated-rules based approach.",
"date": "2020-02-25"
},
{
"vote": 6,
"title": "Visualizing fasttext word embedding w/ t-SNE",
"text": "HI!\n\n\nI'm working with fasttext word embeddings and I would like to visualize them with t-SNE: the main goal is to bring out groupings based on semantic similarity among nouns sharing the Italian suffix -ATA (and-ATA, mazz-ATA, spaghett-ATA, and so on).\n\n\nMy dataset is composed by (more or less) 360 suffixed nouns in -ATA. I've trained the ItWaC corpora with fastText, setting the CBOW Model with a vector dimension of 300. Later on, I've extracted just the -ATA nouns and saved them in a\n.csv file\n, labelling the nouns with numbers representing their semantic grouping, from 1 to 9:\n\n\nlabel    word    embedding\n1        andata    0.74431 -1.5646 -0.17478 -0.31742 0.9255 -0.63189...\n.\n2        mazzata   1.4921 -1.1979 0.46379 -0.52384 2.6954 -0.80644 0.78575...\n.\n3        spaghettata ........................\n\n\n\nI've used this csv file in order to build the t-SNE representation. The code works perfectly for the t-SNE part, and you can find it \nhere\n. This is an \nexample of scatterplot\n I get right now.\n\n\nThe point is that I'm not sure about the code, and of course about the representation I get from it. I'm a beginner, and sometimes I can't figure out what's wrong with the code, or other stuff.\n\n\nEvery time I read about t-SNE, it is applied to huge datasets of word vectors, and my fear is that t-SNE doesn't work so much well with small datasets. Plus, I would like to hear some opinions about using t-SNE with word embeddings: does it work well in general or just for some specific tasks?   \n\n\nHope you can help me!",
"date": "2020-02-24"
},
{
"vote": 4,
"title": "Day 55 of #NLP365 - Learn NLP With Me â€“ Formal Language Theory â€“ Regular Languages â€“ Introduction",
"text": "Day 55.\n\n\nYesterday, I mentioned that we will be exploring three different classes of formal languages, namely:\n\n\n\n\nRegular languages\n\n\nContext-free languages\n\n\nMildly context-sensitive languages\n\n\n\n\nIn today's post, I started learning about Regular Languages. A regular language is any language that can be defined by a regular expression. If you have done NLP projects before, chances are that you would have come across regular expression already but what is a regular expression? Find out in the blog below:\n\n\nhttps://ryanong.co.uk/2020/02/24/day-55-learn-nlp-with-me-formal-language-theory-regular-languages-introduction/\n\n\nBest,\n\n\nRyan",
"date": "2020-02-24"
},
{
"vote": 1,
"title": "LDA VS NMF",
"text": "I have to work on data which is based on student feedbacks/reviews about their course...and most likely these reviews will not be very detailed or perhaps pretty short....like product reviews\nWhich is more appropriate way to perform topic modelling?\nShould I used LDA OR NMF?\nI heard NMF(Non Negative matrix factorization ) performs pretty good in short texts....\nWould love to have comments/recommendations/suggestions on it!!",
"date": "2020-02-24"
},
{
"vote": 2,
"title": "General idea",
"text": "What kind of work in NLP can we set as our targets. While NLP benchmarks like GLUE and SQuAD have on their leaderboard work done by huge companies over months to set major milestones in the field.What exactly can we do to contribute or do something significant at our level which doesn't feel like a waste when seen relatively. \nThe projects that I have been involved in the past include trying to use state of the art models training them for a specific task but it doesn't seem fruitful or the right way to go.Im feeling a bit lost in this context.\nAny guidance would be greatly appreciated!",
"date": "2020-02-24"
},
{
"vote": 1,
"title": "Pre-processing for fasttext embeddings?",
"text": "Hi,\n\n\nI am using fastText embeddings to generate embeddings for a dataset (europarl corpus), however functionality such as analogies (get_analogies) and nearest neighbours return poor results. \n\n\nI was wondering if there are any interesting pre-processing steps to increase the quality of word embeddings. \n\n\nConceptually speaking, the idea of lemmatizing and stemming words does not make sense to me since the whole point of fastText is to have character-level embeddings. \n\n\nSo my questions are:\n\n\n\n\nAre there promising pre-processing steps to take when using fastText embeddings?\n\n\nShould I be concerned with the quality of NN and Analogies prior to training?",
"date": "2020-02-23"
},
{
"vote": 8,
"title": "What does end-to-end mean",
"text": "[deleted]",
"date": "2020-02-23"
},
{
"vote": 1,
"title": "Labelling datasets",
"text": "[deleted]",
"date": "2020-02-23"
},
{
"vote": 1,
"title": "Help me find a tutorial for connectivity clustering ?",
"text": "I am asked to separate all unique tweets with their retweets from the twitter data of tweets and tell the number of original tweets. I have no idea where to begin, but it seems to me that connectivity clustering would be the kind of clustering that is to be implemented here.",
"date": "2020-02-22"
},
{
"vote": 3,
"title": "Parsing for specific Chinese characters in json mapped corpus",
"text": "[deleted]",
"date": "2020-02-22"
},
{
"vote": 10,
"title": "Using DistilBERT to Answer Questions Tutorial!",
"text": "https://www.youtube.com/watch?v=BAKoISDzsGw&list=PL_49VD9KwQ_NzZgEaVu8Qt8JBeVx8aGXS",
"date": "2020-02-22"
},
{
"vote": 19,
"title": "I interviewed the technical founder and CEO of Vector Space AI, Kasian Franks on AI/ML, data, blockchain technology, crypto, startups, and more (podcast)",
"text": "Hi Everyone,\n\n\nI interviewed the technical co founder and CEO of Vector Space AI, Kasian franks on developing  On-Demand Correlation Matrix Datasets for Hidden Relationship Detection in Data & Training in Artificial Intelligence (AI) Systems, blockchain technology, NLP/NLU, and more!\n\n\nI believe the community may find the conversation to be interesting as we discuss various topics that tie into artificial intelligence, blockchain, and machine learning. Kasian Franks is a serial tech entrepreneur with decades of experience in the bio and life science industries as a computer engineer.\n\n\nI hope everyone enjoys the conversation!\n\n\nLibsyn\n // \niTunes\n // \nGoogle Play\n // \nSpotify\n // \nOvercast\n\n\n The episode is also available on most platforms as well.",
"date": "2020-02-21"
},
{
"vote": 0,
"title": "Having problems with OCR",
"text": "[deleted]",
"date": "2020-02-21"
},
{
"vote": 5,
"title": "John Denero, senior Google Tranlate researcher and UC Berkeley faculty, told me not to pursue NLP to my face as an undergrad (was majoring in Linguistics and taking CS classes).",
"text": "[deleted]",
"date": "2020-02-21"
},
{
"vote": 1,
"title": "How to find the correlation between a word/phrase and the overall polarity?",
"text": "I have a body of text with an argument about ocean acidification, and how it's bad for the environment. I want to have an algorithm associate acidification (the phrase) with negativity. I've tried using sentiment analysis with Python and NLTK to find the average polarity of all sentences with \"acid\" in it, but that didn't really work, because it ended up giving me a positive polarity overall (which contradicts the overall claim of the article and thus is wrong). Can anyone point me in the right direction?",
"date": "2020-02-20"
},
{
"vote": 3,
"title": "Good language models to use on dialogue/conversational language?",
"text": "I have used the \nGoogle LM 1B\n corpus in the past for testing written language, e.g. other periodicals.\n\n\nHowever, I'm now using text from an IM chat, which is conversational. I would like to measure predictability, but wasn't sure how accurate probabilities given by the LM 1B would be, given that it was mostly trained on newswire text.\n\n\nDoes anyone know of a language model tailored to conversational language?",
"date": "2020-02-20"
},
{
"vote": 5,
"title": "Is the \"curse of dimensionality\" in clustering valid for word embeddings using cosine similarity?",
"text": "Hi,\n\n\nAs I wrote above I wanted to know if reducing word embeddings dimensionality is necessary and if there's some literature about it. My goal is to cluster around 100-200 semantic topics from my word embeddings.",
"date": "2020-02-20"
},
{
"vote": 8,
"title": "Source for generating sentences with certain grammatical structures?",
"text": "Is there a resource that can produce sentences with certain grammatical structures?  Like a sentence starting with a dependent clause, or one with a semicolon?",
"date": "2020-02-20"
},
{
"vote": 3,
"title": "These apps want to beat ChinaÃ¢Â€Â™s censorship by turning words into a mess",
"text": null,
"date": "2020-02-20"
},
{
"vote": 1,
"title": "Is it possible to update a Doc2Vec Vector?",
"text": "I am working with a steadily growing corpus. I train my Document Vector with Doc2Vec which is implemented in Python. \nIs it possible to update a Document Vector?\nI want to use the Document Vector for Document recommendations.\n\n\nDoes Somebody here have experience with this?",
"date": "2020-02-19"
},
{
"vote": 3,
"title": "Building and analyzing an updateable dataset of posts content for a subreddit &amp; dataset of posts about moral dilemmas from r/AmItheAsshole",
"text": "The following article shares a dataset of collected moral dilemmas shared on r/AmItheAsshole as well as the judgments handed down by the community: \nhttps://blog.dvc.org/a-public-reddit-dataset\n\n\nThe article also explains how to get such a dataset for a subreddit, and some things you can do to research its content.",
"date": "2020-02-19"
},
{
"vote": 12,
"title": "Any paper / github repo about self supervised learning on Text classification?",
"text": null,
"date": "2020-02-19"
},
{
"vote": 7,
"title": "Opinions on Solr vs ElasticSearch?",
"text": "From what I've read online, they sort of copied each others features until both can pretty much do what the other does. \n\n\nThe application I had in mind was document ranking based on text contents.\n\n\n I was wondering if anyone here has worked with either, and what are their opinions on it.",
"date": "2020-02-19"
},
{
"vote": 2,
"title": "What are Search &amp; Recommendation Systems in Machine Learning?",
"text": null,
"date": "2020-02-19"
},
{
"vote": 1,
"title": "[R] Up Close and Personal With BERT â€” Googleâ€™s Epoch-Making Language Model",
"text": "[removed]",
"date": "2020-02-18"
},
{
"vote": 1,
"title": "How is the task of Question Answer ranking modelled in a Neural Network?",
"text": "So given a question with N answers and each answers ranked from 1-10 where 1 denotes best answer and 10 denotes the worst/ bad answer.\n\n\n&#x200B;\n\n\nHow do we model this task as ranking task in a neural network.\n\n\n\n\nto every answer the question will be attached Or\n\n\neach question along with its answer is fed into the NN and we try to maximize the ranking score as one hot\n\n\n\n\n&#x200B;\n\n\nCan anyone shed light on this topic ?",
"date": "2020-02-17"
},
{
"vote": 6,
"title": "Pointer generator network implemented in AllenNLP released with a pretrained PyTorch model",
"text": null,
"date": "2020-02-17"
},
{
"vote": 2,
"title": "NLP News Cypher | 02.16.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n>\nThe Galkin Graphs Cometh\nCompressing BERT\nDeepMind Keeps It PG-19\nDeepSpeed and ZeRO Cool\nOpen Domain QA Strikes Back!\nWhy Models Fail\nKaggle, Colab and Mr. Burns\nDataset of the Week: WinoGrande\n\n\n \nhttps://medium.com/towards-artificial-intelligence/nlp-news-cypher-02-16-20-e3101b7400cb",
"date": "2020-02-17"
},
{
"vote": 2,
"title": "How to copy weights from TF hub model to a local model?",
"text": "I am interested in copying weights from an existing TF hub model lets say Universal Sentence Encoder to my local model.\n\n\n&#x200B;\n\n\nIs this possible ? or my model will be required  to be trained in teacher student fashion ?",
"date": "2020-02-17"
},
{
"vote": 13,
"title": "More than 250 NLP Datasets Found Here! [UPDATE]",
"text": "[removed]",
"date": "2020-02-17"
},
{
"vote": 7,
"title": "Show words/phrases used to make prediction with Roberta (Pytorch transformers)",
"text": "I have a Roberta classification model trained using FastAI and the transformers library. How would I see the words used to make the decision and their importance?\n\n\nKinda like FastAI's show intrinsic attention function ( \nhttps://docs.fast.ai/text.interpret.html\n ) except for transformers? (BERT, Roberta, etc)\n\n\nOr like this:  \nhttps://github.com/sergioburdisso/pyss3/blob/master/examples/extract_insight.ipynb\n  except for pytorch (fastai) models?",
"date": "2020-02-16"
},
{
"vote": 18,
"title": "Data extraction from structured documents",
"text": "I have an NLP \\ data extraction and am hoping someone can nudge me in the right direction on the solution. I have a large set of PDF docs and need to extract about 10 distinct data items from them. There is a some structure to the documents though the grammar will vary somewhat. I need to extract things like â€œdate of treatmentâ€ , â€œfrequency of treatmentâ€, â€œexpected completion dateâ€ etc. The language and vocabulary used is very domain-specific. \n\n\nI managed to get decent extraction results using rules and regular expressions but I know this wonâ€™t necessarily scale to new unseen documents as itâ€™s very dependent of the exact language used and sentence structure.\n\n\nThe approach I was thinking of was to take the thousands of documents I have and to train a language model on those . I would then like to train another model using annotated documents. I think this is a custom NER problem, where the entities are specific to my domain. But Iâ€™m struggling to see how I should start with this. Iâ€™ve looked at the transformers library and I can see how it would help but Iâ€™m just not sure how to tackle this. Would anyone be able to provide a high level approach to tackling this problem?",
"date": "2020-02-16"
},
{
"vote": 0,
"title": "What datasets / testing suites / bench marks for information retrieval models? Particularly for document matching for queries?",
"text": "[deleted]",
"date": "2020-02-16"
},
{
"vote": 2,
"title": "Can I use HuggingFace's byte-level BPE encoding tokenizer for Hindi language",
"text": "In this \nhttps://huggingface.co/blog/how-to-train\n they have used \"byte-level Byte-pair encoding tokenizer\" for Esperanto language. My question is, if I want to train a mix of English, Hinglish & Hindi LM, can I use the same tokenizer?",
"date": "2020-02-16"
},
{
"vote": 3,
"title": "Finding Products and pricing in text?",
"text": "I am working on a project where I would like to identify a product and the price the product is selling for.  The post the products are being sold in are not organized and differ from post to post.  I will give two examples below: \n\n\nexample: 1\n\n\nTitle:  Roku \n\n\nBody: I am selling a roku for $10 message me if you are interested \n\n\n\n\nexample 2: \n\n\nTitle: Roku\n\n\nbody:  I am selling my roku because I just got a new smart tv for my birthday.  I only used the roku for a few months and it is in perfect condition.  I am selling it for 10.  \n\n\n\n\nAs you can see both posts are different but ultimately have the same information.  \n\n\nI have figured out how to solve this problem using regex, but I think NLP would be more interesting. Additionally a lot of the submissions are selling multiple products and not just one which is an issue for matching product to price for regex.  \n\n\nI am new to NLP and looking for advice one where to focus to solve a problem like this.  \n\n\nThe data set I have for this problem is about 15,000 submissions now so i think it is big enough to start working on.  \n\n\nThank you",
"date": "2020-02-15"
},
{
"vote": 1,
"title": "Noun categorization?",
"text": "Iâ€™m wondering if thereâ€™s any existing research on categorizing nouns into buckets, such as:\n\n\n\n\nForest: setting\n\n\nTeacup: object\n\n\nEfficiency: abstract idea\n\n\n\n\nIs there a term I could search for this kind of thing?",
"date": "2020-02-15"
},
{
"vote": 2,
"title": "How to get feature vector from a topic model",
"text": "Hi everyone,\n\n\nI have a problem to solve and I'm not an expert at all in this field. I have a word embedding model from which I get a vector X from a set of terms (using mean or sum). I need a set of topics each with a vector in order to get most similar topics of X. Possible solutions I have seen so far:\n\n\n- Use clustering (like k-means) on the embeddings, I should get a vector for each cluster (I think). Downside: word embeddings are clustered not only by semantics but also part of speech (I think)\n\n\n- Use a topic model like LDA on the same corpus where the embeddings where trained. Downside: The only way I can think of to get topic vectors is a weighted average of the topic words vectors (where the weight is the word-topic probability)\n\n\n- Use lda2vec. Seems like it solves the problem but I'd like a simpler  solution for the sake of my project.\n\n\nI am open to everything that is fairly simple and straightforward. As I said I'm not an expert so don't start citing hard methods for a beginner to understand, because I have to understand the basics.",
"date": "2020-02-14"
},
{
"vote": 1,
"title": "Collaborators for COLING2020 SemEval Task 5",
"text": "Due to very tight deadline I am looking for collaborators/co-authors for \n\n\nhttps://competitions.codalab.org/competitions/21691#learn_the_details-overview",
"date": "2020-02-14"
},
{
"vote": 3,
"title": "Last minute collaboration on SemEval2020 Task 5",
"text": "[deleted]",
"date": "2020-02-14"
},
{
"vote": 9,
"title": "What are some NLP research papers that a beginner would be able to understand?",
"text": "I need to read and summarize two research papers per week as part of my NLP class, but I haven't been able to find many papers that I really understand. I've taken a class on machine learning, so I'm not a complete noob on this stuff. Any suggestions?",
"date": "2020-02-13"
},
{
"vote": 2,
"title": "Hands-on with Hugging Faceâ€™s new tokenizers library",
"text": null,
"date": "2020-02-13"
},
{
"vote": 20,
"title": "State of the Art Keyword Extraction",
"text": "What is the state of the Art of Keyword Extraction without regarding the language of a Corpus?\n\n\nI am reading in KeyGraph, Chi Square, RAKE or even LTSM Networks to extract Keywords. \n\n\nAre there any resources on how Amazon, Facebook or Google handle this?\n\n\nCan anybody here share his experience with Keyword Extraction?",
"date": "2020-02-13"
},
{
"vote": 2,
"title": "Encoder Decoder",
"text": "Why do we one hot our sequence before feeding it to encoder decoder model?",
"date": "2020-02-13"
},
{
"vote": 10,
"title": "Automated system can rewrite outdated sentences in Wikipedia articles",
"text": null,
"date": "2020-02-13"
},
{
"vote": 32,
"title": "Microsoft Open Sources ZeRO and DeepSpeed: The Biggest Language Model in Human History",
"text": null,
"date": "2020-02-13"
},
{
"vote": 6,
"title": "[tutorial] Getting the text fragments involved in the classification decision",
"text": "Some time ago, one of this subreddit's users posted the question \"\nIs there a away to get the part of the text where classification was found?\n\", since he had no substantial response to his problem, I've decided to write a tutorial on how to get the list of text fragments involved in the classification decision-making process.\n\n\nIn case anyone else is interested, the \ntutorial's Jupyter Notebook\n can be opened and executed on Binder, an online executable environment, by clicking on the following link: \nhttps://mybinder.org/v2/gh/sergioburdisso/pyss3/master?filepath=examples/extract_insight.ipynb\n.\n\n\nHope someone else find it useful, have a nice day everyone!",
"date": "2020-02-12"
},
{
"vote": 13,
"title": "Has anyone used Transformer models to predict continious variables?",
"text": "I was tinkering around, trying to model a continuous variable using Bert/Roberta. The Transformer part of the model ending up giving the exact same outputs, to whatever the text input is; such that the output of the overall model was around the average value of the target in the dataset. \n\n\nI tried looking up using Bert to model a continuous variable, but couldn't find much. I am wondering if anyone has worked on this.",
"date": "2020-02-12"
},
{
"vote": 3,
"title": "CCMatrix: A billion-scale bitext data set for training translation models - H Schwenk, A Joulin",
"text": null,
"date": "2020-02-12"
},
{
"vote": 2,
"title": "Calculating Political Bias and Fighting Partisanship with AI",
"text": null,
"date": "2020-02-12"
},
{
"vote": 1,
"title": "The Best Data Collection Tools for Machine Learning",
"text": null,
"date": "2020-02-12"
},
{
"vote": 29,
"title": "Camphr: spaCy plugin for transformers, udify, elmo, etc.",
"text": "I've released [camphr](\nhttps://github.com/PKSHATechnology-Research/camphr/\n), which is a \nNatural Language Processing\n library that helps in seamless integration for a wide variety of techniques from state-of-the-art to conventional ones. You can use \nTransformers\n , \nUdify\n, \nELmo\n, etc. on \nspaCy\n.\n\n\nFeatures:\n\n\n\n\nA \nspaCy\n plugin - Easily integration for a wide variety of methods\n\n\nTransformers\n with \nspaCy\n - \nFine tuning\n, \nEmbedding vector\n\n\nUdify\n - BERT based multitask model in 75 languages\n\n\nElmo\n - Deep contextualized word representations\n\n\nRule base matching with \nAho-Corasick\n, Regex\n\n\n(for Japanese) \nKNP\n\n\n\n\ndocument: \nhttps://camphr.readthedocs.io/en/latest/\n\n\nrepo: \nhttps://github.com/PKSHATechnology-Research/camphr/",
"date": "2020-02-11"
},
{
"vote": 2,
"title": "Top 2020 Voice Conferences",
"text": null,
"date": "2020-02-11"
},
{
"vote": 8,
"title": "Applying NLP on Buttigieg and Sanders Reddit Data",
"text": null,
"date": "2020-02-11"
},
{
"vote": 3,
"title": "Algorithms for topic modeling",
"text": "Hi everybody, i am working on a NLP topic modeling project.\nThe idea is to catch topics from a group of reviews and i tried with bow+lda. To catch more semantic meaning, i used trigram so that when i analyze the topics i can find easily interpretable terms.\n\n\nSadly the most used trigram are actually the same one but with differente sequence : i.e. \"wait long time\", \"long time wait\" or \"wait time time\", ect.\n\n\nMy actual solution is to create a temporary vocab with the most used trigram and use it as input vocabulary for a new vectorizer. This limits the vocabulary but also restricts the same-meaning trigram, cutting them off.\nI use the sklearn CountVectorizer and LDA, and with this escamotage i found better topics.\n\n\nHope you guys got some hints for me, even switch to some thing more intriguing as skip-gram or glove, which i dont know how to use for unsupervised clustering.",
"date": "2020-02-10"
},
{
"vote": 42,
"title": "Turing-NLG: A 17-billion-parameter language model by Microsoft - Microsoft Research",
"text": null,
"date": "2020-02-10"
},
{
"vote": 1,
"title": "Language translation service, trends and process",
"text": null,
"date": "2020-02-10"
},
{
"vote": 2,
"title": "BERT Project - Seeking advice",
"text": "Hi! I'm new to NLP (and Deep Learning) and am interested in specifically applying BERT for a project. The idea is to be able to predict the counts of the reactions (Like, Wow, Sad, Happy etc.) to a Facebook post. \n\n\nThere seems to be two approaches when using BERT which I'm having trouble deciding between: the fine-tuning approach and the feature-based approach.\n\n\n In the BERT paper the fine-tuning approach is applied to Sentence Pair Classification Tasks, Single Sentence Classification Tasks, Question Answering Tasks and Single Sentence Tagging Tasks. However as my project is a multi-target regression problem, is this approach a feasible one? I was thinking to perhaps treat it as a Single Sentence Classification Task (even though I'm treating the collective sentences of a Facebook post as a single sentence), but use the CLS token as an input to a neural network where the outcome is multi-target regression. The paper does mention that some problems may not be suitable for a Transformer-based architecure, though I'm not sure if a regression problem is one of them. \n\n\nThe alternative would be to use a feature-based approach. This means I could just do the embedding pre-processing of my data once, however from looking around I've seen it mentioned that BERT word/sentence embeddings don't hold much advantage over word2vec/glove, unless BERT is first fine-tuned for a task beforehand (though I'm not sure what task this would be or then why not just use fine-tuning), though this itself could cause issues with generalisation. I'd then just use the embeddings to try different models with. \n\n\nAlso, I'm not sure if it would be better to predict ratios of the reactions instead of their counts? And if indeed ratios are better, would I just use linear activation, get the counts and normalise them to 1 or would I use a softmax activation.\n\n\nSo basically I'm a bit confused but hopefully it's clear what are the things I'm confused about. Appreciate the guidance. Thanks!",
"date": "2020-02-09"
},
{
"vote": 1,
"title": "Beginner looking for help",
"text": "Hello everyone,\n\n\nI have just recently started learning python for a course i'm taking, and struggling with an assignment; I'm hoping someone here could help out.\n\n\nAssignment purpose is to build an automatic abstract following a list of steps.\nI'm only sharing the steps up to which I am currently stuck:\n\n\n\n\nGiven a text in a text file, let say text.txt Â \n\n\nOpen and read the content of the file\n\n\nSegment text into sentences based on punctuation (. ! ? etc. )\n\n\nAdd first three sentences (s1, s2, s3) to the abstract\n\n\n\n\nI found an RE function that splits by punctuation, but after that point I can't figure out how to view those splits as sentences to be able to copy them to a new file.\n\n\nI tried using tokenization functions and likewise, after getting a division, I can't figure out how to read tokens as lines to copy them to the new file.\n\n\nHere is the latest failed; it only creates the file but doesn't copy any content to it:\n\n\nwith open('testfile.txt', 'r') as f:\n text = f.read()\n with open('abstract.txt', 'w') as f1:\n for sent in nltk.sent_tokenize(text):\n   PUNCT_RE = regex.compile(r'(\\p{Punctuation})')\n   tokens = (PUNCT_RE.split(sent))\n for tokens in f:\n   f1.write(tokens)\n\n\nHelp? : <",
"date": "2020-02-09"
},
{
"vote": 0,
"title": "A 2020 Guide To Text Moderation with NLP and Deep Learning",
"text": null,
"date": "2020-02-09"
},
{
"vote": 2,
"title": "What is the example of low-level features and high-level features in NLP?",
"text": "In computer vision, first few layers of CNN learns low level features such as line, or a small size of shape. The layer after that learns high level of features like a bigger size of curve or an object.\n\n\nDoes this concept applied to NLP as well? What does lower layer and higher layer of transformer/any architecture learn? Some solid example will helps",
"date": "2020-02-09"
},
{
"vote": 14,
"title": "Code walkthrough huggingface transformere",
"text": "Does anyone know if there is some code walkthrough video what is going on in the different classes of the huggingface transformers source code? A lot of times you see some lines and question what that line is exactly doing.",
"date": "2020-02-08"
},
{
"vote": 1,
"title": "Language Model Integration with CTC",
"text": "[deleted]",
"date": "2020-02-07"
},
{
"vote": 2,
"title": "German University Recommendation For a 3rd World Country Student",
"text": "[removed]",
"date": "2020-02-06"
},
{
"vote": 4,
"title": "NLP conferences anyone?",
"text": "Scholars, I have a tight deadline to publish a paper and putting out a conference paper seems the clearest path. does anyone have links to a database of available conferences or any ideas to help me put out the paper in a week? Cheers. The field of course, is Natural Language Processing.",
"date": "2020-02-06"
},
{
"vote": 27,
"title": "Machine Wisdom: Inspirational Quotes from GPT-2",
"text": "https://machineswisdom.com/\n\n\nI've been experimenting with language generation recently and I made this quick project after training GPT-2 on ~60 thousands quotes from famous authors, artists and philosophers.\n\n\nThe website uses the small version of GPT-2 (117 millions of parameters) which was fine-tuned for ~1700 steps (batch size of 32). A simple sample-and-rank decoding strategy was used similarly to a \nrecent chatbot paper from Google\n.\n\n\nThe model's generation is filtered to \nonly include quotes that don't exist already\n (from those present in the training data). However, since the original GPT-2 model was trained on a large amount of text from the Internet, it is possible that a small number of the quotes generated can be found out there.\n\n\nAny feedback is more than welcome!",
"date": "2020-02-06"
},
{
"vote": 0,
"title": "Google Translate is brilliant.",
"text": "Check this weird ass google translate session. Instructions: Read first line of the left box, then first line of the right box, and so on). Might have to re-read it a couple of times before you really see/get what is going on :-)! Hope you like it. \n\n\nhttps://preview.redd.it/b142nebvz3f41.png?width=1737&format=png&auto=webp&s=40e7f284e35f3f969543701aa6f4b64f1af6bf1b",
"date": "2020-02-05"
},
{
"vote": 2,
"title": "Someone in NLP with linguistics background?",
"text": "Hello, is there someone in this forum who is working in NLP or computational linguistics, but comes from a linguistics or languages background (teaching, translation etc.)?",
"date": "2020-02-04"
},
{
"vote": 1,
"title": "question about transcribers",
"text": "I feel like transcribers (i.e. tools that convert audio of a person speaking to written word) are kind of lousy. Is it possible to \"train\" a transcriber to learn the idiosyncrasies of a person's particular idiolect and become more accurate? How would this be done? If it's possible, can someone link me to resources to learn how this is done? Thanks!",
"date": "2020-02-04"
},
{
"vote": 19,
"title": "John Snow Labs Spark-NLP 2.4.0: New TensorFlow 1.15, Universal Sentence Encoder, Elmo, faster Word Embeddings &amp; more",
"text": null,
"date": "2020-02-04"
},
{
"vote": 1,
"title": "John Snow Labs Spark-NLP 2.4.0: New TensorFlow 1.15, Universal Sentence Encoder, Elmo, faster Word Embeddings &amp; more",
"text": "[deleted]",
"date": "2020-02-04"
},
{
"vote": 5,
"title": "How to train BERT and XLNET models from custom data?",
"text": "I have a massive dataset that I want to train using BERT and XLNET models. Can anyone guide me with links or tips on how to achieve this? Any help is appreciated.",
"date": "2020-02-04"
},
{
"vote": 11,
"title": "Polyglot word embeddings discover sharp monolingual clusters",
"text": null,
"date": "2020-02-04"
},
{
"vote": 8,
"title": "How can i evaluate output generated text by a GPT-2 model?",
"text": null,
"date": "2020-02-03"
},
{
"vote": 2,
"title": "NLP News Cypher | 02.02.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n>\nBERTs Lingua Franca\nDeep Learning Boot Camp\nMeena is Perplexing\nThe Conscious Mind\nA Token of Appreciation\nS&P Global NLP White Papers\nDeployment Headaches\nDataset of the Week: QA-SRL Bank\n\n\n \nhttps://medium.com/towards-artificial-intelligence/nlp-news-cypher-02-02-20-16dda25930bd",
"date": "2020-02-03"
},
{
"vote": 1,
"title": "Help on interview- Cortana",
"text": null,
"date": "2020-02-03"
},
{
"vote": 5,
"title": "I wrote a program to make annotating NER data for spaCy easier",
"text": "[deleted]",
"date": "2020-02-02"
},
{
"vote": 1,
"title": "Problems with Relation Extraction on entities of the same type",
"text": "[deleted]",
"date": "2020-02-01"
},
{
"vote": 23,
"title": "FitBERT - Use BERT for smart string interpolation without deep learning experience",
"text": null,
"date": "2020-01-31"
},
{
"vote": 2,
"title": "Short Text to graph",
"text": "Lately I am trying to rank to encode text as graphs\nWhat I have already found\n\n\nI have seen companies like dbpedia converting text to graph\n\n\nSome directly do it via grammar like use VP- NP to create links,\nOthers use co reference resolution to make links,\nEtc.\n\n\nI was trying to actually like judge which one of these or other would be feasible for converting short sentences to graph \n\n\nLike deriving a graph from a group of short sentences to show the context.\n\n\nAny comments, discussion, blogs, links, paper, etc are welcome",
"date": "2020-01-31"
},
{
"vote": 2,
"title": "How difficult/easy is to learn NLP once you have experience in CV/Deep learning?",
"text": "I have experience in computer vision/deep learning from last 3-4 years. \nNow I am getting a job offer which deals with NLP/chatbots. My confusion/doubts is how much overlap dies these both techniques have? Is it easy to learn NLP given deep learning experience? Having understanding about both is a good choice or shall I stick to only CV?\nWhat are pros/cons of taking this position?",
"date": "2020-01-31"
},
{
"vote": 5,
"title": "How difficult/easy is to learn NLP once you have experience in CV/Deep learning?",
"text": null,
"date": "2020-01-31"
},
{
"vote": 9,
"title": "Best off-the-shelf Open-Domain QA Tool?",
"text": "Hi all, my company is doing a hackathon next week and I had an idea I wanted to explore that uses open-domain question-answering. The gist is: we have a software product with text-based documentation that I'd like to be able to answer search-query like questions about, and display those in search results as cards. My thinking is that this would require a model like one that is successful at \nGoogles Natural Questions Challenge\n. One key limitation: I seriously doubt we have enough data to train from scratch, so I'm looking for something that will be able to produce hackathon-grade results off the shelf with the ability to fine-tune at a later point. Any recommendations? Thanks!",
"date": "2020-01-30"
},
{
"vote": 1,
"title": ".txt to .conll file conversion",
"text": "Dear NLP people,\n\n\nI have a txt file which contains annotations in CoNLL format. For another script, I need to convert this file into a .conll file. I cannot find a program or script that could help with this task. Please share any recommendations (can be Python, Java,.. whatever comes to mind, I'll take it!).\n\n\nThanks!\n\n\nEdit: the solution to this issue is renaming the file with <filename>.conll via Bash.",
"date": "2020-01-30"
},
{
"vote": 2,
"title": "evaluation measurements for clustering algorithms when you don't have a gold standard",
"text": "As the name implies what are the options for evaluating clustering algorithms when you do not have labeled data specifying each cluster. I am aware of the silhouette score but believe this is for choosing the number of clusters required rather than how \"good\" the clusters are.\n\n\nI have visualized my clusters and analyzed the topn terms in each one by frequency which has allowed me personally to judge how useful the clusters are for my application. But a number measurement is often useful for comparison and makes it easier for people to understand. I am aware of MI, AMI, NMI but do not have a gold standard to generate these so as I said are there any other options for evaluation measurements for clustering algorithms when you don't have labeled data",
"date": "2020-01-30"
},
{
"vote": 7,
"title": "More than 200 NLP Datasets Found Here! [UPDATE]",
"text": "Friends,\n\n\nWe've updated the NLP database! We added 26 new datasets!  We had several user contributors: Chandra Bhagavatula and Kiril Gashteovski. Thank you very much!\n\n\nWill continue to update you as more datasets come in.\n\n\nDatabase: \nhttps://quantumstat.com/dataset/dataset.html",
"date": "2020-01-29"
},
{
"vote": 3,
"title": "how to train elmo models to generate embeddings",
"text": "Hi, I am currently using a word2vec model I have trained with my own dataset to generate word embeddings and look at the top n most similar terms. I would like to do the same thing with elmo but am a bit confused as to how it works.   \n\n\nFrom my understanding elmo is similar to word2vec but is able to provide greater context for words that can be used in multiple contexts. Is this done in a way similar to bert where you can take advantage of an existing model and then fine tune it by showing it examples of sentences that use the particular context your interested in? or, do you just train a model and the vectors it generates are more sensitive to the words being used ?  \n\n\nFor example if I want to distinguish the different contexts of the word virus between medical use, cyber attack use and any other contexts. I imagine by fine tuning an existing model I could better classify the context I want whereas using an existing model it may not have a large amount of examples of the cyber attack use case and therefore not accurately represent that context. Can anyone clear this up for me ?, also if anyone has any good straightforward tutorials on how to train an elmo model that they could share it would be much appreciated.",
"date": "2020-01-29"
},
{
"vote": 4,
"title": "Guidance on getting started?",
"text": "Hey all. I'm an experienced software engineer who hasn't done much in NLP. Some folks on one of our teams have been trying to get a workable NLP solution for a while, and in my inexperienced in this field's opinion, I think the use case is fairly uncomplicated.\n\n\nWe have some reports that are written in free text. We are looking only for certain keywords. We are attempting to codify these old reports by searching for certain text strings such as \"thingy type of medical term\". It is all medical data. \n\n\nIs NLP overkill for this? Or a perfect use case? Would it perhaps be simpler to write some regex and then process this way?\n\n\nDon't really know where to start. Happy if the answer is: \"Follow this guide and come back and ask again.\" Thanks in advance.",
"date": "2020-01-29"
},
{
"vote": 0,
"title": "\"Chatbot Tsunami\": The Good &amp; Bad",
"text": null,
"date": "2020-01-29"
},
{
"vote": 3,
"title": "Different stacking/aggregating options for Sentence/Document Embedding.",
"text": "There are now a great amount of embeddings for downstream NLP tasks. I would like to know what are some effective stacking/aggregating methods that you have employed for generating sentence/document embeddings, using embeddings from one or multiple models.",
"date": "2020-01-28"
},
{
"vote": 4,
"title": "Pretrained LDA model for Wikipedia",
"text": "Where can I find a pretrained LDA model for Wikipedia, preferably a gensim model? \nThank you in advance.",
"date": "2020-01-28"
},
{
"vote": 11,
"title": "NLP News Cypher | 01.26.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n-\nThe Dutch RoBERTa\n\n\n-\nHacking GitHub for Blog Posting\n\n\n-\nServerless, VMs and Containers\n\n\n-\nMellonâ€™s Twitter NLP Library\n\n\n-\nA New Dataset for Visual Question Answering\n\n\n-\nAI Content Made Simple\n\n\n-\nmmmmmBART\n\n\n-\nKnow Your Hardware\n\n\n-\nDataset of the Week: DailyDialog\n\n\n \nhttps://medium.com/towards-artificial-intelligence/nlp-news-cypher-01-26-20-626fe7b7c24c",
"date": "2020-01-27"
},
{
"vote": 2,
"title": "Tutorial: Image Captioning with Keras",
"text": "https://blog.paperspace.com/image-captioning-with-ai/",
"date": "2020-01-26"
},
{
"vote": 3,
"title": "Looking for insights for a project",
"text": "I'm essentially trying to build a basic voice authentication program with Python as a step into NLP.\nThe basic idea, as I see it, is going to be to have someone say some phrase to set the template (I'd probably have to create an allowance for vocal variance) and then simply have it compare that template to any following 'attempts'. I'm not looking for it to do much, probably a simple message \"Voice match!\" if the same user uses the same passphrase or \"Not a match!\" in other cases.   \n\n\nI'm fairly comfortable in Python, but I'm curious if anyone has any suggestions about tools or resources to get going, or insights on the idea above, as I've never done an NLP project before.   \n\n\nCurrently, I'm thinking about using Parselmouth so I can utilize spectrograms from Praat. But Praat only comes to mind because it's the default from Phonetics classes, so maybe there's a better tool out there for this?  \n\n\nEventually, I'd like to evolve this into something meatier - analyzing different aspects of different users' voices, but I figure this would be a good starting point.",
"date": "2020-01-26"
},
{
"vote": 1,
"title": "Suggestions on an NLP project.",
"text": "I am a beginner and currently wanna work on a project that will create a chrome extension that analyzes any cold emails and gives you ideas on how to write them using examples off the internet and from our backend data.\n\n\nSomething like this: \nhttps://www.producthunt.com/posts/cold-email-rating\n but I want to focus more on how to writes better emails think of it as Grammarly for sales. \n\n\nI also will have the data to personalize in the sense that it will tell you everything about the person that the user gonna write to e.g. about location, personal news, company news, etc We already have the API that has that data.\n\n\nWhat the NLP techniques and algos I would require and the flow of the steps like how should I proceed with it.",
"date": "2020-01-25"
},
{
"vote": 1,
"title": "Coreference resolution involving clauses?",
"text": "[deleted]",
"date": "2020-01-24"
},
{
"vote": 27,
"title": "What sort of things do you look for when hiring an intern for an NLP position?",
"text": "I'm a researcher on theoretical machine learning and with an interest on NLP. This is from previous research and current unpublished work. But I'm curious on what sort of things the teams look for when hunting for an intern for an NLP related position.",
"date": "2020-01-24"
},
{
"vote": 0,
"title": "The 1st Workshop on Automatic Simultaneous Translation at ACL 2020",
"text": "[deleted]",
"date": "2020-01-22"
},
{
"vote": 3,
"title": "what are the must read textbooks for getting into sentiment analysis",
"text": "textbooks for getting started with sentiment analysis",
"date": "2020-01-22"
},
{
"vote": 41,
"title": "More than 200 NLP Datasets Found Here! [UPDATE]",
"text": "Friends,\n\n\nWe've updated the NLP database! We added 21 new datasets! In addition, we've also added a new column for date \"added\", to know which datasets are the most recent ones. We had several user contributors: Debanjan Chaudhuri, Stefan Larson & Abeed Sarker. Thank you very much!\n\n\nWill continue to update you guys as more datasets come in.\n\n\nDatabase: \nhttps://quantumstat.com/dataset/dataset.html",
"date": "2020-01-22"
},
{
"vote": 1,
"title": "5 step language translation process for multimedia translation",
"text": null,
"date": "2020-01-22"
},
{
"vote": 20,
"title": "Using NLP to parse recipe lines with spaCy",
"text": "Hi all, I'm fairly new to Natural Language Processing and I was hoping I could get some advice. \n\n\nI've been trying to write a command-line program that takes in a list of recipes and returns a grocery list with all the necessary ingredients for all of them, combining like ingredients as necessary. For example, if one recipe called for one cup of flour, and a second recipe called for two cups of flour, then the grocery list would have three cups of flour on it. \n\n\nTo that end, I've been using spaCy's parts-of-speech and dependency taggers to write a pretty simple set of conditionals to parse recipe lines and make a best guess for the ingredient, amount, and measurement is. I have a small writeup of what I've been doing \nhere\n (warning, extremely rough/new personal blog link). What I've got so far works \nmost\n of the time, but as I note above, it's janky and misses edge cases. \n\n\nWhile doing some googling about the problem, I found this article from the New York Times: \nExtracting Structured Data from Recipes using Conditional Random Fields\n, and found it a pretty fascinating read. Now I'm wondering if this is something I can implement in my own program. Thing is, I'm still pretty new to Natural Language Processing and I don't have the best concept of what's viable for one guy to do, especially when he's learning along the way. I have a couple of stupid questions for yall, if you'll tolerate a newbie.\n\n\n\n\nIf I was able to implement a trained model, is that something I can upload to github for others to use, or will anyone who wants to use my program need to train it themselves.\n\n\n\n\nIs it possible for me to hand annotate enough examples to train this sort of program? \n\n\n\n\nIs this something that I train a spaCy model for, or would I need to switch to NLTK? \n\n\n\n\nIs any of this even remotely viable for a guy who got into NLP like a week ago/Python a few months ago? Part of me feels like I'm jumping the gun here, but it's so interesting I don't really want to stop.\n\n\n\n\n\n\nThanks so much for reading and for any advice.\n\n\nEdit: Forgot to post link to actual \ncode\n. Git is still a bit confusing to me, hope I got the upload right.",
"date": "2020-01-21"
},
{
"vote": 2,
"title": "Is there a spanish parser that take clauses/phrase groups into consideration ?",
"text": "Pretty much the title, do you know of any parser that take into account subordinate clauses, prepositional phrases ect. \n\n\n&#x200B;\n\n\nRegards",
"date": "2020-01-21"
},
{
"vote": 3,
"title": "Language-Agnostic Representation Learning for Product Search on E-Commerce Platforms",
"text": null,
"date": "2020-01-21"
},
{
"vote": 9,
"title": "Need a Contractor for an NLP gig",
"text": "Is it ok to post this? If not, where?\n\n\nI have a set of a few thousand articles in the high-tech/computers space that need to be tagged and (if possible) summarized. Iâ€™m not sure if what Iâ€™m asking is doable. Plenty of time, say, a month or two to get it done. Then, Iâ€™d like to use it for other projects as well. (Again, not sure if this is possible.) Iâ€™ve got budget but not massive. Something reasonable. Thanks!",
"date": "2020-01-21"
},
{
"vote": 1,
"title": "Text to speech for a 'noobie'",
"text": "[deleted]",
"date": "2020-01-20"
},
{
"vote": 4,
"title": "Every day I spotlight an NLP or DS GitHub repo that needs more attention",
"text": "[deleted]",
"date": "2020-01-20"
},
{
"vote": 2,
"title": "Trying to build an application which classifies the websites you visit into categories and topics.",
"text": "[deleted]",
"date": "2020-01-20"
},
{
"vote": 2,
"title": "Automated essay scoring using document similarity",
"text": "[deleted]",
"date": "2020-01-20"
},
{
"vote": 73,
"title": "Curated resource list for Deep NLP beginners",
"text": "Hey guys, I just translated my curated list for learning Deep NLP\n\n\nIt was basically written in Korean, but I decided to translate this into English to share more people who want to study Deep NLP.\n\n\nIt contains Mathematics, Deep Learning, and Natural Language Processing courses and books.\n\n\nAnd also introduces famous blogs that publish Deep NLP posts, useful libraries, and Famous Researchers !\n\n\nI hope you guys enjoy it and if you have any recommendations about this project please submit an issue !  \n\n\nhttps://github.com/Huffon/NLP101/blob/master/README_EN.md",
"date": "2020-01-20"
},
{
"vote": 0,
"title": "I need forums and message boards IN HINDI on the topics of Insurance, Banking, Retail, and Telecommunications",
"text": "In short, I need natural language data. Hindi data. Hindi data of conversational text from the web on the topics of Insurance, Banking, Retail, and Telecommunications.\n\n\nI can find plenty of Indian forums and message boards in English. Examples are below.\n\n\nFor Insurance:\n\n\nhttp://www.policywala.com/forum/\n\n\nFor Telecommunications:\n\n\nhttps://broadbandforum.co/\n\n\nThese are good resources for getting real conversational text from real people about these topics, specific to the Indian markets. But the problem is that this is all in English. I need this in Hindi, not in English. Is there any equivalent to forums like the links above, except in HINDI and not in English?\n\n\nI also require discussion boards in the fields of BANKING and RETAIL. Banking can be discussion boards on finances in general, like what financial institutions to use, bank accounts credit card transactions, etc. that are specific to India. Retail is a bit different, maybe I'm looking for forums where people buy, sell, and trade items. Or a discussion forum for a department store in India where products and prices are discussed. But again, I need these all in Hindi and not in English.\n\n\nIs there anything like this available on the web? Could anyone in here provide some help or guidance with this? Thank you.",
"date": "2020-01-20"
},
{
"vote": 1,
"title": "What is the easiest way to create a parse tree from Stanford Parser code ?",
"text": "Hello fellow friends,\n\n\nas an amateur to language technologies, it's quite hard to understand how to convert this code(s)\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/65nui8j2omb41.png?width=1038&format=png&auto=webp&s=5e3794917f5074d0a0371667c7e57d3add0ef2c0\n\n\nto graphic tree structure,\n\n\nif anyone can recommend a simple tool to do that on Windows or Mac, it would make my day.\n\n\n&#x200B;\n\n\nRegards\n\n\n&#x200B;\n\n\nPS ;\n\n\nthe kind of tree I am trying to generate\n\n\nEDIT : Thanks to brunakoch, I found the solution : go to \ncorenlp.run\n and chose \"constituency parse\" from the options. \n\n\nCheers !\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/flg5ea4tmqb41.png?width=1704&format=png&auto=webp&s=eb1c9eba8c9faf5f2154edc0abe589c01f26c284",
"date": "2020-01-19"
},
{
"vote": 7,
"title": "Common Sense + Sentiment",
"text": "Does any have recommendations on resources (paper, code, ...) for how to do sentiment analysis over common sense? Example: \"Delta Airlines dumps fuel on school children.\" Lexical sentiment analyzers won't catch any adjectives or verbs of emotion, sentiment, ... but common sense tells us that this is a negative event.",
"date": "2020-01-18"
},
{
"vote": 2,
"title": "Any papers about how much data you need for asr",
"text": "[deleted]",
"date": "2020-01-18"
},
{
"vote": 2,
"title": "How to find specific rhetorical devices in a body of text with Python?",
"text": "I'm working on a natural language processing project where I need to find specific rhetorical devices. I first need to identify the instances of antithesis (opposite phrases that go together to form a relevant idea), alongside examples of parison (or syntactic parallelism) (sentences that share grammatical structures. For example, \"he gave the red dog a blue ball that has a white stripe.\" This would return red dog, blue ball, and white stripe because they share the grammar).\n\n\nFor the antithesis, I have \nno\n idea where to even begin. However, for the parisons, I have a plan: tokenize all sentences in the text, tag them with the nltk.pos_tag function (parts of speech tagger), then find patterns in the parts of speech. For the parisons, I'm stuck on finding some algorithm for recognizing the patterns. Can anyone help with either of these two problems? Thanks!",
"date": "2020-01-18"
},
{
"vote": 8,
"title": "Sentences classification/assignment",
"text": "Hi all, I am interested in doing a project where I want to assign certain sentences (or text blocks) to not pre-labeled classes. So lets say I want to analyze 20 different sentences and want to know which sentences belong together (are assigned to the same class) and which not.\n\n\nWhat would be a way to achieve that?\n\n\nMy understanding is, that BERT for example needs pre-defined labels (such as Sports/Politics) with the trained examples to assign sentences. But in my case I don't want to limit the outcome to specific classes.\n\n\n&#x200B;\n\n\nTL/DR: How do I assign sentences with similar topic to each other without prelabeling?",
"date": "2020-01-17"
},
{
"vote": 0,
"title": "Help with TF-IDF Vectorizer matching implied indices in resulting DataFrame",
"text": "I'm working on building a tool that will scrape content on a webpage and run TF-IDF scores on it to understand the value of the top keywords for SEO content optimization. \n\n\nI've got the scraping down to extract the webpage content. I've got the pre-process steps of removing stop words, tokenizing, ect. \n\n\nI start with a list of 892 words.\n\n\nI begin by using CountVectorizer() to fit_transform my document. I have a sparse matrix that is (892, 394). From there I'm able to produce IDF scores for each feature word in a desired Pandas DataFrame. Things look great to this point. \n\n\nWhen I go to calculate the TF-IDF scores, I'm met with an error in passing along the final TFIDFVectorizer. See code and error below.\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer  \ntfidf_vectorizer=TfidfVectorizer(use_idf=True)\nfitted_vectorizer=tfidf_vectorizer.fit(words_ns)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(words_ns)\n\n\n\nThen I try to create a Pandas DataFrame out of it\n\n\ndf = pd.DataFrame(tfidf_vectorizer_vectors, index=tfidf_vectorizer.get_feature_names(), columns=[&quot;tfidf&quot;])\ndf.sort_values(by=[&quot;tfidf&quot;],ascending=False)\n\n\n\nThis is the resulting error\n\n\nValueError: Shape of passed values is (892, 1), indices imply (394, 1) \n\n\n\nCan anyone provide a solution to getting the shape of the DataFrame to match the implies indices?",
"date": "2020-01-17"
},
{
"vote": 14,
"title": "Simple explanation of transformers?",
"text": "Hey everyone!\n\n\nTL;DR\n - I haven't been able to understand any examples of transformers that I've read (be it academic paper, medium article, or reddit post). I wrote out my confusion/understanding below - I think a bit of my difficulty comes from the fact that so many of these papers are very formula-dense. Has anyone found a very simple explanation or a guided tutorial with a tiny dataset?\n\n\nI've been dabbling in NLP for the past few years, but I've been avoiding learning any transformer-based applications. If I'm being honest, I just can't seem to follow the formula-heavy documentation from the 'Attention is all you need' paper (and even the medium articles that summarize it). Has anyone found a good explanation that they'd be comfortable sharing that's geared towards a non-academic audience? Maybe some simple code to build your own baby transformer with a small text dataset?\n\n\nI wrote out my current understanding below. Please feel free to correct my ignorance!\n\n\n\n\nA CNN will read the sentence \"I like dogs\" and would treat it the same as \"dogs like I\" because order is not taken into consideration. For this reason, we don't see a whole lot of CNN models in NLP applications.\n\n\nA RNN going left-right in a sentence will take word xi and stir it into the pot with the previous layer, which was every word in the sentence up to that point of the sequence. RNNs were state of the art in NLP for this reason.\n\n\nThe transformer incorporates a self-attention mechanism, which I'm understanding as each word getting scored based on relevance to the words around it. Not entirely sure how we arrive at these values though... But, the transformer's output differs from that of a RNN, as it doesn't necessarily go left to right. Rather, it iterates through every word in the phrase (and outside of the sentence if its BERT/ELMO) and compares it to every other word in that evaluated statement. The vector of those outputs are then fed into a RNN?\n\n\n\n\nAll of the transformer explanations that I'm seeing use language translation to show the input vs output, which is where some of my confusion is happening. I suppose they're losing me when they show the before/after translation of \"yo soy John\" to \"I am John.\" What specific score is calculated for each word and how did they arrive at it? Was there any additional input involved? I understand that you many models use already-trained BERT model - where does that come into play? Is there an assumed large dataset that trained the model shown in examples?\n\n\nThis is the most cohesive article that I've been able to find so far (it loses me on the transformer section):  \nhttps://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n\n\n&#x200B;\n\n\nedit: Grammar.\n\n\nedit#2: Thanks for taking the time to reply so far! I'll take some time to read through these replies over the weekend.",
"date": "2020-01-16"
},
{
"vote": 4,
"title": "Graph knowledge bases",
"text": "Hi, I am starting to dive into the Graph knowledge bases world. I am looking for good tutorials, books and most importantly recommended python packages.\n\n\nWill appreciate any help",
"date": "2020-01-16"
},
{
"vote": 1,
"title": "Difference between Entity and Slot",
"text": "Hi, guys. I've been studying Rasa documentation while trying to create a bot, but the concepts of Entity and Slot are still making me confused, as they seem very similar. After a quick search online, I found some websites that actually refer to these two concepts as synonyms, but it seems that Rasa considers them to be two very different things, so I was kinda lost. If someone could help me with this, I'd very much appreciate it. Thanks!",
"date": "2020-01-15"
},
{
"vote": 2,
"title": "Transfer learning for using large models in other languages",
"text": "A lot of work has been going around regarding transfer learning and I think itâ€™s pretty cool, that you can take a model and use it for a lot of other tasks. What I am curious to know is whether we could some how use transfer learning on large models trained on English to do tasks in other languages. I have looked into some work such as FBs library LASER, on zero shot transfer, but I havenâ€™t really understood how it can be put to use. Thanks in advanced!",
"date": "2020-01-15"
},
{
"vote": 2,
"title": "Best way to understand intent of commands in Python",
"text": "Hi!\n\n\nI'm new to this, and I was just wondering what is the best way to understand commands in Python. \n\n\nLet's say my command is \"Turn the light off\". From that, I want to know that the intent is to have the lights turned off.\n\n\nI understand that natural language processing software already exists for Python (such as NLTK), but it seems overkill just to understand simple commands.\n\n\nThanks!",
"date": "2020-01-15"
},
{
"vote": 15,
"title": "HappyTransformer: A new open-source library that allows you to easily utilize transformer models for masked word prediction, next sentence prediction and binary sequence classification",
"text": null,
"date": "2020-01-14"
},
{
"vote": 3,
"title": "Perform most_similar queries on big models",
"text": "Hi everyone,\n\n\nI'm new to the ML and NLP world. I'm working on a project that needs embeddings from the wikidata knowledge graph. The problem is that it contains something like 80 million nodes and I wonder how to perform queries on a model that does not fit in memory.",
"date": "2020-01-14"
},
{
"vote": 1,
"title": "Table Detection, Information Extraction and Structuring using Deep Learning",
"text": null,
"date": "2020-01-14"
},
{
"vote": 2,
"title": "Help with NLP project",
"text": "i am working on a project where i have to analyse the chats of people with their psychologist.  i have to do sentimental as well as context analysis so that i can tag each user with their specific type of mental illness or problems by analysing their chats at the backend. what according to you is the best way to approach this. also can you recommend any dataset which i can use to do to the sentimental analysis. early reply would be really beneficial.",
"date": "2020-01-13"
},
{
"vote": 7,
"title": "NLP News Cypher | 01.12.20",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\n This Week:\n\n\n>\nGPT-2 for Tweeting\nNeural Module Network Effects\nToo Many Recaps, Not Enough Time\nLexâ€™s Vision is 2020\nTime for a Fireside Chat?\nReading Comprehension Evaluation Server\nUsing BERT for NLU\nDataset of the Week: AQuA\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-01-12-20-1783a8a753e5",
"date": "2020-01-12"
},
{
"vote": 3,
"title": "Need help for project idea",
"text": "Any suggestions for a practical use case of a question answering system? We were planning to do something with resumes but it didn't work out cuz the data we found didn't have a general pattern.The QA model is able to work out only the most basic questions which are directly answered\nAny suggestions for a problem statement where it can be made use of?",
"date": "2020-01-12"
},
{
"vote": 14,
"title": "A Chinese court ruled an algorithmic generated article can be copyrighted, demonstrating sufficient originality",
"text": null,
"date": "2020-01-12"
},
{
"vote": 10,
"title": "Is it possible to use NLP to determine sentence modifiers?",
"text": "I'm a graduate student in writing and I'm currently writing my thesis paper on sentence modifiers. I'm collecting a lot of data, pouring through thousands of sentences and categorizing them based on whether or not the modifier appears at the beginning, middle or end.\n\n\nExample:\n\n\nTwice a day, I take the train to work.\n\n\nTwice a day is the modifier. It could be moved around in the sentence to say:\n\n\nI take the train, twice a day, to work\n\n\nI take the train to work twice a day.\n\n\nIs there a program out there that can recognize the location of a modifier in a sentence, and determine how many of each particular style exist in a chunk of text?\n\n\nAnd if not, is language technology even there yet?\n\n\nThanks, I appreciate your input!!",
"date": "2020-01-11"
},
{
"vote": 2,
"title": "Semantic Similarities in Gensim using python",
"text": "I need help.\n\n\n&#x200B;\n\n\nI know the basics of python so I decided to use Gensim to find semantic similarities in a book. I want to find which chapters have sentences that are highly similar and almost paraphrased to the sentences in the first chapter of the book. So I set it up like this after going through Gensim's tutorial but I am looking for any potential way to get better results.\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nfrom collections import defaultdict\n\n\nfrom gensim import corpora\n\n\n&#x200B;\n\n\ndocuments = [\n\n\n\"Here I input sentences from the chapter that I am comparing to the first chapter of the book\",\n\n\n\"...\",\n\n\n\"...\",\n\n\n\"...\",\n\n\n]\n\n\n&#x200B;\n\n\nstoplist = set('for a of the and to in' .split())\n\n\ntexts = [\n\n\n[word for word in document.lower().split() if word not in stoplist]\n\n\nfor document in documents\n\n\n]\n\n\n&#x200B;\n\n\ndictionary = corpora.Dictionary(texts)\n\n\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n\n&#x200B;\n\n\nfrom gensim import models\n\n\nlsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n\n\n&#x200B;\n\n\ndoc = \"Here I input the sentences from the first chapter\"\n\n\ndoc = \"...\"\n\n\ndoc = \"...\"\n\n\ndoc = \"...\"\n\n\nvec_bow = dictionary.doc2bow(doc.lower().split())\n\n\nvec_lsi = lsi[vec_bow]\n\n\nprint(vec_lsi)\n\n\n&#x200B;\n\n\nfrom gensim import similarities\n\n\nindex = similarities.MatrixSimilarity(lsi[corpus])\n\n\n&#x200B;\n\n\nsims = index[vec_lsi]\n\n\nprint(list(enumerate(sims)))\n\n\n&#x200B;\n\n\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\n\n\nfor i, s in enumerate(sims):\n\n\nprint(s, documents[i])\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nThis outputs which sentences are most similar to the sentences from the first chapter. I would like to know if there is a way I can tell out of the sentences that are most similar to the first chapter, specifically which sentence(s) they are most similar to. Also if there is anything that could be cleaned up, that I am doing wrong or should be changed.\n\n\n&#x200B;\n\n\nThank you for your time and any input you can offer.",
"date": "2020-01-11"
},
{
"vote": 2,
"title": "GPT-2 for Tweets",
"text": "Friends,\n\n\nWe fine-tuned a GPT-2 (medium) model on Gary Marcus's tweets. \n\n\nTweets something \"interesting\" every 30min. \n\n\nHandle:  \nhttps://twitter.com/GaryMarcusAI",
"date": "2020-01-10"
},
{
"vote": 1,
"title": "Natural Language Processing Market Size- KBV Research",
"text": "[removed]",
"date": "2020-01-10"
},
{
"vote": 3,
"title": "What is the best approach for document embedding in order to do document clustering?",
"text": "Hello.\n\n\nI'm currently working on a project which involves document clustering. What I've done so far is utilizing \"Doc2Vec\", or \"Tf-idf + Word2Vec\" for getting my document vectors. While these work somewhat decent in text classification tasks with prior labels, they don't perform that well in text clustering tasks. I've tried using BERT's embedding, but using the \n&lt;CLS&gt;\n token also does not work that good. \n\n\nThe data I'm working on is a set of academic text in linguistics, and I want to cluster them in order to find the sub-fields in the domain. \n\n\nThanks.",
"date": "2020-01-10"
},
{
"vote": 6,
"title": "Meaningful Information retrieval and question answering for unstructured data - Is it even possible ?",
"text": "Hello good NLP people,\n\n\nI am working on a task that gradually seems not solvable for me. My data-set consists of long, messy, unstructured documents (pdfs, doc, docx, scans with tables, graphs, text, etc)  and the client wants to obtain a system that allows to query basically as much information as possible from the documents.\n\n\nAn example query could be: â€œWho are the beneficiaries of project xxx that is implemented by ORG xxx?â€.\n\n\nOr: â€œHow much co-financing was directed to projects that concern focal areas x,y,z?â€\n\n\nMy initial idea was following:\n\n\n\n\nProcess documents (Python, OCR, etc) into machine readable form.\n\n\nPre-process/clean/normalise text.\n\n\nManually annotate entities and build customised NER model.\n\n\nManually annotate entity relations and build Relation extraction model (I am not sure how to do this the best way?)\n\n\nExtract triples.\n\n\nStore triples in knowledge graph/base.\n\n\nQuery.\n\n\n\n\nThe end result could look like this:\n\n\nhttp://semanticparsing.ukp.informatik.tu-darmstadt.de:5000/question-answering/static/index.html\n\n\nHowever, I have the feeling that QA systems and Relation Extraction frameworks can be built on top of large quality datasets (like Wikipedia). In reality, dealing for example with messy documents, it seems very difficult to replicate.\n\n\nLastly, I think the client has to narrow down this task to only some crucial information.\n\n\nPlease let me know about any important work on this lately, or how you would proceed.\n\n\nThanks!",
"date": "2020-01-10"
},
{
"vote": 27,
"title": "2019 â€” Year of BERT and Transformer",
"text": null,
"date": "2020-01-10"
},
{
"vote": 2,
"title": "Transformer Vocab",
"text": "Iâ€™m using huggingfaceâ€™s Transformer library: Does anyone have an idea on how to get the vocab for transformers like XLNet?\n\n\nFor example,\n\n\nbert = BertModel.from_pretrained(...)\nbert.vocab\n\n\nBut XLNet will have no such access to the vocabulary using huggingface.",
"date": "2020-01-10"
},
{
"vote": 11,
"title": "Best methods for unsupervised document clustering?",
"text": "I'm working on a project to analyze short documents where we don't know enough about the data set to start training a supervised model. The documents are on the shorter side, between 1 and 140 characters. I'm working in python.\n\n\nI've tried tfidf vectorizer from sklearn > kmeans. This worked ok\n\n\nDoc2vec > kmeans. This didn't work as well.\n\n\nI'm currently working on yake > brown clustering (I like yake but I'm not sold on brown clustering yet).\n\n\nWhat other methods should I try? What is the best way to compare different methods (DBscan vs. Kmeans etc.)?",
"date": "2020-01-09"
},
{
"vote": 2,
"title": "Mapping parallel data that shares the same vocabulary",
"text": "Hello,\n\n\nLetâ€™s say we want to translate between two sequences that share the same vocabulary.\n\n\nWe assume that the vocabulary is: V = [A,B,C,D,E,F,G]\n\n\nWe have this parallel data:\n\n\nSource: [A B C C , A F G]\n\n\nTarget: [E B C C, E F G]\n\n\nThis was just an example\n\n\nIt we want to represent any sequence. We can use a vector that contains the counts of each element from the vocabulary.\n\n\nSo A B C C = [1,1,2,0,0,0,0]\n\n\nA F G = [1,0,0,0,0,1,1]\n\n\nE B C C = [0,1,2,0,1,0,0]\n\n\nE F G = [0,0,0,0,1,1,1]\n\n\nAs we said that A B C C = E B C C and A F G = E F G, then their vectors must be the same to some extent. Like we can have something like this:\n\n\nA B C C = E B C C = [1,1,2,0,1,0,0]\n\n\nThe first idea was to train a seq2seq model and try to extract the encoder mapping representation of the sequence. But it looks that the encoder encode just the source sequence representation not the mapping.\n\n\nIs there any algorithms that can perform this task?",
"date": "2020-01-09"
},
{
"vote": 1,
"title": "Asking paraphrased or alternate questions",
"text": "I am able to generate questions from a given context passage and given keywords. I would like to know how can I generate alternate questions or paraphrased questions that mean the same but are worded differently or have additional information? \n\n\nAny pointers?",
"date": "2020-01-09"
},
{
"vote": 1,
"title": "Is there a tool or software for semantic textual similarities?",
"text": "I'd like to run a whole book through the software to find where sentences are paraphrased similarly in the book.",
"date": "2020-01-09"
},
{
"vote": 4,
"title": "Text Processing to find Dark Tower (book) in-jokes, cliche phrases. Where to start?",
"text": "Hello everyone!\n\n\nBackground\n\n\nMy brother and I recently read the Dark Tower book series by Stephen King. If you've read the series, you'll know it's got lots of in-jokes, common phrases, and some funny cliches. I thought it'd be funny to run some statistics on the text, to see how often certain phrases appear, are repeated.\n\n\nThe challenge\n\n\nGiven a body of text (~7 books, converted to plaintext UTF-8), I would like to perform ad-hoc queries to see the frequency of certain phrases, and then output (1) the phrase, (2) the phrase's frequency, and (3) contexts of where the phrase appeared (specifically: which book, and several sentences of context where the phrase appears)\n\n\nNote: I'm fairly comfortable with Python & Java, but have never used any text processing / NLP tools, and am unfamiliar with the various frameworks, libraries etc.\n\n\nBasic Example\n\n\nPhrase: \"bombardier eyes\"\n\n\n\n\n- Frequency: Appears 20 times\n\n\nContexts:\n\n\n(1) Book 1, page 50: \"(preceding sentence). The gunslinger looked at him with his *bombardiers eyes*. (following sentence);\n\n\n(2) Book 2, page 120...\n\n\n\n\n\n\n\n\n&#x200B;\n\n\nOther ideas\n\n\n- Top frequency words which don't appear in an English dictionary (e.g. \"lobstrosites\") - tells you about the made-up words in the Dark Tower book universe\n\n\n...\n\n\nQuestions\n\n\n- What's the easiest way to dip my toe in this?  (e.g. newbie-friendly tools, frameworks)\n\n\n- Any examples (code, etc) of finding insights like the above? Would be great to work from an already-working starting point\n\n\n&#x200B;\n\n\nThanks for your opinions and suggestions - excited to play around with this. I'm hoping to get more interested in language processing (I'm learning a language - Spanish - and think it'd be fun to run similar queries on bodies of text, to help me learn common phrases, etc)",
"date": "2020-01-08"
},
{
"vote": 1,
"title": "Analysing large data of chatbot using Python and NLP",
"text": "[deleted]",
"date": "2020-01-08"
},
{
"vote": 3,
"title": "Resources related to BERT?",
"text": "Actually I'm having difficulty in understanding Bert. Can anyone share some useful resources for the same?",
"date": "2020-01-08"
},
{
"vote": 1,
"title": "Looking for a master thesis subject",
"text": "Hello all,\n\n\nI have a pretty big database with posts from a big forum (~5 GB of data) and I would like to use it within my thesis. Forum is extraordinary, that's why I would like to analize it regarding informations inside and I'm looking for ideas for a subject. Dataset is really interesting, but I do not know how to use it to form a thesis out of it.\n\n\nCould you please suggest me something? As my supervisor of the thesis is really cool and I would love to suprise him.",
"date": "2020-01-08"
},
{
"vote": 37,
"title": "FinBERT: Financial Sentiment Analysis with BERT",
"text": null,
"date": "2020-01-06"
},
{
"vote": 1,
"title": "Financial Sentiment Analysis with BERT",
"text": "[deleted]",
"date": "2020-01-06"
},
{
"vote": 3,
"title": "Language-Games: Dead simple games made with word vectors",
"text": null,
"date": "2020-01-06"
},
{
"vote": 0,
"title": "It's in Discord",
"text": "[removed]",
"date": "2020-01-05"
},
{
"vote": 4,
"title": "What is the state-of-the-art for recommendation systems that integrate text data?",
"text": "I want to get some good baselines and compare them to my approach. Current literature seems inconsistent in dataset and evaluation techniques. Appreciate any advice.\n\n\nEDIT::: For anyone interested in the answer.\n\n\nIn general, it seems like there is \"sequential recommendation\", predicting the next item in a sequence where the sequence are the items a user interacted with, \"item prediction\" which is predicting the rating a user will give an item (which apparently has fallen out of favour... \nhttps://twitter.com/sedielem/status/906479855604162561\n) and ranking prediction, which may also be known as \"top-n\" prediction, although that is not clear. For top-n prediction, I found this \nrelevant paper\n which basically says that all the neural network approaches are outperformed by simple baselines or not reproducible.",
"date": "2020-01-04"
},
{
"vote": 0,
"title": "Want to go out for Voice Producer (text-to-speech) job at Apple. Do I have a chance?",
"text": null,
"date": "2020-01-04"
},
{
"vote": 1,
"title": "Google Releases ALBERT V2 &amp; Chinese-Language Models",
"text": null,
"date": "2020-01-03"
},
{
"vote": 1,
"title": "Dialog data from computer games",
"text": "Any chance of getting text dialogs from rpg games such as the witcher 3, horizon zero dawn or other?",
"date": "2020-01-03"
},
{
"vote": 9,
"title": "A simple explanation to TF-IDF algorithm",
"text": null,
"date": "2020-01-03"
},
{
"vote": 5,
"title": "How to contribute to Common Voice",
"text": "Hi guys, I've made this video in order to show you guys how you can help the Common Voice project. Any questions?\n\n\n \nhttps://voice.mozilla.org/pt\n \n\n\n \nhttps://www.youtube.com/watch?v=s9TF4jXxwFM",
"date": "2020-01-02"
},
{
"vote": 20,
"title": "Keyword/Topic Extraction using BERT/BERT like models",
"text": "I am looking to extract the most popular keywords or topics from a list of company documents. Both overall and by time. I have tried a couple of simple statistics and POS based methods like RAKE and TextRank. I was wondering if anyone has experience in extraction of Topics from documents using more advanced methods like BERT/ROBERTA etc. \n\n\nAny help would be appreciated and I would be happy to share the results as well as the core code that worked.",
"date": "2020-01-01"
},
{
"vote": 19,
"title": "Using NLP to help people learn natural languages",
"text": "Iâ€™m curious if there is either research or products which use NLP to help people in some way to learn natural languages? Can anyone point to anything like this?",
"date": "2019-12-31"
},
{
"vote": 0,
"title": "This website leverages linguistics to talk to users",
"text": null,
"date": "2019-12-31"
},
{
"vote": 0,
"title": "10 Deep Learning Best Practices To Keep in Mind in 2020",
"text": null,
"date": "2019-12-30"
},
{
"vote": 18,
"title": "Amazon Language Engineer vs Software Engineer Interview",
"text": "I have an in-person interview scheduled soon at Amazon for a Language Engineer position.\n\n\nI'm wondering if anyone could elaborate on their experience and/or the difference between a Language Engineer vs Software Engineer interview? I'm mostly curious about the proportion of linguistics, natural language processing (NLP)/machine learning (ML) and coding technical questions.",
"date": "2019-12-30"
},
{
"vote": 4,
"title": "BERT sentence perplexity",
"text": "Hello, I am trying to get the perplexity of a sentence from BERT. I switched from AllenNLP to HuggingFace BERT, trying to do this, but I have no idea how to calculate it. I wanted to extract the sentence embeddings and then perplexity but that doesn't seem to be possible. Does anyone have a good idea on how to start?",
"date": "2019-12-29"
},
{
"vote": 3,
"title": "Struggling with BERT feature vectors",
"text": "Hi, I've extracted fixed length feature vectors from BERT by using the script extract_features.py from the official repo but now I'm struggling with the results. The program produces as output a JSON file with one sequence of embeddings (of word in the same sentence) per line, but I'm not able to separate vectors in order to obtain something like a pandas dataframe with one single vector (as an array, not a string) per line. I've tried using json to csv converters without results. Someone can help me? Thanks, I'm not very good with data",
"date": "2019-12-28"
},
{
"vote": 3,
"title": "How to check if a word can be interpreted as a verb in some context or not?",
"text": "I have a list of words and I need to find which of these words are verbs in some context. Consider for example 'bowling': it might be a noun but it is also a verb in some context; 'Africa' on the other hand is never a verb. All I want to find are words that could be verbs. POS tagging will not work since I just have a list of words without context. I would prefer a solution using \nnltk\n and \nWordNet\n. Thanks!\n\n\nEdit: Is it possible to output all possible POS tags that \nnltk\n might provide to a word? If yes, all I need to do is check if 'verb' is a tag.",
"date": "2019-12-27"
},
{
"vote": 1,
"title": "2019 In Review: 10 Open-Sourced AI Datasets",
"text": null,
"date": "2019-12-27"
},
{
"vote": 0,
"title": "Top device, Operation and translation very easy. This was a gift for my mom and weâ€™re fully satisfied after the first attempts. Very good and easy-to-use menu interface, loud volume and large texts (adjustable). The translation operation is pretty simple. You set two languages. If you hold down the",
"text": null,
"date": "2019-12-27"
},
{
"vote": 2,
"title": "Tool that automatically converts articles from an RSS feed into PDFs? Building a corpus",
"text": "Hi! Sorry if I couldn't phrase the question better but I'm building a multilingual text corpus from journalistic texts to study how their translations compare. Because of the nature of the text, it would have to be expanded constantly, as new texts are posted in the websites.\n\n\nI've never built a corpus before and I'm a bit lost. I'd like to know if there's any tool that can regularly and automatically pull the texts from an RSS feed and convert them to PDFs (or better, any other format for further processing). I know there must be something out there, it sounds like the simple thing it could be achieved with native tools like Automator or IFTT, but I'm out of resources to do it. Do you know anything of that kind? Thank you!",
"date": "2019-12-26"
},
{
"vote": 0,
"title": "Any ideas for podcast name?",
"text": "[removed]",
"date": "2019-12-26"
},
{
"vote": 1,
"title": "Exactly as advertised, Easy to use, versatile",
"text": null,
"date": "2019-12-25"
},
{
"vote": 1,
"title": "What is the best way to do domain-specific/ academic collocation extraction",
"text": "[deleted]",
"date": "2019-12-24"
},
{
"vote": 0,
"title": "What if we trained GPT-2 with an AI designed to find text generated by GPT-2?",
"text": null,
"date": "2019-12-23"
},
{
"vote": 1,
"title": "CMU Senior Develops Worldâ€™s First Classical Chinese Programming Language",
"text": null,
"date": "2019-12-23"
},
{
"vote": 8,
"title": "NLP News Cypher | 12.22.19",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis Week:\n\n\n \n-Those Oâ€™Reilly Jupyter Notebooks Live on GitHub!\n\n\n-\nThe World of Conversational AI in 1 Paper\n\n\n-\nNamed Entity Disambiguation (NED)\n\n\n-\nDive In to Complexity\n\n\n-\nSOTA for NER\n\n\n-\nTuning In to Hyper-Parameters\n\n\n-\nMeet ALBERT: BERTâ€™s More Efficient Cousin\n\n\n-\nSocket to Me!\n\n\n-\nIBM Wants All the Smoke!\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-12-22-19-beda4dbca6b8",
"date": "2019-12-22"
},
{
"vote": 15,
"title": "NLP Task: find a word from its definition",
"text": "Hello,\n\n\nI'm quite new to NLP and I struggle with the following task:\n\n\nMy input is the definition of a word (casual, said by a human), and the algorithm has to find the word that best fits the definition. For example: \"It's a very big animal, it's grey, with a good memory and a very long nose\" => \"elephant\".\n\n\nI tried some methods: Bag of words (with fasttext), ad-hoc matching of words using ConceptNet data...without much success.\n\n\nI have the intuition that I should use the sentence structure beyond just the presence of words (because \"It is an animal\" is different from \"It's where you find animals\") but I'm confused by the diversity of NLP algorithms to deal with sentences.\n\n\nWould you have suggestions of tools and algorithms that would be worth investigating for this task ?\n\n\nThanks a lot for your help!\n\n\nFor info: My goal is to help children practice English with games that make them speak to a bot.\n\n\n----\n\n\nEdit: thanks everyone for your help, I'll study your ideas and keep you posted on my conclusions if you are interested",
"date": "2019-12-21"
},
{
"vote": 1,
"title": "12-in-1: Facebook AIâ€™s New Framework Tackles Multiple Vision-and-Language Tasks",
"text": null,
"date": "2019-12-19"
},
{
"vote": 17,
"title": "Does anyone use numpy.char?",
"text": "I've recently started learning about vectorized operations and how they drastically reduce processing time. Whenever talking about vectorization in a Python context, \nnumpy\n inevitably comes up. This discussion is almost always about vectorized \nnumerical\n operations, a.k.a. math, of which \nnumpy\n is the undisputed champion.\n\n\nIn my context though, I work a lot with string data, which is very different from numerical data. So naturally I was wondering if vectorization could apply to operations on string data too, not just numbers. Looking into this, I discovered \nthe numpy.char library\n, which bills itself as\n\n\n> \na set of vectorized string operations for arrays of type\n \nnumpy.string_\n \nor\n \nnumpy.unicode_\n\n\nThis library sounds like exactly what I was looking for: a package of fast, vectorized operations on string data, such as concatenation, duplication, splitting, stripping, counting, and en/decoding. So I got really fired up when I found it, because it could potentially speed up everything I do.\n\n\nBut then I thought about it.... if \nnumpy.char\n is so fantastic, why in all my time on Stack Overflow, Reddit, etc., had I never once even heard of it? Is there something I'm missing here? Is it just that as great as \nnumpy\n is for numerical operations, more NLP-oriented packages like \nNLTK\n, \nspaCy\n, or \ngensim\n are even faster than \nnumpy\n when working with string data, so everyone just skips \nnumpy\n and goes to those?\n\n\nOr should I just forget my hesitation and just start using \nnumpy.char\n for all my string operations going forth?",
"date": "2019-12-19"
},
{
"vote": 0,
"title": "[P] Simple and effective phrase finding in multi-language?",
"text": null,
"date": "2019-12-19"
},
{
"vote": 5,
"title": "Seq2Seq based open-source transliteration for indic languages.",
"text": null,
"date": "2019-12-18"
},
{
"vote": 1,
"title": "Multiclass classification architecture",
"text": "I want to build a multiclass classifier and I am not sure how to approach it. This is my goal:\n\n\nI have defined multiple sub-tasks, for example \"go to the man\" or \"get food\". Now I want to combine the tasks in a sentence and use a classifier, to tell me which sub-tasks does the sentence contain.\n\n\nSo for example, the input could be \"First get food, then go to the man\". My classifier should output [\"get food\",\"go to the man\"]. The tricky part for me is that I want to keep the order of the sub-tasks and that the output might be 3-5 sub-tasks.\n\n\nI would like to use BERT, but I am not sure if it is the best choice for this task.\n\n\nIf you have any suggestions, links or papers for me, it would help me a lot. Thanks!",
"date": "2019-12-18"
},
{
"vote": 2,
"title": "Just got linguistics BA from Cal, considering Data Science/similar MS and pursuing NLP.",
"text": null,
"date": "2019-12-18"
},
{
"vote": 1,
"title": "How to change derivatives of a word to a root using Python?",
"text": "Hi.\n\n\nI'm working on a task which involves scientific text. As you may know, using different derivatives of a word is a very common practice in papers in order to remove \"redundancy\". This makes these data for NLP tasks which don't care much about PoS tags and are revolving around the contextual meanings of a document extremely noisy. In order to tackle this I've tried using \"lemmatization\" but it only changes the words to a simpler lemma, and for every PoS we have a different lemma. This is good for huge amount of data but when we have a small dataset, this does not denoise the data  that much.\n\n\nI've tried using SnowballStemmer but this is still not enough. Many of more complex forms of a word don't get mapped to the same root e.g. \"Morphological\" and \"Morpheme\". LancasterStemmer on the other hand is very aggressive and ruins the contextual differentiation between different roots. \n\n\nAre there any viable solution for this?",
"date": "2019-12-17"
},
{
"vote": 5,
"title": "Current state of the Topic Segmentation problem",
"text": "Recently, I did a little research in the literature for \"Topic segmentation\" since \"Text segmentation\" seems to be more related to identifying text in images. From the results, it appears that the most recent survey is from 2011 [1], while the most recent papers in big conferences are from 2008 to 2013 [2, 3, 4].  \n\n\n&#x200B;\n\n\nIs this the current state of the problem, or there are more recent and relevant works?  \n\n\n&#x200B;\n\n\nIt's also possible that I'm using the wrong terms. So,  for clarification, I'm most interest in segmenting a collection of documents in a small and well-known number of sections / topics.\n\n\n&#x200B;\n\n\n[1]  Purver, Matthew. \"Topic segmentation.\" In \nSpoken language understanding: systems for extracting semantic information from speech\n (2011)\n\n\n[2] Eisenstein, Jacob, and Regina Barzilay. \"Bayesian unsupervised topic segmentation.\" In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (2008)\n\n\n[3]  Riedl, Martin, and Chris Biemann. \"TopicTiling: a text segmentation algorithm based on LDA.\" In \nProceedings of ACL 2012 Student Research Workshop (2012)\n\n\n[4] Du, Lan, Wray Buntine, and Mark Johnson. \"Topic segmentation with a structured topic model.\" In \nProceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (2013)",
"date": "2019-12-17"
},
{
"vote": 13,
"title": "Chrome Extension for NLP annotation/labelling",
"text": "Hey NLP researchers!, I know that extracting and annotating the data for computational linguistics is perhaps the most tedious task in the research. I was thinking of developing a chrome extension using crowd sourcing to make this easier. Take a case where we extract the data from Twitter using it's API and manually annotate it ourselves in a csv/excel file, our crowd source it on platforms like Mechanical Turk. With the use of this extension one can select a piece of text, right click on it and select the appropriate label for it, this text with it's label will be stored in a DB on backend or Google docs.\n\n\nWith the login type functionality we can limit the number of people with annotation rights.\n\n\nI wanted to hear other researchers views on this extension based annotation tool, do you think it will be useful?",
"date": "2019-12-17"
},
{
"vote": 0,
"title": "A great product that will accurately translate several languages in both directions. It's not speedy but it works great!",
"text": null,
"date": "2019-12-16"
},
{
"vote": 10,
"title": "BiLSTM-CRF vs Tranformer for NER with small, labeled dataset and feature engineering",
"text": "Hi all,\n\n\nI have my own, manually-labelled dataset of English tweets (not that big, tbh, 5k tweets or so split into tokens) with some in-built linguistic feature engineering and also pre-trained word embeddings . What NER approach would yield great results and best suit me given my small training dataset and additional features?",
"date": "2019-12-16"
},
{
"vote": 2,
"title": "Need Help in Dependency Parser and Creating Vectors",
"text": "I am trying to implement this paper: \nhttps://arxiv.org/pdf/1903.10117.pdf\n\n\nIt says: \nWe use Stanford Dependency Parser to identify the words intended for each item in a review. We create a vector of these fragments and use a sentiment analyzer framework on top of it to categorize the sentiment of each item.\n \n\n\nCan someone explain me how can I implement this for each review. If not, can someone provide me with links for reference explanation or implementation?",
"date": "2019-12-16"
},
{
"vote": 13,
"title": "Continuency vs. Dependency Parsing",
"text": null,
"date": "2019-12-13"
},
{
"vote": 0,
"title": "Miracle Device for anyone, everyone who travel to other country.",
"text": null,
"date": "2019-12-13"
},
{
"vote": 1,
"title": "AI Reimagines Ancient Chinese Poetry",
"text": null,
"date": "2019-12-12"
},
{
"vote": 3,
"title": "I need to build a Model which takes a bunch of CS question papers(MCQs) as input and outputs specific topic tags for questions.",
"text": "[deleted]",
"date": "2019-12-11"
},
{
"vote": 28,
"title": "Kiev-founded Grammarly raises $90M on unicorn valuation",
"text": null,
"date": "2019-12-11"
},
{
"vote": 2,
"title": "Reshape BERT embeddings for neural network",
"text": "Hello,\n\n\nI have encoded sentences using BERT pre-trained and now I have padded vectors with length 40 (max 40 token per sentences) x 768 (number of hidden states ). I have used bert-as-a-service with pooling strategy equal to NONE to be able to retrieve embeddings for each token. However, the embedding values range from negative to positive floats. How can I extract integer values to feed into a neural network (in this case, a Bidirectional LSTM), or how can I create an embedding/input layer with these embeddings?\n\n\nThank you!\n\n\n(embeddings that BERT generates for illustration)\n\n\nhttps://preview.redd.it/focdxesf7x341.png?width=1866&format=png&auto=webp&s=8864f87caf1b3fbd278a991f63a0b6c4ae5281d4",
"date": "2019-12-11"
},
{
"vote": 10,
"title": "Subword Tokenization - Handling Misspellings and Multilingual Data",
"text": null,
"date": "2019-12-10"
},
{
"vote": 1,
"title": "Classifying sentiment bearing words into categories",
"text": "Hi, I am in search of article which classifies opinionated words into categories. Some words are always positive, unless negated, (like good, worst)  and there are words which change sentiment in presence of other words . Has anyone come across such classification/paper  ?",
"date": "2019-12-10"
},
{
"vote": 1,
"title": "Python grammar parser",
"text": "I'm looking for a library (Ideally python) that can detect things like punctuation and grammar mistakes, with the intention that I could generate an ML model off of the results. Most of the libraries I've seen focus on NLP, which removes all punctuation.",
"date": "2019-12-10"
},
{
"vote": 11,
"title": "Are there any cases where Bert is better than RoBertA?",
"text": "I'm reading the roBertA paper right now. My impression is that its weights are better than Bert. Are there any instances where original Bert is still better?",
"date": "2019-12-10"
},
{
"vote": 3,
"title": "Any attempts at solving Chollet's ARC benchmark?",
"text": "Hi everyone!  \n\n\nTitle is pretty self-explanatory I guess: Is anyone aware of any efforts being made into solving the Abstraction and Reasoning Corpus benchmark created by FranÃ§ois Chollet?\n\n\nHandy links here: \nhttps://github.com/fchollet/ARC\nhttps://www.zdnet.com/article/keras-creator-chollets-new-direction-for-ai-a-q-a/\n\n\nI personally find the concept quite fascinating: the whole new way of seeing things, that \"\ntrue\n\" AI is AI that can generalize to new problems, and that we should be building software that does this by giving it the same priors as humans.",
"date": "2019-12-10"
},
{
"vote": 2,
"title": "Better techniques for weighted average pooling?",
"text": "Disclaimer: All of this is in the context of trying to get better sentence/paragraph/document vectors for similarity searches from language models. I'm not interested in training, but only in using pre-trained language models \n\n\nI had a discussion recently with someone on HN who advocated for a weighted average pooling by \"doing dot product between all vectors in the phrase (m x m), then taking the average over rows for each word and normalizing. Kind of like a poor man's Transformer. \"\n\n\nI implemented this for my semantic search tool and found that it seemed to improve search results, because \"It will boost words that are supported by other similar words in the same phrase.\"\n\n\nThe technique was something that I hadn't thought of before, and had never seen implemented. Usually, people weighed each vector using tf-idf or word movers distance. Those techniques didn't work for me, but this \"poor mans transformer\" seems to. \n\n\nMy question is this: What the heck is this weighted pooling operation doing? Is it actually better for creating document vectors? What's the relationship between this technique and \"Scaled dot-product attention\"? It seems like this is a basic attention mechanism. Why did it take a random HN post for someone to propose this? Did I miss out on this method being talked about in a paper? Are there other even better methods for weighted pooling?",
"date": "2019-12-09"
},
{
"vote": 3,
"title": "Anyone has any good code examples for text generation using huggingface's distilbert models ?",
"text": "I've been struggling with hugging face's DistilBert model for some time now, since the documentation seems very unclear and their examples (e.g. \nhttps://github.com/huggingface/transformers/blob/master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb\n and \nhttps://github.com/huggingface/transformers/tree/master/examples/distillation\n) are extremely thick and the thing they are showcasing doesn't seem well documented.\n\n\nI'm wondering if anyone here has any experience and knows of some good code example for basic in-python usage of their models. Namely:\n\n\n\n\nHow to properly decode the output of the model into actual text (no matter how I change it's shape the tokenizer seems willing to decode it and always yields some sequence of  `[UNK]` tokens)  \n\n\nHow to actually use their schedulers+optimizers to train a model for a simple text to text task.",
"date": "2019-12-08"
},
{
"vote": 2,
"title": "Struggling with find the subject and object of a sentence in Java CoreNLP",
"text": "After searching, I don't get some direct methods to find the subject and object of a sentence. The best I can achieve is this. It is a bit slow to depparse the sentence, and most importantly the format is not good. So How to speed up the program or directly output something like \nnsubj: I\n and \nobj: xx\n. Thank you.\n\n\nroot(ROOT-0, tell-5)\nnsubj(tell-5, I-1)\naux(tell-5, can-2)\nadvmod(always-4, almost-3)\nadvmod(tell-5, always-4)\nadvmod(use-8, when-6)\nnsubj(use-8, movies-7)\nadvcl(tell-5, use-8)\namod(dinosaurs-10, fake-9)\ndobj(use-8, dinosaurs-10)\npunct(tell-5, .-11)\n\n\n\nthe code:\n\n\nimport edu.stanford.nlp.io.IOUtils;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.parser.nndep.DependencyParser;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.AnnotationPipeline;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.semgraph.SemanticGraph;\nimport edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;\nimport edu.stanford.nlp.util.CoreMap;\nimport edu.stanford.nlp.util.PropertiesUtils;\n\nimport java.util.Properties;\n\npublic class testing {\n    public static void main(String[] args) {\n        String text = &quot;I can almost always tell when movies use fake dinosaurs.&quot;;\n        Annotation ann = new Annotation(text);\n\n        Properties props = PropertiesUtils.asProperties(\n                &quot;annotators&quot;, &quot;tokenize,ssplit,pos,depparse&quot;,\n                &quot;depparse.model&quot;, DependencyParser.DEFAULT_MODEL\n        );\n\n        AnnotationPipeline pipeline = new StanfordCoreNLP(props);\n\n        pipeline.annotate(ann);\n\n        for (CoreMap sent : ann.get(CoreAnnotations.SentencesAnnotation.class)) {\n            SemanticGraph sg = sent.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);\n            System.out.println(IOUtils.eolChar + sg.toString(SemanticGraph.OutputFormat.LIST));\n        }\n}\n}",
"date": "2019-12-08"
},
{
"vote": 2,
"title": "Anyone know where I can get some Bert-weights which are pre-trained on Squad data?",
"text": "I don't quite have the resources to train over the 100k squad data. It looks like Google is not going to release these weights \nhttps://github.com/huggingface/transformers/issues/526\n , but I am guessing someone must have done this already. If so, how would I go about trying to find these weights?",
"date": "2019-12-08"
},
{
"vote": 15,
"title": "TinyBert: Distilling BERT for natural language understanding",
"text": null,
"date": "2019-12-07"
},
{
"vote": 7,
"title": "Topic Modelling and Topic Coherence",
"text": "Hi, \n\n\nWhat methods exist to determine the optimal number of Topics for the LDA?\n\n\nDoes somebody here have experience with the LDA/Topic Modelling and can recommend some good resources (preferably books, to get a deeper understanding) regarding LDA and Topic Coherence (or other methods to determine the ideal amount of Topics.) ? \n\n\nI am implementing everything in Python.\n\n\nCheers and thanks for your time",
"date": "2019-12-06"
},
{
"vote": 1,
"title": "What is the goal of cross-lingual embeddings?",
"text": "I mean cross-lingual is done in order to provide some mapping between the embeddings of many languages. Research is done on languages that we already have dictionaries for.\n\n\nFor example cross-lingual for english-french. Using any dictionary we can know that the word \"cat\" is the same as the word \"chat\". So, if we want to do a classification for french using an english trained model we can simply translate the french input to english.\n\n\nTL;DR : We can easily translate the input sentence to the resources-rich language and do the prediction. Why people are looking for cross-lingual embeddings?",
"date": "2019-12-05"
},
{
"vote": 6,
"title": "Do attention/transformer models need RNNs?",
"text": "The question seemed like an obvious NO to me, until today.\n\n\nI had understood attention to be better than RNN variants for two reasons:\n\n\n\n\nSpeed. They process inputs in parallel rather than sequence (which is vastly different from RNNs)\n\n\nDependency modeling. Using scaled dot products, each word (vector) has a relationship to the others. Additionally, positions are encoded as is necessary to maintain positional awareness.\n\n\n\n\nI'm studying up for finals and revisiting attention/transformer models and stumbled on the following resource on \nMedium\n. Here's a quote:\n\n\n>\nIntuition: How does attention\n \nactually\n \nwork?\n>\n>\nAnswer: Backpropagation, surprise surprise. Backpropagation will do whatever it takes to ensure that the outputs will be close to the ground truth. This is done by altering the weights in the RNNs and in the score function, if any. These weights will affect the encoder hidden states and decoder hidden states, which in turn affect the attention scores\n \n\n\nI'm confused because I thought the whole goal was to move away from RNNs because of their limitations with training speed and modeling long term dependencies. It's my understanding that a vanilla NN could have been the \"learning mechanism\" supporting an attention mechanism. Any thoughts?\n\n\nFollow up question: Are transformers and attention mechanisms synonymous terms? I think not (but I'm not certain) because a transformer uses multiple attention \"heads\" in its encoder and decoder architecture.",
"date": "2019-12-04"
},
{
"vote": 3,
"title": "Open source project for deploying deep learning models to production search engines",
"text": "We recently implemented new features such as Kubernetes support, a frontend, etc on an open source project I've been working on called \nNBoost\n.\n\n\nSo I wrote an \narticle\n to talk about some of the hurdles of building a production-scale domain-specific deployment of SoTA models.\n\n\nSome of the main features are:\n\n\n- Open source hosting of finetuned models for domain-specific knowledge\n\n\n- Kubernetes deployment via Helm\n\n\n- A frontend for tracking model and network latency",
"date": "2019-12-04"
},
{
"vote": 1,
"title": "[BERT][Beginner] Create embeddings for multiple sentences",
"text": "Hello,\n\n\nSorry for the beginner question.\n\n\nI am currently using pytorch_pretrained_bert lib to generate word embeddings (following tutorial here: \nhttps://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n).\nHowever almost all examples encode only one sentence or a a sentence pair, and my corpus is composed of several unrelated sentences.\n\n\nTo accomplish this, what I did was to encode each sentence separately (creating the input ids and segment ids) and executing the encoded_layers, _ = model(tokens_tensor, segments_tensors) for each sentence, then I concatenate the embeddings for the last 4 layers of the model. \n\n\nThis works fine, however, I noticed that if I compare the embeddings for the first token of each sentence (the CLS token), the embeddings are different, and I don't understand why they are different. Is this approach correct? If yes, why are the embeddings for the fixed tokens (CLS and SEP) different between sentences?\n\n\nThank you!",
"date": "2019-12-04"
},
{
"vote": 5,
"title": "LSTMs vs attention/transformers",
"text": "I'm very aware that attention/transformer based methods are outperforming LSTMs on sequence data, such as translation, question answering, etc. They're just able to determine which elements are most relevant given context.\n\n\nI'm curious, are there some tasks where the LSTM outperforms attention methods by some noteworthy margin?",
"date": "2019-12-03"
},
{
"vote": 1,
"title": "Randomizing word order in sentences to test models",
"text": "Did anyone read papers where the word order in sentences was randomized and fed as input into an NLP model to check how it will behave/if this modifications affect the output? I think I read somewhere about it a long time ago, but googling didn't help.",
"date": "2019-12-03"
},
{
"vote": 2,
"title": "Character annotating to word annotating",
"text": "Doccano is an annotation tool to label NLP datasets. The output of this labeling is by means of character count. So an output of [15,20, \"verb\"] means that characters beginning from 15 to 20 of a certain document has the label \"verb\". \n\n\nAfter annotating each document I want to tokenize this document (list of words per sentence). But when doing this the info of the annotations (count of the characters) are not accurate anymore as some characters and spaces are not there anymore because of the tokenization.  What I eventually want is  x,y pairs where x is the tokenized list of words in a sentence and y would be the labels of each token. Like:\n\n\nx = [[\"my\", \"name\", \"is\", \"Jack\"], [....], [....]]\n\n\ny = [[\"o\", \"o\", \"verb\", \"o\"], [....], [....]]\n\n\nThe question is, what would be the proper way of doing this without losing or miscalculated labels?",
"date": "2019-12-03"
},
{
"vote": 7,
"title": "How can i Train Bert NLP on specific task after a fine-tuning on the language model",
"text": "Hi\n\n\nI want to use Bert for English topics news. For expecting a better  performance in the end, i would like first tune the pre-trained model  offered by Google on our own domain-specific corpus (title of articles)  and in a second time training it on a specific task.\n I prefer first trying to tune the language model than training from scratch.\n How i can do this?\n I try to pretrain the langage model as explain \nhere \n But after when i train it as i am told \nhere\n,  i want to start at the checkpoint given by the pretraining. I failed to load the pretrained checkpoint.\n Here is the 3 Bert function :\n\n\npython create_pretraining_data.py \\   --input_file=gs://google_bucket/sample_text.txt \\   --output_file=gs://google_bucket/bert-checkpoints/models/tf_examples.tfrecord \\   --vocab_file=gs://google_bucket/uncased_L-12_H-768_A-12/vocab.txt \\   \n--do_lower_case=True \\   --max_seq_length=128 \\   \n--max_predictions_per_seq=20 \\  \n--masked_lm_prob=0.15 \\   \n--random_seed=12345 \\   \n--dupe_factor=5\n\npython run_pretraining.py \\   \n--input_file=gs://google_bucket/bert-checkpoints/models/tf_examples.tfrecord \\   --output_dir=gs://google_bucket/bert-checkpoints/models/output_dir\\   --do_train=True \\   --do_eval=True \\   \n--bert_config_file=gs://google_bucket/uncased_L-12_H-768_A-12/bert_config.json \\   --init_checkpoint=gs://google_bucket/uncased_L-12_H-768_A-12/bert_model.ckpt \\   --train_batch_size=32 \\   \n--max_seq_length=128 \\   \n--max_predictions_per_seq=20 \\   \n--num_train_steps=500 \\   \n--num_warmup_steps=10 \\   \n--learning_rate=2e-5 \\   \n--use_tpu=True \\   \n--tpu_name=test-tpu\npython run_classifier.py \\  \n--task_name=MRPC \\ \n--do_train=true \\ \n--do_eval=true \\ \n--data_dir=gs://google_bucket/MRPC \\ \n--vocab_file=gs://google_bucket/uncased_L-12_H-768_A-12/vocab.txt \\ --bert_config_file=gs://google_bucket/uncased_L-12_H-768_A-12/bert_config.json \\ --init_checkpoint=gs://google_bucket/bert-checkpoints/output_dir \\ \n--max_seq_length=128 \\ \n--train_batch_size=32 \\ \n--learning_rate=2e-5 \\ \n--num_train_epochs=3.0 \\ \n--output_dir=gs://google_bucket/bert-checkpoints/models/MRPC \\ \n--use_tpu=True \\ \n--tpu_name=test-tpu  \n\n\n\nSomeone as already do this?\n Thanks for your help.",
"date": "2019-12-02"
},
{
"vote": 8,
"title": "Various industries are now making use of Natural Language Processing, here's a quick introduction to NLP!",
"text": "[deleted]",
"date": "2019-12-02"
},
{
"vote": 8,
"title": "NLP News Cypher | 12.1.19",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\n This week:\n\n\n-\nTransfer Learning Coming to the Nasdaq\n\n\n-\nPython Tools for Finance\n\n\n-\nDistilBERT + Logistic Regression for Sentence Classification\n\n\n-\nLex Interviews Noam\n\n\n-\nHow Instagramâ€™s Explore Recommendation Engine Was Built\n\n\n-\nHugging Face Releases New Models\n\n\n-\nAWS is Pretty Popular\n\n\n-\nYann LeCun vs. Gary Marcus |Crouching Tiger Hidden Dragon\n\n\n&#x200B;\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-12-01-19-f3d44984633d",
"date": "2019-12-02"
},
{
"vote": 1,
"title": "Is there a reason why unpadding outputs of a padded inputs isn't more common in Bert?",
"text": "Bert can only take in a batch of tensors of the same shape, so tensors less than the length of the largest vector need to be padded. \n\n\nI am wondering though, why not unpad the output in cases where the output is the contextualized vectors.",
"date": "2019-12-02"
},
{
"vote": 3,
"title": "New problems in NLIDB and different solutions to resolve the cross domain constraints in NLIDBs",
"text": "Where is the overlap between NLP and NLIDB? And what are the different problems in NLIDB and what are the different solution for generalizing NLI across multiple domains?\n\n\nPlease help me in understanding this?",
"date": "2019-12-01"
},
{
"vote": 4,
"title": "Which is the most recommended corpus for training a POS Tagger for English?",
"text": "The corpus should be one that best represents present-day English.",
"date": "2019-12-01"
},
{
"vote": 11,
"title": "Newcomer to NLP",
"text": "I am considering a PhD degree in NLP but know so little about it.  I've looked at YouTube videos and Udemy courses.  Is there a current beginner textbook, website, or course that this sub recommends ?",
"date": "2019-11-30"
},
{
"vote": 8,
"title": "Summarising Multiple Articles",
"text": "Hi, I'm currently writing a PhD project idea where I want to summarise the current state of a given biomedical fields research. What is this large scale summarisation called in research? I'm not sure what to google to get any meaningful results for my background reading.",
"date": "2019-11-29"
},
{
"vote": 0,
"title": "Help downloading huggingface GPT-2 demo app for iOS",
"text": "[deleted]",
"date": "2019-11-28"
},
{
"vote": 11,
"title": "tf-idf weighting for document collections",
"text": "Does anyone with NLE/NLP experience have recommendations for applying a tf-idf weighting to compare two document collections? I'm currently stuck on building a term frequency function and would appreciate any input. Thanks in advance!",
"date": "2019-11-27"
},
{
"vote": 0,
"title": "Papers on multiple word-class based language modelling (in the context of speech recognition)",
"text": "Currently, the N Gram language modelling in speech recognition uses the words directly. I have seen papers using word class based language modelling (in ASR, NLP). I am now looking for an extension to this idea, where multiple levels of classifications are used. Each word gets classified multiple times based on its properties. For example - a class based on grammar context, a class based on word length, a class based on origin and so on. We can use the first class to form an n-gram and the next oneâ€™s ngram to improve on that. Since most of the ideas on language modelling in ASR are directly â€˜hijackedâ€™ from NLP, I wanted to know your thoughts. Have you come across papers related to this? \n\n\n// Some of the \"multi-class\" papers I have seen are different from the above approach",
"date": "2019-11-27"
},
{
"vote": 0,
"title": "Speech recognition",
"text": "[deleted]",
"date": "2019-11-27"
},
{
"vote": 2,
"title": "How to Build a Semantic Search Engine in 3 minutes",
"text": null,
"date": "2019-11-27"
},
{
"vote": 2,
"title": "Ideas/help for a school project (beginner)",
"text": "Hi everyone,\n\n\nIm a 21 year old accountancy student from Amsterdam and I have to write a thesis this year, partly consisting of a \"practice oriented\" research. \n\n\nThis could be on all kinds of topics, but I would love to write about NLP. I know in the world of accountancy there is a lot of information to be processed in text-form (annual/integrated reports) and I feel like there's a chance in it for me to develop myself.\n\n\nThe office Im interning at sadly can not let me use the tools that they're developing since they're still WIP, so Im afraid I'm on my own here.\n\n\nMy idea was to somehow make a connection to SDGs (terms like poverty, hunger, education and gender) within integrated reports - which pay a lot of attention to these things. \n\n\nAre there any tools I could process PDF documents in and get useable information from on such key terms? Perhaps even models on how many times these terms (in their right context) appear in what documents? \n\n\nAny help is appreciated!",
"date": "2019-11-27"
},
{
"vote": 1,
"title": "Why doesn't the WordNet synset of the word 'may' contain the modal verb sense?",
"text": "According to WordNet the word 'may' has only two senses: 1) the month, 2) a shrub. \n\n\nWhy isn't the modal verb sense included in the synset?",
"date": "2019-11-27"
},
{
"vote": 10,
"title": "Looking for attribution of a quote about NLP",
"text": "Sorry if this is not quite topical, but one of the proverbs in the field of Computer Science that has been popularized is:\n\n\n>â€œThere are only two hard things in Computer Science: cache invalidation and naming thingsâ€.\n\n\n- Phil Karlton (as related by \nTim Bray\n)\n\n\nI'm pretty sure I read a variation of this as applied to the field of Natural Language Processing that went something like:\n\n\n>â€œThere are only two hard things in Natural Language Processing: ambiguity and OOV errors.â€\n\n\nDoes anyone know who might have said this?\n\n\nI vaguely recall maybe it was Peter Norvig or Chris Manning, but since I don't have the exact quote it's proving impossible to get relevant results via Google or DuckDuckGo.  Even if nobody knows the exact quote or who said it, I'd also be interested in other proverbs of this nature as relate to hard problems in NLP.",
"date": "2019-11-27"
},
{
"vote": 1,
"title": "Need help formulating a NLP Parsing problem",
"text": "Hello all,  \n\n\nI am a complete newbie to the field of NLP and I have been given a task that needs NLP to solve it but I need some help with figuring out what terms I need to learn more about.  \n\n\nEssentially I have a large body of text reports with varying formats, and these reports will contain basic demographic data (age, date, ethnicity etc) and scoring of a variable number of tissue samples. My first priority is extracting the scores for the tissue samples and I have come up with the following questions:  \n\n\n\n\nHow can I design a model that can output labels for a variable number of fields (tissue samples)?  \n\n\n\n\nIs there a specific name for this kind of problem? If I know this I can do my own research\n\n\n\n\n\n\nThanks for any help!",
"date": "2019-11-26"
},
{
"vote": 1,
"title": "Natural Language Processing is redefining human interaction",
"text": null,
"date": "2019-11-25"
},
{
"vote": 1,
"title": "Document classification (help)",
"text": "So, my task is to make ML models which can classify receipts based on their data.\n\n\nThe approach that I have in mind is as follows-\n\n\nText preprocessing : removing all non-alphabetical data, removing common stop words as well as custom stop words.\n\n\nWord embedding : generate word vectors for the processed data using word2vec. I'm not sure how to handle out-of-dictionary word problem here.\n\n\nClassification algorithm : svm or naive Bayes\n\n\nIs this approach fine/ are there any improvements or a different approach altogether which will give good results?",
"date": "2019-11-25"
},
{
"vote": 20,
"title": "Is there any research on how to determine a speaker's knowledge of a topic based on the text?",
"text": "For example, say someone posts an article on a topic. Has any research been done on how to determine the author's expertise on their respective topics based on the reports? \n\n\nI am working on a project where I have a large number of articles, and how far off their predictions are. The goal is to build a system that can take in the text of their documents and try to predict how far off their prediction is.",
"date": "2019-11-25"
},
{
"vote": 2,
"title": "What is Sentiment Bias?",
"text": "In working with lexicon/rule-based sentiment analysis, I came across a term called \nsentiment bias\n, which causes false classification of polarity in a lexicon model. \n\n\nLink provided:  \nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6108458/#pone.0202523.ref003\n \n\n\nUnfortunately, I cannot find a proper definition of this term.",
"date": "2019-11-24"
},
{
"vote": 5,
"title": "NLP News Cypher | 11.24.19",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis week's highlights:\n\n\n-ðŸ¤— \nand the French Connection\n\n\n-\nA Montage of Cheat Sheets\n\n\n-\nWalmart Woos Siri (Into Buying Groceries)\n\n\n-\nYann LeCun Welcomes SONY to the Party!\n\n\n-\nYou Stay Classy San Diegoâ€¦\n\n\n-\nNew Cross-Lingual Question Answering Benchmark\n\n\n-\nBERTâ€™s Big Squeeze\n\n\n-\nNLU is Hard!\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-11-24-19-60b5b21032fa",
"date": "2019-11-24"
},
{
"vote": 18,
"title": "What is one software you'd like to exist for language tech?",
"text": "Hi all!  \n\n\nThe title of the post is pretty self-explanatory: what is one piece of language tech software (let's say something realistic. Like, maybe we're not at the level to have a perfect language model \njust yet\n) that you would like to exist, for personal use or otherwise?\n\n\nBetter software for drawing syntactic trees? Programs that transcribe language into phoneme sequences? A graph maker for Linguistic Variation?\n\n\nI'm curious about what you can come up with, and about what's lacking in the field in general.",
"date": "2019-11-24"
},
{
"vote": 6,
"title": "A Breakdown of Language Analyzers for Elasticsearch",
"text": null,
"date": "2019-11-24"
},
{
"vote": 13,
"title": "NLP initiatives I can contribute to?",
"text": "Are you familiar with some actual NLP-for-good projects that can benefit from DS/NLP contribution?\n\n\nI'm passionate about:\n\n\n\n\nPost truth/fake news detection (preventing misleading headlines, lies spreading, bots)\n\n\nMaking relevant information more transparent and accessible\n\n\nSpreading factual knowledge that can empower people to make more informed decisions\n\n\n\n\nI've gained some valuable real-world work and research experience in language and would love to make some use of it for causes I believe in.",
"date": "2019-11-23"
},
{
"vote": 2,
"title": "Syllogisms generation",
"text": "I want to build a tool that generates sensible syllogisms. An example of a syllogisms is: all A are B. all C are A. all C are B). I want the triplet (A, B, C) to be semantically related to each other in the way described by the syllogisms. That is, I would like my tool to generate 'a=humans; b=mortal; c=greeks' and not 'a=chickens; b=burgers; c=frogs'.\n\n\nThere's a syllogism generator online (\nhttp://krypton.mnsu.edu/~jp5985fj/courses/609/Logic/Silly%20Syllogisms.htm\n) but it doesn't generate syllogisms that are semantically related, it generates random terms for A, B, and C which may or may not be related.\n\n\nMy question is, in NLP, are there any research papers for generating semantically valid syllogisms? What topics would I need to research to build this tool?",
"date": "2019-11-22"
},
{
"vote": 21,
"title": "How did BERT and other recent models overcome catastrophic forgetting?",
"text": "ULMFit is one of the earliest Transfer Learning model in NLP, where we can fine-tune the network based on the downstream task. Their paper explicitly mentions Catastrophic forgetting as the big hurdle in fine-tuning, and how they overcome it by some novel techinques. But more recent papers such as BERT and GPT don't even mention about the catastrophic forgetting. Surprisingly. Even Multifit - the successor of ULMFit paper doesn't talk about it. Why is this phenomenon not seen now, while it was supposed to be a big problem during the release of ULMFit?",
"date": "2019-11-22"
},
{
"vote": 2,
"title": "Exploring NLP concepts using Apache OpenNLP",
"text": "https://blog.valohai.com/exploring-nlp-concepts-using-apache-opennlp-1?hs_preview=sFTbZHbV-21281035511",
"date": "2019-11-22"
},
{
"vote": 1,
"title": "What English corpus for syntactic parsing should I work with?",
"text": "I'm working on a syntactic parser (constituent structure based) and I need a corpus with just  sentences, not phrases. What corpus should I use?",
"date": "2019-11-21"
},
{
"vote": 5,
"title": "Questions on Symbolic Reasoning for NLU from an enthusiast",
"text": "Hello everyone,\n\n\n&#x200B;\n\n\nJust a bit of context before my questions:\nI'm interested in NLP and have been learning it by myself whilst studying computer science and linguistics.\nI have fair knowledge of the most well-known language models out there, as well as classic machine learning techniques.\n\n\n&#x200B;\n\n\nLately though, I've been thinking about the approaches that are currently used to make computers understand language. From what I understood (and I might be wrong, please correct me if so), most models use statistical / probabilistic approaches. What I mean is, NNs and supervised/unsupervised learning somehow \njust try to mimic language\n, and not to \nreally understand it.\n\n\nAnd that's kinda where symbolic approaches come: wouldn't it make more sense to use symbols to create a representation of language? Also, from what I understand there are some symbolic AI used for NLP: would anyone be able to point me to where I could learn more about them?\n\n\nAnd the more general, background question: What do you all think about this? Do you also see these 2 separate approaches? Also, anything on models that try to mimic the way a child learns language (without massive amounts of data like current models do)?\n\n\n&#x200B;\n\n\nSorry if this post is a bit all over the place. I tried to be as clear as possible.\nAnd thank you to anyone who takes the time to answer, even if just with some links.",
"date": "2019-11-21"
},
{
"vote": 1,
"title": "Ang good resource for cross lingual sentiment analysis?",
"text": "Guys any leads on this topic",
"date": "2019-11-21"
},
{
"vote": 5,
"title": "Why isn't GPT-2 on the GLUE leadership board?",
"text": "[deleted]",
"date": "2019-11-20"
},
{
"vote": 1,
"title": "Any good resources for categorical grammar?",
"text": "Struggling to find anything described in Homer Simpson terminology. \n\n\nThe wikipedia is the best I found as well as the Staford Encyclopedia of Philosophy, but I would love more of a ramp up to the complicated stuff. \n\n\nThanks!",
"date": "2019-11-20"
},
{
"vote": 5,
"title": "Facts extraction from a text approach",
"text": "Hi all,\n\n\nI am a new to NLP field so I am sorry if I am asking things that is annoying to someone here. I was trying my google-fu but with no results I can understand.\n\n\nIn short: I need to extract facts from a text. I was trying spacy + benepar constituency parser and was walking the tree benepar produces. I can parse simple sentences but when it comes to complicated sentences with, say, multiple preposition phrases and so on, I am getting garbage.\n\n\nThe output benepar produces has a lot of combinations of parts-of-a-speech. And different combinations has different meaning.\nI saw some examples in the NLTK book (Chapter 7 - Extracting Information from Text) that are basically create some noun chunkers. I am sure the real deal is starting from here but the book isn't very broad on this topic.\n\n\nSo my questions are:\n\n\n\n\nWhat is the discipline in NLP that is doing facts extraction from unstructured text?\n\n\nWhere can I learn about chunking grammar?\n\n\nWhere can I learn about constituency trees produced by constituency parsers? How to read them and how to extract facts from them?\n\n\n\n\nThanks,",
"date": "2019-11-20"
},
{
"vote": 1,
"title": "Looking for APIs, Tools or Consultants for word level sentiment analysis",
"text": "[deleted]",
"date": "2019-11-19"
},
{
"vote": 1,
"title": "Neural tabular data summarization and explanation",
"text": "I am looking for any resources on automatic tabular data summarization and explanation. Let's say you have a table of match (also past) and team statistics and you would like to generate few sentence summary of most important points. For example \"Team A won 3 times in a row\" or \"Team B haven't scored since 10 hours\". Those data do not explicitly appear in the dataset, but can be reasoned/calculated from other fields ergo cannot be copied. Some sort of reasoning is necessary (compare \"2 items plus 3 items\" with GPT-2 and Wolfram Alpha). Textual dataset could be obtained e.g. from match commentaries/summaries, but I am afraid standard encoder-decoder (data2text) will not handle that.",
"date": "2019-11-19"
},
{
"vote": 1,
"title": "The future of conversational AI",
"text": "[deleted]",
"date": "2019-11-19"
},
{
"vote": 3,
"title": "Why is it that GPT-2 large and X-large can't be fine-tuned, given that the medium and small versions can?",
"text": null,
"date": "2019-11-19"
},
{
"vote": 11,
"title": "Papers on Entity Linking",
"text": "I'm writing a short overview of the task of Entity Linking in NLP. For this reason I'm looking for important research papers or surveys on the topic, preferrably recent ones (2015 or later).\n\n\nI'm focusing on the disambiguation part rather than the recognition.",
"date": "2019-11-18"
},
{
"vote": 2,
"title": "Is there a way to use a dictionary (e.g. Oxford Dictionary) as a feature?",
"text": "Hello. I'm currently trying to get the dictionary definitions of words as features. However, I'm having trouble finding a source that actually has such a source.\n\n\nI understand that there are many techniques that allow us to leverage the information that words carry (e.g. WordNet, Word2Vec, GloVe, ELMo, etc.) but these aren't exactly the \"dictionary definition\" that I'm looking for. For example, WordNet takes advantage of the hierarchical relationship among words, and Word2Vec and GloVe tell us how similar two words are. Not exactly a \"dictionary definition\" in my opinion.\n\n\nDoes anybody know if there exists any source out there that provides such features? Thanks in advance.",
"date": "2019-11-18"
},
{
"vote": 3,
"title": "Looking for English word list excluding names",
"text": "I've spent a while now, without success, looking for an English word list that \ndoesn't\n include names.\n\n\nE.g. it \nshould not\n include \"John\", \"Alfred\", \"Steven\", etc., but it \nshould\n include \"April\", \"Summer\", \"Mark\", etc.\n\n\nMy ultimate goal is to take a list of names, and exclude the names that could be words. In order to do that, I need a list of words that aren't names.\n\n\nDoes such a word list exist, and if so, where can I find it? Thank you so much.",
"date": "2019-11-17"
},
{
"vote": 3,
"title": "Help needed! NLP - organization-executive relation extraction techniques",
"text": "Hi everyone, I'm relatively new to NLP. Hence, may not be aware of the most useful nlp libraries in python yet. I have an understanding of the capabilities and features of Spacy and Stanford NLP. \n\n\nMy current problem statement is that of extracting entities (PERSON/ORG/GPE) from NEWS ARTICLES and then developing rules to extract organization-executive relationships from the textual data. If possible, extract organisation-location relationships as well. \n\n\nDisclaimer - I don't have training(labelled) data to help me in developing a ML model. I have read a bunch of papers and blogs related to this. Research papers mostly encompass predictive modeling, and blogs cater to very specific chunks of the relationships which don't help extract relations from.different sentence structures.\n\n\nI'm done with the NER part using both spacy and Stanford NLP tagger. \n\n\nCurrently,on the relation extraction part, I'm.using Spacy's phrase matcher and dependency tree parsing (dependencies and POS tagger) to deduce the an exhaustive set of rules for the relations, but the relationship mentions in sentences are more varied than what my algorithm extracts and I've not been able to get very good results for the same. \n\n\nIf someone can help me with any alternative approach or public datasets available for this task, that would be extremely helpful. \n\n\nThanks in advance!",
"date": "2019-11-17"
},
{
"vote": 1,
"title": "Weekly Papers | EMNLP 2019 Best Paper; Facebook XLM-R and More!",
"text": null,
"date": "2019-11-15"
},
{
"vote": 2,
"title": "Looking for ways to squeeze more performance from Transformer-based models (e.g. BERT)",
"text": "I am running a number of experiments to improve the performance of NLP transformer-based models. There is no shortage of papers attempting to do the same but would gladly consider your 'tried and true' strategies. Do you think adding bigrams and trigrams to the vocabulary help?",
"date": "2019-11-13"
},
{
"vote": 6,
"title": "References on controlled text generation",
"text": "I am looking for examples (both research and commercial) of neural controlled text generation. Everyone heard about GPT-2 and its ability to generate text and also its inability to control what is generated  (you can play with it \nhttps://transformer.huggingface.co/\n) . I am looking for examples where this approach is pushed towards some control over what is created. I managed to find three examples:\n\n\n\n\nSalesforce Research: \nhttps://github.com/salesforce/ctrl\n\n\nAI21: \nhttps://ai21.com/haim-post\n\n\nSamsung R&D Poland: \nhttps://arxiv.org/abs/1910.00337\n\n\nUber's plug and play language models: \nhttps://arxiv.org/abs/1912.02164\n\n\nGoogle's LaserTagger: \nhttps://arxiv.org/abs/1909.01187\n\n\nOpenAI's fine-tuning from human preferences: \nhttps://arxiv.org/abs/1909.08593\n\n\n\n\nDo you know of any other companies, research groups that word on controlled text generation? Just to give you a flavor, imagine that you need to write a text that needs to include specific keywords and keyphrases picked up beforehand.",
"date": "2019-11-13"
},
{
"vote": 6,
"title": "Robust sentence splitter?",
"text": "Is there a really good sentence splitter?\n\n\nIt should leverage lexical and sequence information, eg long-distance clues as well as the style of the sequence.  (So the usual \nrules-based\n ones, even those with exceptions lists, do \nnot\n qualify.)\n\n\nIdeally it\n\n\n\n\nworks on all types of content, incl eg Reddit comments  \n\n\nworks for 100+ languages  \n\n\nerrs towards not splitting  \n\n\nnotifies low confidence rather than just splitting incorrectly  \n\n\nhas some basic configurability  \n\n\nis open source and/or available via an API\n\n\n\n\n(And if you're very interested in working on this, do ping me.)",
"date": "2019-11-13"
},
{
"vote": 8,
"title": "Why isn't it more common to have NLP programs ask follow up questions?",
"text": "Ok, so full disclosure: I am a graduate student in philosophy currently taking a philosophy of language class. I am familiar with the basics of how machine learning works, but don't really feel like my understanding extends far enough to know how else to get my question answered.\n\n\nThe question is as follows: it seems like in the way natural language happens between humans (call them human 1 and human 2), the content of person 1's \"turn\" tends to be underdetermined (its meaning is ambiguous), and person 2 has to \"guess\" what the person is saying. We normally do this simply by taking our turn with the response based on the assumption that we have guessed correctly (because we normally do), but there are things that we do if we get stuck, as well. Young children will exhibit these behaviors pretty much all the time, but adults will too. \n\n\nA few examples that seem like they would have clear value for a NLP would be the following circumstances:\n\n\n\n\ndidn't hear you clearly\n\n\n\n\nIt sounded like you said \"gam\"? Is that right, or did I mishear you?\n\n\n\n\nDoesn't recognize a word you said\n\n\n\n\nI don't know what \"gam\" means, could you explain it to me?\n\n\n\n\nDoesn't comprehend the meaning of the sentence/turn\n\n\n\n\nIn some ways, I think this one is more complicated, and consequently more difficult to address. That is, it can be relatively easy to determine that one hasn't understood a thing, but it can be much harder to know \nwhy\n. \n\n\nStill, it seems like there might be some response to indicate an utter failure to compute (so to speak). \n\n\nSorry if this is simply a flagrant display of my own ignorance on the subject.",
"date": "2019-11-13"
},
{
"vote": 4,
"title": "Using Bert for encode phrases?",
"text": "Hello everyone! Could I use Bert for just embedding some phrases? I have seen some example about use it encoding sentences but not sure for phrases. \n\n\nAnother confusion is if I use Bert for encoding sentences or phrases, does that mean I have to use BertTokenize as my tokenizer or in other words I couldn't have my own vocabulary for these sentences any longer?\n\n\nThanks for your help!",
"date": "2019-11-12"
},
{
"vote": 5,
"title": "Context2Vec",
"text": "[deleted]",
"date": "2019-11-12"
},
{
"vote": 2,
"title": "OCR review and moderation - Human in the loop workflows for deep learning solutions",
"text": "article link\n \n\n\nthere's a lot of paranoia these days about how adverse automation's effects might be on society. there's some tasks that can definitely be automated and will cause certain segments of the workforce their jobs. but alongside, other opportunities will get created that didn't exist before. it discusses how businesses are transforming, about digital information,  automation, the future of work and human in the loop workflows as a possible solution to these concerns.\n\n\nhttps://i.redd.it/7czi13a7d8y31.gif",
"date": "2019-11-12"
},
{
"vote": 1,
"title": "How to sample z from normal distribution?",
"text": "Recently, I learned about variational autoencoder(VAE) from this post\nhttps://towardsdatascience.com/deep-generative-models-25ab2821afd3\n. I think it is really a nice post about VAE introduction and I think that I have a clear understanding after read it.\n\n\nHowever, when I read some code from github, I am very confused about one thing. We all know that we need to sample a vector z from prior distribution like N(0,1) for reparameterize. In some code, the implement of sampling z is using (assuming implement by pytorch) torch.randn() or torch.normal() or something like these. These methods are just drawing numbers from normal distribution. Mathematically, the vector z may be not following normal distribution.\n\n\nSo, have I misunderstanding about something?\n\n\nAny help is great! Thank you so much!",
"date": "2019-11-12"
},
{
"vote": 14,
"title": "What are the best places to look for NLP jobs?",
"text": "So far I'm using linkedin, and occasionally finding interesting companies on crunchbase.",
"date": "2019-11-12"
},
{
"vote": 5,
"title": "Robust Multi-domain Sentiment Classifier",
"text": "Hey Reddit, any resources for how to build a robust multi-domain (products, movies, politics, etc.) sentiment analysis classifier? \n\n\nI tried grouping multiple datasets into one training set, but the classifier didn't generalize in an acceptable manner (because of different datasets sizes and distributions)",
"date": "2019-11-11"
},
{
"vote": 16,
"title": "NLP News Cypher | 11.10.19",
"text": "Friends,\n\n\nWe are back for another week of research papers and NLP news:\n\n\nThis week's highlights:\n\n\nðŸ¤¯ \nEMNLP 2019\n ðŸ¤¯\n\n\nNew QA Leaderboard Attempting to Mitigate SQuAD Problems\n\n\nGPT-2 Doesnâ€™t Bring Armageddon\n\n\nCholletâ€™s New Formulation of Intelligence\n\n\nUnsupervised Cross-lingual Representation Learning\n\n\nCompute Growth Goes Hyperbolic\n\n\n \nhttps://medium.com/@quantumstat/nlp-news-cypher-11-10-19-567ea762c963",
"date": "2019-11-11"
},
{
"vote": 4,
"title": "Converting HuggingFace GPT2 Models to Tensorflow 1.x",
"text": null,
"date": "2019-11-10"
},
{
"vote": 7,
"title": "Attention maps for text classification",
"text": "I had a colleague ask me whether there is a text classification version of attention maps  used to help understand neural networks in image classification. I am still quite a beginner in NLP and searched for what I could, but perhaps there is something obvious that this community knows? TIA",
"date": "2019-11-10"
},
{
"vote": 1,
"title": "Good research topic for computational linguistics MA",
"text": "Hello NLP community, as the title of my post suggests, I need your honest opinion on what would make a good topic for an MA research in comp ling/NLP. I come from a languages background (~7 years of work experience) and have recently started my MA in computational linguistics. Ideally, I would like to pursue a topic that is more on the computational side of the spectrum. What are some research topics that would \"open\" doors to better jobs or PhD opportunities? Thank you for your input :)",
"date": "2019-11-10"
},
{
"vote": 1,
"title": "Good topic for a MA research in NLP",
"text": "[removed]",
"date": "2019-11-10"
},
{
"vote": 4,
"title": "Need help in spacy NLP on pandas Dataframe, my program takes forever",
"text": "Hi, there,\nI have data['non_lexical_tweets'] as dataframe who has 10,000 rows of tweets.\nWhen I implement in jupyter notebook: \n\n\nimport spacy\nnlp = spacy.load(&#039;en_core_web_sm&#039;)\n\ndata[&#039;NER_tweet&#039;] = data[&#039;non_lexical_tweets&#039;].apply(lambda x: nlp(x))\n\n\n\n It never ends its executing. \n\n\nAny anyone figure out why?\n\n\nThx in advance",
"date": "2019-11-10"
},
{
"vote": 2,
"title": "Ways to Compare Machine Learning and Lexicon Based Models for NLP Purposes",
"text": "I am developing a lexicon and machine learning models and plan to compare each on their accuracy with sentiment classification. Are there any other ways I can compare two such models?",
"date": "2019-11-09"
},
{
"vote": 1,
"title": "What are good challenges I could use to validate my word prediction model?",
"text": "[deleted]",
"date": "2019-11-09"
},
{
"vote": 1,
"title": "Need some help interpreting the following model.",
"text": "[deleted]",
"date": "2019-11-09"
},
{
"vote": 1,
"title": "www.lingocall.com",
"text": null,
"date": "2019-11-09"
},
{
"vote": 1,
"title": "Help for building a Speech to Speech Translation system",
"text": "[deleted]",
"date": "2019-11-08"
},
{
"vote": 1,
"title": "GPU Cluster Projects",
"text": "[deleted]",
"date": "2019-11-08"
},
{
"vote": 4,
"title": "Lemmatization for medical latin words",
"text": "Hi folks, do you know any existing package which can do lemmatization for medical terms originated in Latin?  i.e. cocci vs. coccus (bacteria... single or multi) \n\n\nI have tried classic language tool kit (CLTK), it didn't work on those special vocabularies. \n\n\nThanks in advance!",
"date": "2019-11-08"
},
{
"vote": 1,
"title": "Documentation for Performing Named Entity Recognition on Reviews in a Data Frame(technical question) (python)",
"text": "I have scraped reviews from yelp and put the into a pandas data frame and have worked with NLTK to clean/normalize the data. I am trying to perform named entity detection on the significant words left in each review after cleaning. Is there any helpful documentation or implementation that somebody might have",
"date": "2019-11-08"
},
{
"vote": 2,
"title": "[Advice] Finding similarities between large (and already similar) amount of texts",
"text": "I am looking into different ways of finding similarities between large (from 5 to 10 thousand documents) amount of texts within same domain. It means that all of text are already 'kind of' similar and they will not share highly distinctive features. \n\n\nThis are some I already tried or plan to try (with different success rates).\n\n\nTD-IDF for single words.\n\n\nTD-IDF for ngrams (bigrams and trigrams).\n\n\nSVO? (Subject-verb-object) I didn't test this one.\n\n\nNER\n\n\nOf course, all of this gives me just a set of keywords from every text. Any advice on how to actually make similarity connection with highest probability using a mix of those or more?\n\n\nAll of this depends on dataset, but my current idea is to use one of those named above and 'build-down' semantic connections. I.e Extract all entities, find most common phrases around those entities and locate specific sentence where they are used to try to somehow get the subject/object relation. Weight it. Somehow. Get the probability of connection between texts - Text[0] shares Entity with Text[52], Text[0] shares X terms/N-grams with Text[52]. We could also get into the debate of supervised vs unsupervised approach at this point.\n\n\nI am pondering here about idea as a whole. Did anybody done something similar? In what context? What were the effects? Maybe someone knows some nice case studies?",
"date": "2019-11-07"
},
{
"vote": 4,
"title": "Need to write a program to extract problems users face from tweets",
"text": "Hey guys \n\n\nI have never worked on NLP before and I've got a hackathon coming up in a few days . My team and I want to extract problems faced by users who tweet about it on company Twitter pages and put it on a dashboard . How do i go about extracting issues they face from their tweets about them .\n\n\nExample tweet  : @user1 : Hey @company , your service is shit. The connection near my locality in [area],Allentown is never stable and despite calling you multiple times , the issue never seems to get resolved . \n\n\nExtracted info : @user1 - Unstable/Poor connection in [area],Allentown",
"date": "2019-11-07"
},
{
"vote": 2,
"title": "State of the art for classifying sequences?",
"text": "I am looking for a classifier - which takes in two inputs (word sequences) - A, B and classifies whether B is related to A or not.\n\n\nA - Can be a question or product description.\n\n\nB - Can be the answer.\n\n\nWhat is the state of the art for this type of task? Any suggestions or pointers?",
"date": "2019-11-05"
},
{
"vote": 1,
"title": "How can I get started with doing natural language generation?",
"text": "Hello, I am a beginner in NLP and interested in NLG. I have some experience in machine learning before. Can anyone give me some guidance about studying NLG? Any paper, git repo or specific task will be helpful. Thanks!",
"date": "2019-11-05"
},
{
"vote": 18,
"title": "Made sentence-similarity calculator",
"text": "repo: \nhttps://github.com/Huffon/sentence-similarity\n\n\nHi, folk. I made sentence similarity calculator using various pre-trained models and libraries. \n\n\nYou can easily make various combinations using pre-trained models and calculating methods.\n\n\nPlease feedback this project to me!\nThank you :)",
"date": "2019-11-05"
},
{
"vote": 1,
"title": "What do you call text classification where the targets for each input are 1 or more classes?",
"text": "Not multi class classification. For instance, say, fine grained emotion classification, where some texts can be exclusively happy, whereas some texts can be both happy and excited, others happy, excited, and sad. Is it as simple as changing the softmax function to accept a certain threshold in logistic regression? Like make a function that checks the difference between the top 2 or 3 classes, and based on some rule, outputs 1 or more classes?",
"date": "2019-11-05"
},
{
"vote": 1,
"title": "How can I separate different articles in Penn TreeBank Data",
"text": "I downloaded Penn TreeBank text data (from \nhttps://corochann.com/penn-tree-bank-ptb-dataset-introduction-1456.html\n), and The data format I get is sth like this: \nhttps://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt\n\n\nMy question is: How can I separate different articles in such text?  I know each line in the text corresponds to a sentence. But seems there are no article separator tags?\n\n\nThanks!",
"date": "2019-11-04"
},
{
"vote": 1,
"title": "Sentiment analysis on entities?",
"text": "Hi All,\n\n\nI was wondering what type of research is going on with entity based sentiment analysis domain. My end goal is to explore any available model or train my own model to accomplish this task.\n\n\nExample: \"I hate city X but I loved city Y.\"\n\n\nLet's say \"city X\" and \"city Y\" are two popular cities which one can identify with any popular NER model. Our sentiment model score range is 0-100%. Ideal sentiment analysis model would give me about 50% neutral score for above sentence. I want to break this sentence into pieces and want to get \"city X\" : 10% and \"city Y\": 90% because of the words association of those two cities with \"hate and loved\" respectively.\n\n\nI don't know what exactly to search to get more information on this. I will really appreciate if anyone can point me to any research papers, blogs or Github link related to this.\n\n\nThanks!",
"date": "2019-11-04"
},
{
"vote": 1,
"title": "NLP Bootcamp - Zero to Hero full day workshop - Bengaluru, 9 November",
"text": "[removed]",
"date": "2019-11-04"
},
{
"vote": 5,
"title": "Are there any research projects regarding learning how to infer the meaning of words based on their etymological roots?",
"text": "Hello. I had an idea for a project that I was interested in and wanted to ask a question.\n\n\nDoes anyone know if there have been any research projects that have to do with teaching neural networks to learn how to infer the meaning of words based on their roots?\n\n\nFor example, the word \"ophthalmologist,\" particularly referring to medical doctors who specialize in the eye, is made up of the Greek words \"\nophthalmos\n\" + \"\nlogos\n.\" The former meaning \"eye\" and the latter \"science\" or \"study.\"\n\n\nThe model I have in mind would look at the word \"ophthalmologist\" and learn how it should split the word and use the corresponding roots to infer the meaning of \"a specialist for eyes.\"\n\n\nSome of my lab mates have told me that it sounds very similar to segmentation, which is true to some extent, but what I'm looking for is something that goes beyond simple segmentation.\n\n\nAnother aspect that's troubling me about this idea aside from my lack of knowledge of prior research is how I would get the data for such a project. If anyone has any idea about that as well, any bits of information are greatly appreciated.\n\n\nThank you!",
"date": "2019-11-04"
},
{
"vote": 3,
"title": "Cleaning scraped documents at scale",
"text": "Hi redditors,\nSince a while I am facing a problem with quick preparation of scraped documents for various machine learning tasks. Typical scenario is the following: scraping of millions of online websites, storing them in csv file, then filtering and cleaning (sentence segmentation, tokenization, hashtag, email, phone number, links removal, lowercasing and also other operations) and finally writing to a file (ok then deduplication). All these operations rely on regexes or ml models (spacy) which is pretty slow and can take up to few days (Python). I am lokking for a way to speed it up significantly with some parallel processing on a cluster (I have access to gcp and can spin up a cluster). Instead of few days I would like to finish it in 1 hour. Any ready to go frameworks in Python? Pyspark, dask, ray?",
"date": "2019-11-04"
},
{
"vote": 3,
"title": "Any ideas for an algorithm to attach affixes to words in this corpus?",
"text": "Hello everyone,\n\n\nI'm currently in the process of generating a dataset from the UMich Middle English Dictionary. One thing I would like from the corpus is all attested forms of the word. The problem is that many of these forms are simply the cannonical form with a list of different spellings of prefixes and suffixes, or generally alternate spellings of a certain segment of the word. Here is an example of the \nentry\n for \"partridge\"\n\n\n> partricche, -ritch, -rech, -riz, -rige, -rigch, -rick, -erich\n\n\nWhat I'd like is a list of forms like\n> partricche, partritch, partrech, partriz, partrige, partrigch, partrick, parterich\n\n\nsome other examples are:\n> fÄÌ†derhÅd, -hÄ“de n. Also father-.\n\n\n> provocar, -our. \n\n\nThe problem is that there doesn't really seem like an trivial/straighforward way to get these forms for the given words in this particular dataset. There is no indication in the dataset that the cannonical form should be split after the segment \"part-\", however it is pretty clear in most examples what the root that the affix is attaching to is, but I can't think of an algorithm for generating these forms that's intuitive. Does anyone know of any possible solutions? (I'm working in pyhton)",
"date": "2019-11-04"
},
{
"vote": 3,
"title": "Calculating distance between words",
"text": "So I am working on a problem where I have a python list with word positions. The data is something like:\n\n\nhello : [1, 3, 5]\n\n\nworld : [2, 4]\n\n\nbye : [8, 9]\n\n\ncode : [6, 7]\n\n\nThe numbers in the list means the word position in the whole sentence. So basically the sentence is: \"hello world hello world hello code code bye bye\" Is there an efficient way to calculate the distance between these words so for example if I send a query like, \"hello world bye\" I get a value which determines the distance between these words in the sentence? For example, \"hello\" and \"world\" are always closer to each other with a difference of one, while \"bye\" is far apart from both of these words. Maybe there's an algorithm already available which I'm not aware of? Or the same problem can be solved in a different manner? Any type of guidance would be appreciated! Thanks!",
"date": "2019-11-04"
},
{
"vote": 2,
"title": "Does anyone have an idea of how the ranking-type cost works? What do we try to maximize? and what do we try to minimize? And how the backpropagation is done for it?",
"text": null,
"date": "2019-11-03"
},
{
"vote": 48,
"title": "One sentence highlight for every single EMNLP 2019 Paper (~680 in total)",
"text": "Browse all EMNLP-2019 papers in two hours!\n\n\nhttps://www.paperdigest.org/2019/11/emnlp-2019-highlights/",
"date": "2019-11-02"
},
{
"vote": 0,
"title": "Any ideas for podcast name?",
"text": "[removed]",
"date": "2019-11-01"
},
{
"vote": 6,
"title": "Is it possible to highlight parts of a sentence with USE?",
"text": "TLDR: this is an example of what I wanted to accomplish:\n\n\nhttps://www.linguee.com/english-french/search?query=oiseaux\n\n\n&#x200B;\n\n\nAfter doing a semantic search - with a few words as a query, and over sentences - I would like to \nhighlight\n the words that are related to the query. If I have, for example:\n\n\nquery = &quot;xmas&quot;\nsentence = &quot;We make french toasts for Christmas and sometimes, new year&#039;s eve&quot;\n\n\n\nI want this to highlight:\n\n\n&quot;We make french toasts for [Christmas] and sometimes, [new year&#039;s eve]&quot;\n\n\n\nI could tokenize it into words, get the embeddings and compare, this works to an extent - it will capture Christmas, but may not capture all of \"new year's eve\" correctly. Likewise if my query has too many words, it won't relate to individual words. Additionally, I can't segment this sentence because it can be in different languages.\n\n\nMy intuition / heuristic would be to start comparing at token level, and if I detect some similarity, try combinations of adjacent words, but I need think this careful because it is costly and complicated to implement (like avoiding overlaps).\n\n\nAny advice?",
"date": "2019-10-31"
},
{
"vote": 1,
"title": "Picking a Programming Language for NLP",
"text": "Hello!\n\n\nIâ€™m going to be starting a capstone project in the spring that will involve some NLP inside a text-based game, specifically in regards to finding synonyms of both words and phrases. Iâ€™m looking for a programming language to begin work on this in, but I lack experience with any NLP and machine learning, and I have a few stipulations. \n\n\nPart of this project is using tools Iâ€™ve never used before, otherwise Iâ€™d just pick Python since I know it has packages for this type of work and is quite relevant, and then go on my way. I also have experience with Java, C, and JavaScript. Otherwise, I have no idea what would have the proper packages or functionality for this work.\n\n\nTo clarify, Iâ€™m not looking for the â€œbestâ€ language, nor one thatâ€™s the most likely to land a job. Instead, Iâ€™m looking for one that would teach me something.\n\n\nTo that extent, Iâ€™ve looked at both R and Rust, but R seems to be much more statistical and Iâ€™m concerned it might not fit what Iâ€™m trying to do, and Rust seems like it might lack relevant packages. \n\n\nAm I wrong about either of those? Would there be another good fit?\n\n\nThank you!",
"date": "2019-10-31"
},
{
"vote": 4,
"title": "Computational Linguistics research labs for undergraduate internships",
"text": "I have been doing computational linguistics projects for past one and half year and I'm determined to make my career in it. For past one month I'm trying to get an internship in this field, I am trying to contact multiple University's computational linguistics groups/labs. But most of them just don't reply or are interested in Masters/PhD students. I wanted to know if there are research groups or some proper channels (not just mailing the professor and hope they revert) through which an undergrad can get some research experience at a decent computational linguistics lab. Thank you :)",
"date": "2019-10-31"
},
{
"vote": 0,
"title": "An exchange of ideas",
"text": "Most of us would accept that we exchange ideas in conversation, and any exchange requires a medium of exchange (like money). But what is a conversation's medium of exchange? Well, that medium is in plain view; the medium is words. So how many words does an idea cost? I once thought the cost, or value, of a idea in words is a phrase. Now, I think that's wrong. An idea's cost in words is at least a sentence. Or a sentence can express an idea, but sometimes it takes more than a sentence.\n\n\nA chatbot developer doesn't have to worry about this because a chatbot developer essentially writes both sides of a conversation. He or she imagines what a user says and then writes the response.  But I want to develop software that can think for itself. I don't worry about what a user says, I'm programming software with the ability to take part in an exchange of ideas; it's important for me to know what an idea costs.\n\n\nIn light of this, I'll be rolling out changes in the way Ri responds. If you're at all interested, I suggest trying Ri now and then trying Ri in a week or so, after I've rolled out the changes. Then you can tell if this idea of mine is right, or not. :)\n\n\nhttp://representi.com",
"date": "2019-10-31"
},
{
"vote": 5,
"title": "How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG",
"text": null,
"date": "2019-10-30"
},
{
"vote": 1,
"title": "â€˜ChineseGLUEâ€™ â€” New NLU Benchmark for Chinese NLP Models",
"text": null,
"date": "2019-10-30"
},
{
"vote": 0,
"title": "Finding value in my tech project",
"text": "Hi All,\n\n\nI've been working on a technology that is able to find many online reviews regarding a product category, analyse these reviews and rank the products in each aspects by how people think this products is good regarding this aspect(i.e. how good is the battery life of iphone 11 vs. the iphone pro vs. Samsung Galaxy S10). I can also sum up what are the intresting aspects of a product category.\n\n\nWho do you think can find some value in it?\n\n\nI would love some ideas,\n\n\nThanks",
"date": "2019-10-30"
},
{
"vote": 6,
"title": "CONLL-X to tree ?",
"text": "Hi,\n\n\nI am working with texts in the CONLL-X format. I would like to be able to work on their sentence structures (ideally as a tree). I found a lot of things to work with the CONLL-U format but they are not compatible with X.\n\n\nDo you know a way to convert automatically from X to U or to the parenthesis format* ?\n\n\nThanks !\n\n\n*I don't know the standard name, it looks like this : (S (NP I) (VP (V saw) (NP him)))",
"date": "2019-10-29"
},
{
"vote": 0,
"title": "NLP Weekly News Cypher",
"text": "Friends,\n\n\nBegan an NLP weekly trends column on medium to keep us up-to-date on latest news | open-sourced code releases from the NLP world. You can also follow us on the Twitter: @Quantum_Stat\n\n\n&#x200B;\n\n\n \nhttps://medium.com/@quantumstat/nlp-cypher-628100874585",
"date": "2019-10-29"
},
{
"vote": 1,
"title": "Human-human conversation dataset?",
"text": "I'm trying to find a human-human conversation dataset in order to create a simple, non-goal-oriented chatbot. I found \n\n\n\n\nhttps://catalog.ldc.upenn.edu/LDC2010T05\n\n\nhttp://convai.io/2017/data/\n\n\n\n\nHowever, the first one costs $150 and the second one only has 441 human-human conversations. I'm looking for at least a couple thousand conversations.",
"date": "2019-10-28"
},
{
"vote": 13,
"title": "Advice wanted, new to NLP and need to classify emails at work in Python",
"text": "Hi, I just got assigned to an NLP project at work, and I have little to no experience in this. I would love some pointers in the right direction as to what type of algorithm/framework to use for this, so that I can research the right thing. I'm decently experienced in Python, and I'm familiar with basic NLP and ML terms, but that's about it.\n\n\nI have a labeled database of ~3000 emails that I can use to train a model to classify emails into two categories. The thing is, only ~7% of the emails are in one of the categories (same thing for the labeled database), and it's not just a few simple keywords to filter but a relatively abstract category.\n\n\nThank you so much, please tell me if there is any key information I'm missing with my question!",
"date": "2019-10-28"
},
{
"vote": 4,
"title": "How is combinatorics used in NLP?",
"text": "I have my eye set on a career in NLP, so Iâ€™m brushing up on some of my quantitative skills. Currently Iâ€™m working on stats. The resource Iâ€™m using to learn includes a section on probability within the stats module, and \nwithin that probability section are a few lectures on combinations and permutations\n.\n\n\nIâ€™ve never studied combinatorics, but I cannot for the life of me see how it might ever be relevant to application of or topics in NLP. But Iâ€™m an NLP neophyte, so I could be mistaken. \nSo my questions are, is it useful for NLP, and if so, how?\n\n\nSince NLP is my end goal, if combinatorics is not useful, Iâ€™ll deprioritize it.",
"date": "2019-10-28"
},
{
"vote": 1,
"title": "Identify and fusion similar persons",
"text": "Hello, I am new in this field of NLP, so my question is probably something obvious that I missed, vut I 'm currently working on a project where right now I am stuck on the NER.\nI want to extract how frequent something extracted is. \nTo do that I want to fusion NE that are actually the same person.\nFor example, if I have in the same text:\n'The president Trump as been to the summit.' and 'Donald trump is the president of the United States'\nThen in the end I would have 'Donald Trump' with a frequency of 2.\n\n\nIs it even possible?\nThank you",
"date": "2019-10-28"
},
{
"vote": 1,
"title": "Vae training problem",
"text": "Hello everyone, recently, I am using variational auto encoder and I have a strange problem. When I trained a VAE and give a noise to the decoder, it always output one strange sentence â€œwhat what what...â€ whatever the noise is. However, when I check the reconstruction ability of this model, it works well. Even when I input a sentence to the encoder an then add some noise to the output and feed it to the decoder, the decoder could still output proper sentence. So, why this happened ? Any help is appreciated! Thanks!",
"date": "2019-10-28"
},
{
"vote": 28,
"title": "T5, Googleâ€™s New Transformer",
"text": ">This new transformer achieved new SOTA performance on SuperGLUE leaderboard scoring a total score of 88.9, just 0.9 away from human performance. \n\n\nThis is the paper. (kind of long!) \nhttps://arxiv.org/pdf/1910.10683.pdf\n\n\nHas anyone read it yet?",
"date": "2019-10-27"
},
{
"vote": 0,
"title": "Google on using BERT in Search",
"text": null,
"date": "2019-10-26"
},
{
"vote": 7,
"title": "Voice2text: how to create dataset?",
"text": "I have around 4000 phrases for which corresponding voice is available in the database which might be around 5 hours of speech. Phrases are of varying length.\n\n\nHow to create a training set/test set out of this for voice to text model ?\n\n\nDoes my training/test set look different if i choose different model(Hmm, rnn etc )? Or the training set can be independent of the model that i will choose.\n\n\nI saw one example in kaggle where for a simple case like for recognizing \"yes\" there was a dedicated folder for \"yes\" and it had multiple voice for \"yes\" but then for a long sentence how does one create a training set. ?",
"date": "2019-10-25"
},
{
"vote": 4,
"title": "NLP Exam Study Question",
"text": "Hey guys, I am studying for my exam in an NLP based course, looking over previous exam questions and one question is:\n\n\n\"If Sentence A is: â€œwhere is itâ€, Sentence B is: â€œit is Auckland'â€, and Sentence C is: â€œyes, it isâ€. If we hope to search for a sentence â€œwhere is itâ€, how would you get the search result based on the posting lists?\"   \n\n\nTo me this makes 0 sense to me, I was wondering if anyone could help me find the answer? :' D",
"date": "2019-10-25"
},
{
"vote": 2,
"title": "tesseract ocr",
"text": "Hello, I'm very new to programming and im doing a study on optical character recognition, planning to use tesseract, specifically pytesseract. Question is, can pytesseract measure font size? For example i have the word \"Sun\", i want it to say that \"S\" character is bigger than \"u\" or \"n\".",
"date": "2019-10-24"
},
{
"vote": 9,
"title": "Understanding WordPieceModel",
"text": "This\n paper mentions the technique (WordPieceModel) to build word inventory. The algorithm looks like this:\n\n\n&#x200B;\n\n\nWordpiece Algorithm\n\n\nI found point 2 is really confusion. It's saying build a language model, while the later part in paper says:\n\n\n>This inventory is then used for language modeling, dictionary, and decoding.\n\n\nSo, how can we build a language model, if inventory isn't there? Isn't wordpiece is used for developing word inventory first and then build a language model with preferred technique (statistical/neural)?",
"date": "2019-10-24"
},
{
"vote": 5,
"title": "Pronunciation evaluation",
"text": "I want to do a project that deals with pronunciation evaluation, but I do not know what are the languages and right tools to do it. My background is computer science so I don't really know fft and signal processing, however I'm into neural networks. Can somebody point me to the right tools and resources?",
"date": "2019-10-23"
},
{
"vote": 1,
"title": "OWL language tags",
"text": "Hi everyone, \n\n\nI posted this question in r/ontologies but it seems this sub is not very well frequented. I have a question regarding which language tags are allowed in OWL. I have data in English (@en) and Malayalam. I naively used @ml as a language tag. However, the Fuseki server I run my ontology on gives me a \"bad language tag\" warning for both tags. \n\n\nPI Google did not yield anything helpful. \n\n\nMaybe there is someone here who can help me out! Thank you in advance!",
"date": "2019-10-23"
},
{
"vote": 5,
"title": "Creating semantically similar but syntactically different variations of sentences",
"text": "I am working on a chatbot and looking to implement FAQ functionality. There is only one example of how a question can be asked by a user and so, there are some cases which my current evaluation metric, cosine similarity fails.\nI thought, if I could generate variations in the questions and create an intent, I'll be able to map them as training phrases to an intent and generate my response accordingly.\nAny help will be appreciated.",
"date": "2019-10-23"
},
{
"vote": 0,
"title": "Speak your mind with hundred language s!!ðŸ¦¸",
"text": "[deleted]",
"date": "2019-10-23"
},
{
"vote": 17,
"title": "[P] A BertSum (Bert extractive summarizer) model trained on research papers. Access to datasets also included.",
"text": null,
"date": "2019-10-22"
},
{
"vote": 2,
"title": "How were the GPT-2 token embeddings constructed? Are representations of semantically related tokens like 'king' and 'queen' close together in the embedding space?",
"text": null,
"date": "2019-10-22"
},
{
"vote": 1,
"title": "Machines Beat Humans on a Reading Test. But Do They Understand? | Quanta Magazine",
"text": null,
"date": "2019-10-22"
},
{
"vote": 7,
"title": "I built a python package to make boolean text searches",
"text": "I   built eldar, a python/cython package to make boolean queries. You  can  search in list of texts (or filter pandas dataframes) with queries containing the usual \"OR\", \"AND\", and \"AND NOT\", like:\n\n\n(\"gandalf\" OR \"frodo\") AND NOT (\"movies\" OR \"adaptation\")\n\n\nYou can search in lists of dicts as well. The readme is still in progress, and additional tests will be added in the next few days: \nhttps://github.com/kerighan/eldar\n\n\nGiven a gensim Word2vec model for example, I'll also add the ability to match similar words as queried.",
"date": "2019-10-22"
},
{
"vote": 10,
"title": "How important is it to have a Masterâ€™s/PhD degree?",
"text": "[deleted]",
"date": "2019-10-21"
},
{
"vote": 8,
"title": "Inference over a knowledge base",
"text": "Hi, \n\n\nI was looking do some survey over the inference capability of a knowledge base built using RDF vs  logic programming rules.\n\n\nI was wondering if anyone has any resources to help me with this? I was reading about inference systems built on top of RDF using Apache jena. any one has experience with it? \n\n\n&#x200B;\n\n\nAny help will be appreciated. \n\n\n&#x200B;\n\n\nThanks,",
"date": "2019-10-21"
},
{
"vote": 19,
"title": "A general approach for fine-tuning BERT in Pytorch",
"text": "https://medium.com/@f2015791/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa\n\n\n&#x200B;\n\n\nI have written a tutorial on how to fine tune BERT for some NLP problem in pytorch. The purpose of this post is to provide a general method to use BERT as just another pytorch module in your network architecture. Any questions and suggestions are welcome.",
"date": "2019-10-20"
},
{
"vote": 8,
"title": "Bert: padding all inputs to 512, vs padding to maximum length in a batch.",
"text": "I see that a popular practice in bert training is padding a batch to the match the size of the largest sample in the batch. \n\n\nI am wondering if there are some solid benefits to doing this, vs just padding all samples to 512.",
"date": "2019-10-20"
},
{
"vote": 1,
"title": "Manipulating syntactic trees ?",
"text": "Hi all,\n\n\nCan you guide me towards softwares/libraries/packages that one can use to manipulate syntactic representations of sentences ?\n\n\nI know of tregex, I am finding other stuff here and there and I am interested in anything you may know. \n\n\nThanks !",
"date": "2019-10-19"
},
{
"vote": 9,
"title": "Text generation evaluation problem",
"text": "Hi everyone, thanks for your attention. I am puzzled about how to evaluate a unsupervised text generation model. I found lots of paper use BLEU, self-BLEU and Perplexity. BLEU and self-BLEU are easy to understand and calculate because of some existed tools such as NLTK. Perplexity is really confusing. How can I calculate perplexity for my generated text corpus? Is there any existed evaluation tools? Or I have to train a RNN model and then calculate loss then get perplexity?\n\n\nReally thanks for your help !!!",
"date": "2019-10-19"
},
{
"vote": 13,
"title": "logic behind tf-idf calculation",
"text": "Does anybody know a good resource for understanding tf-idf? I know how to calculate tf (term frequency) and idf (inverse document frequency), I wanted to have deeper understanding of it; how it works and why it is effective? If anyone can suggest any research paper or any other source it will be really helpful. Thank you!\n\n\n(this is my first post so sorry if I made any mistake)",
"date": "2019-10-18"
},
{
"vote": 3,
"title": "Custom annotator tool?",
"text": "So yea looking for a custom annotator tool. It's not for NER, but rather for psychological applications. Like say this particular phrase can be classified into \"hopelessness\", and another into another category. As for output, I'd like to be able to collect all those annotations, and use them on some ngram classification model to classify or detect bits of text.\n\n\nEDIT: Thanks to everyone who commented. I decided to use doccano after trying a few since it seems to be the easiest to set up and run. My next step would be to host this on a web server like maybe AWS so I can have another person do the annotation with me.",
"date": "2019-10-18"
},
{
"vote": 14,
"title": "I want to apply for master and I have to choose a research area. Which topic of NLP do you think has a better future and is better to research on?",
"text": null,
"date": "2019-10-17"
},
{
"vote": 1,
"title": "How would you go about identifying body parts and type of injuries from text data?",
"text": "[deleted]",
"date": "2019-10-17"
},
{
"vote": 1,
"title": "adviceneeded: Recognition/Counting of Drawing Blocks for Eng Drawings?",
"text": "Hello all,\n\n\nI am looking for some advice.\n\n\nThe Problem: My company is large and old-fashion. Currently, we have to manually read through hundreds of eng drawings counting 'blocks'. \n\n\nThe solution: Some type of software/method that is capable of \"reading an eng drawing PDF\" and counting all of the blocks using some type of reading software.\n\n\nIs it possible? Does it exist? Help!\n\n\nFor example:\n\nhttps://www.acm.ca/sites/default/files/email-diagram.png",
"date": "2019-10-17"
},
{
"vote": 1,
"title": "Sotabench: Benchmarking Open Source Models Directly From GitHub",
"text": "[deleted]",
"date": "2019-10-17"
},
{
"vote": 21,
"title": "Would you like a Chrome extension to train NER?",
"text": "Our team is planning to build a Chrome extension to make NER training dead easy for anyone.\n\n\nWorks like this:\n\n\n\n\ndownload the extension and install it;\n\n\nbrowse to any real-life web page;\n\n\nmark a phrase, and add a tag.\n\n\n\n\nHas these features:\n\n\n\n\ndefine new tags;\n\n\nexport trained data for use in any NLP tool/library;\n\n\nsupport for multiple users, so that you can hire as many mturks as needed, and manage them efficiently.\n\n\n\n\nThoughts?",
"date": "2019-10-16"
},
{
"vote": 10,
"title": "Using GloVe word embeddings in translation tasks with PyTorch",
"text": "For an assignment, I need to translate from English to Italian, using GloVe 100 dimensional embeddings. We were given this English-to-German \nnotebook\n, using PyTorch, as a reference. The reference builds its own embeddings, rather than using GloVe as the assignment requests. I'm unsure how to go about making this change.\n\n\nI'm not asking for anyone to tailor the code in the linked notebook for my task by any means, but rather, just point me in the right direction. Does this appear to a somewhat simple fix where I swap out embedding layers? Or more of a major overhaul?",
"date": "2019-10-15"
},
{
"vote": 3,
"title": "Information Extraction/Semantic Search for long, unstructured documents",
"text": "Hello good NLP people,\n\n\nI am stuck with a particular task of \ninformation extraction\n. I have a few hundred, long (5-35 pages) \npdf, doc and docx\n project documents from which I seek to extract specific information and store them in a structured database.\n\n\nThe ultimate goal is to extract and store information in a way that we can query those and any new incoming documents for fast and reliable information. For instance, I want to query a combination of entities from the knowledge base and then return the n-most relevant paragraphs/sentences from the documents. Since some entities like â€œWorld Bankâ€ are extracted dozens of times for some documents, I need a way to query the entity in context. Otherwise I just end up with a database that contains the names of specific entities without any way to map them back.\n\n\nNER\n usually seems like a good solution for this, however, the documents all have very unique structures which also change from document to document. For instance, a lot of relevant information is stored in tables, but also in long paragraphs.\n\n\nAs far as I understand, \nNER\n uses the surrounding words to identify entities, hence loading in the whole documents as raw text and manually tagging terms in the tables will probably not serve as good training data.\n\n\nFor now I built a function that extracts the raw text from pdf, doc and docx documents and extracted entities with spaCys NER model, however, I need to define my own entities and the domain is to scientific for \nspaCy\n to deliver good results. I also obtained a prodigy license for annotation.\n\n\nAlso, it seems I have to find a way to distinguish between \"relevant\" text and table entries and \"junk\" - aka footnotes, annexes, titles, subtitles, etc.\n\n\nI have the feeling this could be a usual corporate task and perhaps some of you have made experiences on this and are willing to share their wisdom!\n\n\nThanks a lot!",
"date": "2019-10-15"
},
{
"vote": 2,
"title": "7th International Conference on Artificial Intelligence and Applications (AIAPP 2020)",
"text": "7th International Conference on Artificial Intelligence and Applications (AIAPP 2020)\n\n\nJanuary 25 ~ 26, 2020, Zurich, Switzerland\n\n\nhttps://preview.redd.it/eu464mixgos31.png?width=1346&format=png&auto=webp&s=aca3f7efbc77eeb60c546c0b8b42aa451dd9485b\n\n\nhttps://cosit2020.org/aiapp/index.html\n\n\nSubmission system :\nhttps://cosit2020.org/submission/index.php\n\n\nHereâ€™s where you can reach us : \naiapp@cosit2020.org\n or \naiapp_secretary@yahoo.com\n\n\nSubmission Deadline : October 19, 2019",
"date": "2019-10-15"
},
{
"vote": 3,
"title": "How to build a NER tagger from scratch?",
"text": "I'd like to build my own NER tagger from scratch. I'm aware that there are easier options, such as using SpaCy's NER tagger and (re)training it to tag my own supplied entities. However, I'd like to \"see\" into the black box a bit more.\n\n\nI have experience with \"vanilla\" ANNs as well as RNNs, including LSTMs, using PyTorch. Are these familiarities suitable for what I'm trying to do? I might have read that NNs are overkill, and this sort of problem is more usually solved with graphical models. Not sure where I came across this, so it might be hearsay..\n\n\nAny python specific recommendations would be appreciated!\n\n\nEdit1: Stanford thinks LSTMs are a good tool for this \nchallenge\n\n\nEdit2: For work, I'm parsing resumes and job descriptions for skills. A web developer's resume might include \"java\" which I would want to tag as a skill. Other entities are not of consequence to this specific work task.",
"date": "2019-10-14"
},
{
"vote": 1,
"title": "Retrieving sentences containing particular words from web ?",
"text": "Hi everyone,\n\n\nDoes anyone know any api or a pretrained model which returns/generates sentences based on given words ?",
"date": "2019-10-13"
},
{
"vote": 1,
"title": "sentiment analysis , key words extraction NLP",
"text": "Hello , \n\n\nfor a project that i'm working on i have 287 articles ( talking about developpmeent , IT ,..... )written in french   and i have 3 classes positive , neutral and negative  and i have to extract information for example if a user searches the word NLP i have to provide him all the articles that talk about Natural language processing   i also have to tag the articles that contain names of certain companies like microsoft , google  and also the acronyms meaning  if an article has the word  \nIBAN\n  the program has to know that its  \nInternational Bank Account Number\n ( i can provide the list of all the abreviations and companies names )   \n\n\nfor the sentiment analysis i know that 287 articles aren't  enough to do the training so i was thinking to use TRANSFER LEARNING  \n\n\nfor the second phase  i really have no idea how start can anyone please help",
"date": "2019-10-13"
},
{
"vote": 0,
"title": "PewDiePieâ€™s $300 Million Personality from His 5 Million Words",
"text": null,
"date": "2019-10-13"
},
{
"vote": 0,
"title": "Please suggest alternative for LIWC",
"text": "So I want to use LIWC for analysing text, but I am unable to get access to it. I simply do not get a reply from them.\n\n\nDoes anyone here know of an alternative?",
"date": "2019-10-13"
},
{
"vote": 0,
"title": "Which program is used to compute average precision?",
"text": "Which program is mainly used to compute average precision in Information Retrieval? In particular, I'm looking for uninterpolated average precision. By googling I have found only the sklearn module in Python.",
"date": "2019-10-12"
},
{
"vote": 0,
"title": "Master Thesis - data needed for an application about Controlling robots with NLP.",
"text": "[deleted]",
"date": "2019-10-11"
},
{
"vote": 4,
"title": "[Open Beta] The Newsphere: A News Visualization Platform Powered by NLP",
"text": "Hey guys, thought you might be interested in this. My friends and I are working on a website called The Newsphere and weâ€™re looking for others who share our interest in understanding both current and past events to help us beta test our platform.\n\n\nThe Newsphere acts as a virtual map of world news, enabling users to see the news as the result of a complex web of relationships between people, places, organizations, and countries. You can watch our intro video on youtube here: \nhttps://www.youtube.com/watch?v=Wpwgat54KXg\n or keep reading to learn more about the Newsphere.\n\n\nBasically, our model analyzes thousands of news articles every day, and uses natural language processing to extract the different entities in them, as well as to identify relationships between them. Over time we construct narratives, which the user can see illustrated on a graphical timeline. The goal here is to give context to current events, enabling users to view recent developments in light of past patterns of behavior.\n\n\nIf youâ€™re interested in learning more, you can comment below or email me directly at \nalex@newsphere.org\n and I'd be happy to get back to you. If you're interested in signing up for the open beta (which we're planning to launch by the end of the month!), you can find the form here: \nhttps://newsphere.org/signup\n Thanks again!",
"date": "2019-10-11"
},
{
"vote": 0,
"title": "NLP basics, hands-on: a language classifier deployed online in 3 steps",
"text": null,
"date": "2019-10-10"
},
{
"vote": 1,
"title": "Is there a way to create a timeline from text without explicit dates?",
"text": "I'm using NER to grab the dates of events in a text and put them into a timeline. This works great... for explicit dates (July 9th or August of 1942), but has there been anything that can organize say last week and today or next wednesday?",
"date": "2019-10-10"
},
{
"vote": 0,
"title": "5 Use Cases of NLP. From Text Mining to Sentiment Analysis",
"text": "[deleted]",
"date": "2019-10-10"
},
{
"vote": 16,
"title": "If I want to add vocabulary to transformer models like XLNET, BERT,GPT Do I need to retrain the models from scratch?",
"text": "I'm kind of new to NLP so im not sure if my understanding is correct or not. From my understanding, models like XLNET are trained based on a vocabulary that has a predefined size. XLNET uses a sentencepiece model which was trained on a prior data set to create its vocabulary set and then subsequently tokenises text based on the vocabulary during the training procedure. During the training, it's building context based word embeddings. \n\n\nIf I wanted to add new words to the vocabulary of the existing sentencepiece model, would they not contain any additional context embeddings because the XLNET model trained on that one specific vocabulary set or would I be able to essentially build a new sentencepiece model with a larger vocabulary and hotswap them with an existing XLNET model?",
"date": "2019-10-10"
},
{
"vote": 2,
"title": "Query Wikidata for lexeme/lemma and subclass for all tokens?",
"text": "I'm currently downloading the latest-all.json.gz as a last resort but I've poked around in the wiki SPARQL interface, to see if I can get the dataset i'm after: I would like a set of data that contains a particular lexeme and it's corresponding subclass and superclass ('instance of'). \nIt looks like i can query for such a thing if i know the particular token,\n but not if i just want all the tokens in the english language.  Plus the example provided gives the entire graph and all its parents, where I would like to have a row by row representation of such a graph so that I can build my graph programmatically.  \n\n\n&#x200B;\n\n\nThe goal is to replicate something like a simplified wordnet using wikidata.  Such a graph would be very large, so I'm hoping to limit by a specific set of super classes.  Yes, I know BabelNet exists, but the python API is commercial-only, and even the java API has a daily 1K query limit.  \n\n\n&#x200B;\n\n\nCan anyone give insights on how to I might achieve this?",
"date": "2019-10-09"
},
{
"vote": 0,
"title": "Seeking NLP Expert as Co Founder - Curation Platform - Top Growth Industries",
"text": "[deleted]",
"date": "2019-10-09"
},
{
"vote": 0,
"title": "No Code Natural Language Processing: Feedback needed!",
"text": "[deleted]",
"date": "2019-10-08"
},
{
"vote": 6,
"title": "Extract statement out of simple questions",
"text": "I have a set of simple questions such as \"Is the silver string Osmophoto making sound ? \" or \"Does the door lock is broken\" and i would like to extract the statement from the question\n\n\nIs the silver string Osmophoto making sound  ? --> silver string Osmophoto making sound\n\n\nDoes the door lock is broken ? --> door lock is broken \n\n\nOne way to handle it will be with regex and to extract the is/does/WH  etc. But i'm wondering if there are more options using parsing the sentence or chunking it .\n\n\n&#x200B;\n\n\nAny idea?",
"date": "2019-10-07"
},
{
"vote": 2,
"title": "Advice on how to work on projects as a beginner?",
"text": "I am sorry, I know that this question is probably commonly asked.  \n\n\nThe only vague-ly project-esque thing that I have done so far is classifying stackoverflow questions using a dataset that I found. It had the titles of the questions and the tags, just had to classify them. Used BoW and TF-IDF as features and used sklearn's classification algorithms. Didn't really get a sense of accomplishment after I was done with it.  I don't have any intuition for what I am doing and am just following random articles that I find online.   \n\n\nI would like to do something more practical and more challenging. So does anyone have any suggestions or advice?   What sort of concepts should I be consolidating my grasp on? What outcomes should I be focusing on while working on any project?  \n\n\nI have just started with NLP and am relying on self-study. I'm following Dan Jufarsky's introduction book.  So far I am up to learning about sentiment analysis.\nWhat should be the rough roadmap that I should be following?   \n\n\n Would greatly appreciate all help.",
"date": "2019-10-07"
},
{
"vote": 1,
"title": "Connecting accurate data",
"text": null,
"date": "2019-10-07"
},
{
"vote": 19,
"title": "Unofficial release of Chinese pretrained weights for the Albert transformer model (pytorch, keras, tensorflow)",
"text": null,
"date": "2019-10-06"
},
{
"vote": 5,
"title": "Clustering documents using multiple models",
"text": "Hello!\n\n\nI'm working on news article clustering. I've tried many approaches and I've noticed that when I use two inputs from different models (e.g. LSI and word2vec+SIF, or LSI and named entities, etc.) precision increases.\n\n\nInitially I was using my own naive clustering algorithm (was focusing on getting similarity right) and for that I was using two different thresholds. I.e. cosine similarity using both models has to be over their respective thresholds for two documents to be clustered together.\n\n\nNow I want to use DBSCAN which is the algorithm that seems to work better for my use case of the ones I've tried (OPTICS, HDBSCAN and others).\n\n\nI was reading a \npaper\n where they propose two alternatives:\n\n\n\n\nconcatenate vectors; they first normalise them (Euclidean) and also weigh them (they use word2vec and named entities, and they weigh the named entities vector with the number of named entities)\n\n\na second method which is pretty much what I was doing. From the paper: \nThe second method, similar to that used in (Kumaran and Allan, 2004), requires that both word distance and NE distance should be sufficiently closeâ€”closer than corresponding thresholds\n\n\n\n\nHowever it's not clear to me how they use both thresholds when clustering. Also they use agglomerative clustering, not DBSCAN. This part is the one I don't understand:\n\n\n> In this case, we cannot use the complete linkage metric since a maximum of distances is not defined if the distance is a pair of numbers. \n\n\nI also tried a third approach, simply averaging (possibly weighed) the two distance matrices calculated separately.\n\n\nFor approach #1 I was testing with this code:\n\n\ndef normalize_vector(vector):\n    norm = np.linalg.norm(vector)\n    return vector if norm == 0 else vector / norm\n\ndef normalize_matrix(matrix):\n    return np.array([normalize_vector(v) for v in matrix])\n\ndef stack_horizontally(matrices):\n    return np.hstack(tuple([m for m in matrices]))\n\ndef get_distance_matrix(matrices):\n    stacked_matrices = stack_horizontally([normalize_matrix(m) for m in matrices])\n    return pairwise_distances(stacked_matrices, metric = &#039;cosine&#039;, n_jobs = cores).tolist()\n\n\n\nHopefully I'm normalising the matrices right for this task.\n\n\nI get the embedding matrices for two models, then call \nget_distance_matrix\n and get the distance matrix that I pass to DBSCAN for clustering (with \nmetric = precomputed\n). I'm not weighing the matrices before stacking them yet. Seems to work ok.\n\n\nHowever I'd like to know if there's a way to use two different thresholds when clustering, as it's what I was doing and I feel gives me more control.\n\n\nI thought of getting the two distance matrices separately and then combining them so that when a specific pair of values is not over their respective thresholds, I would increment the distance. This seems wrong to me though. Any thoughts?\n\n\nThanks!",
"date": "2019-10-05"
},
{
"vote": 6,
"title": "Finding tokens in raw text",
"text": "Hello.\n\n\nI'm working in a NER project where we are trying to fetch names from different documents. We have tried different tools, but right now we are testing Spacy and Polyglot.\n\n\nSo far, we get the best results from polyglot, however, in the entities we dont get the position in the original text as we do in Spacy or in Duckling.\n\n\nThe only information that we get is the token index for the entity:\n\n\nres.entities\n[I-ORG([&#039;France&#039;, &#039;-&#039;, &#039;Presse&#039;]),\n I-PER([&#039;Jacques&#039;, &#039;Chirac&#039;]),\n I-LOC([&#039;Paris&#039;]),\n I-LOC([&#039;Matignon&#039;]),\n I-PER([&#039;ValÃ©ry&#039;, &#039;Giscard&#039;, &quot;d&#039;Estaing&quot;]),\n I-PER([&#039;FranÃ§ois&#039;, &#039;Mitterrand&#039;]),\n I-PER([&#039;Philippe&#039;, &#039;Goulliaud&#039;]),\n I-ORG([&#039;Figaro&#039;]),\n I-PER([&#039;Emmanuel&#039;, &#039;Serot&#039;]),\n I-PER([&#039;Michel&#039;, &#039;Martin&#039;, &#039;-&#039;, &#039;Roland&#039;]),\n I-PER([&#039;Sylvie&#039;, &#039;Maligorne&#039;]),\n I-PER([&#039;Chirac&#039;])]\nres.words\nWordList([&#039;Au&#039;, &#039;fil&#039;, &#039;des&#039;, &#039;ans&#039;, &#039;,&#039;, &#039;de&#039;, &#039;nombreux&#039;, &#039;journalistes&#039;, &#039;de&#039;, &quot;l&#039;Agence&quot;, &#039;France&#039;, &#039;-&#039;, &#039;Presse&#039;, &#039;ont&#039;, &#039;croisÃ©&#039;, &#039;Jacques&#039;, &#039;Chirac&#039;, &#039;:&#039;, &#039;notamment&#039;, &#039;quand&#039;, &#039;il&#039;, &#039;Ã©tait&#039;, &#039;maire&#039;, &#039;de&#039;, &#039;Paris&#039;, &#039;,&#039;, &#039;Ã &#039;, &#039;Matignon&#039;, &#039;,&#039;, &#039;oÃ¹&#039;, &#039;il&#039;, &#039;fut&#039;, &#039;le&#039;, &#039;Premier&#039;, &#039;ministre&#039;, &#039;de&#039;, &#039;ValÃ©ry&#039;, &#039;Giscard&#039;, &quot;d&#039;Estaing&quot;, &#039;et&#039;, &#039;FranÃ§ois&#039;, &#039;Mitterrand&#039;, &#039;puis&#039;, &#039;au&#039;, &#039;Palais&#039;, &#039;de&#039;, &quot;l&#039;ElysÃ©e&quot;, &#039;.&#039;, &#039;Philippe&#039;, &#039;Goulliaud&#039;, &#039;,&#039;, &#039;qui&#039;, &#039;a&#039;, &#039;depuis&#039;, &#039;rejoint&#039;, &#039;le&#039;, &#039;quotidien&#039;, &#039;Le&#039;, &#039;Figaro&#039;, &#039;,&#039;, &#039;Emmanuel&#039;, &#039;Serot&#039;, &#039;,&#039;, &#039;ex&#039;, &#039;-&#039;, &#039;correspondant&#039;, &#039;Ã &#039;, &quot;l&#039;ElysÃ©e&quot;, &#039;,&#039;, &#039;le&#039;, &#039;journaliste&#039;, &#039;et&#039;, &#039;romancier&#039;, &#039;Michel&#039;, &#039;Martin&#039;, &#039;-&#039;, &#039;Roland&#039;, &#039;et&#039;, &#039;Sylvie&#039;, &#039;Maligorne&#039;, &#039;,&#039;, &#039;ancienne&#039;, &#039;cheffe&#039;, &#039;du&#039;, &#039;service&#039;, &#039;politique&#039;, &#039;de&#039;, &quot;l&#039;AFP&quot;, &#039;,&#039;, &#039;nous&#039;, &#039;livrent&#039;, &#039;quelques&#039;, &#039;grands&#039;, &#039;moments&#039;, &#039;de&#039;, &#039;sa&#039;, &#039;vie&#039;, &#039;politique&#039;, &#039;et&#039;, &#039;leur&#039;, &#039;&quot;&#039;, &#039;Chirac&#039;, &#039;&quot;&#039;, &#039;,&#039;, &#039;&quot;&#039;, &#039;animal&#039;, &#039;politique&#039;, &#039;&quot;&#039;, &#039;Ã &#039;, &#039;la&#039;, &#039;fois&#039;, &#039;&quot;&#039;, &#039;jovial&#039;, &#039;&quot;&#039;, &#039;,&#039;, &#039;&quot;&#039;, &#039;proche&#039;, &#039;des&#039;, &#039;gens&#039;, &#039;&quot;&#039;, &#039;.&#039;, &#039;.&#039;, &#039;.&#039;, &#039;et&#039;, &#039;&quot;&#039;, &#039;menteur&#039;, &#039;invÃ©tÃ©rÃ©&#039;, &#039;&quot;&#039;, &#039;.&#039;])\n\n\n\nIf I check, for example:\n    res.entities[0].start\n\n\nI get: 10, which is the index for the WordList, but not the actual position in the original text.\n\n\nPart of the problem is that sometimes we get tokens like:\n\n\nI-PER(['M', '.', 'Jacques', 'Chirac']) from the text 'M. Jacques Chirac', so using index or find methods are risky since we can't rebuild the original string and if I use the first token, chances are that I will find other 'M' within the original text.\n\n\nHas anyone worked with Polyglot or had a similar situation?\n\n\nSomeone already asked in \nSO\n\nbut that solution fails in the example that I mentioned and if an entity overlaps.\n\n\nThanks",
"date": "2019-10-04"
},
{
"vote": 1,
"title": "Hugging Face Implements SOTA Transformer Architectures for PyTorch and TensorFlow 2.0",
"text": null,
"date": "2019-10-04"
},
{
"vote": 18,
"title": "Using Pytorch's dataloaders to deal with big size text files.",
"text": "I was recently working on a problem where I was required to read data from an 11 GB file. The traditional Dataset API of PyTorch for this problem will require you to store whole data in memory, which in this case wasn't feasible. So I came across this IterableDataset class introduced in Pytorch 1.2 which helped me solve this problem. However, I couldn't find a good documentation on how to use this feature, so I decided to write a \ntutorial\n on the same. I hope someone will find this useful.",
"date": "2019-10-04"
},
{
"vote": 1,
"title": "Hangman",
"text": "[deleted]",
"date": "2019-10-04"
},
{
"vote": 27,
"title": "Releasing PAWS and PAWS-X: Two New Datasets to Improve Natural Language Understanding Models",
"text": null,
"date": "2019-10-02"
},
{
"vote": 6,
"title": "Introductory example for building a Speech Recognition Decoder",
"text": "Hey there, I'm looking for a good example/tutorial on building a Speech Recognition Decoder. I've already built the encoder, which takes the raw audio stream and calculates the Log melspectogram, but i'm unclear on how to predict individual words via a CTC-based decoder. I've looked through github, but I haven't seen a clear example. Presumably the decoder will use a word embedding to match the vocabulary. Please let me know if you know of any good examples, Thanks!",
"date": "2019-10-02"
},
{
"vote": 6,
"title": "Language Models using LSTMs",
"text": "What is the goto language model for autoregressive language modelling?\n\n\nSo far I know the AWD-LSTM \nhttps://arxiv.org/abs/1708.02182\n\n\nBut are there other newer architecture out there, preferably with open source implementations?",
"date": "2019-10-01"
},
{
"vote": 1,
"title": "5 Text Annotation Services for Voice Assistants",
"text": null,
"date": "2019-09-30"
},
{
"vote": 7,
"title": "[P] DeepSegment v2 now supports English, French and Italian. tf-serving docker images also supported.",
"text": "Hi all, around a year ago, I open sourced \nhttps://github.com/bedapudi6788/deepsegment\n. A sentence segmenter that is aimed towards segmenting text with bad/no punctuation. This release is a major update with much cleaner interface and many more performance improvements. Do check it out and any feedback is appreciated.",
"date": "2019-09-29"
},
{
"vote": 13,
"title": "Introduction to NLP",
"text": null,
"date": "2019-09-28"
},
{
"vote": 2,
"title": "How to look for keywords specifying area of improvement in sentimental analysis of reviews",
"text": "Hi. I am new to practically applying NLP but have studied about it from a lot of resources. Recently I picked up a project on product reviews which requires classifying a sentiment based on its polarity and then extracting the information of area of improvement on the negative polarity.\nI have completed the first part but is struck as how to extract the keywords specifying thr information of improvement through NLP techniques.\nIt would be very helpful if you guide me to a resource where I can find the solution to my problem.\nThanks in advance.",
"date": "2019-09-28"
},
{
"vote": 2,
"title": "[D] What search terms have you used when looking up ML papers, either on Arxiv, Arxiv-Sanity, Google Scholar, Semantic Scholar, etc? I'm trying to make a model for searching through ML papers, and I am looking for test cases for the model.",
"text": null,
"date": "2019-09-28"
},
{
"vote": 4,
"title": "Generating jokes with probabilistic sentence embeddings",
"text": "Hi guys.\n\n\nIs there anything in NLP today like probabilistic sentence embeddings?\n\n\nMy idea is to use such a thing to generate or identify jokes. Jokes are often funny cause a sentence has 2 meanings. What if you can get a \"sentence embedding distribution\", then you could get the distance between sentence embeddings components for the same sentence. If the distance is large, then it's a joke.",
"date": "2019-09-28"
},
{
"vote": 12,
"title": "GitHub CodeSearchNet challenge - using natural language to find source code",
"text": null,
"date": "2019-09-27"
},
{
"vote": 0,
"title": "Neural machine translation code",
"text": "I was looking around neural machine translation and found these codes\n\n\n\n\nnmt github\n\n\nnmt notebook\n\n\n\n\nWhat is the difference between these codes, I found notebook one easier to setup and run",
"date": "2019-09-27"
},
{
"vote": 1,
"title": "Yooul. Where the world chats.",
"text": null,
"date": "2019-09-26"
},
{
"vote": 2,
"title": "Anyone have experience with the stony brook compLing masters?",
"text": "[deleted]",
"date": "2019-09-24"
},
{
"vote": 0,
"title": "Is there a curated list of AI project I can test and interact with on the internet somewhere to have a broad idea of what the tech is capable of?",
"text": null,
"date": "2019-09-24"
},
{
"vote": 0,
"title": "AI Solves: King is to Queen as Brother is to ... ?",
"text": null,
"date": "2019-09-24"
},
{
"vote": 1,
"title": "High-tech Interpreters",
"text": null,
"date": "2019-09-23"
},
{
"vote": 1,
"title": "Khan Academy Transcript and Comment Downloader",
"text": "Does anyone know of a khan academy GitHub repo (or other code) that can download the comments and transcripts, I have only found ones for their videos. But I only require transcripts and comments for a chatbot that I am creating.",
"date": "2019-09-22"
},
{
"vote": 1,
"title": "Grade 3 Math Questions Dataset",
"text": "Hello everyone, Would anyone happen to have or know where to find a dataset of conversation between students and teachers talking about mathematics, specifically grade 3 students? Texts such as: \n\n\nStudent: Mr./Ms. ..., could you explain .... \n\n\nTeacher: Sure (Student), .....",
"date": "2019-09-22"
},
{
"vote": 27,
"title": "One of the more interesting submissions from the Pytorch Hackathon: Single Headed Attention RNN. Compares the Transformer with single headed RNNs with a few other additions. Great results with a fraction of the computations, and huge gains over regular LSTMs.",
"text": "https://devpost.com/software/single-headed-attention-rnn\n\n\nCompares the Transformer with single headed RNNs with a few other additions. Great results with a fraction of the computations, and huge gains over regular LSTMs. Can give insights to where a lot of the gains from the Transformer comes from.\n\n\nHere's the rest of the submissions\n\n\nhttps://pytorch.devpost.com/submissions\n\n\nAnother one that caught by eye \n\n\nhttps://devpost.com/software/ava-toward-a-machine-translation-platform-for-deaf\n\n\nIt looks like this is the first machine learning model for American sign language translation.\n\n\nFinally, a shout out to the project I submitted SpeedTorch.\n\n\nhttps://devpost.com/software/speedtorch-6w5unb\n\n\nIt can Speed up data transfer to/from Pytorch Cuda variables, in some instances (mostly CPUs with low numbers of cores) up to 410x by using alternative indexing kernels.",
"date": "2019-09-22"
},
{
"vote": 2,
"title": "can anyone share a copy of FrameNet?",
"text": "[removed]",
"date": "2019-09-20"
},
{
"vote": 2,
"title": "Project recommendations for deep learning NLP",
"text": "Currently Iâ€™m enrolled in a deep learning class at uni, with applications in computer vision and NLP. Rather than a final exam, the course culminates in a practical project using methods discussed. We have GPU servers available for students.\n\n\nIâ€™m curious what are some good project ideas for NLP using deep learning?\n\n\nFor example, Iâ€™m interested in query detection/answering - but this seems like the â€œdeep end of the pool.â€ Is this an appropriate project?\n\n\nIf not, what alternatives should I consider? Iâ€™ve done text classification projects before and would like to do something different this time around.",
"date": "2019-09-20"
},
{
"vote": 2,
"title": "Auto Tagging Customer Survey Data",
"text": "[deleted]",
"date": "2019-09-20"
},
{
"vote": 2,
"title": "Are there any datasets/tasks related to Materials Science NLP?",
"text": "I am working on a materials science project analyzing texts from MS research papers, I am wondering if there are any datasets/tasks that we can text our model on.",
"date": "2019-09-19"
},
{
"vote": 1,
"title": "Suggestions request - books about interpreting text for chunking",
"text": "Hi, I'm working on a system to understand unstructured text by extracting meaning from it. I am considering chunking (hierarchy of ideas) to achieve this. While I get my hands dirty, I would like to learn more about the English language's structure. That way, my approach is formal, rather than creating a grammar based on what I think or feel is right.\n\n\nI am not new to NLP in general, but I would appreciate any suggestions for books (or papers) that could help me.",
"date": "2019-09-19"
},
{
"vote": 8,
"title": "Evaluation of word embeddings",
"text": "Is there any way to evaluate the quality of word embeddings to get a numeric score?",
"date": "2019-09-19"
},
{
"vote": 1,
"title": "How do I find relevance of a company to a piece of news about bankruptcy?",
"text": "I want to find bad news about a specific set of companies. I decided to divide separate topics into different models. Let's assume I want to find bad news about a company, that it is going to be bankrupt. I created a model that returns the probability that a piece of news is about bankruptcy, but I don't know what to do with relevance. For example, news about bankruptcy can contain names of other companies, \nreal example:\n\n\n\"\nForever 21 Inc.\n is in discussions to give a stake in the company to its two largest landlords as part of a restructuring that would also let co-founder Do Won Chang retain a share, according to people familiar with the matter.\n\n\nThe ailing fast-fashion retailer is in talks with mall owners \nSimon Property Group Inc\n. and \nBrookfield Property Partners\n about the proposal\"\n\n\nSo if company \"Forever 21 Inc\" is in the set, I want to return that the news is relevant, but not for Simon Property Group Inc. and Brookfield Property Partners. The only my ideas are to account for how many times the name of a company appears in the text and use n-grams in some way to understand around what words relevant companies appear in comparison to non-relevant. Also, I'm interested, is it possible to automate the process data sample creation, or I need to manually read news and mark relevant and non-relevant companies? Also, I want to mention, that I'm creating this model for non-English texts (but it's one of the largest, so it has good NER, POS and other models).",
"date": "2019-09-19"
},
{
"vote": 5,
"title": "NER, transfer learning, and SpaCy",
"text": "Does anyone have experience training a SpaCy NER Tagger to tag new entities, not included in the baseline? For example, tagging technology skills like â€œjavaâ€, â€œms excelâ€ etc \n\n\nIm curious what volume of data is needed to leverage transfer learning and train SpaCy to tag this new entity (ie 5,000 observations, 50,000,  500,000 etc)\n\n\nIm aware that prodigy (I think itâ€™s called) helps you tag data quickly. Any experience with this would be noteworthy, too.",
"date": "2019-09-19"
},
{
"vote": 11,
"title": "Field extraction with NLTK",
"text": "Hi everyone,\n\n\nI'm currently working on a PDF scraper to extract certain numerical values from enormous PDF files filled with words. They're not uniform, so I can't write one general scraper, I'll have to find each value based on the words around it (or so I believe, please let me know if there's something I'm overlooking). I've started looking at NLTK to help me do this, but I'm not sure if it's exactly what I want. What methods would you suggest using for this task?\n\n\nThank you!\n\n\nEdit: For example, the PDFs I'm searching in contain specs for pipelines. I would want to pull information such as diameter, lifespan, install dates, materials used, etc.",
"date": "2019-09-18"
},
{
"vote": 1,
"title": "Generating 100% grammatically correct sentences.",
"text": "[removed]",
"date": "2019-09-18"
},
{
"vote": 0,
"title": "Why Business Use NLP? 5 prominent Use Cases",
"text": "[deleted]",
"date": "2019-09-18"
},
{
"vote": 1,
"title": "[RL+NLP] Why there are only a few papers about using RL to solve dependency parsing?",
"text": null,
"date": "2019-09-17"
},
{
"vote": 22,
"title": "A community suggestion (and challenge): have pity on r/NLP",
"text": "Obviously our NLP is the real NLP, but a lot of posts meant for r/languagetechnology have to first get redirected from r/nlp, which sucks for posters and readers in both groups. It's not my area, but has anyone here made a Reddit bot before? It would be great if we could automate the wrong-number process. No idea how hard that would be to write.",
"date": "2019-09-17"
},
{
"vote": 0,
"title": "Additional custom features in BERT fine-tuned downstream tasks",
"text": "Interested to know if anybody could provide pointers to how I would use BERT in addition to some custom additional features for NER:\n\n\nFor example:\n\n\n# TWO INPUTS \nè¿™æ˜¯ä¸€åªç‹—å’Œè¿™æ˜¯ä¸€åªçº¢çŒ«&lt;div&gt;This is a dog and that is a panda \n0 0 0 0 B-TERM 0 0 0 0 0 0 0&lt;div&gt;0 0 0 0 0 0 0 0 0  \n\n# OUTPUT \n0 0 0 0 B-TERM 0 0 0 0 0 0 0&lt;div&gt;0 0 0 B-TERM 0 0 0 0 0",
"date": "2019-09-17"
},
{
"vote": 13,
"title": "A PyTorch implementation of \"Capsule Graph Neural Network\" (ICLR 2019).",
"text": "&#x200B;\n\n\nhttps://preview.redd.it/ej5r6ju8o6n31.jpg?width=1337&format=pjpg&auto=webp&s=de7d076a9619601edc4b202f896c2ebdbe25b15c\n\n\nPyTorch: \nhttps://github.com/benedekrozemberczki/CapsGNN\n\n\nPaper: \nhttps://openreview.net/forum?id=Byl8BnRcYm\n\n\nAbstract:\n\n\nThe high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings. Inspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs. Our extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.",
"date": "2019-09-17"
},
{
"vote": 2,
"title": "Creating Text from data",
"text": "I'm interested in training a model to convert data (think a sporting event box score or financial statements) into a text summary. A training set of both the data with corresponding article would be provided and Ideally the box score would be passed as the input once a model is trained similar to how gpt2 take short user generated text as an input.  Can anyone point me in the direction of any research that has been done in this area.  I tried googling around but wasn't able to find anything related to this use case.",
"date": "2019-09-17"
},
{
"vote": 1,
"title": "Benefits of Using Machine Learning in Supply Chain",
"text": null,
"date": "2019-09-17"
},
{
"vote": 0,
"title": "Setting up Smart Homes with Artificial Intelligence",
"text": null,
"date": "2019-09-17"
},
{
"vote": 14,
"title": "Where do these weight matrices come from in the transformers architecture?",
"text": "I'm fairly new to NLP and was reading this article explaining how the transformers architecture works  \nhttp://jalammar.github.io/illustrated-transformer/\n . There are a few portions where it was explaining how to obtain attention scores by generating key,value,and query vectors based on multiplying an embedding vector by a matrix corresponding to the key, value, and query. Where are these matrices coming from and what do they represent? There seems to only be mentions of them coming from a training process but i dont know what this training process is.",
"date": "2019-09-15"
},
{
"vote": 2,
"title": "Interested in app for determining relatedness of various words",
"text": "Does anyone know of any word association apps/codebases? I'm looking for something that can compare different words and determine how related they are to each other (I'm not too concerned about how the app groups words - it could be based on word meaning, structure, etymology. Looking for any apps or links related to word association)",
"date": "2019-09-15"
},
{
"vote": 1,
"title": "Neural Attentive Bag-of-Entities Model for Text Classification",
"text": "[deleted]",
"date": "2019-09-14"
},
{
"vote": 2,
"title": "Are there topic modeling techniques outside of the statistical and probabilistic realm?",
"text": "All I've seen is LSA, LDA, tfidf and others for topic modeling. I'm just wondering if theres something I've missed that can pull topics from a passage.",
"date": "2019-09-12"
},
{
"vote": 1,
"title": "QAit AI Learns by Interacting With Its Environment",
"text": null,
"date": "2019-09-11"
},
{
"vote": 19,
"title": "What are cross-lingual word embeddings?",
"text": "So I've found this survey (\nhttp://ruder.io/cross-lingual-embeddings/\n) that sorts of explains them, but it is not quite what I'm looking for, since it doesn't explain them in detail.\n\n\nSearching for \"cross-lingual word embeddings\" or similar only results in articles, and I am looking either some chapter of a book or a blog explanation. Does anyone know of something like that?",
"date": "2019-09-11"
},
{
"vote": 1,
"title": "difflib - order of sentences in the text",
"text": "Hi,\n\n\nI am currently trying to compare two texts using difflib but I noticed that the order of the sentences matters. Please check following two different outputs based on the order of the sentences in the text:\n\n\nExample 1: [Hello world at the end of text 2]\n\n\ntext1 = \"\"\"Hello world too. We like python and.\"\"\"\n\n\ntext1_lines = sent_tokenize(text1)\n\n\ntext2 = \"\"\"We like python. I like to code python. Hello world. \"\"\"\n\n\ntext2_lines = sent_tokenize(text2)\n\n\ndiff = difflib.ndiff(text1_lines, text2_lines)\n\n\nprint('\\n'.join(diff))\n\n\nOutput:\n\n\n- Hello world too.\n\n\n- We like python and.\n\n\n?               ----\n\n\n&#x200B;\n\n\n+ We like python.\n\n\n+ I like to code python.\n\n\n+ Hello world.\n\n\n&#x200B;\n\n\nExample 2:\n\n\ntext1 = \"\"\"Hello world too. We like python and.\"\"\"\n\n\ntext1_lines = sent_tokenize(text1)\n\n\ntext2 = \"\"\"Hello world. We like python. I like to code python. \"\"\"\n\n\ntext2_lines = sent_tokenize(text2)\n\n\ndiff = difflib.ndiff(text1_lines, text2_lines)\n\n\nprint('\\n'.join(diff))\n\n\nOutput:\n\n\n- Hello world too.\n\n\n?            ----\n\n\n&#x200B;\n\n\n+ Hello world.\n\n\n- We like python and.\n\n\n?               ----\n\n\n&#x200B;\n\n\n+ We like python.\n\n\n+ I like to code python.\n\n\n&#x200B;\n\n\nThanks!",
"date": "2019-09-10"
},
{
"vote": 1,
"title": "Finding gendered Job Ads",
"text": "[deleted]",
"date": "2019-09-10"
},
{
"vote": 9,
"title": "Can \"deep\" language models (XLNet , BERT ..) be used for words semantic similarities?",
"text": "Usually word embeddings from GLOVE, fasttext and Word2vec can be used for finding synonyms to words.\n\n\nCan we use \"deep\" language models such as XLNet and BERT for finding synonyms for words based on their semantic similarities?",
"date": "2019-09-10"
},
{
"vote": 1,
"title": "5 ways AI is transforming Document Management Systems",
"text": null,
"date": "2019-09-10"
},
{
"vote": 3,
"title": "NLP in Edtech Conference 2019-2020",
"text": "Hi kind folks,\n\n\n&#x200B;\n\n\nI am a data scientist currently working in Bangalore, India. I've been working on a product in the edtech space and want to present the same as an industry paper at an edtech conference this year or the next. The project involves a lot of NLP work and hence I am looking for a conference that specializes in applying NLP to the edtech domain. Can you please provide any lead here? TIA",
"date": "2019-09-10"
},
{
"vote": 1,
"title": "R.I.P. Python 2: October 16, 2000 â€” January 1, 2020 | Survey indicates 84 percent Python developers had adopted Python 3",
"text": null,
"date": "2019-09-09"
},
{
"vote": 1,
"title": "Experimenting with Sequence to Sequence Architecture",
"text": "[deleted]",
"date": "2019-09-09"
},
{
"vote": 5,
"title": "5 Major open problems in NLP",
"text": "https://deeps.site/blog/2019/09/09/nlp-problems/\n\n\nHave \ncompiled\n 5 major problems/\nopportunities\n \nfor students, researchers and NLP enthusiasts\n to work on with \nopen pointers to resources\n.",
"date": "2019-09-08"
},
{
"vote": 7,
"title": "Semantics-aware BERT for Language Understanding",
"text": null,
"date": "2019-09-08"
},
{
"vote": 0,
"title": "BERT fine-tuning for question answering.",
"text": "I'm a student and I'm doing a project where I would like to use BERT for question answering. In particular I will work on natural question by Google. I was wondering what are the fine tuning algorithm with better performance on natural question or on SQuAD 2.0? I wasn't able to find the most recent paper on it.\nThank you :)",
"date": "2019-09-08"
},
{
"vote": 9,
"title": "[R] Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange",
"text": "[deleted]",
"date": "2019-09-07"
},
{
"vote": 0,
"title": "Looking for a measure (that perhaps doesn't exist)",
"text": "I have some vectors and I want to rank them through a measure that \"rewards\" vectors with higher overall sum values but also vectors with a higher of high values. For example, given a vector with lots of high values and lots of low values, I still want to \"reward\" it (I apologize for the terminology). Someone can suggest me a measure that I can get on Pandas? I thought a quantile like 0.75. Other ideas?",
"date": "2019-09-07"
},
{
"vote": 22,
"title": "Announcing Two New Natural Language Dialog Datasets (Google AI Blog)",
"text": null,
"date": "2019-09-07"
},
{
"vote": 12,
"title": "ACL 2019 Conference: ALL papers Word Cloud.",
"text": "https://deeps.site/blog/2019/09/06/conference-word-clouds/\n\n\nHere is a Rich word cloud containing the most recurring words occurring in the ACL 2019 anthology,\nthis might help aspiring students, researchers to have a quick visual glance as to what is happening in the NLP Community.",
"date": "2019-09-06"
},
{
"vote": 6,
"title": "02 History of Natural Language Processing",
"text": null,
"date": "2019-09-06"
},
{
"vote": 2,
"title": "Intro resources to knowledge bases?",
"text": "At work we need to build a (or find an off-the-shelf) knowledge base for identifying food and online shopping products. I was wondering if there were any \n\n\n\n\nopen source \n\n\npaid API \n\n\nintro to building a KB myself\n\n\n\n\nkind of solution that this community could recommend.\n\n\nThanks!",
"date": "2019-09-05"
},
{
"vote": 20,
"title": "Wondering if people here are interested in NLU and test automation",
"text": "I built a test automation framework that uses Natural Language Understanding which is still in the proof-of-concept stage. \n\n\nYou can find it in Github: \nhttps://github.com/roniemartinez/HumanFramework\n\n\nWondering if people here are interested to help build the the tool and contribute in an open source project.\n\n\nThanks!",
"date": "2019-09-04"
},
{
"vote": 1,
"title": "cheap tic for chatbot conference 2019",
"text": null,
"date": "2019-09-04"
},
{
"vote": 4,
"title": "Datasets of argumentative structures",
"text": "[deleted]",
"date": "2019-09-03"
},
{
"vote": 0,
"title": "Document anonymization platform based on named entity recognition technology",
"text": null,
"date": "2019-09-03"
},
{
"vote": 5,
"title": "Beginner looking for BERT tutorial",
"text": "Hello there, I am looking to learn more about NLPs in general and would like to test some of these open source algorithms. Thing is, I am having trouble getting some of them to work, specifically BERT. I would ideally like to test if I can have a NLP find \"definitions\" within a document, and perhaps match them to similarly defined terms in other documents. \n\n\n&#x200B;\n\n\nWould appreciate any help or suggestions!",
"date": "2019-09-03"
},
{
"vote": 1,
"title": "Emerging Applications of AI in Marketing",
"text": null,
"date": "2019-09-03"
},
{
"vote": 4,
"title": "What is difference between sentence similarity, paraphrase and textual entailment models?",
"text": "I am unable to understand the difference between models trained for these 3 tasks. Could someone explain this?\nThanks",
"date": "2019-09-03"
},
{
"vote": 1,
"title": "Most successful use-cases of Artificial Intelligence for Businesses - ParallelDots",
"text": null,
"date": "2019-09-02"
},
{
"vote": 25,
"title": "Tongue-in-cheek (NLP) Paper Summaries",
"text": null,
"date": "2019-09-01"
},
{
"vote": 27,
"title": "Best podcasts for NLP?",
"text": "I'm looking to find some great podcasts on the subject of NLPs and deep learning. Do you have any suggestions on the best ones?",
"date": "2019-09-01"
},
{
"vote": 10,
"title": "Random Walk based feature vector for improved Text Classification",
"text": "Read more at \nhttps://prakhartechviz.blogspot.com/2019/09/random-walk-term-weighting-for-text.html",
"date": "2019-09-01"
},
{
"vote": 27,
"title": "Created a compendium for the latest NLP reasearch that is going on from top conferences.",
"text": "https://github.com/soulbliss/NLP-conference-compendium\n\n\nThe compendium is created with an objective to organise all the latest research carried in the direction of NLP, so that interested researchers, students can head directly to papers and other relevant resources.",
"date": "2019-08-31"
},
{
"vote": 1,
"title": "Aargh! The â€œBullshitspeakâ€ and Bullshit Generators â€” A Headbangerâ€™s Journey",
"text": null,
"date": "2019-08-31"
},
{
"vote": 3,
"title": "What do you use to detect repeated patterns in text",
"text": "I am trying to analyze system logs. It's a messy unstructured text. I would like to detect repeated patterns.\n\n\nAs an example:\n\n\n>Feb 24 06:48:03 circle vpopmail[12039]: vchkpw-pop3: **password fail [EMAIL PROTECTED]:**67.109.191.46 Feb 24 06:49:03 circle vpopmail[12043]: vchkpw-pop3: **password fail [EMAIL PROTECTED]:**67.109.191.46 Feb 24 06:50:03 circle vpopmail[12099]: vchkpw-pop3: **password fail [EMAIL PROTECTED]:**67.109.191.46 Feb 24 08:13:31 circle vpopmail[13042]: vchkpw-pop3: **password fail [EMAIL PROTECTED]:**70.104.21.208 Feb 24 08:13:32 circle vpopmail[13046]: vchkpw-pop3: **password fail [EMAIL PROTECTED]:**70.104.21.208\n\n\n&#x200B;\n\n\nThe pattern can be\n\n\npattern = \".password fail [EMAIL PROTECTED]:($ip)\"\n\n\n&#x200B;\n\n\nI didn't know that in advance. I just discovered by eyeballing the text. You might say, you can tokenize the words and count frequencies. In my case,  it's hard to tokenize and decide on windows of substring. the patterns might vary.\n\n\nIs there a name for such techniques. I couldn't find ML techniques appleid to this problem",
"date": "2019-08-31"
},
{
"vote": 1,
"title": "What do you use to detect repeated pattern detection in NLP",
"text": "[deleted]",
"date": "2019-08-31"
},
{
"vote": 4,
"title": "KBSET -- Knowledge-Based Support for Scholarly Editing and Text Processing",
"text": null,
"date": "2019-08-31"
},
{
"vote": 26,
"title": "GitHub - microsoft/nlp: Natural Language Processing Best Practices &amp; Examples",
"text": null,
"date": "2019-08-30"
},
{
"vote": 16,
"title": "Natural Language Processing: the age of Transformers",
"text": null,
"date": "2019-08-30"
},
{
"vote": 0,
"title": "[Internship: Nov-Dec 2019] speech signal analysis project: feature extraction from 0.5M min of call centre speech",
"text": null,
"date": "2019-08-29"
},
{
"vote": 38,
"title": "DistilBERT: A smaller, faster, cheaper, lighter BERT trained with distillation!",
"text": "HuggingFace released their first NLP transformer model \"DistilBERT\", which is similar to the BERT architecture: only 66 million parameters (instead of 110 million) while keeping 95% of the performance on GLUE.\n\n\nThey released a \nblogpost detailing the procedure with a hands-on\n.\n\n\nIt is also available on their repository \npytorch-transformers\n alongside 7 other transformer models.",
"date": "2019-08-28"
},
{
"vote": 6,
"title": "Picking a major",
"text": "I have always been interested in linguistics, but I recently have become really intrigued by cs and I think I'd like to combine the two for my career! I found out about computational linguistics and I think that's the route I'd like to go. I have been trying to decide between a double major or a major minor and was hoping someone here could give me some advice. I am also considering a masters program after I get my bachelor's. I just started school 2 terms ago so I have only taken basic prereqs.  I was hoping someone her could make a recommendation for me. Thanks for the help!",
"date": "2019-08-26"
},
{
"vote": 9,
"title": "Language Modeling on Low-Resource Domains - Discussion",
"text": "Hi friends! I am starting to explore a new project and would like to get your opinion on some ideas I brainstormed.\n\n\nI wish to perform domain-specific Language Modeling on low-resource languages. These languages of interest typically have less unsupervised data (for specific domains atleast) and also lack availability of well-structured grammar (if I had the language's grammar and also examples of lexical units corresponding to grammatical units, I would be able to able to synthesize sentences from this language). In such a case, is it advisable to train a Machine Translation system which can be later used to generate sentences in the low-resource language or train a LM directly? There are advantages and disadvantages with either approach.\n\n\nIn the case of MT approach, we may have parallel-corpora for many languages though they tend to be small. Not to forget unsupervised MT systems which exploit neighborhood properties in the semantic spaces. One advantage of using noisy translation systems is that they provide some implicit regularization benefits. But the problem with this approach is that MT itself is an ill-solved domain and adds more complexity to our problem.\n\n\nFor the second approach, we can try to learn a LM directly on low-resource languages. Though this path has been explored previously, the dominant approach seems to be using multilingual word embeddings to get a good initializations for neural LMs. Even this approach can help us to some extent with domain extrapolation (a LM trained on huge amounts of open-domain data can be fine-tuned using a few domain-specific examples). But this approach doesn't work for a system like conventional ASR pipeline, where statistical n-gram based LMs dominate performances. Also, n-gram based LMs are appealing due to their O(1) retrieval time for real-time cloud-based systems. Is there a multi-lingual embeddings equivalent for n-gram based LMs? Any other good approach for learning n-gram LMs on low-resource domains?\n\n\nIs there any idea you think would be particularly interesting to explore and learn? Pls let me know. I will continuously update this post with resources I come across related to this problem. \n\n\nThanks for your time!\n\n\n&#x200B;\n\n\nReferences:\nSurvey on Cross-Lingual Word Embedding models\n\n\nNeural Network LMs for Low-Resource Languages\n\n\nUnsupervised Machine Translation",
"date": "2019-08-25"
},
{
"vote": 7,
"title": "How to label text for sentiment analysis?",
"text": "Are there any guidelines, rules, and tips on how to label text for sentiment analysis?\n\n\nThank you.",
"date": "2019-08-25"
},
{
"vote": 1,
"title": "Errudite: Scalable, Reproducible, and Testable Error Analysis",
"text": null,
"date": "2019-08-25"
},
{
"vote": 17,
"title": "[R] FacebookAI releases Adaptive attention span and All-attention layer to reduce decrease computation time / memory footprint",
"text": null,
"date": "2019-08-24"
},
{
"vote": 20,
"title": "Heads up: NLTK security vulnerability",
"text": null,
"date": "2019-08-24"
},
{
"vote": 4,
"title": "Sentence Classifications with Neural Networks",
"text": null,
"date": "2019-08-24"
},
{
"vote": 2,
"title": "Can Machine Learning Really Estimate How Well You Know a Foreign Language?",
"text": null,
"date": "2019-08-23"
},
{
"vote": 29,
"title": "Introducing the New Snorkel",
"text": null,
"date": "2019-08-23"
},
{
"vote": 3,
"title": "Calculating lexicon scores",
"text": "I have created a lexicon based on a  list of words and Wordnet related words. Now, I want to score entries in  my corpus based on the lexicon.\n\n\nWhat  are the best ways to do so? I suppose I would need to stem the words in  the lexicon and the corpus in order to calculate the score. Are there  other ways of doing so (e.g., using diffleb for small text  differentials)?",
"date": "2019-08-22"
},
{
"vote": 4,
"title": "Creating every possible logical phrase for words with certain length.",
"text": "How can I create list of every possible logical sentece of some phrase like XX XX XXX XXXX XXX XXXXXXXX.\nI want to use list of 1k most common english words for this and without language processing and creating only logical sentences I'm getting more than 11k possibilities from 8-letter and 5-leter words staying next to each other(63*185 because this is amount of words with certain letters length). I need to cut down huge part of possibilities so I will have less data.",
"date": "2019-08-22"
},
{
"vote": 3,
"title": "Theoretically, what would be the best way to go about doing GloVe with triple co-occurrences?",
"text": "My best guess would be to take a randomly average two of the 3 embeddings, and then do a dot product against the 3rd embedding.",
"date": "2019-08-22"
},
{
"vote": 4,
"title": "Getting list of characters for given language",
"text": "[deleted]",
"date": "2019-08-21"
},
{
"vote": 2,
"title": "Creating NLP model/pipeline",
"text": "After reading a couple of research papers in the Q & A domain, I am thinking to create my own pipeline but I am not sure how to choose the correct components (activation function, RNN or LSTM or GRU or CNN or attention or ) and how to place them. Any suggestion, thanks.",
"date": "2019-08-21"
},
{
"vote": 4,
"title": "Help us record data for NLP research on Radio Data for Agriculture",
"text": null,
"date": "2019-08-21"
},
{
"vote": 1,
"title": "Analyse survey responses using machine learning - ParallelDots",
"text": null,
"date": "2019-08-21"
},
{
"vote": 1,
"title": "Guidance needed for Fastai BERT Multi input - Multi output",
"text": "[deleted]",
"date": "2019-08-21"
},
{
"vote": 2,
"title": "Understanding word2vec failings",
"text": "In the GloVe paper (\nhttps://nlp.stanford.edu/pubs/glove.pdf\n), it is mentioned that \"Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the en- tire corpus, which fails to take advantage of the vast amount of repetition in the data.\" The matrix factorisation methods refer to the likes of LSA, while the shallow window-based methods refer to Skip-gram, CBOW etc.\nWhen the paper says \"fail to take advantage of vast amount of repetitions\" and \"suffer from disadvantage that they do not operate directly on the co-occurrence statistics of the corpus\", what exactly are shallow window-based methods missing out on? Is it reducing complexity, or learning new information using co-occurrence statistics? It's not clear to me",
"date": "2019-08-20"
},
{
"vote": 6,
"title": "Word Sense Disambiguation",
"text": "How would one do a sense disambiguation from a set of contexts? Say, there are several words, each of which can have an arbitrary number of senses, a few (1-5) contexts for each, as well as labels for ground truth.\n\n\n&#x200B;\n\n\nMy first guess would be to somehow obtain embeddings for words and then use clustering (probably hierarchical). However, I am not sure how I could obtain the vectors. Maybe one could average the (pretrained) vectors of the context word? Or run a skip-gram with the target word missing to tune the pretrained vectors for the word? Or something else?\n\n\nMaybe you could provide useful articles/papers/etc. on the topic?",
"date": "2019-08-19"
},
{
"vote": 2,
"title": "Classifying text active or passive advice to get started.",
"text": "[deleted]",
"date": "2019-08-19"
},
{
"vote": 24,
"title": "Are there any embedding models worth checking out besides word2vec, glove, and fasttext?",
"text": "It seems all other embedding models are just variations on the first three, excluding the contextualized RNN/Transformer developed embeddings. \n\n\nFor example, Facebook's StarSpace has a lot of praise, but when I looked at the paper, maybe I'm missing something but it's basically skip-gram but testing another loss function and cos similarity at train time.",
"date": "2019-08-19"
},
{
"vote": 2,
"title": "How to use NLP to identify work/job activity from emails?",
"text": "[deleted]",
"date": "2019-08-19"
},
{
"vote": 0,
"title": "Hi /r/LanguageTechnology! I am trying to brainstorm ideas for roles in language tech that involve 3D design/3D printing, does anyone have ideas?",
"text": "[deleted]",
"date": "2019-08-19"
},
{
"vote": 3,
"title": "Poetry Generation Update.",
"text": "I managed using only statistical methods (ngrams and HMM) to create a generative model that writes poetry with proper grammar (in my language, portuguese) However, of course there is still a semantic problem. Although one can argue poetry is subjective,  most of the verses seem disconnected and i would like to guarantee some kind of meaning in the poetry being generated. Is there any algorithm that can help me link the contents of all the verses or  tell the model if it is generating good or bad poetry ? I have though of increasing/decreasing the weights on the ngram moodel , but that seems a bad idea prone to overfitting. Does anyone know any algorithm/theoric concepts i should look into ? (Note: I'd like to program the algorithms myself instead of using some deep learning framework). Thanks",
"date": "2019-08-18"
},
{
"vote": 3,
"title": "BERT for sentence classification with very bad loss",
"text": "[deleted]",
"date": "2019-08-18"
},
{
"vote": 14,
"title": "[P][D] Pytorch Sparse training library. Sparse training = fraction of all parameters updated each step. Non-used parameters saved to disk -&gt; reduce GPU Memory Usage + Increase Training Speed. If you are working with such an architecture, let us know and we'll optimize and include it in our release.",
"text": "Hello,\n\n\nWe are creating a sparse training library for Pytorch. Sparse training is when only a fraction of the total parameters go through a forwards pass / backwards pass / update during each step.\n\n\nHaving \nall\n parameters takes up a lot of GPU memory, and in some cases may limit the total number of parameters your system can hold. By having the parameters stored on disk when not in use, that would significantly reduce the GPU memory used at any given instance, allowing you to use many more parameters.\n\n\nA concern is that generally disk are not low enough latency to make this work. But we were able to figure out a pipeline to make it work. Not only that, but through a few Pytorch tricks we inadvertently discovered along the way, we think our set up may be (very slightly) faster, though we'll need to do a bunch of test to absolutely confirm.\n\n\nAt the moment we need to code each adapt each architecture individually. If you or anyone you know have sparse training architecture you have in mind, point us to the paper or code and we'll optimize and include it.\n\n\nSo far we've only been able to find recommender systems that make use of such architectures, such as word2vec and GloVe. If you know of any other architectures, please point them out.",
"date": "2019-08-17"
},
{
"vote": 21,
"title": "Learning about Natural Language Processing (NLP)",
"text": "Hello there,\n\n\nI'm trying to get into Natural Language Processing but I am still somewhat confused about the basics and the plethora of different technologies available for NLP. Has anybody got a good resource about what technologies are available, their application and the pros and cons? Anything from a textbook to a website would be much appreciated.\n\n\nDoes it make more sense to group the technoligies per algorithm they deploy or per function they serve? Or is there a better way to group and remember them?\n\n\nThank you.",
"date": "2019-08-17"
},
{
"vote": 2,
"title": "I used a dqn to beat the hardest flappy bird level",
"text": "[deleted]",
"date": "2019-08-17"
},
{
"vote": 8,
"title": "Has anyone used tf-idf scores in addition to word embeddings to improve the prediction accuracy/precision/recall of the network (LSTM's) for text classification? If so, how did you do it?",
"text": "Anyone tried to use tf-idf scores in combination with word embeddings (fast-text, glove etc.) for text classification? If so how did you? hstack? Multiply tf-idf score with the n-dimentional word embedding?",
"date": "2019-08-17"
},
{
"vote": 0,
"title": "NLP/Text Mining/ Deep Learning Trending Research topics",
"text": "What are the current trending research topics/problems/questions in the areas of NLP/Text Mining/Deep learning which are also researched at the top AI industries?\n\n\nThanks!",
"date": "2019-08-16"
},
{
"vote": 5,
"title": "Facebook, Georgia Tech &amp; OSU â€˜ViLBERTâ€™ Achieves SOTA on Vision-and-Language Tasks",
"text": null,
"date": "2019-08-15"
},
{
"vote": 3,
"title": "How to treat proportions of GPUs are multiple devices?",
"text": "Hello, I am using pytorch data parallel to distribute my model over multiple GPUs. A key requirement of my model is that I need to run it in smaller batches. The model itself is kinda not deep, so the per GPU utilizations is usually around 2-3%. Is there a way I can take 5% of each GPU, so that I can create 20 \"virtual\" GPU devices, so that instead of running the small batch and utilizing only 2-3%, I can run 20 batches and utilize upto 100%.",
"date": "2019-08-14"
},
{
"vote": 46,
"title": "Text Corpus: 40 Million Pages of U.S. Case Law from Harvard Law School Library",
"text": "Sharing a text corpus from the Harvard Library Innovation Lab!\n\n\nIn October we launched the Caselaw Access Project (\nhttps://case.law/\n) API and Bulk Data Service, making 40 million pages of case law freely available online in machine readable format, digitized from the collections of the Harvard Law School Library.\n\n\nWe want to see what themes people can pull out of the data! Here are some ways to get started (\ncase.law/tools/\n), including:\n\n\n\n\nCAP API: \nhttps://case.law/api/\n\n\nBulk Data Service: \nhttps://case.law/bulk/\n\n\nHistorical Trends (Visualize how words are used in U.S. case law over time!): \nhttps://case.law/trends/\n\n\n\n\nWe're excited to see what people find in this data set about how law and language develop over time. Share what you find with us at: \nhttps://case.law/contact/\n\n\nThanks!\n\n\nhttps://preview.redd.it/og5icja4o8g31.png?width=917&format=png&auto=webp&s=ddd74eb586a4ff7eaaa2fc7e44726352e09fe8a5",
"date": "2019-08-13"
},
{
"vote": 8,
"title": "Is it possible to learn machine learning for translation within a couple months to apply for graduate school?",
"text": "Hi, all. So, I graduated with a B.A. in language degree, and want to get into M.Sc. graduate program in Computer Science with research focus on machine translation.\n\n\nI have around 4-5 months until graduate school opens for application with 0 knowledge concerning machine learning.\n\n\nIs it possible for me to learn about machine learning just enough that I can write decent research plan to apply for graduate school? (Also, I can only learn part-time since I will be working full-time as an in-house translator).\n\n\nMy current plan is to get these two online certificates in hope that they will boost my chance.\n\n\n\n\n'IBM Data Science Professional Certificate'\n from Coursera\n\n\n'Harvard Professional Certificate in Data Science'\n from EdX\n\n\n\n\nI choose these two because the first one focuses on Python and the second one focuses on R. Or, should I just focus on 1 language?\n\n\nI don't even know that I will get accepted into M.Sc. program with a B.A. degree tbh. Thoughts?\n\n\nEdit: Thanks for the replies so far. I would assume that a course like this would be useful as well: \nTensorFlow in Practice Specialization by deeplearning.ai\n ?",
"date": "2019-08-13"
},
{
"vote": 7,
"title": "Training Coreference resolution for world knowledge",
"text": "I have tried with NeuralCoref, End to End Coresolution, AllenNLP, CoreNLP to solve for coreference resolution. All of these do not perform well (and mainly does statistical guesswork) when faced with sentences containing pronouns which require world knowledge to resolve the pronoun. \n\n\n&#x200B;\n\n\nEg: The ball crashed through the table since it was made of Styrofoam. What does the it refer to? table or ball.\n\n\nAnswer is table, but most models misclassify since it does not know the physical propoerties of styrofoam etc,\n\n\nHow does one go about training the coreference models to resolve these kind of situations?",
"date": "2019-08-12"
},
{
"vote": 14,
"title": "Looking for a particular paper against English-as-default",
"text": "Hey,\n   I recall reading a nice, thorough position paper which critiqued the (quite common) implicit assumption in our field of English as the \"default language\", and the issue that tests showing method A > method B on English don't necessarily generalize because English is in many ways (enumerated in that paper) a 'weird' language with properties that differ from most other languages worldwide.\n\n\nBut I can't find it now anymore, I don't recall who wrote it nor the exact title ... Perhaps someone here has heard of it and can remind me?",
"date": "2019-08-12"
},
{
"vote": 4,
"title": "Please could you complete my short language comprehension experiment?",
"text": "[deleted]",
"date": "2019-08-11"
},
{
"vote": 19,
"title": "AI Analyzes Chess Commentary to Learn to Play Chess",
"text": null,
"date": "2019-08-09"
},
{
"vote": 0,
"title": "OCR in 2019 - From Character Recognition to Information Extraction",
"text": "Optical Character Recognition has been the driving force behind text based automation and information digitization. This article covers where current OCR APIs fall short, how to improve them and what that can mean for developers, product managers and company CEOs.\n\n\nCheck out - \nhttps://nanonets.com/blog/ocr-apis-to-extract-text-from-images/\n\n\n&#x200B;\n\n\nhttps://i.redd.it/k75lg9ptref31.gif",
"date": "2019-08-09"
},
{
"vote": 11,
"title": "Active learning and model explainability for document classification",
"text": null,
"date": "2019-08-09"
},
{
"vote": 1,
"title": "Java library to score text using various metrics",
"text": "My goal is to find a way, with Java, to assist writing short/medium length texts, so I'm looking for a  library (made with Java) that given some text can:\n\n\n\n\nproduce metrics: like readability, sentence analysis, spot grammar errors \n\n\nsupport most of languages: not only english, but also french, spanish, german, italian\n\n\nnot an online service, but a library\n\n\nopen source\n\n\n\n\nDisclaimer: I know it's an huge topic ! What I'm looking for is a simple small library to play with it  before move to bigger stuff, so I can familirize on this topic starting  with small examples",
"date": "2019-08-08"
},
{
"vote": 5,
"title": "NLP Features Categorization",
"text": "I am doing some reading on papers that approaches the text classification problem with classic NLP techniques. I usually get confused when the author categorizes the features used during the feature engineering step by using words like semantic features, syntactic features, shallow (??) features, content features(???). Dont get me wrong, I do understand the meaning of these words, but sometimes it seems to me that this categorization is kind of random.\n\n\nFor example:\n\n\n- A bag of words / TF approach would fall into one of these categories?\n\n\n- Count of words, words ratio, count of characters?\n\n\n- Presence of uppercase words, count of uppercase letters?\n\n\n- Punctuation count? Any categories here?\n\n\nSumming up, the final doubt would be what are the major categories for NLP features?",
"date": "2019-08-08"
},
{
"vote": 1,
"title": "Where to get SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT) Dataset?",
"text": "[deleted]",
"date": "2019-08-07"
},
{
"vote": 13,
"title": "Telegram group for NLP",
"text": "[deleted]",
"date": "2019-08-07"
},
{
"vote": 7,
"title": "[P] Open source an NLP/speech library DELTA",
"text": null,
"date": "2019-08-06"
},
{
"vote": 4,
"title": "BERT variable input length",
"text": "[deleted]",
"date": "2019-08-06"
},
{
"vote": 2,
"title": "Looking for suggestions on structure to build a rules based chatbot / personal assistant",
"text": "I am looking to build a chat bot that can be called to execute commands like get the scrape information off the web like weather or csv, do some simple math, query a database and run some functions. Not really looking for anything with intense deep learning but more that handles language patterns.\n\n\nI know how to write the functions but not really sure how to structure the commands and how the bot interacts with them. Most bots I have seen online are used for conversation. Are there any examples of github projects or libraries to build something like this in python?",
"date": "2019-08-06"
},
{
"vote": 11,
"title": "What parts of BERT, XLNET, Transformers etc. could I \"take\" in order to reinforce my current Deep Learning architectures and make them more robust?",
"text": "Hello,\n\n\nI have implemented some models in pytorch for text classification.  I want to get influenced by the recent publications, like the ones that I mentioned in the title, and try to create something hybrid.  More specifically, I'd like to keep most or some of the structure of my current architectures but add some pieces that come from the more recent stuff.\n\n\nRegarding the embeddings, I know that the current SOTA models do some crazy stuff in order to produce their embeddings.  However, I am not on an advanced level or confident enough to implement something like this.  So i want to avoid that part.\n\n\nTo conclude, except for the embeddings, what other parts can I \"borrow\" from the SOTA models to implement them, in order to examine how my models' performance is affected?\n\n\nThank you very much!",
"date": "2019-08-06"
},
{
"vote": 48,
"title": "Trends in Natural Language Processing: ACL 2019 In Review",
"text": null,
"date": "2019-08-05"
},
{
"vote": 1,
"title": "Moving on faster with NLP",
"text": "[removed]",
"date": "2019-08-05"
},
{
"vote": 1,
"title": "Improving syntactic structure of ngrams results.",
"text": "What steps should I take to improve the syntatical/grammatical correctness of my short automatically generated poems ? At first, I found the results better than my expectations but now I feel like it is time to improve and the way to do that is pick on the generated text and correct it. However I don't know what grammatical problems I should tackle first. OF course, there is some gender/person mismatch, but that seems like the smallest of the problems, the big one seems to resolve the verses that have like 3,4 articles following each other.  I would like some guidance, namely names about algorithms or techniques. Thanks",
"date": "2019-08-05"
},
{
"vote": 11,
"title": "In word embedding models, why is the softmax matrix initialized as zeros, where as the input embedding matrix is initialized using a normal distribution?",
"text": "In every word2vec implementation I've seen, the softmax embeddings are initialized as zero, but the target embeddings are initialized with a random distribution. Anyone know why this is?",
"date": "2019-08-04"
},
{
"vote": 1,
"title": "Simple document and query processing for the browser",
"text": null,
"date": "2019-08-04"
},
{
"vote": 4,
"title": "Story Completion Advances",
"text": null,
"date": "2019-08-04"
},
{
"vote": 41,
"title": "[P] (Facebook AI) Advances in Conversational AI: \"We've made scientific advancements in 5 areas that open-domain chatbots fail in today: specificity, consistency, empathy, knowledgeability, and multimodal understanding\"",
"text": null,
"date": "2019-08-03"
},
{
"vote": 27,
"title": "ACL 2019 | Best Papers Announced",
"text": null,
"date": "2019-08-01"
},
{
"vote": 11,
"title": "Thoughts about spacy IRL?",
"text": "I was wondering what other spacy users here thought about spacy IRL? \nhttps://www.youtube.com/watch?v=hNPwRPg9BrQ&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc\n \n\n\nI have only watched two talks so far:  \n\n\nSpanish ruled based lemmatiser \nhttps://www.youtube.com/watch?v=88zcQODyuko&t=1s\nIt was interesting to see the performance of a rule based matcher compared to a neural lemmatiser. I have not used a neural lemmatiser before but I was thinking that for non English languages lemmatisation may be more important.  \n\n\nQuartz's NLP pipeline \nhttps://www.youtube.com/watch?v=azrVX8JksMU&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc&index=11\nThe pipeline described seemed to be pretty standard would have been interesting to hear a bit more about the use cases around the text graph. It was also interesting to hear how they used the Harvard Inquirer corpus to identify active and passive voice to keep journalists in line with the style guide.  \n\n\nI will update this post when I have watched more of the talks.",
"date": "2019-08-01"
},
{
"vote": 5,
"title": "Suitable models for Information Retrieval problem",
"text": "Hi all, \n\n\nSo I am having a challenge where there are a list of queries and a list of answers (as paragraphs), I need to use a model to classify an answer as being in top 5 most relevant for the given question. \n\n\nPlease suggest some models that are suitable for this kind of problem.\nThanks in advance!",
"date": "2019-08-01"
},
{
"vote": 5,
"title": "[D] Is there a framework or library that you wish existed? Looking for ideas for a 2 day machine learning hackathon.",
"text": null,
"date": "2019-07-31"
},
{
"vote": 4,
"title": "Text equivalent of image flipping/scaling/rotation?",
"text": "In image classification, a model can be more robust if variants of the training samples are added by flipping, scaling, and/or rotating. It seems we could do the same for text, essentially rewriting a sample, however I haven't seen much about this approach. Any papers/articles that address this kind of technique? I could imagine it's non-trivial to do so, possibly ineffective, but just looking for discussions on the topic.",
"date": "2019-07-31"
},
{
"vote": 0,
"title": "(Self promotion) would appreciate if you voted for my (@mat_mto) NLP meme here",
"text": null,
"date": "2019-07-30"
},
{
"vote": 27,
"title": "Baiduâ€™s ERNIE 2.0 Beats BERT and XLNet on NLP Benchmarks",
"text": null,
"date": "2019-07-30"
},
{
"vote": 0,
"title": "What is the SOTA machine translation method for training data of 700k word?",
"text": "Hello, I am going to do a machine translation project.\n\n\nI have 700k word of bilingual dataset.\n\n\nWhat is the SOTA machine translation method for training data of 700k word?\n\n\nI want to start from it first.",
"date": "2019-07-29"
},
{
"vote": 4,
"title": "GLTR aka Giant Language model Test Room",
"text": null,
"date": "2019-07-29"
},
{
"vote": 3,
"title": "Generating Adversarial Examples for Semantic Textual Similarity",
"text": "[deleted]",
"date": "2019-07-29"
},
{
"vote": 1,
"title": "Dataset Required similar to SQuAD?",
"text": "Hi\n\n\nI am looking for a Questioning and Answering dataset which  can be equivalent to SQuAD or a dataset which can be converted to SQuAD format. Do you have any suggestions or ideas?",
"date": "2019-07-27"
},
{
"vote": 18,
"title": "ACL2019 paper code \"Simple and Effective Text Matching with Richer Alignment Features\"",
"text": null,
"date": "2019-07-27"
},
{
"vote": 3,
"title": "Testing spacy NER model on coNLL 2003 corpus",
"text": "Really dumb question, but I am trying to test spacy's pretrained NER model on coNLL's 2003 dataset. I have the annotated test set, but I don't have the unannotated corpus to the run on. I tried to get around this by creating a script to grab the text in the test set and then store it in a list and then convert that to a string with the text tokens separated by spaces and then pass that into spacy's ner, but this solution isn't perfect as it introduced new characters(I am still a newbie in programming lol) and it seems to confuse Spacy's tokenizer. \n\n\n&#x200B;\n\n\nI have noticed that people are able to to test on the conll 2003 dataset without the unannotated set; is there a different way I could approach this?",
"date": "2019-07-27"
},
{
"vote": 1,
"title": "Short-Term Memory: Maintaining Conversation Context",
"text": null,
"date": "2019-07-26"
},
{
"vote": 10,
"title": "Random idea: Is there a way to partition the word-vectors distribution space?",
"text": null,
"date": "2019-07-25"
},
{
"vote": 3,
"title": "Word Sense Disambiguation Function that works with single word for context",
"text": "I am looking for word sense disambiguation functions that can work with a single word. For example, the website \nContext Function\n provides an api that can predict the context of an adjective when combined with a noun (actually it compares two nouns and an adjective and determines if the context is the same or different).\n\n\nI am looking for more functions that can differentiate context, based on a single word. For example, suppose I want to disambiguate the adjective â€œhotâ€ in each of the following sentences:\n\n\n\n\nâ€œHot womanâ€\n\n\nâ€œHot stoveâ€\n\n\nâ€œHot sauceâ€\n\n\n\n\nIn each sentence, it is clear what meaning of â€œhotâ€ is used, even though â€œhotâ€ is paired only with a single noun.\n\n\nI am looking for functions that already exist, and/or information about methods to accomplish this task.",
"date": "2019-07-24"
},
{
"vote": 2,
"title": "Information extraction from text, and why BERT?",
"text": "I'm trying to do this task of looking at a lengthy article and if from the article there is a match of keyword (lets says terms and conditions and if it matches \"fraud\"), somehow to extract the entire line which relates to it. I have been thinking of creating a bag of words from the text (for each section, 1 bag) and if the term matches one of those bags, go to them and extract the phrase. \n\n\nIs it the most feasible way to do it? I think I'd be able to use spacy to extract the sentence. Searching this sub and internet has mentions of SGRank and BERT. But how would embeddings be used for something like this? I'd appreciate if you anyone could give a resource to look at for this, thank you",
"date": "2019-07-24"
},
{
"vote": 27,
"title": "spaCy : Industrial Strength NLP and itâ€™s online interactive course",
"text": null,
"date": "2019-07-24"
},
{
"vote": 2,
"title": "Using Deep Learning to Classify a Reddit User by their Myers-Briggs (MBTI) Personality Type",
"text": null,
"date": "2019-07-24"
},
{
"vote": 1,
"title": "[D] Volunteer opportunities in Machine Learning ? Or, other ways to get professional-ish experience in ML learning?",
"text": "I am semi-self taught in machine learning (my degree covered most of the math) and have done a few projects with ML. I think the next step to prove myself to employers would be to work in an enviroment which is a little more professional, and more likely to emulate the ML enviroment at a company.\n\n\nSince I am not in school anymore, I do not qualify for most internships.\n\n\nAnother thought would be to find some very early stage startups, and offer to work for really cheap, I would be willing to work for minimum wage for a while. I am in the Bay Area, and there seem to be a ton of startups, some very early stage.",
"date": "2019-07-23"
},
{
"vote": 10,
"title": "GLUE Is Not All You Need: Discourse Based Evaluation of Language Understanding",
"text": null,
"date": "2019-07-23"
},
{
"vote": 1,
"title": "GLUE Is Not All You Need: Discourse Based Evaluation of Language Understanding",
"text": "[deleted]",
"date": "2019-07-23"
},
{
"vote": 3,
"title": "Find associate of multi-word expressions",
"text": "I'm working on a model capable to predict fillers of a sentence starting from the other words of the sentence using association measures (for example, the best filler of the verb \"sip\" in the object position is something like \"wine\" or \"cognac\").\n\n\nNow i'm looking for the best fillers for multiword sentences (for example, the best filler of the expression \"leave the stage\" in the position of subject must be something like \"actor\").\n\n\nCan anyone suggest me a way to extract multiword expression from a parsed corpus (ex. leave-stage=obj) and check for the most frequent subjects?",
"date": "2019-07-22"
},
{
"vote": 15,
"title": "[P] Scientific summarization datasets w/accompanying (Beginner Friendly) Colab notebooks to train them with Pointer-Generators, Transformers or Bert. Sources are paper sections (Background, Methods, Results, Conclusions etc.), summaries are corresponding sections in abstract. ~11 Million data points",
"text": null,
"date": "2019-07-21"
},
{
"vote": 2,
"title": "Vectorizer giving different shape of an element before and after saving it",
"text": "&#x200B;\n\n\nI am using tf_idf vectorizer on dummy data below:  \n\n\n    file_list=[&quot;Google headquarters is in California&quot;, &quot;Steve Jobs was a great  man&quot;, &quot;Steve \nJobs has done great technology innovations&quot;]\n\n\n\nIf I print the shape of an element after vectorization, I get the output (14,)\n\n\nWhen I save the vectorizer and later transform using that vectorizer then the shape is (1,14).  May I know if anything is done wrong?\n\n\nIf we have large data similar to this and if I pass the data after vectorization to a machine learning model, then will the model predicts and gives the same output in both the cases?\n\n\n    from sklearn.feature_extraction.text import TfidfVectorizer    \n    import pickle \n    transformer = TfidfVectorizer().fit(file_list)\n    file_data = transformer.transform(file_list) \n    with open(&#039;transformer.pickle&#039;, &#039;wb&#039;) as handle:\n        pickle.dump(transformer, handle, protocol=pickle.HIGHEST_PROTOCOL)  \n    file_data = file_data.toarray() \n    with open(&#039;transformer.pickle&#039;, &#039;rb&#039;) as handle: \n        transformer = pickle.load(handle)    \n    transformed_file = transformer.transform([file_list[0]]).toarray(\n\n\n\n Running the two below two lines gives different outputs. May I know why is it so?\n\n\n    print(file_data[0].shape)\n    output: (14,)\n\n    print(transformed_file.shape)\n    output: (1,14)",
"date": "2019-07-21"
},
{
"vote": 5,
"title": "Any good database of adverbs in their according categories (i.e. degree, frequency, stance)",
"text": "^ Thanks (perhaps this would be more specifically for adverbs that are not morphologically derived from English adjectives like surprising - surprisingly, etc. - more specifically like \"now\", \"never\", \"always\", \"hardly\", \"mostly\", \"totally\", etc.)",
"date": "2019-07-21"
},
{
"vote": 2,
"title": "NLP in LISP",
"text": "I got an NLP textbook for LISP a while ago through a university library getting rid of books it didnâ€™t want to keep.\n\n\nIs learning through LISP a good idea?\nI am quite familiar with linguistics, coding a bit less so, but I would like to work on that. Any input on how relevant this book is for present-day learning and practice of NLP is more than welcome.",
"date": "2019-07-21"
},
{
"vote": 51,
"title": "BERT's success in some benchmarks tests may be simply due to the exploitation of spurious statistical cues in the dataset. Without them it is no better then random.",
"text": null,
"date": "2019-07-21"
},
{
"vote": 1,
"title": "Maximizing Cosine similarity between 2 Ecommerce products.",
"text": "Hi All,\n\n\nI am currently working on a content based filtering on an E-commerce dataset which have features like product title, description and tags like 'men, women, accessories' etc. I want to maximise the cosine distance between 2 products if they are of opposite gender even though their title and description might me somewhat similar. I have tried Word2vec for converting those tags into word vectors but word2vec learns that man and women are not that different and are very similar and it's not giving me good results. Any suggestions on how to tackle this?",
"date": "2019-07-19"
},
{
"vote": 19,
"title": "RoBERTa : Latest Facebook AI Model outperformed XLNet ( Current State of the Art) on GLUE Benchmark",
"text": null,
"date": "2019-07-19"
},
{
"vote": 13,
"title": "pros and cons of lexical vs machine learning methods for text mining",
"text": "I posted this on another site but haven't gotten any replies, so wanted to try here.\n\n\nI wanted to know what are the pros and cons are of using lexical methods and machine learning methods for classifying texts based topic.\n\n\nI have used a simple method of mining documents related to a specific topic based on a keyword list. Basically, if the document contains one of the words from the keyword list it will retrieve it. If that particular word could be used in a different context it checks the post again for other associated words which would usually be found in similar types of documents. This is a simple method but seems to work well, and can be applied to any topic quickly and easily. The main detractor seems to be the keyword lists need to be created and maintained which can be time consuming and inefficient.\n\n\nIn recent times machine learning methods have been used for this type of document classification. It seems this method is able to judge \"context\" better in documents but requires large datasets to be trained on and also require continual training if new data needs to be classified.\n\n\nIt feels like people dismiss lexical methods since the emergence of machine learning methods but is this warranted? It seems like lexical methods can still get good results, especially on small documents which don't contain much context.\n\n\nWhat are the pros and cons of each?",
"date": "2019-07-19"
},
{
"vote": 2,
"title": "Japanese tokenization and stemming mature?",
"text": "I have an idea for something like an Anki plugin that would take in a sentence(s), break it into its words and stemming/lemma-tize (so Iâ€™m not tracking both è¦‹ã‚‹ and è¦‹ã¾ã™for example) them to help keep track of my progress when Iâ€™m changing decks, or adding/removing sentences.  [Sorry if my jargonâ€™s doesnâ€™t quite match my question]. This is specifically in regards to breaking down sentences into vocabulary and deduplicating things like Anki flash cards.\n\n\nIs this feasible in Japanese (and other languages?) I know that tokenization is a pretty simple most western languages, but lemma-tizing is trickier and in asian languages the opposite is true, but my impression is that both are pretty mature technologies for many languages nowadays.  Is this impression correct?  If so any resources I should look at for something like this?",
"date": "2019-07-18"
},
{
"vote": 11,
"title": "Extracting info from text, suggestions for a rule based approach?",
"text": "Hi, I'm trying to extract entities from text. Have been playing around with spaCy and got something to work but doesn't seem to be getting good results. What I tried was vanila spacy and bunch of hand crafted rules for extracting information entities.\n\n\nE.g.: 1 tablespoon sugar\n\n\n1 = Quantity , tablespoon = unit and sugar = entity/food\n\n\na purely rule based would work but there are some instances, \n\n\n1 (1/2 oz) can of milk. which throws off the rules.\n\n\nAny ideas on how to do this? I think my rules method isn't going nowhere and something I should use inbuilt with spaCy. Thank you",
"date": "2019-07-17"
},
{
"vote": 3,
"title": "Should i study text analytics?",
"text": "I am not a good student when it comes to data analytics.So should i take text analytics?What is the learning curve as compared to data analytics?",
"date": "2019-07-17"
},
{
"vote": 0,
"title": "Samsung Releases Blockchain and Decentralized Application SDK",
"text": null,
"date": "2019-07-17"
},
{
"vote": 5,
"title": "Text classification for Natural Language Processing (NLP)",
"text": "I want to classify a corpus into five different categories whether it's descriptive, comparative, cause/effect, sequential and problem/solutions. And build a sequential model from Keras. What approach should i follow to do that ?",
"date": "2019-07-17"
},
{
"vote": 4,
"title": "Has anyone here treaded in bioinformatics?",
"text": "I have seen the Global Sequence Alignment (optimal matching of 2 DNA sequences with parameters for error and skipping symbols) applied from normal text to generated speech-to-text output. This is what triggered my curiosity. (I also fell in love with the algorithm, it's so... satisfying.)\n\n\nThen I googled some stuff, it seems bioinfo relates to language, and I have been looking into some slides, diagonally. They do seem to dabble a lot over sequences of symbols, which could be characters but also words maybe?\n\n\n&#x200B;\n\n\nI'm no expert in that domain though, but that Global Sequence Alignment algorithm's usefulness was self-evident. I was wondering if any of you are acquainted with that field and if it's worth exploring, for us who engage in NLP.",
"date": "2019-07-17"
},
{
"vote": 6,
"title": "XLNet on unicorns",
"text": "XLNet is here and it beats BERT on 20 NLP tasks. But does it beat GPT-2 on text generation? When XLNet was released the answer was unclear but now \nrusiaaman has\n managed to get some decent generated text out of it (not a simple task). This is what it has to say about unicorns.\n\n\n>======Example 3 SAMPLE 1======\n>The group of researchers managed to speak with the unicorns as well as with human being. In addition, the unicorns could also speak the rare language of Tao. Among the scientific marvel that the researchers found was the discovery of a hidden environmental threat to the canyon.\n>\n>Pro-Female Evolutionists are known to think that the female branch of the species appears to be greater in similarity to the male branch. These people also tend to believe that the species was originally created to serve as a civil war instrument. Though that theory does not hold today, it still has its adherents among experts. Unfortunately, the subspecies and branches of the species are in a peculiar and unfamiliar place in Bolivia.\n>\n>In 1891 Bolivia, an ancient red stone fortress, Cusco, stands amidst a spectacular landscape and peaceful mountains. The English New Testament is in its thousand year anniversary translation. In the beginning of the century, Cusco was the capital city of a large German Empire and was the northeastern of four distinct kingdoms. But its population has declined to a little over two thousand people today. The possibility of the female branch of the species being breeding with a male branch is almost highly likely.\n>\n>The surrounding district of P. Tuy Au Grande is known as \"The Choca Island\". There is only one grocery store and one restaurant. The city of Cusco is as old as the city of Istanbul, but it does not appear to be as close to the historical center of Istanbul as it is to the capital of Bolivia. The Inca Empire was the largest, and most powerful, of the empire of Peru. But it has shrunk since the birth of the modern world. The city of Cusco has suffered from many cultural and economic changes. Although this is an important city, the population of its districts has declined as a result of the \"human migration\" that has occurred during the past hundred and thirty years.\n>\n>The current location of the subspecies and branches of the species is in Bolivia. One was found in Bolivia in 2001. The other was discovered in Bolivia in 2003. The gorilla is known to live in a major canyon in the Andes Mountains, where it has been found at least twenty-four times in the past decade.\n>\n>The tag used to identify the individual of the species is by a unique numerical code, both in the world-wide census and the national census. The text is in the same non-textless format as a human being is used to write on paper, but the names are pronounced as it would\n\n\nfull \nwriteup\n and \ncode\n\n\nCompare this to \nGPT2 on unicorns\n Is that better? It's hard to tell until we get metrics, but they both seem to be in a similar league. The amount of training data, and model architectural makes me suspect XLNet is better.",
"date": "2019-07-17"
},
{
"vote": 10,
"title": "Text summarization",
"text": "What are some latest state of the art techniques for text summarization",
"date": "2019-07-16"
},
{
"vote": 1,
"title": "older career transitioners: MSc's versus niche-specific online courses",
"text": "[removed]",
"date": "2019-07-15"
},
{
"vote": 1,
"title": "In subword processing / volab generation, what do you typically set your maximum subtoken length ?",
"text": "I was going through the Tensor2Tensor library, the default subtoken length is huge, 200\n\n\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_problems.py#L324\n\n\nThat's bigger than most words. I am wondering if there's a gap in my understanding, or that default was meant to be changed.",
"date": "2019-07-15"
},
{
"vote": 6,
"title": "A curated list of community detection / graph clustering methods with implementations (deep learning, factorization edge cuts)",
"text": null,
"date": "2019-07-15"
},
{
"vote": 1,
"title": "Idea to train a dataset with grammatical errors",
"text": "[deleted]",
"date": "2019-07-15"
},
{
"vote": 1,
"title": "Understanding what is learnt when RNNs are stacked and some extra information",
"text": "[removed]",
"date": "2019-07-14"
},
{
"vote": 0,
"title": "[D] Is anyone interested in doing small machine learning tasks for small dividends?",
"text": "In my projects I get stuck at times, and while I eventually get unsuck, it still adds time and friction to my projects. Stuff like setting up new tools, pipelines, deployment, frameworks, programming, etc.\n\n\nI have been using /r/slavelabour and /r/ProgrammingTasks to get help time to time, but since I am doing ML projects, I am wondering if there any people here who would be up for that.",
"date": "2019-07-13"
},
{
"vote": 1,
"title": "I have the same question, but haven't received any answers yet:",
"text": null,
"date": "2019-07-13"
},
{
"vote": 2,
"title": "Google's latest attempt at a single universal model for translation",
"text": null,
"date": "2019-07-12"
},
{
"vote": 22,
"title": "bistring: A Python library for non-destructive text processing",
"text": "Hi everybody!  I've just recently released a new Python library called \nbistring\n that can be useful for text processing tasks when you need to keep track of how your changes map back to the original text.  The GitHub is \nhere\n, and the \nIntroduction\n in the \ndocumentation\n provides some more background on exactly what it's useful for.\n\n\nLet me know if you find it useful, or if you can see room for improvement.  It's in the fairly early stages right now, but still quite useful for some tasks I've been doing at work.",
"date": "2019-07-12"
},
{
"vote": 3,
"title": "Lexical differential highlighting instead of syntax highlighting",
"text": null,
"date": "2019-07-12"
},
{
"vote": 3,
"title": "Fine tuning bert or gpt-2 for your document",
"text": "I was wondering if you could fine tune bert or gpt2 on your specific document, lets say on some paper or text book with the following purpose: If you would start a sentence with a question about your document, would it be giving the right answer with itâ€™s language model. Or will it give some unrelated sentence based on the attention mechanism of the combined sentence and model?",
"date": "2019-07-10"
},
{
"vote": 0,
"title": "Using a LSTM over temporal data, and another inner LSTM for the actual words inside the temporal document?",
"text": "Hey folks,\n\n\n&#x200B;\n\n\nIm in the process of learning the craft, but need to still grasp the basic practicality of how to do this. I have a task (NLP+LTR) and a large collection of documents which can be batched into bins of dates (January, February, March). Each document is a large string of course, and the way I know LSTMs to work, is to go through a document while maintaining knowledge of the previous words.\n\n\n&#x200B;\n\n\nHowever, temporal data similarily also uses LSTM in a similar matter. If I want to learn some representation of data over time, where each unit of time consists of multiple documents, would I have to use multiple LSTMs? Would the 'outer' LSTM be the temporal data? Or am I missing some crucial thinking pattern.\n\n\n&#x200B;\n\n\nFor an example: I want to predict something for the month of June. For each month, I have a set of items which change in ranking. I want to go over the months, maintaining some knowledge about the previous predictions (forget-gate for instance), but also all of the weights learned which relate the embedding-weights to the ranking through SGD for instance.",
"date": "2019-07-09"
},
{
"vote": 2,
"title": "How the hell did we get from LSTM Seq2Seq models to transformers?",
"text": "I am reading about transformers right now and the transformer architecture is so much more complex than the LSTM it makes me wonder how we even figured this out in the first place...",
"date": "2019-07-08"
},
{
"vote": 8,
"title": "Analogies Explained: Towards Understanding Word Embeddings (ICML 2019 Best Paper Runner Up)",
"text": null,
"date": "2019-07-08"
},
{
"vote": 2,
"title": "Polish stemmer in Python",
"text": "Hey! I am looking for a stemmer for the Polish language with Python API.  \n\n\nI see that there are a lot of academic solutions (\nhttp://clip.ipipan.waw.pl/benchmarks\n) with quite good efficiency, but requiring a lot of refactoring to be used as an independent library. In addition, these are lemmatizers + disambiguation, and a simple stemmer is enough for my needs. Unless some tools are ready for use :-)  \n\n\nSpacy boasts of support for the Polish language, but PR with the lemmatizer is still pending: \nhttps://github.com/explosion/spaCy/pull/3711\n.  \n\n\nI see that Stempel stemmer has entered Apache Lucene, a great Java framework, but I cannot find it ported to Python.\n\n\nDoes anyone know of Polish stemmer in Python ready to use?",
"date": "2019-07-08"
},
{
"vote": 12,
"title": "Decision/classification/regression tree research papers from the last 30 years",
"text": null,
"date": "2019-07-08"
},
{
"vote": 7,
"title": "How to keep paragraph and page breaks when converting PDFs to plain text?",
"text": "I need to convert many English-language PDFs to txt files. I'm using a text extraction Python library called textract. It works wonderfully except I  haven't found a way to keep the paragraph and page breaks; the plain text is just one big continuous block of text. Does anyone know how I can demarcate paragraph and/or page breaks when converting PDFs to plain text? I am working with the text extraction method of converting PDFs to plain text, not OCR. Thanks in advance!",
"date": "2019-07-07"
},
{
"vote": 12,
"title": "How to Fine-Tune BERT for Text Classification?",
"text": null,
"date": "2019-07-07"
},
{
"vote": 1,
"title": "Advice needed please. Extracting action from tickets resolutions and train a seq2seq model for new ones.",
"text": "[deleted]",
"date": "2019-07-06"
},
{
"vote": 3,
"title": "Installing PyTorch on RHEL 7.6 ppc64LE architecture",
"text": "Hi,\n\n\n&#x200B;\n\n\nI am trying to install PyTorch on RHEL 7.6 ppc64LE architecture using this package \nhttps://anaconda.org/jjhelmus/pytorch\n using conda 4.6.14. It downloads all the packages except pytorch0.40 which is ~273 MB. It gives this error:\n\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url <\nhttps://conda.anaconda.org/jjhelmus/linux-ppc64le/pytorch-0.4.0-py36h53588c4_0.tar.bz2\n&gt;\n\n\nElapsed: -\n\n\n&#x200B;\n\n\nAn HTTP error occurred when trying to retrieve this URL.\n\n\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n\n\n&#x200B;\n\n\nI tried almost all the available solutions online, as in:\n\n\nupdate conda; install openSSL, update certifi, set SSL to False, add/remove proxy, add channels, add whitelist, download the tar file and build manually, etc. but none helped me the HTTP error.\n\n\n&#x200B;\n\n\nAny suggestions are much appreciated!\n\n\n&#x200B;\n\n\nThanks!",
"date": "2019-07-05"
},
{
"vote": 5,
"title": "SpaCy for ner PRODUCT tagging (?)",
"text": "Hello all,\n\n\nI see from SpaCy's documentation that it can be trained to recognize products from text.\n\n\nhttps://spacy.io/usage/linguistic-features\n  (scroll down to visualizing named entities)\n\n\nI also see that the tutorial is loads a \"custom_ner_model\". In the example shown, SpaCy is able to detect that Siri and Alexa are products, separate entities from organizations. This is exactly what I need SpaCy to do for me! When I used the text and code provided in this example, I was unable to tag products but picked up everything else.\n\n\nI'm quickly starting to believe that this super useful custom_ner_model is just that, customized and not available for download. (Perhaps it serves as motivation to train my own using prodigy or other methods.)\n\n\nSo my ask is - does anyone have a GitHub link or reference to a pre-trained model that can tag products?\n\n\nCheers!\n\n\n&#x200B;\n\n\nEdit: This \nlink\n seems to indicate that product tagging is supported by modes trained on the OntoNotes5 corpus. But I'm still not sure what I need to do.",
"date": "2019-07-04"
},
{
"vote": 1,
"title": "Implementing text prediction based on words typed in addition to the 'sentence to be typed' in a different language",
"text": "How would you implement a text prediction ML model based on the words inputted by the user in addition to having a translation of the sentence in a different model? \n\n\nI'm hoping this would make a cool model to improve the efficiency of translators and am looking for thoughts/suggestions/ideas.",
"date": "2019-07-04"
},
{
"vote": 0,
"title": "Accessing GoogleNews-vectors-negative300.bin online",
"text": "Hi,\n\n\nI am using some limited shared space for my work. Working with pre-trained word vectors needs, for instance, GoogleNews-vectors-negative300.bin stored in the local memory which takes up to 3.6 GB. Is there any way that this word2vec file can be referenced online? I found this \nStackOverflow post\n where it is stored on the AWS but it is in the .tz format. Another \npost\n (point #16) shows how to use the gensim API but after trying out this option, this also downloads the file (~1662 MB = 1.662 GB).\n\n\nThanks!",
"date": "2019-07-04"
},
{
"vote": 2,
"title": "K Means Clustering visualizations for TF-IDF BOW ?",
"text": "Hello all,\n\n\nI'm looking for a tutorial on using K-Means to cluster my text documents, using TF-IDF word vectors as inputs. Further, I need to visualize these clusters so that I can inspect the level of overlap between clusters.\n\n\nIs this possible? I understand that my feature space is the size of my BOW, so a 2D graphic representation might be difficult to achieve. \n\n\nI'm using python FYI.\n\n\nThank you",
"date": "2019-07-03"
},
{
"vote": 9,
"title": "How to fine tune the BERT model over custom dataset?",
"text": "Hi,\n\n\nI know that BERT is trained on general domain namely wiki and some book corpus. But How can i use it over a domain specific dataset. Because the the vocab list for domain specific dataset willl be different from BERT - BASE vocab list. Or if I need to retrain from scratch like SCIBERT or  BIOBERT I would need GPU's or TPU's Which I cannot affford. Also from Bert Git Repo, I found that the author mentioned to utilize some '[unusedxxx]' item to replace with our domain specific vocab. BUt most of them quoted that they haven't got good results. Any leads would be appreciated......!",
"date": "2019-07-03"
},
{
"vote": 1,
"title": "How natural language changes the way we visualise data",
"text": null,
"date": "2019-07-03"
},
{
"vote": 30,
"title": "Share your transformer/BERT/GPT2 training tips",
"text": "Transformers are an exciting new technology for language, but it's unclear what the best way to fine tune them is, what heads to use for new tasks, and so on.\n\n\nA few people here are experimenting with them. Lets share tips on how to engineer transformers.",
"date": "2019-07-03"
},
{
"vote": 1,
"title": "Share you transformer tips",
"text": "[deleted]",
"date": "2019-07-03"
},
{
"vote": 3,
"title": "How does RF classifier go about making splits on text data?",
"text": null,
"date": "2019-07-02"
},
{
"vote": 12,
"title": "Looking for some good NLP/Computational linguistics lecture notes",
"text": "Before starting to read any book , good lecture notes help to grasp overall idea about the topic and help to understand  quiet fast and easily",
"date": "2019-07-02"
},
{
"vote": 5,
"title": "Predicting word from a set of words",
"text": "My task is to predict relevant words based on a short description of an idea. for example \"SQL is a domain-specific language used in programming and designed for managing data held in a relational database\" should produce words like \"mysql\", \"Oracle\", \"Sybase\", \"Microsoft SQL Server\" etc...\n\n\nMy thinking is to treat the initial text as a set of words (after lemmatization and stop words removal) and predict words that should be in that set. I can then take all of my texts (of which I have a lot), remove a single word and learn to predict it.\n\n\nMy initial thought was to use word2vec and find words similar to my set. But this doesn't work very well, as I don't want similar words but words that go together in many sentences, which is sort of the task that word2vec is training on to create its embedding...\n\n\nHow would you model this problem? I don't think RNNs are relevant here because I want to use a set of words - they do not have any order, I'm not trying to predict the next word in any way. However, the size of the set could vary I think...\n\n\nWhat kind of NN architecture would you use for this sort of problem?",
"date": "2019-07-02"
},
{
"vote": 3,
"title": "Predict if different parts of a document are about a certain topic",
"text": "[deleted]",
"date": "2019-07-01"
},
{
"vote": 3,
"title": "Is there any dataset for polite and impolite words in English?",
"text": "[deleted]",
"date": "2019-07-01"
},
{
"vote": 3,
"title": "Oversampling data for text classification",
"text": "Hello all,\n\n\nI need a little help oversampling my input data in order to get better prediction results.\n\n\nMy independent variable is job_category, which can take on 16 values (construction, finance, education, etc). I have app. 120,000 rows of data, which are greatly imbalanced.\n\n\nNaively, I re-balanced the data so that there were 7,000 observations randomly sampled (or over-sampled) of each category. I pre-processed the text, trained a random forest model, which achieved really good results. F1 avg of 0.95, with no precision or recall lower than 0.8. So I was pretty happy with the performance.\n\n\nIn practice, however, I am predicting the smallest minority class about 5x more often than the others. I attribute this to the fact that there were only ~1000 unique samples of this class. However, I'm not positive that this is the cause. (feedback?)\n\n\nIn non-NLP settings, I would use SMOTE to re-balance my data. However, my google searches have returned little in the way of implementing this technique on textual data.\n\n\nThoughts?",
"date": "2019-07-01"
},
{
"vote": 5,
"title": "Making a poetry model round2:",
"text": "I've already posted 2 related posts on this sub, so i thought it was time to share some updates and ask for more guidance:\n\n\nMy hobbie goal was to make a program generate 4 lines poetry, or atleast coherent text in portuguese. Here goes an example of a poem from one of our most famous poets, Fernando Pessoa.\n\n\n\"O poeta Ã© um fingidor.Finge tÃ£o completamente Que chega a fingir que Ã© dor A dor que deveras sente\"\n\n\n&#x200B;\n\n\nMy first approach was a simple ngram-model. I assembled a dataset of 700 poems and managed to generate some data.\n\n\nThis was my conclusion from that simple model: The results we good in terms of syntax, the person of the entities matched the person of the verbs, the gender of the determinants matched the person and gender of the nouns etc... (not perfect but quite good). However there were 3 major drawbacks for me:\n\n\n\n\nDue to using ngrams, despite the dataset having a lot of different verbs that are conjugated with the same person, gender, time, in the generation step the only verbs considered are the ones that only appear after the actual word in the model, when there were more valid options to be considered in the corpus.(\noverfitting\n)\n\n\nThere was a rhyming issue, i found no rhyming dictionary available online, for words that rhyme in portuguese. (I managed to solve this problem by creating my own rhyming dictionary which has around 10.000 words at the moment)\n\n\nI had no way to know if a verse ended properly.  I'm not being rigid like syllabic metric level, i just wanted the sentence to make sense, for example:\n\n\n\n\nThe sentence should end like:\n\n\n\"The cat sat on the mat.\"\n\n\nBut instead was ending like:\n\n\n\"The cat sat on the mat furthermore\"\n\n\n&#x200B;\n\n\nSo i thought i would take the problem in a complete different way:\n\n\n\n\nFirst, I would use a tagger. spaCy's tagger to have a concrete verse structure and create a dictionary with all the vocabulary of my corpus. Here are the keys:\n\n\n\n\ndict_keys([**'VERB', 'DET', 'NOUN', 'PRON', 'SCONJ', 'ADP', 'ADJ', 'ADV', 'PUNCT', 'CCONJ', 'SYM', 'X', 'AUX', 'NUM', 'PROPN'**]\n\n\n&#x200B;\n\n\n\n\nThen i would selected a random poem from my corpus and get it's structure. I like to think of it as a skeleton.\n\n\nThen the first words i'd add to the token were the last one ones, that are supposed to rhyme (i used help from my rhyming dictionary).\n\n\nThen i would just fill the rest of tags, with words from the newly created dictionary.\n\n\n\n\n&#x200B;\n\n\nThe results were worse when compared to the ngram-model. This new model provides much more freedom, and since the tags are too broad, a \n\"VERB\"\n can be in very tenses, the tokens picked to fill \n\"DET\"\n tags sometimes pick wrong gender etc...\n\n\n&#x200B;\n\n\nHowever this way of constructing poems has  2 features i like a lot:\n\n\nThe structure of the poem is defined. You know already how many elements a verse will have (the same as the number of tags), so you don't need to provide an arbitrarily limit of tokens.\n\n\nAnd It offers more freedom, there are more words to be considered when selecting a token to fill a tag, which increases the diversity of the produced text.\n\n\n&#x200B;\n\n\nAnyone wants to provide an ideia on how i can make a better model ?  I'm avoiding using neural networks. I've thought of two things:  Make a better tagger which takes in account the time/gender/person of the verbs, determinants etc... Or increase my corpus (I really don't want to that). I'd much rather add new words to a dictionary and create a better tagger than searching for more poems to increase my corpus size.",
"date": "2019-06-30"
},
{
"vote": 6,
"title": "What are some fields sentiment analysis can be applied to that it hasn't been used for yet?",
"text": "[deleted]",
"date": "2019-06-30"
},
{
"vote": 2,
"title": "NER for short text",
"text": "Hi guys,\nI am working on Japanese named entity extraction task. The goal is to extract the name of location in the article. The output of my model is not good when I test with short texts (title / subtitle of article). It maybe due to the length of input is too short (titles are often short and include only entity and few words). \nCan you suggest me any solution? I am building NER using biLSTM+charCNN+CRF.\nDo I need to train with more short sentences? Or can I switch to another model (seq2seq)?\nThanks in advance.",
"date": "2019-06-27"
},
{
"vote": 14,
"title": "[P] AI Against Humanity: How an experiment with GPT-2 turned into an AI-driven browser game (offensive language)",
"text": "This is my current side project. A game like Cards Against Humanity, but all the cards were generated by an AI. Your opponent is controlled by another one. Let me know what you think :)\n\n\nHere's a blog post detailing the idea and development: \nhttps://cpury.github.io/ai-against-humanity/\nAnd here's the link directly to the game: \nhttps://www.aiagainsthumanity.app/",
"date": "2019-06-26"
},
{
"vote": 0,
"title": "Implementing K-Means Clustering From Scratch: Simply Explained",
"text": null,
"date": "2019-06-26"
},
{
"vote": 7,
"title": "Can anybody recommend an Natural Language Generation open source platform?",
"text": "Currently I'm using very complex unscaleable php templates to generate news articles from data. I'm looking for an open source platform which would help me create a more structured, manageable system.",
"date": "2019-06-25"
},
{
"vote": 1,
"title": "Neural Machine Translation With Attention Mechanism: Step-by-step Guide",
"text": null,
"date": "2019-06-25"
},
{
"vote": 1,
"title": "Neural Machine Translation With Attention Mechanism: Step-by-step Guide",
"text": null,
"date": "2019-06-25"
},
{
"vote": 3,
"title": "Information extraction help",
"text": "I'm in the process of writing a script to associate job titles and descriptions specified by my company's clients to the titles (and skills if possible) to those found on ONET. \nhttps://www.onetonline.org\n\n\nGenerally, speaking - I'm hedging my bets with two different strategies:\n\n\n===\n\n\n1.) Build adequate sized training data to train an ML model. There are app 973 unique ONET skills, which apply to my company, so the size of my training data will need to be >50,000 rows, imo. Time consuming and I haven't started yet..\n\n\n===\n\n\n2.) The latter approach has been through text processing and word vector similarity. Specifically,\n\n\n- Transform job title and ONET job titles to word vectors (lowercase, lemmatize, and remove stopwords.) Find cosine similarity with each ONET title. Index the skills from the ~top 5 most similar (above an arbitrary similarity threshold.)\n\n\n- Create a counter for each candidate ONET title. Parse job description for skills. The ONET title corresponding to the highest number of skills found is the \"winner.\" Parsing so far has been comparing the length of a set intersection (ONET skill, description) divided by the length of the ONET skill skill. So \"java\" (common in my company's data) and \"oracle java\" (ONET skill) would return a score of 0.5 (just an example.) In order to catch things like this, I've had to drop the required similarity to about 0.5; this creates several good and bad matches..\n\n\n- Assign all skills, whether or not they come from the \"winning\" ONET title.\n\n\nWhen approach 2 works, it really works. But the choke point is finding highly similar job titles. If we start with a bad \"seed\" so to speak, few skills will be found and of those, many could be \"false positives.\"\n\n\n===\n\n\nNeedless to say, approach 2 works about 10% of the time. But it often spits out garbage. And advice or recommendations would be appreciated. If you were to suggest topic modeling I'd be surprised because there are 973 topics, roughly, but I'd explore it - any progress is good progress.\n\n\nEDIT: One neat thing I noticed is that there are some skill overlap from job to job. So some jobs are highly similar in that they share many skills; other jobs are dissimilar in that they share little or no skills in common. And vice versa holds true too; based on the jobs, which a given skill can be found under, some skills are highly similar (co-occur frequently) and some skills are quite dissimilar (co-occur infrequently, if at all.) \n\n\nSo I created a matrix where the columns are jobs and the rows are skills, and if a given job required a given skill, the corresponding cell received a 1, if not, it received a 0. Then I was able to use cosine similarity to compare job and skill vectors. Using similarity as a threshold/parameter, I can supply a given job (or skill) and find others as or more similar than specified. \n\n\nI haven't had a use for this just yet, but I get a feeling that it has promise!",
"date": "2019-06-25"
},
{
"vote": 29,
"title": "CMU &amp; Google XLNet Tops BERT; Achieves SOTA Results on 18 NLP Tasks - Medium",
"text": null,
"date": "2019-06-24"
},
{
"vote": 26,
"title": "[D] Those who hire/interview for NLP positions, what can self taught people include in their projects that would convince you they would be able to fit in and keep up with those with a more standard background ?",
"text": "I posted a similar thread in r/ml but wanted to hear about NLP specific positions.",
"date": "2019-06-24"
},
{
"vote": 5,
"title": "Similarity between list of keywods",
"text": "Hi, i have 10 lists each containing keywords which are semantically similar to each other. Also i have one more list which contain some keywords, i have have to identify to which category out of other 10 this list is most similar to, also there is possibility of multi label, i.e. this list might be similar to more than 1 category of keywords.",
"date": "2019-06-24"
},
{
"vote": 0,
"title": "Probability and NLP hand-in-hand",
"text": "Read about my latest writing at \nhttps://prakhartechviz.blogspot.com/2019/05/probability-for-linguists.html",
"date": "2019-06-23"
},
{
"vote": 2,
"title": "I have a lot of trouble understanding github files.",
"text": "I am sorry this isn't much about NLP, BTW, IDK where else to ask.\n\n\nI come from a Maths background, I programmed most of my life in simple IDEs, where pressing F5 runs the whole script, and I struggle with these things.\n\n\n&#x200B;\n\n\nI am able to handle NLP tasks programmatically in python and install and fiddle with python APIs without much trouble, but when it comes to setting up anything, unless the instructions are very clear, I have trouble doing them alone. Usually, I have coworkers who can help me, \nbut I'd like to be a bit more independent about these issues\n. BTW, the docker itself is something I just access because I follow the instructions, I never set one up myself and have trouble understanding what it even is.\n\n\n&#x200B;\n\n\nWhenever I see someone posting github links I am curious about these amazing things they can do but it always baffles me how I am supposed to use them. Am I also supposed to read and understand each .py code file? What order should I make sense of them?\n\n\n&#x200B;\n\n\nI was once able to install GPT-2 in the docker at work, but it took me like 2 hours hitting my head against commands and trying to figure out some steps in the console.\n\n\n&#x200B;\n\n\nAnyone could advise me what I should read in order to overcome this difficulty? My coworker said it comes with practice, but that doesn't seem the case.",
"date": "2019-06-23"
},
{
"vote": 15,
"title": "RDRPOSTagger provides 300+ pre-trained POS and morphological tagging models for about 80 languages",
"text": "RDRPOSTagger is a robust and easy-to-use toolkit for POS and morphological tagging. It obtains a fast tagging speed and achieves a competitive accuracy in comparison to the state-of-the-art results. It now provides pre-trained UPOS, XPOS and morphological tagging models for about 80 languages.\n\n\nFind more information about RDRPOSTagger at: \nhttp://rdrpostagger.sourceforge.net\n\n\nCurrent release (41MB .zip file containing about 330 pre-trained tagging models) is available to download at: \nhttps://github.com/datquocnguyen/RDRPOSTagger/archive/master.zip",
"date": "2019-06-23"
},
{
"vote": 0,
"title": "Shouldnâ€™t N-LP be H-LP?",
"text": "Shouldnâ€™t it be:\n\n\n/Natural/Human/ Language Processing?\n\n\n/s",
"date": "2019-06-23"
},
{
"vote": 5,
"title": "Where can i find a Part Of Speech and Phoneme Tagger for Portuguese (in Python)",
"text": "Anyone knows where i cant these 2 taggers to label my own text ?",
"date": "2019-06-22"
},
{
"vote": 1,
"title": "Finding ambiguities within documents",
"text": "Hi, Iâ€™m a software developer working on a small NLP & ML project to identify ambiguities within documents. For e.g. to check whether documents meet certain quality criteria or not. \n\n\nDo I need to acquire training data on â€œwhat is ambiguousâ€ and â€œwhat is notâ€ to train my algorithms? Or are there pre-existing libraries or data set that can be used? Really appreciate your guidance.",
"date": "2019-06-22"
},
{
"vote": 9,
"title": "Poem Generation",
"text": "Hello, everyone. I am writing a series for generating poems. My first article tackling that problem uses N-grams and is explained in detail for beginners. Please check it out!\n\n\n \nhttps://divybramhecha.tech/n-grams-for-poetry-generation/\n\n\nFor someone who is quite good at this stuff, could you please take a quick look at the article, the section where I discuss the data structure I use, is it the most efficient way? I didn't see a lot of people discuss that, but I think it was better than what is generally used. (N-nested dictionary for N-grams) \n\n\nHere's the Poem from 4-gram model:\n\n\nhttps://preview.redd.it/fvgf8oi1vt531.png?width=595&format=png&auto=webp&s=2d6b7d8578a4d1459de30ba21df3c769dbf86ae0",
"date": "2019-06-22"
},
{
"vote": 3,
"title": "Hallo multilingual BERT, cÃ³mo funcionas?",
"text": null,
"date": "2019-06-21"
},
{
"vote": 45,
"title": "NLP for starters",
"text": "I've created a tutorial explaining the current non-DL techniques for one of the common problems in NLP, i.e. text classification. I go over multiple topics, Bag of Words and Embedding approach, along with challenges in real problems. It also covers fasttext and transfer learning. It's tailored towards practitioners with basic knowledge of ML for structured data and python who want to start working on NLP projects. Thought sharing it with this community and any feedback is appreciated.\n\n\nPart1: \nhttps://youtu.be/TGgOpBtlH4Y\n\n\nPart2: \nhttps://youtu.be/KvoTRvEcvhg\n\n\nPart3: \nhttps://youtu.be/muoXbpbApmc\n\n\nPart4: \nhttps://youtu.be/ALgcfvuMcP8\n\n\nPart5: \nhttps://youtu.be/SMDwh8DJ5jo\n\n\n&#x200B;\n\n\nOverall 1 hour and 40 minutes. Speed 1.5x should be comfortable for the most parts.\n\n\n&#x200B;\n\n\nEdit: Uploaded an edited version of part 1 and updated the link above.",
"date": "2019-06-21"
},
{
"vote": 2,
"title": "Phrase similarity dataset",
"text": "Iâ€™m looking for a dataset to train a model on phrase/clause similarity. For example:\n\n\nIt is television / Itâ€™s not television â€”> 0 Itâ€™s television / Itâ€™s TV â€”> 1\n\n\nOr even:\n\n\nIdentify errors in a system / Detect problems in a program â€”> 1\n\n\nI have found some similar data sets but for full sentences, Iâ€™m looking for more compact phrases/clauses.\n\n\nAny ideas help!",
"date": "2019-06-21"
},
{
"vote": 13,
"title": "Is anyone working on NLP for healthcare and willing to answer a few questions about what youâ€™re using it for?",
"text": null,
"date": "2019-06-20"
},
{
"vote": 1,
"title": "Synthetic Parallel Dictionary",
"text": "Could someone please help me understand synthetic dictionary? Or synthetic parallel vocabulary? Thanks",
"date": "2019-06-20"
},
{
"vote": 25,
"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding (from Google, beats BERT on some key tasks like SQuAD)",
"text": null,
"date": "2019-06-20"
},
{
"vote": 4,
"title": "Sentiment Analysis in R - Polarity and Counts functions output problem (QDAP package)",
"text": "I'm working through a Datacamp course on Sentiment Analysis. I'm applying the `polarity()` function from the QDAP package. However, there is a difference in my output when I call the `counts()` function to visualize which are the positive and negative words in the strings.\n\n\nlibrary(QDAP)\n\n#Creating the data frame: \n\nperson &lt;- c(&quot;Nick&quot;, \n            &quot;Jonathan&quot;, \n            &quot;Martijn&quot;, \n            &quot;Nicole&quot;, \n            &quot;Nick&quot;, \n            &quot;Jonathan&quot;, \n            &quot;Martijn&quot;, \n            &quot;Nicole&quot;)  \n\ntext &lt;- c(&quot;DataCamp courses are the best&quot;, \n         &quot;I like talking to students&quot;, \n         &quot;Other online data science curricula are boring.&quot;, \n         &quot;What is for lunch?&quot;, \n         &quot;DataCamp has lots of great content!&quot;, \n         &quot;Students are passionate and are excited to learn&quot;, \n         &quot;Other data science curriculum is hard to learn and difficult to understand&quot;,\n         &quot;I think the food here is good.&quot;)  \n\ntext_df &lt;- data.frame(person, text) \n\n#Check the data frame. The strings should be factors \n#for the QDAP polarity function\ntext_df  \n\n#Create a new object `datacamp_conversation` by\n#forwarding `text_df` with `%$%` to `polarity()`. \n#Pass in text followed by the grouping person column.\n#This will calculate polarity according to each\n#individual person. Since it is all within parentheses\n#the result will be printed too. \n\n(datacamp_conversation &lt;- text_df %$% polarity(text, person)) \n\n#Apply `counts()` to `datacamp_conversation` to print\n#the specific emotional words that were found. \n#NOTE: This is where I cannot see the positive nor\n#negative words.\n\ncounts(datacamp_conversation)\n\n\n\nIn the Datacamp server, I can visualize the  negative and positive words right away in the table.\n\n\nHowever, locally, on my computer I have save to save the `counts()` as an object and call the \"pos.words\" and \"neg.words\" column to visualize the list contents:\n\n\nc &lt;- counts(datacamp_conversation)\nc$pos.words\nc$neg.words\n\n\n\nHas anyone experienced this? Can you replicate it in your computer to check if you can visualize the neg.words and pos.words as you call the counts() function?\n\n\n&#x200B;\n\n\nThanks.\n\n\n&#x200B;\n\n\n[My system: Windows 10 Pro, RStudio Version 1.2.1335, R x64 2.6.0.]\n\n\n&#x200B;\n\n\nIn the image you can see my pos.words and neg.words output \"char [1]\" and \"char [2]\" , while on datacamp it shows the specific words:\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nMy system\n\n\nDatacamp's server output",
"date": "2019-06-19"
},
{
"vote": 5,
"title": "spaCy IRL 2019 - July 5-6, Berlin",
"text": null,
"date": "2019-06-19"
},
{
"vote": 0,
"title": "Improving Business Communications and Human Interactions with NLP",
"text": null,
"date": "2019-06-19"
},
{
"vote": 18,
"title": "Is there a point in competing against BERT when pre-training is incredibly expensive?",
"text": "Hi everyone,\n\n\nI'm not trying to troll, this is a very real question I'm asking myself.\n\n\nWith models like ELMO and BERT being SOTA or very close to SOTA for most common NLP tasks and costing thousands dollars in GPU / TPU computations, is there a point in doing NLP research as a single person?\n\n\nI love that NLP is getting all these shiny new improvements but I can't help but feel doomed to just use whichever pre-trained word embeddings crumbs FAANG companies make available to the public.\n\n\nI've seen some papers that try to reduce the BERT model size but we're still a long way out.\n\n\nUpdate: Just to make clear, I think \"competing\" was maybe the wrong term but more doing non-out-of-the-box-NLP, like retraining a model on custom data for example.",
"date": "2019-06-18"
},
{
"vote": 0,
"title": "Using doccano for Aspect Based Sentiment Analysis annotation",
"text": "Currently looking for a good tool to annotate sentences regarding aspects and their respective sentiment polarities.\n\n\nI'm using SemEval Task 4 as a reference. The following is an example in the training dataset:\n\n\n&#x200B;\n\n\n><sentence id=\"2005\">\n>\n><text>it is of high quality, has a killer GUI, is extremely stable, is highly expandable, is bundled with lots of very good applications, is easy to use, and is absolutely gorgeous.</text>\n>\n><aspectTerms>\n>\n><aspectTerm term=\"quality\" polarity=\"positive\" from=\"14\" to=\"21\"/>\n>\n><aspectTerm term=\"GUI\" polarity=\"positive\" from=\"36\" to=\"39\"/>\n>\n><aspectTerm term=\"applications\" polarity=\"positive\" from=\"118\" to=\"130\"/>\n>\n><aspectTerm term=\"use\" polarity=\"positive\" from=\"143\" to=\"146\"/>\n>\n></aspectTerms>\n>\n></sentence>\n\n\n&#x200B;\n\n\nCan I easily use doccano for such a task? Or would I be better off using some other tool, such as brat?",
"date": "2019-06-18"
},
{
"vote": 1,
"title": "[D] What are some examples of malicious tasks that language models can trained for?",
"text": "AI safety and ethics is a hot topic. Besides fake news generation, and toxic comment generation, what other malicious tasks could language models be trained for?",
"date": "2019-06-18"
},
{
"vote": 9,
"title": "Python medical Library for WordNet; OR; any good python medical library",
"text": "I have started using NLTK's WordNet library and find it very useful for content analysis in Python\n\n\n&#x200B;\n\n\nJust wondering if WordNet has a library for medical words as well\n\n\n&#x200B;\n\n\nor if not wordnet then what Python library could be used for medical terminologies",
"date": "2019-06-18"
},
{
"vote": 16,
"title": "Generating Poems Artificially, please check out my new series!",
"text": null,
"date": "2019-06-15"
},
{
"vote": 13,
"title": "First NLP project?",
"text": "Hi, \n\n\nI am a beginner in NLP and recently finished a course at my university covering topics such as vector semantics, hidden markov models, syntactic parsing, dependency parsing. We used mainly Jurafsky during the course.\n\n\nSummertime is here and I want to try to apply my knowledge.\n\n\nAny tips for simple and small projects?",
"date": "2019-06-15"
},
{
"vote": 5,
"title": "What are some techniques to split sentences into smaller phrases?",
"text": "I'm trying to get from this:\n\"To hack into an organization system with their permission and check if there are any problems\"\n\n\n&#x200B;\n\n\nTo this (1):\n\n\n\"To hack into an organization system\",\n\"With their permission\",\n\"Check if there are any problems\"  \n\n\nTo this (2):\n\"Hack organization system\"\n\n\n\"With Permission\"\n\"Check Problems\"  \n\n\nDo you have any ideas for steps 1 and 2?",
"date": "2019-06-14"
},
{
"vote": 4,
"title": "Do you know of a good phrase generator / paraphraser in Pytorch?",
"text": "Hi!   I'm looking for a phrase generator (paraphraser) in Pytorch, ideally  already trained on Quora's Duplicate Question dataset, but also on other  types of datasets is fine.\n\n\n&#x200B;\n\n\nI found one in Lua / Torch, but unfortunately I don't know how to use Lua.\n\n\nhttps://github.com/badripatro/Question-Paraphrases\n\n\n&#x200B;\n\n\nPretty much something like this repo but in Pytorch would be perfect!\n\n\n&#x200B;\n\n\nThanks for the wisdom!\n\n\n&#x200B;\n\n\nEric",
"date": "2019-06-14"
},
{
"vote": 1,
"title": "How can memory networks perform well in lists/set type tasks?",
"text": "[deleted]",
"date": "2019-06-14"
},
{
"vote": 1,
"title": "What Benefit Can You give for Business by Translation Services?",
"text": "[removed]",
"date": "2019-06-14"
},
{
"vote": 2,
"title": "What are current state-of-the-art techniques for phrase similarity?",
"text": "Iâ€™m looking for a model that will help in the abstraction of meaning. For example, the meaning of:\n\n\nâ€œIt helps fixing errorsâ€\n\n\nIs really close to:\n\n\nâ€œItâ€™s good to correct mistakesâ€\n\n\nCurrently Iâ€™m using spaCyâ€™s large model for word embeddings.\n\n\nPlease share your ideas ðŸ’¡",
"date": "2019-06-14"
},
{
"vote": 4,
"title": "Evaluating topics generated by topic models",
"text": "I am working on topic modelling of Amazon reviews and descriptions. I read that the coherence measure is better because it is correlated with human topic interpretation. The online resources explain what u_mass and c_v measures are through formulas which makes sense and they also mention the usage of reference corpus. I have no clue about this.\nI wanted to know what are intrinsic and extrinsic measures and what is a reference corpus. Why use an external reference corpus and what is the problem of using the same corpus to calculate coherence which was used in learning of topics.\nAlso, can I use part of my 500000 review dataset as reference corpus and part of it to learn the topics?",
"date": "2019-06-13"
},
{
"vote": 3,
"title": "How to do large scale train test splits without overlaps for named entity recognition?",
"text": "I have 7 million sentences and the relevant entities and I am looking to create train test splits ensuring none of the entities are both in the train and test set. \n\n\nINPUT\n\n\nENTITIES TEXT\ne1       TextA\ne1, e2   TextB \ne3       TextC \n\n\n\nI would like to have the outputs:\n\n\nTRAIN SPLIT\n\n\nENTITIES TEXT\ne1       TextA\ne1, e2   TextB \n\n\n\nTEST SPLIT\n\n\nENTITIES TEXT\ne3       TextC\n\n\n\nMy initial approach was with Dataflow but getting no overlaps led to an ugly \ngroupby\n bottleneck. I was wondering if anyone else has done any work around something like this.",
"date": "2019-06-13"
},
{
"vote": 1,
"title": "Metrices to evaluate semantic similarity between two sentences.",
"text": "I am working on a sentence generation task and trying to evaluate the generated sentence based on following metrices :\nBLEU 1\nBLEU 2\nBLEU 3\nBLEU 4\nMETEOR\nROUGE_L\nCIDEr\nSkipThought cosine similarity\nEmbedding Average cosine similarity\nVector Extrema cosine similarity\nGreedy Matching score\n\n\nI am not sure which metrices are robust as  when my two sentences have the same meaning, some of these metrices indicates them to be different.\nPlease suggest metric that evaluates sentences based on semantic meaning.",
"date": "2019-06-12"
},
{
"vote": 1,
"title": "Sentence Similarity Task - What are the options available for implementing this task using sentence embeddings on a mobile application?",
"text": "I have a set of functions(dataset with nearly 400 samples) along with a description for each. The input would be an user trying to describe a function that they need in natural language. It maybe a word or a sentence. Similarity Task has to be done on the search query and the functions with the closest semantic relation should be shown as results. \nNow that Apple has added support for embeddings in their Natural Language framework, which paper/implementation might be suited for this task on iOS ? \nTried forming sentence embeddings by averaging the word embeddings from word2vec, but the results were not what I expected.",
"date": "2019-06-12"
},
{
"vote": 4,
"title": "Microsoft QnA maker - how does it work?",
"text": "I just stumbled upon this and I am curious as to how it might work, or easy might be to (partly) replicate: \nhttps://www.qnamaker.ai/\n\n\n\"Build, train and publish a simple question and answer bot based on FAQ URLs, structured documents, product manuals or editorial content in minutes.\"\n\n\nIt seems like there is a front-end where one writes pairs of question-answer. Are they treating it simply as a classification problem, where they make use of word embeddings and sentence similarity?\n\n\nI would imagine this might not work without a significant amount of sentence samples, so perhaps there is some kind of relationship extraction/semantic parsing + NER, and then the a learning algorithm is used to score the most likely answer.\n\n\nWhare are your thoughts on this? Any ideas, suggestions or resources I should know about?",
"date": "2019-06-11"
},
{
"vote": 0,
"title": "most optimal model",
"text": "I have a dataset with some 6000 rows and 3 columns. I have to use the two columns 'travel_agency' and 'code' to predict 'time_taken'\n\n\nMay I know what would be the best model for this kind of regression problem? and how to proceed into this? \n\n\nthe dataset looks something like this:\n\n\ntravel_agency               code   time_taken\n\n\n&#x200B;\n\n\nalfredo travel agency   9887  10.45674\n\n\nfast travels                     8569  99.9873\n\n\nhappy journey tours    3621  5.5698\n\n\ncomfort travels            5687  50.689\n\n\nmake your trip            3697  27.67\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n6000 rows",
"date": "2019-06-10"
},
{
"vote": 13,
"title": "Does anyone have any interesting use cases for named entity recognition?",
"text": "Any articles or papers would be greatly appreciated!",
"date": "2019-06-10"
},
{
"vote": 2,
"title": "Parallel Text Datasets for Machine Translation Training",
"text": "[deleted]",
"date": "2019-06-10"
},
{
"vote": 5,
"title": "Need guidance to pull medical data using NLP",
"text": "I am newbie to NLP. I want to extract patient information like age, gender, drug name, tablet etc from PDF journal. Can anyone guide me how can I proceed. Thanks",
"date": "2019-06-09"
},
{
"vote": 3,
"title": "CYK algorithm - how to handle unknown terminals given in a sentence to parse?",
"text": "There is a given treebank which we derive the Probabilistic context free grammar. I wonder how do one handles with a given sentence which includes terminals that don't exist in the derived rules? Is there a kind of smoothing for this case? Considering we use CYK to get the parse tree of the given sentence.",
"date": "2019-06-09"
},
{
"vote": 12,
"title": "Can BERT be used for semantic (meaning) search?",
"text": "Same with ELMo.\n\n\nI am experimenting with a pre-trained BERT, trying some sentence/token similarities like what can be done with word embeddings, but the contextualized embeddings' similarity just fluctuate a lot. Also, the first and end tokens, as the punctuations, seem to be similar in general.",
"date": "2019-06-08"
},
{
"vote": 1,
"title": "Weak annotation for emotion analysis dataset",
"text": "I am currently working on a classifier which is supposed to determine the emotion of a tweet.\n\n\n&#x200B;\n\n\n I have 7 classes derived from Ekman emotions. I collected about 100 000 tweets using emojis for various emotions. For example ðŸ˜€for \"joy\", ðŸ˜¡ for \"anger\" etc... (I obviously removed these emojis during the preprocessing steps).\n\n\nI am using Keras and CNN to classify and I have around 55% accuracy on a balanced dataset.\n\n\n&#x200B;\n\n\nThe accuracy is not great and I guess my dataset need some refinements. \n\n\nProblem: \n\n\n\n\nEmojis aren't perfect to convey emotions\n\n\nthere are no datasets with 7 emotions, most of them are (positive / negative)\n\n\n\n\nMy idea is to use weak annotation to help me annotate tweets.\n\n\nHas anyone here used Snorkle (\nhttps://github.com/HazyResearch/snorkel\n), to annotate a dataset ?",
"date": "2019-06-08"
},
{
"vote": 1,
"title": "Understanding the parsed sentence",
"text": "Hi\n\n\nBeginner to NLP. \n\n\nOnce I have parsed and tokenized a sentence what can be done to 'understand' it and turn it into computer commands to act? \n\n\nMay be a million dollar question but I have to start somewhere.\n\n\nThanks\n\n\nRegards",
"date": "2019-06-07"
},
{
"vote": 1,
"title": "Praat - Error when processing Pitch of .wav file: â€œminimum pitch must not be less thanâ€¦â€",
"text": "I've been Praat scripting for only a few weeks now, so bear with me. I'm trying to extract the pitch contours of each spoken word in a corpus of annotated speech data. I can extract the pitch contours of the whole long sound file no problem, and I can chomp the .wav file into smaller .wav files of a single spoken word, but when I try looping through the smaller .wavs I frequently get this error:\n\n\n>To analyse this Sound, 'minimum pitch' must not be less than [some number].\n\n\nwhere that number seems to be pretty arbitrary.\n\n\nIs there a good way to dynamically change the parameters for\n \nTo Pitch:\n \nbased on the minimum pitch for a given .wav?\n\n\nHere's the code:\n\n\n#initialize counter \ni = 0  \n#loop through .wav files \nfor current_file from 1 to file_count    \n    i = i + 1    \n    select Strings list\n    filename$ = Get string... current_file\n    Read from file... &#039;source_directory$&#039;/&#039;filename$&#039;\n    appendInfoLine: string$(i)+ &quot;/&quot; + string$(file_count) + &quot; &quot; + filename$\n\n    To Pitch: 0.1, 75, 600\n\n    #create pitch file\n    pitch_file$ = filename$ - &quot;wav&quot; + &quot;pit&quot;\n\n    #write to pitch file\n    no_of_frames = Get number of frames\n    for frame from 1 to no_of_frames\n        time = Get time from frame number: frame\n        pitch = Get value in frame: frame, &quot;Hertz&quot;\n        appendFileLine: pitch_file$, &quot;&#039;time&#039;,&#039;pitch&#039;&quot;\n    endfor\nendfor",
"date": "2019-06-06"
},
{
"vote": 8,
"title": "Recommendations on how to analyse the differences in speech between people/groups of people",
"text": "I have two large text files from political parties. I want to see how they use language differently. \n\n\nI initially just looked at the frequency of words used however the results were disappointing. I'm not sure if sentiment analysis would be useful? Are there any NLTK concepts you would recommend me learning to extract some useful differences?",
"date": "2019-06-05"
},
{
"vote": 3,
"title": "How to fix duplicate content distribution bias in NLP training tasks?",
"text": "Hi everyone,\n\n\nI have a question about document, paragraph, sentence and clause distribution bias in training tasks.\n\n\nIn lots of NLP training tasks, the dataset is coming from either one source (e.g: Wikipedia) or an ensemble of controlled, unique sources (e.g: BooksCorpus).\n\n\nWhat I'm wondering is when you have an ensemble of sources that lack this publishing and copyright control like websites, how can you remove the bias of duplicate articles, large citations, etc.\n\n\nRemoving the tails of the distribution or even applying outlier detection would only remove the worst offenders. The produced word vectors would still be heavily biased or influenced by the non-uniqueness of the content though.\n\n\nExact hashing could be used to remove duplicates. But duplication isn't always exact and could also be at the document, paragraph, sentence or clause level so not sure how fine-grained that check should be.\nEspecially since nearby sentences are important for context analysis.\n\n\nLocal sensitive hashing and string distance algorithms can be used to find similar pieces of text but the granularity question still applies with the added problems that a \"canonical\" version has to be picked as well as a threshold distance after which the content is considered to be original again.\n\n\nThanks in advance for your help!",
"date": "2019-06-05"
},
{
"vote": 3,
"title": "Building an end-to-end speech-to-text model for traditional Chinese",
"text": null,
"date": "2019-06-05"
},
{
"vote": 7,
"title": "CX_DB8 - A state of the art, biasable, word-level extractive summarizer",
"text": null,
"date": "2019-06-04"
},
{
"vote": 1,
"title": "CX_DB8 - A state of the art, biasable, word-level extractive summarizer Universal Sentence Encoder, Flair)",
"text": "[deleted]",
"date": "2019-06-04"
},
{
"vote": 3,
"title": "What the mandatory first steps in natural language processing projects?",
"text": "In numerical analysis, there is many ways to plot data to have a first overview of the content of a given dataset using statistics e.g. min, max, avg, deviation... For graphical data, there is various way to visualize and well-known algorithms that provide information about the nature of the data.\n\n\nRegarding texts is more fuzzy to me what I can do to quickly gain insights about the content of a text or corpus without reading all of it.\n\n\nI think about computing ngrams and doing statistics about them. I think about word clouds. What else?\n\n\nWhat are the first steps you take in your projects?",
"date": "2019-06-04"
},
{
"vote": 2,
"title": "predicting Y label using CuDNNLSTM",
"text": "I want to use CuDNNLSTM to predict the Y label. I have a dataset and I want to use CuDNNLSTM to predict the code. I am considering the sentences as X label and the codes as Y label. the model is actually giving probability matrix of every class. I want to know 1. How can I predict the actual sentence code 2. The dataset is somewhat this kind of:\n\n\nGoogle headquarters is in California 98873 Google pixel is a very nice phone 98873 Steve Jobs was a great man 15890 Steve Jobs has done great technology innovations 15890 Microsoft is another great giant in technology 89736 Bill Gates founded Microsoft 89736\n\n\n\nI took help from this link: \nhttps://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n\n\nThe below code I am using predicts the probability matrix, I want to know how can it predicts the actual sentence code. Also, can we use tfidf vectorizer?\n\n\nmy code:\nhttps://www.codepile.net/pile/P8A7Yr6Z",
"date": "2019-06-03"
},
{
"vote": 2,
"title": "Language Detector for social media Language Detect - [Google's Compact Language Detector 3] in Kotlin",
"text": "[removed]",
"date": "2019-06-03"
},
{
"vote": 2,
"title": "Back-Translation model for German and English",
"text": "Hey there,\n\n\ndo you know of pre-trained models for back translation between German and English? I am aware that there are ways in which one can include a monolingual corpus into the training of a machine translation model (which is often referred to as back translation). What I am looking for is a model that would allow me to generate new, semantically similar German texts by translating them into, say, English and then back to increase my training corpus with examples that are similar to the existing ones, but with slight variation in sentence structure and choice of words.\n\n\nObviously, I could just stick together two separate models (one for translating the original texts into English and one for translating the English version back to the original language), but it would be great if you knew of sth I could use.\n\n\n&#x200B;\n\n\nI'd very much appreciate any kind of help!",
"date": "2019-06-03"
},
{
"vote": 4,
"title": "How to use BoTorch to tune the hyperparameter of a one layer LSTM model for text data?",
"text": "Does anyone have an example on how to fine-tune parameters of a model with BoTorch ?",
"date": "2019-06-03"
},
{
"vote": 0,
"title": "Learn to write another language",
"text": null,
"date": "2019-06-01"
},
{
"vote": 2,
"title": "A question on Mihalcea's TextRank",
"text": "I am trying to wrap my head around textRank and would appreciate any input people might have on this. The formula for text rank is as follows WS(V_i) = (1 - d) + d(sum(V_j) in (V_i) * W-ij/V_k out(V_j) W_jk * WS(V-j)  .. or ahem here ( \nhttps://raw.githubusercontent.com/iamprem/temp/master/assets/tr_form.png\n ) might be a little clearer. \n\n\n&#x200B;\n\n\nSo as I understand it WS(V_i) is the weight of the node, which is updated iteratively.  WS(V_J) is the connected node, these values are changing as the iterative updates to the weights finds a level point. Presumably w_ij is the weight that is assigned to them , informed from whatever weighting scheme you are using. I think in the original paper she was saying the number of times they connect in a given sliding window. In and out is related to a directed graph , the number of nodes feeding into the variable. But what in the hell is W_jk ? \n\n\n&#x200B;\n\n\nIn a worked example. Lets say we have a  sentence \n\n\n sent = [ a , b, c , d, b,  c] \n\n\nc has 3 incoming and 3 out going nodes. \n\n\nWhere does W_jk relate to C ? \n\n\n&#x200B;\n\n\nHopefully a few people have been engaging with this and have some insight, would love to hear peoples take on this as the paper is pretty unclear on a few points. Thanks",
"date": "2019-06-01"
},
{
"vote": 11,
"title": "Confused about TF-IDF",
"text": "I don't quite understand the need for the IDF piece of TF-IDF. I get that it can give you a measure of how \"unique\" a word is to a document, but isn't it constant for all documents given the same word? If so, that seems to make it kind of meaningless.\n\n\nHere's the example from \ntfidf.com\n:\n\n\n>Consider a document containing 100 words wherein the wordÂ \ncat\nÂ appears 3 times. The term frequency (i.e., tf) forÂ \ncat\nÂ is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the wordÂ \ncat\nÂ appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n\n\nIf you search for \"cat\", all 10 million documents have the same IDF score for \"cat\". So in the context of ranking, how is it helpful?",
"date": "2019-06-01"
},
{
"vote": 4,
"title": "Can I use my own tokenizer with BERT?",
"text": "[deleted]",
"date": "2019-06-01"
},
{
"vote": 1,
"title": "Really stupid question but can somebody explain a point from this one slide to me? (related to word2vec training)",
"text": "https://i.imgur.com/TaLH1DR.png\n\n\nThe second last bullet point, how does each word have two vectors? Does it mean that when the word is the context/center word? In terms of surrounding words its bound to repeat loads of times but its only going to be the context word one when we're creating that type of one-shot vector, no?",
"date": "2019-05-31"
},
{
"vote": 1,
"title": "Need to webcrawl for a bunch of data in different categories, have never webcrawled before and have no idea where to start, what do I do?",
"text": "[removed]",
"date": "2019-05-31"
},
{
"vote": 2,
"title": "Using Transformer Models to Generate Comments from Titles",
"text": null,
"date": "2019-05-31"
},
{
"vote": 2,
"title": "Is calculus useful for NLP?",
"text": "Iâ€™ve read that probability theory and linear algebra are the primary workhorses of NLP technologies, but what about calculus? Is there some role for calculus concepts like limits, derivatives, integrals, or other when creating or reading research about NLP tech? How specifically, if at all, has knowing calculus helped you in doing anything with NLP?\n\n\nIâ€™m learning calculus just for fun, and am just curious to know whether it might be helpful in mastering NLP. So Iâ€™m just hoping this thread will elicit some general discussion about that. Would love to hear your experiences. The more concrete and specific, the better. Thanks!",
"date": "2019-05-31"
},
{
"vote": 3,
"title": "Context vectors in Word2vec",
"text": "What are context vectors in Word2vec and how are they obtained?",
"date": "2019-05-31"
},
{
"vote": 1,
"title": "18 best blogs need to read every translator",
"text": null,
"date": "2019-05-31"
},
{
"vote": 1,
"title": "Best 18 blogs need to every translator read-First language solutions",
"text": null,
"date": "2019-05-31"
},
{
"vote": 3,
"title": "Complementary NLP books for Sentiment Analysis study",
"text": "I am relatively new to the area of sentiment analysis. I recently purchased Liuâ€™s book â€œSentiment Analysis: mining sentiments, opinions, and emotionsâ€. In the preface it is stated that:\n\n\nâ€œThis book also does not detail the basics of linguistics or natural language processing, such as part-of-speech tagging, syntactic parsing, shallow parsing, and grammar.â€\n\n\nWhat are the best complementary books on these basic topics?\n\n\nThanks",
"date": "2019-05-30"
},
{
"vote": 5,
"title": "Ask AI: Is Bob Dylan an Author or a Songwriter?",
"text": null,
"date": "2019-05-30"
},
{
"vote": 2,
"title": "Recorded Future acquired for $780M... by a VC",
"text": null,
"date": "2019-05-30"
},
{
"vote": 8,
"title": "What are some decent quality open-source TTS models ?",
"text": "Are there currently any open source TTS models that give results somewhat close to the bleeding edge in terms of proprietary technology (Alex, google voice... etc).\n\n\n&#x200B;\n\n\nUp until now I've found tacotron, which seems to be not that well maintained, built by some google guys, and a pain to actually get running (+ a host of libaries based on it, such as mozilla's TTS)\n\n\n&#x200B;\n\n\nI've also had luck with: \nhttps://github.com/r9y9/deepvoice3_pytorch\n , but the output quality of the pre-trained models is lacking to say the least.\n\n\n&#x200B;\n\n\nSo, my question is, are there any high quality OS TTS models out there that one can easily use ? They don't necessarily have to be based on sequential CNN, I just gave two such models as examples since that's, to my very amateurish knowledge, the infrastructure which is most popular and give the best results presently.\n\n\n&#x200B;\n\n\nIf the code is actually well-maintained enough that outside contributors can read and work on it, and if there's an active community around the library, that would be a huge plus.",
"date": "2019-05-30"
},
{
"vote": 9,
"title": "State of Smart Question Generators?",
"text": "What is the current cutting edge in smart question generating? In other words, if I give a program a paragraph, will it be able to create the questions implied from that paragraph in the same way a human can? If not, what are the current bottlenecks. What resource would you suggest for further learning?",
"date": "2019-05-29"
},
{
"vote": 2,
"title": "Document/Sequence vector generation with multihead self attention",
"text": "I coded this model, implementing Transformer(Vaswani et.al.)'s encoder part only, with fully connected layers following that, basically its partial BERT(contextual attention from both sides of a given token), but the objective is to rank a given sequence, trained end to end. The learnt attentionised vectors can be transferred to anything ranging from sentiment analysis to clustering sequences or documents. I further aim to do heirarchical clustering of the inputs.\n[Here is the git link]\nI would like for people on this subreddit to go through it and suggest improvements\nThanks",
"date": "2019-05-29"
},
{
"vote": 8,
"title": "[R] What the Vec? Towards Probabilistically Grounded Embeddings",
"text": null,
"date": "2019-05-29"
},
{
"vote": 5,
"title": "Four Mistakes You Make When Labeling Data",
"text": null,
"date": "2019-05-28"
},
{
"vote": 9,
"title": "[R] Gradient boosting research papers from the last 25 years",
"text": null,
"date": "2019-05-28"
},
{
"vote": 8,
"title": "Looking to volunteer, either academic, or Non-profit. Or even non-ML organizations.",
"text": "I am looking to volunteer approx ~20 hours of my time to Machine Learning research and tools. \n\n\nIdeally it would be for a Machine Learning organization, either academic or non-profit, but I am open to even non-ML organizations as well. If they had a lot of interesting text data, I would be very open to volunteering, and if the data was interesting enough, I could pull others to help me. But how would one go about identifying organizations with interesting text data.",
"date": "2019-05-28"
},
{
"vote": 3,
"title": "Need opinions: I'm trying to obtain low-key entity-related information...",
"text": "But the problem is that the entities aren't popular, so they do not exist in either DBpedia or wikipedia. I'm talking about identifying names of restaurants, no-name bands, start-ups, and such.\n\n\nRight now my approach consists of using textacy (for Python, built over spaCy) which has a keyterm extraction algorithm called SGRank, which returns n-gram keyterms (composite keywords, seems to work really well), then I'll do a google search with an API, which allows 100 queries per day for free and get the results' snippets. I might actually use the paid service, but I'd rather not depend on it.\n\n\nOptionally, I could try and pass context to select the likely results, but I haven't thought too much about that. One idea would be computing cosine similarities between keyword embeddings with the source context and the results'.\n\n\nAdditionally, I wanted to get the google's knowledge graph information, but I don't know how to do that. If possible, besides the snippets which describe the entities enough to get more related keywords, I'd like to be able to extract geo-coordinates, job/roles, who produced which products, and so on.\n\n\nFor reference, the types of text I work with are news articles and subtitles (mostly interviews). Articles are very concise and objective, I could give up retrieving more entity info from these as they almost invariably already contain descriptions within them, but the subtitles are a pain to deal with, also because of the expressions / ways of saying things.\n\n\nThe information I'm extracting will be used to perform searches, so if I input honey, I might wanna get companies and people related to the honey-making business, even if honey isn't directly mentioned in the text.",
"date": "2019-05-27"
},
{
"vote": 4,
"title": "NLP techniques and the balance between model accuracy and explainability",
"text": null,
"date": "2019-05-27"
},
{
"vote": 8,
"title": "Looking for Approaches to build/Expand Ontologies.",
"text": "I have a set of data and I would like to build Ontologies from the data. I found many existing Ontologies which were really useful for me but I have to expand those for my vocabulary. An Unsupervised approach would be a cherry on the top.\n\n\n&#x200B;\n\n\nAre there any good papers or blogs w.r.t this?",
"date": "2019-05-27"
},
{
"vote": 7,
"title": "Process to build a similar sentence predictor.",
"text": "I am looking to build a model that can identify similar sentences based on an input sentence.\nOne thing I am not understanding is how to make it where terms can change but still be of the same type .\nIe  In 2018 there was a record number of crimes.\nVs In 2019 crime fell from record levels.\nWould the 2018->2019 be recognized as a similar type or be treated as unique terms and thus be weighted differently?  Figure same process would apply for proper nouns or Named Entities to make it comparable across sources.\n\n\nI am thinking a classifier would have to be trained on word types and terms rather than exact terms.\nIs there better way to do this?",
"date": "2019-05-25"
},
{
"vote": 5,
"title": "What has been your experience using multilingual BERT",
"text": "Hi,\n\n\nI have a multilingual data(8 languages) and my downstream task is classification.\n\n\nI was wondering if some one has already used multilingual bert and what were your results ? Does it wok ?\n\n\nI know that it supports more than 100 languages and that i cannot modify the vocabulary without per-training it again. But i know that i will have only 8 languages in my data. In your experience what is the best way to use it, do i just take the pre trained model and fine tune it on the downstream task, or does it help if i for e.g. continue the per-training(Masked-LM and next sentence tasks) on a big corpus that contains only this 8 languages and then fine-tune on the downstream task ?\n\n\nPlease share any experience you had with multilingual bert.",
"date": "2019-05-24"
},
{
"vote": 9,
"title": "Besides summarization, are there any other areas of NLP where Reinforcement Learning plays a significant role in SoTA architectures?",
"text": null,
"date": "2019-05-24"
},
{
"vote": 13,
"title": "How to use LSTM or CNN or any other method for classifying very long text sequences?",
"text": "I am doing document level text classification and the longest is ~80,000 words. I used doc2vec (gensim) for the classification task but I am not convinced with the results. I want to use LSTM/CNN but setting the max_sequence_length to 80000 is not practical. I am using the tokenized format of the document such that all the punctuations (including the period) are removed and so all sentences are like below:\n\n\nDocument\n: Place 25 items in the box. Look for the adjacent ones.\n\n\nTokenized\n: place # item in the box look for the adjacent ones\n\n\nI am also wondering why I need to use max_sequence_length if I use CNN because CNN uses a sliding window for feature extraction, unlike LSTM. Or maybe if I could chunk the sequence but I am unclear as to how to do that.\n\n\nI also tried \nHAN\n but not convinced by the results. (compared to the baseline in the paper I am referring to).\n\n\nAnother thought would be to just use CNN to extract features from the long document but again this feature vector will be quite long too.\n\n\nAny suggestions? Thanks a lot!",
"date": "2019-05-22"
},
{
"vote": 0,
"title": "Context-Sensitive Review Generation",
"text": "I'm working on a project in witch I have to summarize reviews from a same hotel into a generated meta review. I wanted to use some Seq2Seq-like networks that peform well in the task of Single Document Summarization. However, as I don't have any already well formed meta-reviews as a training set, I gave up on this technique.\n\n\nI have rather implemented a summarization technique using word graphs, paths extraction, and feature-based clustering that gives some good results. However, the format of the summary that I get is for example :\n\n\n- Great location in Prague\n\n\n- The staff is really friendly and helpful.\n\n\n- The continental breakfast is just wonderful.\n\n\n- etc\n\n\nIt is a listing of summaries of each category among [\"location\", \"catering\", \"room confort\", staff\", ...] if tere is enough reviews to make it. As this technique is quite limited, the sentences I get are rather poor syntactically. What I want to do now is to generate a single sentence with a better syntax, such as : \n\n\n\"This well-located hotel in Prague offers a wonderful continental breakfast and a really friendly staff.\"\n\n\n(It is of course an idealistic example)\n\n\n&#x200B;\n\n\nThus, do you know some text generation algorithms that can be fed with some context, such as the sentences I've generated with the word graphs ?\n\n\n&#x200B;\n\n\nThank you!",
"date": "2019-05-22"
},
{
"vote": 3,
"title": "Beginner question: Only positive dataset",
"text": "I've got a dataset where some of the data are labelled as True, but the remaining dataset is not labelled. Is it possible to create a model that predict if the unlabelled data is true or not. In other words is it possible to make a classification of only one type?\n\n\n&#x200B;\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nExample of data:\n\n\n\n\n\n\n\n\nIndex\n\n\ndata\n\n\nLabel\n\n\n\n\n\n\n\n\n1\n\n\n\"bla bla bla\"\n\n\n1\n\n\n\n\n\n\n2\n\n\n\"bla bla bla\"\n\n\n?\n\n\n\n\n\n\n3\n\n\n\"bla bla bla\"\n\n\n1\n\n\n\n\n\n\n4\n\n\n\"bla bla bla\"\n\n\n1\n\n\n\n\n\n\n5\n\n\n\"bla bla bla\"\n\n\n?",
"date": "2019-05-22"
},
{
"vote": 3,
"title": "Help Needed regarding extracting skills from Job Description",
"text": "I am working on a project which aims to extract skills from a given job description. The end goal is to extract the relevant skills from the Job Description which in turn will help the job seekers to find the suitable jobs. \n\n\nI have been trying out various approaches both supervised and unsupervised. I am unable to get any good results using NLP. So I have for now tried to detect the skills in a hardcoded manner which is not good for the long run as the industry keeps on evloving so it is imposible to maintain the dictionary with all the latest skills.\n\n\nAny help regarding this would be highly appreciated.",
"date": "2019-05-22"
},
{
"vote": 5,
"title": "Topic modeling: Similar topics",
"text": "I applied a topic model algorithm on a corpus with feedback from gig workers and employers (temporary work).  Iâ€™m getting plausible but several very similar topics (many top 10 words in common). It doesnâ€™t matter if I define 8 or 48 topics. Does anyone have an idea how to handle this situation?",
"date": "2019-05-21"
},
{
"vote": 3,
"title": "Comparing Sentiment Analysis Libraries for Arabic Language",
"text": "https://medium.com/@mapmeld/sentiment-analysis-in-arabic-f08cd20a76d\n\n\nWith most NLP code samples focused on English, I went looking for projects which analyze Arabic Tweets. For this project I scraped 11k Tweets to and from @NetflixMENA and tried analyzing them with 4 projects that I found on GitHub.\n\n\nRequirements were:\n\n\n- open source\n\n\n- easy to install with current versions of Python and sklearn\n\n\n- including training data (because I don't know enough Arabic)\n\n\nMost options that I found use NLTK to tokenize the text and then evaluate many sklearn classifier options (Random Forest, Linear SVC, etc). I identified \none project\n that seemed the best (in part because it has Tweets and emojis in its training data), and an otherwise rigorous \nproject which needs to be updated\n for newer versions of scikit.",
"date": "2019-05-21"
},
{
"vote": 10,
"title": "Free Wolfram Engine for Developers",
"text": null,
"date": "2019-05-21"
},
{
"vote": 2,
"title": "How Dementia Affects Conversation: Building a More Accessible Conversational AI",
"text": null,
"date": "2019-05-21"
},
{
"vote": 2,
"title": "Visualize Attention in BERT for SQuAD Q&amp;A Dataset",
"text": "Hello, this is my first post on this forum. I have created a self-contained Colab Notebook to visualize attention weights for BERT (fine-tuned on the SQuAD 2.0 Q&A dataset).  You can find the notebook \nhere\n.\n\n\n&#x200B;\n\n\nHere is  a sample visual (x-axis is the Question and y-axis is the Question+Context) - darker the color, higher the probability. (While this visual appears pretty much blank, I think it's expected given the top answer to this Question is one word, \"France\".)\n\n\n&#x200B;\n\n\nHope you find it useful, and I welcome any feedback!\n\n\nhttps://preview.redd.it/o9po0xmkegz21.png?width=4457&format=png&auto=webp&s=73f661e3e164317353a56eefa85e483e42d57b9e",
"date": "2019-05-20"
},
{
"vote": 35,
"title": "How do you keep up?",
"text": "Rather than seeking specific answers, I'd just like this post to spark some general discussion. It is aimed primarily at NLP professionals, not just enthusiasts.\n\n\nThe NLP field changes so quickly, with paradigm shifts bringing significant technological step-changes every year. Word2Vec, PyTorch, TensorFlow, MXNet, BERT, the list of important tools is ever-expanding. Unfortunately, these tools tend to be fairly complex, so require time to understand master. But as soon as I learn one, I feel like it's already outdated and the industry has moved onto the next big thing.\n\n\nSo I'm wondering how other NLP professionals cope with the field's rapid rate of evolution.",
"date": "2019-05-20"
},
{
"vote": 1,
"title": "Image Recognition with Convolutional Neural Networks: Simply Explained",
"text": null,
"date": "2019-05-19"
},
{
"vote": 15,
"title": "[arXiv] What can linguistics and deep learning contribute to each other?",
"text": null,
"date": "2019-05-18"
},
{
"vote": 3,
"title": "translation fails 1: fun with Unicode whitespace chars",
"text": null,
"date": "2019-05-18"
},
{
"vote": 12,
"title": "FranÃ§ois Chollet trolls GPT-2",
"text": null,
"date": "2019-05-17"
},
{
"vote": 25,
"title": "Microsoft makes Googleâ€™s BERT NLP model better",
"text": "Anyone care to explain in simple English why and how is the MT DNN useful or more effective?\n\n\nhttps://www.google.com/amp/s/venturebeat.com/2019/05/16/microsoft-makes-googles-bert-nlp-model-better/amp/\n\n\nhttps://github.com/namisan/mt-dnn",
"date": "2019-05-17"
},
{
"vote": 8,
"title": "Google AI yesterday released its latest research result in speech-to-speech translation, the futuristic-sounding â€œTranslatotron.â€",
"text": null,
"date": "2019-05-16"
},
{
"vote": 11,
"title": "Tokenize and Transliterate Japanese, Chinese, Korean",
"text": "Hello. We're making a Chrome extension for studying languages with Netflix (\nhttp://languagelearningwithnetflix.com\n). The extension needs to tokenize subtitles so we can feed individual words to a dictionary API (Microsoft, not entirely happy with the quality, but it works for any language pair, and affordable options are a little thin on the ground).\n\n\nAnyway, tokenization for most languages is fairly straightforward, it's basically break-on-space, while handling some special cases. Japanese, Chinese, no spaces! So our dictionary breaks. ( see \nhttps://www.reddit.com/r/ChineseLanguage/comments/bijwok/language_learning_with_netflix_chinese_dictionary/\n :-)\n\n\nWe tried ( \nhttps://wanakana.com/\n  ), which is a nice small js library that can run on the frontend, but it doesn't seem to be very aggressive in breaking up the subs, the returned tokens sometimes correspond to 3-4 English words (according to machine translation).\n\n\nGoogle has an API, \"analyzesyntax\", that does tokenisatiion, but we need to make a seperate call for every sub (~600 or so per episode), which is a bit fiddly and a bit costly. \n\n\nAfter some research, Mecab seems to be where it's at for Japanese tokenization, so we'll try get that set up as a Amazon lambda function, so we can call it.\n\n\nMaybe some of you nice people have some ideas about the best way to go about tokenization for Korean (Mecab support Korean?) and Chinese? There's also the question of transliteration, microsoft have an API that we'll try out.",
"date": "2019-05-16"
},
{
"vote": 5,
"title": "[Resource] Chinese Language Datasets for NLP",
"text": "[deleted]",
"date": "2019-05-16"
},
{
"vote": 1,
"title": "An Algorithm Predicts the Emotion of Alan Turing",
"text": null,
"date": "2019-05-16"
},
{
"vote": 9,
"title": "Worth reading: The Curious Case of Neural Text Degeneration.",
"text": null,
"date": "2019-05-15"
},
{
"vote": 1,
"title": "I wordnet the best tool to link taxonomies?",
"text": "I am tasked in linking taxonomies to texts.\n\n\nSuppose a text mentions \"tea\". With wordnet, I could define find the synsets related to \"tea\" and check if their hypernyms contain any of the synsets on my taxonomies. (such as \"beverage.n.01\" or \"drinks.n.01\", writing their IDs from memory).\n\n\nBut is wordnet the best tool for this, or should I look into Knowledge DBs such as DBpedia? KDBs look like they contain more worth information (like products' names), but their usage appears to be very complicated, plus the information is structured in an 'organical' manner it's hard to generalize patterns. From what little I understand about it, I mean.\n\n\nWith wordnet, I can take full advantage of hypernyms (more general concepts) and hyponyms (more particular concepts) to try and group and also subgroup taxonomies and and see if some thing belongs to a tree.",
"date": "2019-05-15"
},
{
"vote": 28,
"title": "We release OPIEC: the largest Open Information Extraction corpus to date (341M triples), rich with metadata",
"text": "We release OPIEC: the largest Open Information Extraction corpus to date (341M triples), rich with metadata (conf. score, syntax, semantics, gold Wiki entity links,...) Paper: \nhttps://arxiv.org/abs/1904.12324 \nData (and all the other resources related to the corpus): \nhttps://bit.ly/2JOLNVp \nCode: \nhttps://github.com/gkiril/OPIEC",
"date": "2019-05-15"
},
{
"vote": 4,
"title": "Create your own text classification models based on BERT with 1 API call",
"text": "https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6",
"date": "2019-05-15"
},
{
"vote": 1,
"title": "Best language translation apps to ease everyoneâ€™s life",
"text": null,
"date": "2019-05-14"
},
{
"vote": 2,
"title": "GPT-2 Among the philosophers: Prompting a language bot with philosophical content",
"text": null,
"date": "2019-05-14"
},
{
"vote": 4,
"title": "Simple yet tough-to-beat sentence embedding baseline (Paper Summary)",
"text": "This post talks tries to explain the approach taken by the paper for generation of sentence embeddings. Read at\n\n\nhttps://prakhartechviz.blogspot.com/2019/05/baseline-sentence-embeddings.html\n\n\nPlease feel free to share your thoughts.",
"date": "2019-05-14"
},
{
"vote": 5,
"title": "play with GPT-2",
"text": null,
"date": "2019-05-14"
},
{
"vote": 5,
"title": "EAL: Toolkit &amp; Dataset for Entity-Aspect Linking",
"text": "We release EAL: Toolkit & Dataset for Entity-Aspect Linking. Input: a sentence; Output: the most relevant aspect for each mentioned entity (joint work with \n@\nf_nanni\n) Paper: \nhttps://ub-madoc.bib.uni-mannheim.de/49596/1/EAL.pdf \n Demo: \nhttp://tools.dws.informatik.uni-mannheim.de/eal \n Data: \nhttps://federiconanni.com/eal-d/",
"date": "2019-05-13"
},
{
"vote": 8,
"title": "MinScIE: Open Information Extraction system for scientific literature",
"text": null,
"date": "2019-05-13"
},
{
"vote": 8,
"title": "ACL 2019 accepted paper list",
"text": "The acceptance notification deadline for ACL is today so I was wondering when they'd put up a list of accepted papers.",
"date": "2019-05-13"
},
{
"vote": 13,
"title": "Microsoft Research Asia: Past, Present, and Future of NLP",
"text": null,
"date": "2019-05-12"
},
{
"vote": 2,
"title": "Career Prospects for Speech and Language Processing MSc (opportunities with NGOs?)",
"text": "Prospective NLP-er here. \n\n\nI'm from a languages background currently figuring out my career pathway. I'm looking to retrain in a more quantitative/technical area as I've missed that since finishing my Maths A Level and studying a BA. It also seems to me that quantitative/technical qualifications are much more in-demand in the jobs market than anything my generalist background can offer. (I'd also like to get into international development as a career and I know the market is saturated with many well-intentioned generalists lacking the right technical skills).  \n\n\nI've seen this 'Speech and Language Processing' degree at Edinburgh which looks really interesting ( \nhttps://www.ed.ac.uk/ppls/linguistics-and-english-language/prospective/postgraduate/msc/speech-language-processing/programme-overview\n ). I volunteer with Translators Without Borders and looking at their careers page I can see they're searching for NLP experts - I wonder if there are many of these roles in crisis response and international development?  \n\n\nDoes anyone know how likely a NLP graduate is to be employed in their field? Do you think this degree would make me industry-ready? If many of them work on humanitarian and social projects?   \n\n\nI'm a bit worried by how very specific the training would be - meaning that if I were unable to make a career in NLP, the masters I'd invested in wouldn't be of much use in any other spheres. So I wanted to make sure it was a good bet before applying.   \n\n\nSo far, I have an offer in France for Econ conversion and I'm thinking of applying to a generalist International Development MSc, too. Econ especially would give me a lot of options if I didn't immediately get a role as a development economist. But NLP would put my linguistics skills to use and just sounds so much fun tbh.   \n\n\nAny insights at all appreciated, I can't find much info at all upon Googling.",
"date": "2019-05-12"
},
{
"vote": 1,
"title": "Working with ChatScript - Questions",
"text": "Originally, I was planning to post this on chatbot.org but as I have to wait for them to let me sign up (they're super paranoid about bots joining their forum) I figured I'd post my question here as well. \nIn the context of our project, we have begun works on building a chatbot - using ChatScript. I have a few questions regarding ChatScript in particular. \n\n\nI'll just post whatever I was going to ask them in that forum: (post actually beings here then)\n\n\nHello! \n\n\nI am new to Chatscript. I am trying to make a Chatbot that (optimally) uses an interface with Postgresql\nso I can run a product recommendation. \n\n\nI have a few questions: \n\n\n\n\nIs it alright for me to jump from one top file to another by using gambit? For instance, I have the following line:\n\n\n a: AFFIRMATIVE (~yes) In diesem Fall kann ich dir dabei helfen, was aus dem Uni-Shop zu suchen, $kunde. gambit(~keyEx)\n\n\n\n\n\nHow do I map words that consist of multiple words as one in a concept?\nFor example, I have:\n\n\n concept: ~example[word one wordtwo]\n\n\n\n\n\n\n\nIn this case, how would I make it so \"word 1\" is recognized as one unit - instead of being \"word\" and \"1\"?\nDo I maybe do this by with:\n\n\n    ~example[&quot;word one&quot; wordtwo]\n\n    \n\n\n\n\n\nHow do I include an already existing database (PostgresSQL) in ChatScript? So far, we only know how to build it ourselves\nbut we want to use an exisiting one.  \n\n\n\n\nThis is the most complicated one I figure -\nHow exactly would a keyword extraction work in ChatScript? If you've read the other points: I want to run a query \nin a database. My first idea was to find any words that appear in one of my columns (Name, Purpose, Color) and \nbuild concepts around it.\n\n\n\n\n\n\nImagine: Say, I have a database for vehcicles called vehicle, with the following columns: Power, Color, Seats\n\n\nSo if my user input was: I want a red car with 2 seats and at least 250 horse power \n\n\nmy query in postgresql would look like this:\n\n\n    select *\n    from vehicle \n    where power &gt;= 250 and color = &#039;red&#039; and seats = 2\n\n\n\nMy approach to this would have been like this: Make a concept for every distinct entry in every column\n\n\nLike so:\n\n\n    concept: ~colors [green red blue]\n    concept: ~seats [2 4 6]\n\n\n\nThe next step to take would be to look for said \"keywords\" in user input. To be more precise:\nThe user says what he wants, like \"I want a red car with 2 seats and at least 250 horse power\". \n\n\nQuestion 1: How do I make the bot look for matching words from the three concepts at the same time?\nSo in \"I want a red car with 2 seats and at least 250 horse power\" it would have to look for matching \nwords in (\nseats) and (\ncolors). It does not really have to be concepts - I am very new ChatScript and\nif there is a better approach to this than using concepts, please let me know.\n\n\nQuestion 2: Assuming I get the bot to  match user input with multiple concepts, what \nI'd like to do next is to save said matches in words such as $color and $seats. I've done this \nwith $name before but I wonder how that'd be done.",
"date": "2019-05-11"
},
{
"vote": 5,
"title": "Semantic parsing using Lojban â€“ On the middle ground between semantic ontology and language",
"text": null,
"date": "2019-05-11"
},
{
"vote": 7,
"title": "Converting English numbers (in words) into digits",
"text": "I'm looking for libraries (or algorithms) to parse and convert English words in a sentence/phrase into digits.\n\n\n&#x200B;\n\n\nFor e.g\n\n\nMy dog is five years old -> My dog is 5 years old\n\n\nI have two and a half million rocks -> I have 2500000 rocks\n\n\nThe elephant weights twenty two point five pounds -> The elephant weights 22.5 pounds\n\n\nI have three green balls, two red balls, and 100 yellow balls -> I have 3 green balls, 2 red balls, and 100 yellow balls\n\n\n&#x200B;\n\n\nThe closest thing I have found is this ruby library but it doesn't parse from an input sentence: \nhttps://github.com/markburns/numbers_in_words\n\n\n&#x200B;\n\n\nAnybody with recommendations?",
"date": "2019-05-10"
},
{
"vote": 11,
"title": "[P] Keras BERT for Medical Question Answer Retrieval using Tensorflow 2.0 ! With GPT-2 for Answer Generator. Pip installable. Weights/Data readily available. Reduced version for Google Colab instantly available in premade notebook.",
"text": null,
"date": "2019-05-10"
},
{
"vote": 8,
"title": "Merhods of automatic text labeling",
"text": "What are the most popular (research and industry) methods for automatic labeling of simple text ? In manner of full automation or labeling recommendation for the user ?\n\n\nMy texts are sentences like \"The boy felt and broke his left arm \" and the label will be \"broken arm\" . I have 500k samples for each dataset i'm working with and unknown number of labels before i start. I am aiming to find a way that will reduce the labeling into 5000-10000 samples for each data set like that .\n\n\nI have played with google's universal sentence encoder to find similar labels to similar sentences but i'm looking for more rigorous methods.\nPapers , links and even just open ideas without full proofs will be great as well for me .",
"date": "2019-05-09"
},
{
"vote": 1,
"title": "Concept grouping",
"text": "Hey! So I've been searching around and I can't seem to find anything related to what I want to do. Let's say I'm analyzing sentences, and want to count number of times a country is mentioned. Sometimes 'U.S' pops up, sometimes it's 'USA', sometimes it's 'America', 'United States', etc\n\n\n&#x200B;\n\n\nIs there any library for Python that would tell me, okay, these words are different in writing, but they refer to the same thing. And add them up to the count tally.\n\n\n&#x200B;\n\n\nThanks!",
"date": "2019-05-09"
},
{
"vote": 1,
"title": "[push in the right direction] Finding subjects in documents",
"text": "I am a noob in NLP and I am looking for a solution to recognise sections in a document. My documents have different words for for example a 'subject' or an 'end date' or 'skills' etc. Mostly like a job oppertunity and hence in a narrative way.\n\n\nAnd in each document this can be on a different place. Sometimes it's easy is just gives a colon like:  \"Subject:  bla di bla\"\n or\n\"Skillset: some other bla\"\n\n\n&#x200B;\n\n\nWhat I want do to eventually is based on pre-defined subjects like \"subject\", \"end date\", \"skillset\" etc. I want to filter only those subjects out of a document.\n\n\n&#x200B;\n\n\nSo my question is what kind of NLP task is this called and what techniques I can use the for the best result. The best choice would we unsupervised learning if it is possible and otherwise i need to label the data what is a big dataset.\n\n\n I am experienced with other fields of data science and deep learning but I am very new to NLP. \n\n\nEach push in a good direction is appreciated.",
"date": "2019-05-08"
},
{
"vote": 6,
"title": "Enlarging Labeled Handwritten Character Datasets With Capsule Networks",
"text": null,
"date": "2019-05-07"
},
{
"vote": 0,
"title": "Get a summary of virtually anything in seconds with SummarizeBot",
"text": null,
"date": "2019-05-06"
},
{
"vote": 2,
"title": "Sentence generation by using keywords",
"text": "Hey guys.. I am looking for a sentence generator given a few keywords.\n\n\nI am able to identify facts from text using semantic role labeling and now having the Verb and its arguments as keywords, i would want to create an action sentence. I could go on to write rules by checking the presence of what type of Argument to the verb is present as per Semantic role labels. But this sounds tedious.\n\n\nJust wondering if there exists technique where i could generate syntactically(almost) correct sentences given a few keywords.\n\n\n&#x200B;\n\n\nThanks.",
"date": "2019-05-06"
},
{
"vote": 2,
"title": "Writing Consistency Checker",
"text": "[deleted]",
"date": "2019-05-06"
},
{
"vote": 1,
"title": "Contextual Originality Checker",
"text": "[deleted]",
"date": "2019-05-06"
},
{
"vote": 8,
"title": "Fine tune a chatbot on new questions/intents.",
"text": "[deleted]",
"date": "2019-05-05"
},
{
"vote": 0,
"title": "What software would I use for a text to data program",
"text": null,
"date": "2019-05-04"
},
{
"vote": 1,
"title": "How useful is a BLEU score calculated across 2 datasets?",
"text": "Beginner at NMT here so apologies if this is a very obvious question. \n\n\nSuppose I have a dataset for machine translation.\n\n\nIn practice, suppose researcher R^(A) builds a machine translation model, and randomly splits 300 parallel sentences from datase  for their test set T^(A) and reports BLEU score B^(A).\n\n\n&#x200B;\n\n\nSuppose researcher R^(B) builds a different MT model and randomly splits out 500 parallel sentences from dataset, as their test set T^(B), for their test set and reports BLEU score of B^(B.) \n\n\n&#x200B;\n\n\n T^(A) and T^(B) don't have the same sentences. \n\n\n&#x200B;\n\n\nAre the BLEU scores B^(A) and B^(B) *AT ALL* comparable? Can I say that model A is better then model B based on those scores?\n My (brief) understanding of the BLEU score is No, I can't.   \n\n\nIs that correct?",
"date": "2019-05-04"
},
{
"vote": 1,
"title": "Video &amp; Audio Transcription Tools for NLP",
"text": "[deleted]",
"date": "2019-05-03"
},
{
"vote": 6,
"title": "[Help]Approach to building chatbot?",
"text": "Hello,  \n\n\na friend of mine and I are supposed to build a chatbot.   \n\n\nOur idea was to build a chatbot that helps a customer decide what to buy based on a database. Our products in our sample database have certain features (price, type of article, size etc.) and what we'd like to do is for the bot to recognize these features (what size, what price etc.) in the given user input.   \n\n\nSo: Chatbot greets customer -> User states intent (what they want to buy) and actually finish describing what they want to buy -> Bot analyses input -> convert to SQL -> Query output is what the bot recommends the customer afterwards.   \n\n\nWe'll most likely be using python (friend and I know Java, Ruby, SQL and also things like XML and R)  \n\n\nUnfortunately, neither I nor my partner have really done chatbots in theory. While I have done some googling and come across [this](\nhttps://mlwhiz.com/blog/2019/04/15/chatbot/?utm_campaign=shareaholic&utm_medium=reddit&utm_source=news\n) \nI would still like to ask:\n \n\n\nHas anyone built a chatbot before?   \n\n\nWhat concepts would we need to link into and implement to realize what we want to do?\nWhat would be some (if not all) the steps we need to take to build a bot that acts like this?\nDo you know any useful literature on this topic? My \"googling\" for this was really off, all I found was articles aimed for a more general audience, that explained what a chatbot is.  \n\n\nWhat libraries, plug-ins etc. would you recommend we work with?  \n\n\nThanks in advance!",
"date": "2019-05-02"
},
{
"vote": 2,
"title": "What data formats/pipelining do you use to store and wrangle data which contains both text and float vectors ?",
"text": "I have a lot of data points that contain both text and float embeddings, and it's very tricky to deal with. CSVs take up a ton of memory and are slow to load. But most the other data formats seem to be meant for either pure text or pure numerical data. \n\n\nThere are those that can handle data with the dual data types, but those are generally not flexible. For example, for pickle you have to load the entire thing into memory if you want to wrangle anything. You can just append directly to the disk like you can with hdf5. \n\n\nAlso, any alternatives to Pandas for wrangling Huge datasets? Sometimes you can't load all the data into Pandas without causing a memory crash.",
"date": "2019-05-02"
},
{
"vote": 12,
"title": "I have millions of unlabeled news articles in 3 different languages. What's the best topic classification approach?",
"text": "Hi, I have unlabeled datasets containing millions of news pieces in 3 different languages (italian, german, french). \n\n\nThe first thing that I want to do is topic modelling as the core idea of the project is to analyse the frequency of a particular topic across all the sources. \n\n\nI am a newbie to NLP, and from what I read around guidedLDA could be a good technique to apply to recognize the topic. For instance, let's say I want to measure the frequency of topic X over the total amount of articles T in the time period of interest, I could define X with a set of keywords that define it.\n\n\nThis is a bit the idea that I tough as the most adapt to this case study, but clearly I'll need to build several different classifiers and compare them. (tf-idf, stm, lda, lsa cnn?). \n\n\nWould you have any suggestions on how to recognize a particular topic X (in order to measure X/T)? And in terms of technologies (Python or R), or approaches for big unlabeled datasets?\n\n\nThanks!!",
"date": "2019-05-02"
},
{
"vote": 1,
"title": "Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",
"text": null,
"date": "2019-05-02"
},
{
"vote": 23,
"title": "Resource for German datasets for NLP, translation, sentiment analysis, etc.",
"text": null,
"date": "2019-04-30"
},
{
"vote": 2,
"title": "Anomaly detection",
"text": "Hi,\nI'm developing an nlp classification model, and now it is about  83% accuracy. The thing is that some of  my data might be labeled a wrong way. Are there any methods to visualize it and see if there is any anomalies? I'm using tfidf matrix for data encoding and svm as a model.",
"date": "2019-04-30"
},
{
"vote": 7,
"title": "NLPCraft - open source API to convert natural language into action.",
"text": "NLPCraft\n is an open source library for adding a natural language interface to any applications. Think Amazon Alexa that is developer friendly, works with any private data source, has no hardware or software lock-in while giving you more NLP powers:\n\n\n\n\nDownload\n and Maven/Grape/Gradle/SBT \ninstructions\n\n\nDocumentation\n, \nJavadoc\n, and \nREST APIs\n\n\nExample\n data models\n\n\nLicensed under \nApache 2.0 License\n with \nCommons Clause\n.",
"date": "2019-04-29"
},
{
"vote": 1,
"title": "Is it easier to overfit a complex(multiple layers) or a simple neural network ?",
"text": "EDIT: is it easier to overfit a network with more than with less parameters ?",
"date": "2019-04-29"
},
{
"vote": 1,
"title": "Pretrained english &lt;--&gt; spanish translation?",
"text": "I googled around but couldn't find anything. Is anyone aware of free english <--> spanish translation software? I was poking around with some python libraries that are wrappers around web APIs but burned through the free tiers pretty quickly.\n\n\nI don't need state of the art. Statistical or neural, I don't really care. I just don't want to spend time training my own model if I can avoid it easily enough.",
"date": "2019-04-28"
},
{
"vote": 8,
"title": "Developing interpretable models in NLP?",
"text": "I'm developing a model to predict a binary output given a string, but I'm interested in grouping the data points by the \"reason\" they're expected to have a given output. Imagine the example of restaurant reviews. I would ideally come up with a model that predicts when a review will be bad, along with some latent state information, which I could then analyze by looking over some examples. So maybe the algorithm divides the bad reviews into 3 categories, which correspond roughly to price, poor service, or bad food. I could then build a more interpretable model for detecting reviews that are bad because the service was poor. What models would you consider in such a situation? Is Deep Learning inadequate for this purpose? Do you have any recommended resources? I guess I'm trying to do some form Topic Modeling in conjunction with classification, and then reduce each topic to a more interpretable model.",
"date": "2019-04-27"
},
{
"vote": 1,
"title": "Arabic lessons part 1",
"text": null,
"date": "2019-04-27"
},
{
"vote": 1,
"title": "Metatagging textbooks",
"text": "We have a large number of textbooks (hundreds) that have already been metatagged for our industry -- content, audience, etc. by humans, but I'm looking for a way to metatag new textbooks with AI or machine learning. I have converted our existing textbooks to text files and run them through a topic modelling system, but that seems to be a dead end because topic modelling seems to be inherently stochastic -- each time you run the system, you get slightly different topics. \n\n\nWhat would be another approach to train a system with existing textbooks and metatags to come up with a system to metatag new textbooks?",
"date": "2019-04-26"
},
{
"vote": 3,
"title": "Encoding syntactic dependency paths in Sklearn",
"text": "I want to use the path that connects two words in a syntactic dependency tree as a feature/set of features in Sklearn for different purposes.\n\n\n\n\nThe first one is to train a SMV classifier (one of the classifiers of an ensemble).\n\n\nThe second one is as a sequence to find out the direction of a relationship (binary classification in this case).\n\n\n\n\nThe depedency paths looks like in this example:\n\n\n[WORD_1] - nsubjpass -&gt; xcomp -&gt; advcl -&gt; dobj -&gt; compound - [WORD_2]\n \n\n\n[WORD_1] - nmod -&gt; acl -&gt; dobj - [WORD_2]\n\n\nDoes somebody have experience on it? Iâ€™m specially interested on the implementation of it in Sklearn.",
"date": "2019-04-26"
},
{
"vote": 9,
"title": "Important tips when working with small dataset(2K samples)",
"text": "Hey,\n\n\nWhat are some important things that I need to take into account when working with a small dataset(text data) for classification.\n\n\n\n\nDo I need a smaller model (less parameters) ?\n\n\nShould I use a smaller batch size ?(Is something like 4 too small ??)\n\n\nHow do I avoid overfitting ? Do I use more Dropout ?\n\n\nIs there a relationship between the number of samples and the number of parameters in the network ? I mean how do I know the min number of samples a need for a certain network.\n\n\n\n\nP.S Imagine a scenario where I cannot get more data and I have to use neural networks",
"date": "2019-04-25"
},
{
"vote": 2,
"title": "What does coarse-grained mean in machine learning",
"text": "What does a corse-grained representation mean in NLP ?",
"date": "2019-04-25"
},
{
"vote": 2,
"title": "Does anyone know Stanfordâ€™s CL programâ€™s acceptance rate, or have any experiences in the program they would like to share?",
"text": null,
"date": "2019-04-24"
},
{
"vote": 3,
"title": "Need help with entity tagging",
"text": "I need to design a system which can identify \nmovie\n and \nproduction company\n names in a sentence.\n\n\nThe approach that comes to my mind is to train a \nNER\n system on labeled data so that it identifies the corresponding entities. But what about new entities (movie or production company name) that trained system hasn't seen, how can we tag them.\n\n\nLabeled data: Sentences with the position of words that corresponds to movie or production company name\n\n\n\nI am a beginner in NLP, any help would be appreciated",
"date": "2019-04-24"
},
{
"vote": 1,
"title": "Ideas Needed",
"text": "I have a language tutor system, where there is A virtual agent which interacts with the user and teaches the user to learn a new language. \n\n\nRight now it's following the dualingo approach of showing pictures and helping the user to memorize the words. \n\n\nAnother approach could be to take A particular situation like talking to A cab driver and teaching the user smaller sentences to be able to do that. \n\n\nApart from these, is there any algorithm Or an approach which could be used to help the user learn a new language easily and faster.",
"date": "2019-04-24"
},
{
"vote": 1,
"title": "Severely imbalanced dataset",
"text": "[removed]",
"date": "2019-04-23"
},
{
"vote": 1,
"title": "Lots of AI news on one page",
"text": null,
"date": "2019-04-23"
},
{
"vote": 2,
"title": "Best language model for 'middle-out' sentence construction?",
"text": "[deleted]",
"date": "2019-04-23"
},
{
"vote": 39,
"title": "A series I wrote to explain NLP intuitively",
"text": null,
"date": "2019-04-21"
},
{
"vote": 19,
"title": "Rotating an LSTM unit can make all the difference for machine long term memory and abstractive summarization of long complex documents",
"text": null,
"date": "2019-04-20"
},
{
"vote": 27,
"title": "A neural network can read scientific papers and render a plain-English summary",
"text": null,
"date": "2019-04-20"
},
{
"vote": 8,
"title": "Chatbots aren't as difficult to make as You Think",
"text": null,
"date": "2019-04-19"
},
{
"vote": 0,
"title": "Struggling with the environement more than the coding",
"text": "[deleted]",
"date": "2019-04-19"
},
{
"vote": 0,
"title": "Looking for good OCR macbook app. I prefer this to be somethig that exists in the Mac App store, so don't mind paying some for it.",
"text": "[deleted]",
"date": "2019-04-18"
},
{
"vote": 0,
"title": "Get the mundane done by AI - How many keywords can you come up with and how to find more relevant keywords",
"text": "The most common options for keyword research are:\n\n\n\n\nHaving someone else try and come up with more keywords.\n\n\nLooking through your whole webpage for inspirations.\n\n\nUse one of the keyword research tools available on the market to look through a really big pile of keywords that the users of the web have also searched for while they searched for the keywords you already have. But most of them will not be really relevant.\n\n\n\n\nOne of the things machines are pretty good at is detecting which words or phrases seem to be the most important in the text they are given - we can automatically extract keywords from a website with no manual labour. You just give the URL of your webpage to a program and you get a list of relevant keywords. \n\n\nThanks to the natural language processing (NLP), we also have the tools to easily search for words similar to a given word, so you can easily get a quite long a list of keywords that are at least somewhat relevant to your content: \nGet the mundane done by AI\n (full article)",
"date": "2019-04-18"
},
{
"vote": 1,
"title": "Some entertaining limitations of word2vec",
"text": null,
"date": "2019-04-18"
},
{
"vote": 1,
"title": "Extracting action sentences (HELP NEEDED)",
"text": "Hey guys.\nI want to extract action sentences from a sentence. Best understood by an example:\n\n\nScott threw the ball   ---> Throw the ball. \n\n\nScott is eating a mango ---> Eat the mango. \n\n\n&#x200B;\n\n\nI used dependency relations to extract the ROOT of a sentence (the verb most of the time) and then extracted the Object. \n\n\nThis seems to work okay but fails to handle a lot of cases like where I have multiple objects or it sometimes makes a Name the root word. \n\n\nI used spacy to do this. \n\n\nHelp needed please.",
"date": "2019-04-18"
},
{
"vote": 11,
"title": "Topic modelling with NLTK and Gensim -- getting different topics with each run for same textbook",
"text": "I'm using the examples from here:\n\n\nhttps://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n\n\nto do topic modelling on some of my company's text books. I've gotten it to work, but I'm getting a different set of topics for each run on the same textbook. Is this the normal state of affairs for topic modelling? \n\n\nFor instance, one number one:\n\n\n(0, &#039;0.073*&quot;student&quot; + 0.041*&quot;develop&quot; + 0.030*&quot;explicit&quot; + 0.026*&quot;explain&quot;&#039;)\n(1, &#039;0.076*&quot;question&quot; + 0.064*&quot;energy&quot; + 0.045*&quot;guide&quot; + 0.044*&quot;solution&quot;&#039;)\n(2, &#039;0.070*&quot;material&quot; + 0.035*&quot;idea&quot; + 0.029*&quot;equipment&quot; + 0.022*&quot;light&quot;&#039;)\n\n\n\nRun number two:\n\n\n(0, &#039;0.042*&quot;standard&quot; + 0.028*&quot;alignment&quot; + 0.021*&quot;mathematics&quot; + 0.021*&quot;conservation&quot;&#039;)\n(1, &#039;0.038*&quot;table&quot; + 0.030*&quot;review&quot; + 0.024*&quot;require&quot; + 0.024*&quot;valid&quot;&#039;)\n(2, &#039;0.053*&quot;water&quot; + 0.048*&quot;change&quot; + 0.040*&quot;object&quot; + 0.033*&quot;safety&quot;&#039;)",
"date": "2019-04-17"
},
{
"vote": 1,
"title": "Looking to do a PhD In CS with NLP as main core",
"text": "[deleted]",
"date": "2019-04-17"
},
{
"vote": 2,
"title": "Need help with texts classification task",
"text": "I am very New in the nlp field of machinelearning, but i have a task in  a company where I am working. The task is to classify messages comming to the support-centre for 8 groups. When I tried to visualize the data from tfid matrix, I saw that there is no visisble  consequences between groups.  Then I tryed with random forest and it gave 36% accuracy. \nWhat are the ways to increase it? Is it possible to use words chunks or words tagging and how can I apply this techniques to my model?\n\n\nP. S. Sorry for my English",
"date": "2019-04-17"
},
{
"vote": 1,
"title": "Generating new text with a transformer ?",
"text": "Hey friends, \n\n\n&#x200B;\n\n\nI'd like to know if it is doable to train a text generator based on a transformer architecture without using the full pre-trained models from OpenAI. More specifically, I'd like to train from scratch a Harry Potter generator using attention. \n\n\nDoes this sound doable ? What would be your workflow ? \n\n\n&#x200B;\n\n\nThanks !",
"date": "2019-04-17"
},
{
"vote": 3,
"title": "Training time Unsupervised SMT vs Unsupervised NMT",
"text": "Hi guys, I am trying to train an unsupervised machine translation model referring the following link \nhttps://github.com/facebookresearch/UnsupervisedMT\n. Roughly speaking which trains faster NMT or PBSMT ?",
"date": "2019-04-17"
},
{
"vote": 2,
"title": "Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",
"text": null,
"date": "2019-04-15"
},
{
"vote": 0,
"title": "Chatbots aren't as difficult to make as You Think",
"text": null,
"date": "2019-04-15"
},
{
"vote": 1,
"title": "Could anyone describe at a high level how to Train a voice from start to end? or know a guide? I have a general idea but it still feels disconnected and incomplete.",
"text": "I have tacotron, TF, google cloud tools available.   \n\n\nFrom what I understand, I need to build a dataset by parsing a particular voice into its own word and then label the data with <word, audio>. Then feed the dataset into tacotron and let magic happen -> trained voice.   \n\n\nDoes this sound generally correct, am I missing any big steps? Is the idea there?  \n\n\nThanks!",
"date": "2019-04-15"
},
{
"vote": 1,
"title": "Recurrent Neural Networks: Algorithms and Applications",
"text": null,
"date": "2019-04-15"
},
{
"vote": 0,
"title": "How Neural Networks Work: Simply Explained",
"text": null,
"date": "2019-04-15"
},
{
"vote": 0,
"title": "Developing a Robust Face Generative Adversarial Network with Tensorflow",
"text": null,
"date": "2019-04-15"
},
{
"vote": 7,
"title": "[N] Tensorflow 2.0 Hackathon coming up. Also our team could use 1 more person if you're interested. It's an NLP project, and we got some great team members, including an advisor who has published current SoTA ML architectures.",
"text": "Here's the link\n\n\nhttps://tensorflow.devpost.com/\n\n\nWe are looking for one more member, ideally someone with experience some of the current SoTA NLP models (Elmo, Transformer, BERT, GPT/2, ULMFiT, etc.) and wrangling data for those datasets (Our adviser may have had their name published in the official paper for one of those papers ;) ) . But really, we're just looking for someone who has solid practical experience with Tensorflow and can data wrangle.\n\n\nIf you're interested, PM me with what are your time commitments for the next 3 weeks, and your experience with Tensorflow.",
"date": "2019-04-14"
},
{
"vote": 7,
"title": "Convolutional Neural Networks: An Intuitive Approach",
"text": null,
"date": "2019-04-14"
},
{
"vote": 3,
"title": "Cause -&gt; Effect Extraction",
"text": "I'm new to NLP and I'd like to know if I'm on the right track with a task I'm trying to do.\n\n\nI have a massive body of text containing descriptions of two sets of entities (A, B).  I'm trying to process blocks of this text and accurately extract A -> B direct cause/effect relationships.  This is the only relationship I care about, and only in the A-> B direction.\n\n\nI've trained a spaCy model on the two sets of entities and it does pretty well with recognizing them in the text. \n\n\nNow, I'm trying to write a set of rules for the dependency parser results that can detect A->B CE relationships.\n\n\nThe simplest example would probably be:\n\n\nIf sentence root is a \"cause\" verb and it connects to an A subject and B direct object.\n\n\nIt seems like this could get complicated fast though, so I'm wondering if there's an easier approach, or if someone else has already codified rules for this somewhere.  I've read a few recent papers, but most of them were approaching it with much broader/harder goals like not having predefined entities or detecting several other types of relationships.\n\n\nDoes anyone have any tips or experience with this?",
"date": "2019-04-14"
},
{
"vote": 3,
"title": "[D][P]Feedback About Project on Unsupervised Document Classification and Summarization",
"text": "Hi everyone,\n\n\nI have recently completed a project as part of my undergraduate curriculum and would like to get some feedback about it from you guys. The project falls under Multi Document Summarization and Document classification.  \n\n\nAim of the project :\n To classify and summarize valuable player feedback(identification of Bugs, Useful Suggestions etc.) found on discussions from a particular video game subreddit. The classification will happen according to a number of pre-defined topics given by the end user. Then the classified discussions will be summarized for easy digestion.\n\n\n&#x200B;\n\n\nApproach\nÂ : To use \ntopic modeling\n to classify discussions and using the output matrices of the topic model with word vectors retrieved by a Word2Vec model, to do a simple scoring of sentences to generate an extractive summary. Each discussion will be considered as a document in this project. The approach is explained in detail in \nthis\n flow chart.  \n\n\nAny feedback about the project is much appreciated! Thanks!",
"date": "2019-04-13"
},
{
"vote": 6,
"title": "Automated voice translator(s) {crosspost}",
"text": null,
"date": "2019-04-13"
},
{
"vote": 3,
"title": "Any ideas on how to map a variably sized sequence of vectors into a fixed size vector?",
"text": "Hi redditors,\n\n\nI  am currently working on a project which includes a topic extraction  pipeline and now I want to create document embeddings using the Google  BERT model instead of good old Tf-Idf. Sadly BERT has a limit on the  input size and therefore I cannot push whole texts into it. Now I need  to encode each sentence and generate a document feature vector out of  it. I had a look at the literature, but that does not seem to be an active research problem. Are you aware of any techniques which encode e.g. the semantic structure of the sequence into such a vector?",
"date": "2019-04-12"
},
{
"vote": 11,
"title": "What tools are available for sentence/textual similarity?",
"text": "I have a problem where I need to determine the similarity between two phrases.  These will often be composed only of adjectives and nouns, and not full sentences.  e.g. \"Man with brown hat\", \"Spotted brown dog\", etc.\n\n\nAre there any tools currently available (any language is ok, C# preferred if available) that would fit this need?  Pre-trained models preferred, but am not opposed to throwing some resources at training something myself.  \n\n\nIf no good tools exist for this, can anyone point me to a direction I can go to implement myself?  I have some experience working with WordNet and other basic NLP research tools.",
"date": "2019-04-11"
},
{
"vote": 2,
"title": "TF-IDF in a nutshell",
"text": null,
"date": "2019-04-09"
},
{
"vote": 0,
"title": "The model trained using LSTM is predicting the same value for all",
"text": "I have a dataset with 4000 rows and two columns. The first column contains some sentences and the second column contains some numbers for it.\n\n\nThere are some 4000 sentences and they are categorized by some 100 different numbers. For example:\n\n\n&#x200B;\n\n\nSentences                                         Codes\n\n\n&#x200B;\n\n\nGoogle headquarters is in California              87390\n\n\nSteve Jobs was a great man                        70214\n\n\nSteve Jobs has done great technology innovations  70214\n\n\nGoogle pixel is a very nice phone                 87390\n\n\nMicrosoft is another great giant in technology    67012\n\n\nBill Gates founded Microsoft                      67012 \n\n\n&#x200B;\n\n\nSimilarly, there are a total of 4000 rows containing these sentences and these rows are classified with 100 such codes\n\n\nI have tried the below code but when I am predicting, it is predicting one same value for all. IN othr words y_pred is giving an array of same values.\n\n\n&#x200B;\n\n\nMay I know where is the code going wrong\n\n\nimport pandas as pd\n\n\nimport numpy as np\n\n\nxl = pd.ExcelFile(\"dataSet.xlsx\")\n\n\ndf = xl.parse('Sheet1') \n\n\n&#x200B;\n\n\n#df = df.sample(frac=1).reset_index(drop=True)# shuffling the dataframe\n\n\n&#x200B;\n\n\n&#x200B;\n\n\ndf = df.sample(frac=1).reset_index(drop=True)# shuffling the dataframe\n\n\nX = df.iloc[:, 0].values\n\n\nY = df.iloc[:, 1].values\n\n\n&#x200B;\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\nimport pickle \n\n\n&#x200B;\n\n\ncount_vect = CountVectorizer()\n\n\nX = count_vect.fit_transform(X)\n\n\n&#x200B;\n\n\ntfidf_transformer = TfidfTransformer()\n\n\nX = tfidf_transformer.fit_transform(X)\n\n\n&#x200B;\n\n\nX = X.toarray()\n\n\n&#x200B;\n\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n\nlabelencoder_Y = LabelEncoder()\n\n\nY = labelencoder_Y.fit_transform(Y)\n\n\ny = Y.reshape(-1, 1)  # Because Y has only one column\n\n\n&#x200B;\n\n\nonehotencoder = OneHotEncoder(categories='auto')\n\n\nY = onehotencoder.fit_transform(y).toarray()\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\n\n&#x200B;\n\n\ninputDataLength = len(X_test[0])\n\n\noutputDataLength = len(Y[0])\n\n\n&#x200B;\n\n\nfrom keras.models import Sequential\n\n\nfrom keras.layers import Dense\n\n\nfrom keras.layers import LSTM\n\n\nfrom keras.layers.embeddings import Embedding\n\n\nfrom keras.preprocessing import sequence\n\n\nfrom keras.layers import Dropout\n\n\n&#x200B;\n\n\n# fitting the model\n\n\nembedding_vector_length = 100\n\n\nmodel = Sequential()\n\n\nmodel.add(Embedding(outputDataLength,embedding_vector_length, input_length=inputDataLength))\n\n\nmodel.add(Dropout(0.2))\n\n\nmodel.add(LSTM(outputDataLength))\n\n\nmodel.add(Dense(outputDataLength, activation='softmax'))\n\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\nprint(model.summary())\n\n\nmodel.fit\n(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=20)\n\n\ny_pred = model.predict(X_test)\n\n\ninvorg = model.inverse_transform(y_test)\n\n\ny_test = labelencoder_Y.inverse_transform(invorg)\n\n\ninv = onehotencoder.inverse_transform(y_pred)\n\n\ny_pred = labelencoder_Y.inverse_transform(inv)",
"date": "2019-04-08"
},
{
"vote": 2,
"title": "Which algorithm should I use to map an input sentence to an output sentence?",
"text": "I am new to NLP realm. If you have an input text \"The price of orange has increased\" and output text \"Increase the production of orange\". Can we make our RNN model to predict the output text? Or what algorithm should I use?",
"date": "2019-04-07"
},
{
"vote": 3,
"title": "Integrating or unifying different scores for a task",
"text": "Many NLP tasks require usage of different techniques, and scoring functions to unify different concepts to do the task.\n\n\nWhat are some good statistically sound ways of integrating scores from different methods, in order to achieve a single score for the desired goal, except normal concatenation and dot products of scores?\n\n\n&#x200B;",
"date": "2019-04-06"
},
{
"vote": 0,
"title": "How Neural Networks Work- Simply Explained by a Machine Learning Engineer",
"text": null,
"date": "2019-04-05"
},
{
"vote": 2,
"title": "Classification using word embeddings",
"text": "If we train a classification model using word \nembeddings\n, where let's say we have about 100 words as input and some specific label.\n\n\nThen is it possible that the classifier model learns in such a way that for some unknown new word not present in training dataset, but which is somewhat related to the word in the train data, like: 'run'(in \nTrain\n data) is related to 'jog'(\nnew\n in \ntest data\n), the model is able to classify for jog in \naccordance\n with what \nrun\n is labeled for.",
"date": "2019-04-05"
},
{
"vote": 1,
"title": "Python library to normalize job titles",
"text": "[deleted]",
"date": "2019-04-03"
},
{
"vote": 5,
"title": "Find Independent Clause from a statement",
"text": "I'm currently working on a project which requires to find Independent clause from a statement.\n\n\nI have tried Stanford CoreNLP, spaCy library, but I cannot find proper answer.\n\n\nIf anyone knows how to find Independent clauses from statement, please help or give a pointer or a reference from where I can study implement it.\n\n\nThank you in advance.",
"date": "2019-04-03"
},
{
"vote": 3,
"title": "Exploratory project on Deep Learning based Question Answering on Amazon Dataset",
"text": "Hi folks,\nI would like to share a recent project I did while exploring the field of deep learning based Question Answering systems. It is built on a curated dataset based on Amazon reviews, product descriptions and other fields. Feel free to explore it, any comments would be appreciated. The link to the code is \nhttps://github.com/shubham14/AQUAA",
"date": "2019-04-02"
},
{
"vote": 11,
"title": "What tools to use for feature extraction from large documents?",
"text": "Hi there! I am interesting in knowing what tools are most popular for data extraction from large unstructured documents? I am referring for instance to pension plans documents. From those I would like to automatically extract different financial facts and features. Any recommendations?",
"date": "2019-04-02"
},
{
"vote": 1,
"title": "Displaying Word2Vec Similarities",
"text": "Hi Guys,\n\n\n&#x200B;\n\n\nI am struggling to find a way to display my Word2Vec cosine similarities on a graph and wondered how I should be doing it? I tried using T-SNE on a 2D Graph but for some reason I am dropping over 0.7 accuracy when they are plotted due to the dimension drops. Can you guys give me any pointers as what tech I could look at implementing to fix this problem?\n\n\n&#x200B;\n\n\nI can provide code or whatever if you guys need any information to help.",
"date": "2019-04-02"
},
{
"vote": 4,
"title": "Abstractive text summarization for hotel reviews",
"text": "I'm working on a project in which I have many hotel reviews and I want to create an abstract summary (called a meta-review) for each hotel. I found that the SOTA techniques for abtractive summarization were using Deep Learning with RNNs and Attention Mechanisms (Seq2seq architecture, TextSum by Google, ...)\n\n\n(Seq2seq for text summarization :  \nhttps://aclweb.org/anthology/D15-1044\n )\n\n\nHowever, most of the work available online is about Single Document Summarization, in which you want to produce a summary for each review.  In my case, it's more about Multi-Document Summarization, with a lot of repetitions in the inputs. And unfortunately I don't have a database of (hotel reviews --> meta-review) to train directly a seq2seq-like network...\n\n\n&#x200B;\n\n\nSo I was wondering if you knew some techniques that would be adapted to this problem ?",
"date": "2019-04-02"
},
{
"vote": 6,
"title": "iFLYTEK &amp; HIT Reading Comprehension Model Betters Humans, Tops SQuAD2.0 Leaderboard",
"text": null,
"date": "2019-04-01"
},
{
"vote": 2,
"title": "IUI2019 Keynote: Getting Virtually Personal: Making Responsible and Empathetic â€œHerâ€ for Everyone",
"text": null,
"date": "2019-04-01"
},
{
"vote": 2,
"title": "Trying to reproduce the experimental results with Flair Embeddings",
"text": "I'm trying to reproduce the results in \nFlair\n. But I couldn't reproduce 93+ F score performance for CoNLL-2003.\n\n\nWhat I did:\n\n\n\n\nDownload the suggested embeddings in Flair: Glove, Pool (news forward) Pool (news backward)\n\n\nUse them in my BiLSTM-CRF model (which I have successfully reproduced the results in ELMo and BERT).\n\n\n\n\n&#x200B;\n\n\nAny ideas in the neural architectures design?",
"date": "2019-04-01"
},
{
"vote": 6,
"title": "NLP in medical diagnostics (ML + Rule based system)",
"text": "[deleted]",
"date": "2019-03-29"
},
{
"vote": 10,
"title": "Mining Discourse Markers for Unsupervised Sentence Representation Learning",
"text": null,
"date": "2019-03-29"
},
{
"vote": 6,
"title": "Ternary Search Tree / N-Gram Model in Python",
"text": "Hi.\n\n\n&#x200B;\n\n\nI implemented a word n-gram model using a character ternary search tree. It is intended to be passed a generator that yields a long  sequence of words (from a corpus) and its requirements are that it\n\n\n\n\ncan return frequencies and probabilities for word n-grams\n\n\nallows providing a vocabulary, such that n-grams containing words not in the vocabulary are not counted\n\n\nallows providing a list of targets, so that only n-grams ending in a target are counted (as probabilities for these are needed)\n\n\n\n\nI find that it works as expected, but it is quite slow and consumes a  lot of memory. For n-grams of length 4, trained on a corpus of about 1  billion words, right now it consumes >120GB of memory, despite providing a vocabulary that consists of words with a minimum frequency of 5, and it didn't finish within three days (job was cancelled, had to resubmit). I know that Python  requires a lot of memory, but I'm wondering if I'm missing something that would make it faster and maybe less memory intensive.\n\n\nThe only two things I can think of is that I might want to re-balance the tree (maybe after every million insertions, but I don't quite understand how to do that) and to increase the minimum frequency, maybe 5 occurrences in a one billion corpus is too optimistic.\n\n\n&#x200B;\n\n\nAny ideas? Here's the code:\n\n\n&#x200B;\n\n\ntst.py\n\n\nclass Node():\n    def __init__(self, char):\n        self.char = char\n        self.count = 0\n\n        self.lo = None\n        self.eq = None\n        self.hi = None\n\n\nclass TernarySearchTree():\n    &quot;&quot;&quot;Ternary search tree that stores counts for n-grams\n    and their subsequences.\n    &quot;&quot;&quot;\n\n    def __init__(self, splitchar=None):\n        &quot;&quot;&quot;Initializes TST.\n\n        Parameters\n        ----------\n        splitchar : str\n            Character that separates tokens in n-gram.\n            Counts are stored for complete n-grams and\n            each sub-sequence ending in this character\n        &quot;&quot;&quot;\n        self._root = None\n        self._splitchar = splitchar\n        self._total = 0\n\n    def insert(self, string):\n        &quot;&quot;&quot;Insert string into Tree.\n\n        Parameters\n        ----------\n        string : str\n            String to be inserted.\n        &quot;&quot;&quot;\n        self._root = self._insert(string, self._root)\n        self._total += 1\n\n    def frequency(self, string):\n        &quot;&quot;&quot;Return frequency of string.\n\n        Parameters\n        ----------\n        string : str\n\n\n        Returns\n        -------\n        int\n            Frequency\n        &quot;&quot;&quot;\n        if not string:\n            return self._total\n\n        node = self._search(string, self._root)\n\n        if not node:\n            return 0\n\n        return node.count\n\n    def _insert(self, string, node):\n        &quot;&quot;&quot;Insert string at a given node.\n        &quot;&quot;&quot;\n        if not string:\n            return node\n\n        char, *rest = string\n\n        if node is None:\n            node = Node(char)\n\n        if char == node.char:\n            if not rest:\n                node.count += 1\n                return node\n            else:\n                if rest[0] == self.splitchar:\n                    node.count += 1\n                node.eq = self._insert(rest, node.eq)\n\n        elif char &lt; node.char:\n            node.lo = self._insert(string, node.lo)\n\n        else:\n            node.hi = self._insert(string, node.hi)\n\n        return node\n\n    def _search(self, string, node):\n        &quot;&quot;&quot;Return node that string ends in.\n        &quot;&quot;&quot;\n        if not string or not node:\n            return node\n\n        char, *rest = string\n\n        if char == node.char:\n            if not rest:\n                return node\n            return self._search(rest, node.eq)\n\n        elif char &lt; node.char:\n            return self._search(string, node.lo)\n\n        else:\n            return self._search(string, node.hi)\n\n    def __contains__(self, string):\n        &quot;&quot;&quot;Adds &quot;string in TST&quot; syntactic sugar.\n        &quot;&quot;&quot;\n        node = self._search(string, self._root)\n        if node:\n            return node.count\n\n        return False\n\n    @property\n    def splitchar(self):\n        return self._splitchar\n\n\n\nlanguage_model.py\n\n\nfrom collections import deque\nfrom tst import TernarySearchTree\n\nclass ContainsEverything:\n&quot;&quot;&quot;Dummy container that mimics containing everything.\nHas .add() method to mimic set.\n&quot;&quot;&quot;\n\n    def __contains__(self, _):\n        return True\n\n    def add(self, _):\n        pass\n\n\nclass LanguageModel():\n    &quot;&quot;&quot;N-gram (Markov) model that uses a ternary search tree.\n    Tracks frequencies and calculates probabilities.\n\n    Attributes\n    ----------\n    n : int\n        Size of n-grams to be tracked.\n    vocabulary : set\n        If provided, n-grams containing words not in vocabulary are skipped.\n        Can be other container than set, if it has add method.\n    targets : container\n        If provided, n-grams not ending in target are counted as\n        ending in &quot;OOV&quot; (OutOfVocabulary) instead, so probabilities\n        can still be calculated.\n    boundary : str\n        N-grams crossing boundary will not be counted,\n        e.g. sentence &lt;/s&gt; or document &lt;/doc&gt; meta tags\n    splitchar : str\n        String that separates tokens in n-grams\n    &quot;&quot;&quot;\n\n    def __init__(self, n, boundary=&quot;&lt;/s&gt;&quot;, splitchar=&quot;#&quot;,\n                 vocabulary=None, targets=None):\n        &quot;&quot;&quot;\n        Parameters\n        ----------\n        n : int\n            Size of n-grams to be tracked.\n        boundary : str\n            N-grams crossing boundary will not be counted,\n            e.g. sentence &lt;/s&gt; or document &lt;/doc&gt; meta tags\n        splitchar : str\n            String that separates tokens in n-grams\n        vocabulary : set\n            If provided, n-grams with words not in vocabulary are skipped.\n            Can be other container than set, if it has add method.\n        targets : container\n            If provided, n-grams not ending in target are counted as\n            ending in &quot;OOV&quot; (OutOfVocabulary) instead, so probabilities\n            can still be calculated.\n        &quot;&quot;&quot;\n        if not targets:\n            targets = ContainsEverything()\n\n        if not vocabulary:\n            vocabulary = ContainsEverything()\n\n        self._n = n\n        self._counts = TernarySearchTree(splitchar)\n        self._vocabulary = vocabulary\n        self._targets = targets\n        self._boundary = boundary\n        self._splitchar = splitchar\n\n    def train(self, sequence):\n        &quot;&quot;&quot;Train model on all n-grams in sequence.\n\n        Parameters\n        ----------\n        sequence : iterable of str\n            Sequence of tokens to train on.\n\n        Notes\n        -----\n        A sequence [A, B, C, D, E] with n==3 will result in these\n        n-grams:\n          [A, B, C]\n          [B, C, D]\n          [C, D, E]\n          [D, E]\n          [E]\n        &quot;&quot;&quot;\n        n_gram = deque(maxlen=self.n)\n        for element in sequence:\n            if element == self.boundary:\n                # train on smaller n-grams at end of sentence\n                # but exclude full n_gram if it was already trained\n                # on in last iteration\n                not_trained = len(n_gram) &lt; self.n\n                for length in range(1, len(n_gram) + not_trained):\n                    self._train(list(n_gram)[-length:])\n                n_gram.clear()\n                continue\n\n            n_gram.append(element)\n\n            if len(n_gram) == self.n:\n                if element not in self.targets:\n                    self._train(list(n_gram)[:-1])\n                    continue\n\n                self._train(n_gram)\n\n        # train on last n-grams in sequence\n        # ignore full n-gram if it has already been trained on\n        if len(n_gram) == self.n:\n            n_gram = list(n_gram)[1:]\n        for length in range(1, len(n_gram) + 1):\n            self._train(list(n_gram)[-length:])\n\n    def probability(self, sequence):\n        &quot;&quot;&quot;Returns probability of the sequence.\n\n        Parameters\n        ----------\n        sequence : iterable of str\n            Sequence of tokens to get the probability for\n\n        Returns\n        -------\n        float or list of float\n            Probability of last element or probabilities of all elements\n        &quot;&quot;&quot;\n        try:\n            n_gram = sequence[-self.n:]\n\n        # if sequence is generator (cannot slice - TypeError),\n        # run through it and return probability for final element\n        except TypeError:\n            n_gram = deque(maxlen=self.n)\n            for element in sequence:\n                n_gram.append(element)\n\n        probability = self._probability(n_gram)\n        return probability\n\n    def frequency(self, n_gram):\n        &quot;&quot;&quot;Return frequency of n_gram.\n\n        Parameters\n        ----------\n        n_gram : list/tuple of str\n\n        Returns\n        -------\n        int\n            Frequency\n        &quot;&quot;&quot;\n        n_gram_string = self.splitchar.join(n_gram)\n        frequency = self._counts.frequency(n_gram_string)\n        return frequency\n\n    def _train(self, n_gram):\n        # test for OOV words\n        for idx, word in enumerate(n_gram):\n            if word not in self.vocabulary:\n                n_gram = list(n_gram)[:idx]\n\n        n_gram_string = self.splitchar.join(n_gram)\n        self._counts.insert(n_gram_string)\n\n    def _probability(self, n_gram):\n        frequency = self.frequency(n_gram)\n\n        if frequency == 0:\n            return 0\n\n        *preceding, target = n_gram\n        total = self.frequency(preceding)\n\n        probability = frequency / total\n        return probability\n\n    def __contains__(self, n_gram):\n        return n_gram in self._counts\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def vocabulary(self):\n        return self._vocabulary\n\n    @property\n    def targets(self):\n        return self._targets\n\n    @property\n    def boundary(self):\n        return self._boundary\n\n    @property\n    def splitchar(self):\n        return self._splitchar\n\n\n\n&#x200B;",
"date": "2019-03-29"
},
{
"vote": 8,
"title": "[D] Why does all of NLP literature use Noise contrastive estimation loss for negative sampling instead of sampled softmax loss?",
"text": null,
"date": "2019-03-29"
},
{
"vote": 4,
"title": "Methods of document classification",
"text": "[deleted]",
"date": "2019-03-28"
},
{
"vote": 4,
"title": "Help with implementing doc_stride in Huggingface multi-label BERT",
"text": "As you might know, BERT has a maximum wordpiece token sequence length of 512. \n\n\nThe SQuAD example actually uses strides to account for this: \nhttps://github.com/google-research/bert/issues/27\n\n\nI want to implement something like what Jacob Devlin described in that post for Kaushal's multi-label BERT classifier: \nhttps://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n\n\n&#x200B;\n\n\nWhat I did was basically just copy and edit a bit the doc_stride functions from the SQuAD example in huggingface into Kaushal's code: \nhttps://colab.research.google.com/drive/1aqcIdm2Pn2rvmWHvOSEMUTzw_CNmoseA\n\n\n&#x200B;\n\n\nHowever, this is excruciatingly slow, I don't know if it's an issue of stride length?\n\n\n&#x200B;\n\n\nAlso, I'm not sure how to implement the thing about reshaping the combined minibatches and getting predictions from there. \n\n\n&#x200B;\n\n\nAnyone who has tried to adapt BERT for longer texts, could you please help?\n\n\n&#x200B;\n\n\nThanks\n\n\n&#x200B;\n\n\n&#x200B;\n\n\n&#x200B;",
"date": "2019-03-27"
},
{
"vote": 3,
"title": "Train NLP model on text content and font styles.",
"text": "I want to create a more efficient process for extracting information from resumes. I am trying to optimize this by training a model on text content and font style, but I'm having trouble finding a parser to pull both from files. I'm currently using Java and have been trying apache tika, but it doesn't seem to be pulling accurate xhtml info. Any help would be welcome.",
"date": "2019-03-26"
},
{
"vote": 23,
"title": "Baiduâ€™s ERNIE Tops Googleâ€™s BERT in Chinese NLP Tasks",
"text": null,
"date": "2019-03-25"
},
{
"vote": 2,
"title": "Pre-trained models for sentence/ multi -sentence compression.",
"text": "[deleted]",
"date": "2019-03-24"
},
{
"vote": 15,
"title": "Skip gram is an important algorithm in context prediction for a given word. This blog may help you in understanding it.",
"text": null,
"date": "2019-03-24"
},
{
"vote": 7,
"title": "How Neural Networks Work- Simply Explained by a Machine Learning Engineer",
"text": null,
"date": "2019-03-23"
},
{
"vote": 3,
"title": "I made a downloadable version of Thesaurus.com for all your synonym needs!",
"text": "[deleted]",
"date": "2019-03-23"
},
{
"vote": 3,
"title": "Sentence representation in BERT Transformer",
"text": "[\nx-post from SE\n]\n\n\nI am trying my hand at BERT and I got so far that I can feed a sentence into BertTokenizer, and run that through the BERT model which gives me the output layers back. Modified from \nPyTorch code\n over at HuggingFace.\n\n\nimport logging\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nlogging.basicConfig(format = &#039;%(asctime)s - %(levelname)s - %(name)s -   %(message)s&#039;, \n                    datefmt = &#039;%m/%d/%Y %H:%M:%S&#039;,\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass InputFeatures(object):\n    def __init__(self, tokens, input_ids, input_mask, input_type_ids):\n        self.tokens = tokens\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.input_type_ids = input_type_ids\n\ndef convert_sentences_to_features(sentences, max_seq_length, tokenizer):    \n    features = []\n    for sentence in sentences:\n        # tokenizer will also separate on punctuation\n        # see https://github.com/google-research/bert#tokenization\n        tokens = tokenizer.tokenize(sentence)\n        \n        # limit size of tokens\n        if len(tokens) &gt; max_seq_length - 2:\n            tokens = tokens[0:(max_seq_length - 2)]\n        \n        # add [CLS] and [SEP], as expected in BERT\n        tokens = [&#039;[CLS]&#039;, *tokens, &#039;[SEP]&#039;]\n\n        \n        input_type_ids = [0] * len(tokens)\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) &lt; max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            input_type_ids.append(0)\n        \n        features.append(InputFeatures(tokens=tokens, \n                                      input_ids=input_ids, \n                                      input_mask=input_mask, \n                                      input_type_ids=input_type_ids)\n                       )\n    return features\n\n\ndef main(sentences, layers=&#039;-1, -2, -3, -4&#039;, max_seq_length=512, bert_model=&#039;bert-large-uncased&#039;, \n         do_lower_case=True, batch_size=32, no_cuda=False):\n    device = torch.device(&#039;cuda&#039; if torch.cuda.is_available() and not no_cuda else &#039;cpu&#039;)\n    \n    # &#039;layers&#039; indicates which layers we want to concatenate\n    layer_idxs = [int(l) for l in layers.split(&#039;,&#039;)]\n    \n    # init tokenizer\n    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n    \n    # returns a list of &#039;InputFeatures&#039;\n    features = convert_sentences_to_features(sentences, max_seq_length, tokenizer)\n    \n    # init model and move to device\n    model = BertModel.from_pretrained(bert_model)\n    model.to(device)    \n    \n    # extract IDs and mask from the features\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    \n    # prepare dataset and dataloader\n    eval_data = TensorDataset(all_input_ids, all_input_mask)\n    eval_sampler = SequentialSampler(eval_data)\n    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n\n    model.eval()\n    \n    for input_ids, input_mask in eval_dataloader:\n        input_ids = input_ids.to(device)\n        input_mask = input_mask.to(device)\n\n        all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n        \n        # put layers to concatenate in list, and use torch.cat\n        layers_to_concat = [all_encoder_layers[idx] for idx in layer_idxs]\n        concat = torch.cat(layers_to_concat, dim=-1)\n        \n        logger.info(concat.size())\n        logger.info(concat)\n\nif __name__ == &quot;__main__&quot;:\n    proc_args = {\n        &#039;sentences&#039;: [&#039;I saw Bert today !&#039;, &#039;Do you like bananas ?&#039;, &#039;Some sentences are really horrendous to parse .&#039;],\n        &#039;max_seq_length&#039;: 32\n    }\n    main(**proc_args)\n\n\n\nThis works and gives me an output size of \n3, 32, 4096\n in this case. This comes down to \nbatch_size, seq_length, layers*hidden_size\n. In practice, this means that I have a representation of each token in its context. But I would like to extract the sentence representation from this. In an (bidirectional) RNN, you would typically take the top leftmost node's output which contained the latest hidden state and latest output, but I am not sure whether this is also true for transformers, as the architecture is different.\n\n\nWhat is the best way to extract the sentence representation from a sequence of representations in a transformer? The size would be \nbatch_size, hidden_size\n.",
"date": "2019-03-22"
},
{
"vote": 2,
"title": "Book for neural NLP",
"text": "I have a decent experience with Deep Learning for Computer Vision (I can use/modify a model such as Mask-RCNN for semantic segmentation). I need to learn about neural NLP (NLP using deep nets - is that the right term?) because we have multiple use cases in the company. Which books could I use? I like some theoretical background, but I need to be able to solve practical problems, rather than proving theorems or inventing new architectures. I can't decide between these two: \n\n\nhttps://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984\n\n\nhttps://www.amazon.com/gp/aw/d/1491978236\n\n\nCan you help? Thanks\n\n\nPS I can give some (not too detailed) info about the use cases, if you'd need it to give me an advice. Also, is cross-posting allowed on Reddit? I was unsure whether to ask here only, or on r/MachineLearning too.",
"date": "2019-03-22"
},
{
"vote": 6,
"title": "[D] How to fine tune BERT embeddings to your own Corpus similar to w2v?",
"text": "Hi!\n\n\nIâ€™m looking to use BERT embeddings for downstream tasks and as such want to fine tune the embeddings to my own corpus. This is often done with antecedents to BERT (w2v, FastText, Infersent etc.) where the model could just be fed a new corpus and no preprocessing was required.\n\n\nBut as BERT requires sentence pairs, I think I would need to generate the sentence pairs by creating pairs of sequential sentences from my corpus. In the preprocessing script available in the repo,  I believe the script generates the masked language task. From that Iâ€™d be able to run the fine-tuning script.\n\n\nHas anyone else in the community fine-tuned bert on a custom corpus and knows the steps to  pre-process and fine tune (and if Iâ€™m missing anything)?",
"date": "2019-03-21"
},
{
"vote": 1,
"title": "Detect blocks of text inside of text (instead of images)",
"text": "Hey folks!\n\n\n&#x200B;\n\n\nYou can find lots of articles on the net for detecting text blocks in images and how to extract them. Great stuff! But what if I want to extract text blocks from plain text, gathered from PDF files or web sites?\n\n\n&#x200B;\n\n\nI thought about converting the website / PDF to an image and then use OCR but this is prone to fail a lot.\n\n\nThen I wondered if you can convert to an image and later use the coordinates to get the text out of the original document. But some times it is not possible to use the coordinates in PDF...\n\n\n&#x200B;\n\n\nI did not find a solution directly on plain text.\n\n\n&#x200B;\n\n\nThe data I want to segment contains simple paragraphs but sometimes also has multiple columns. You see this ofthen in job advertisements for example.\n\n\n&#x200B;\n\n\nEvery once in a while I search for new terms / ideas to solve this problem. Maybe some guys here can point me in another direction?\n\n\n&#x200B;\n\n\nMany thanks for any hint in advance!",
"date": "2019-03-21"
},
{
"vote": 0,
"title": "Unable to run RUST calls for SNIPS",
"text": "While running Rust/Rustling, getting an error\n\n\n&#x200B;\n\n\nerror: Could not compile 'snips-nlu-utils'\n\n\n&#x200B;\n\n\nDev Platform details:\n\n\nOS: Ubuntu Server 18.04 LTS 64 bit \n\n\nRAM: 2 GB \n\n\nDrive: 80 GB (more than 20% available)\n\n\nSNIPS Version latest pulled from GitHub\n\n\nUsing the RUST reference library from Git - \nhttps://github.com/snipsco/snips-nlu-rs\n\n\nError is triggered while runinng  Cargo command for interactive parsing.\n\n\n&#x200B;\n\n\nIf anyone can share some light or path to debug this error.\n\n\n&#x200B;\n\n\n- Noman",
"date": "2019-03-20"
},
{
"vote": 0,
"title": "A stupid-simple way to send your NLP compute jobs to a remote machine. Hit me up if you want to learn more!",
"text": null,
"date": "2019-03-20"
},
{
"vote": 2,
"title": "[Job Posting] Postdoc in clinical NLP at Harvard Medical School/Boston Children's Hospital",
"text": "We are looking for a postdoctoral research fellow in the HealthNLP lab at Boston Children's Hospital's Computational Health Informatics Program (\nhttps://www.chip.org/\n) and Harvard Medical School. Come work on NLP that makes a difference in a rich intellectual environment! See here for more information and how to apply: \nhttps://scholar.harvard.edu/tim-miller/jobs",
"date": "2019-03-19"
},
{
"vote": 54,
"title": "spaCy v2.1 finally released",
"text": null,
"date": "2019-03-19"
},
{
"vote": 18,
"title": "Using BERT as a feature extractor in PyTorch",
"text": "[deleted]",
"date": "2019-03-18"
},
{
"vote": 3,
"title": "spacy_conll: Parse text with spaCy and print or use the output in CoNLL-U format",
"text": "[deleted]",
"date": "2019-03-18"
},
{
"vote": 2,
"title": "How to use Conv2d with embeddings in Pytorch",
"text": "Hi,\n\n\nSo I have embeddings in the shape: batch_size x max_len x embeddings_dimension (16, 32, 300)\n\n\nThe embeddings are the input of a Conv2d laye. But I don't know how to set the parameters.\n\n\nThis is what I have tried Conv2d(300, 100, kernel_size=(3,3))\n\n\nWhat should be the input dimension of Conv2d ?",
"date": "2019-03-18"
},
{
"vote": 1,
"title": "Progress in my language processor script",
"text": "Here's the output of the language processor I've been working on for a few months now.  A few things to notice:  The system recognizes many patterns in language and \"standardizes\" them (such as 'there is/there are' statements, which are turned into simple declarative statements, conversational postulates are standardized to the actual question that is being asked, and if the question is part of the (very limited) fields of knowledge the system currently has access to (a dictionary, a quote reference and date/time functions) then it actually answers.  It also handles several contextual environments, like greetings, goodbyes, etc.  The system has conversational memory, both short-term (in conversation) and long-term (all interactions are logged to a text file and scanned as the conversation proceeds.   It also recovers proper noun capitalization since the speech recognition engine doesn't capitalize properly.  Proper Noun recovery (capitalization) is important because without it, NLTK mis-classifies them as regular nouns (NLTK is truly great, but it definitely has needed some patches.)  The system handles I/you reflections of all types and translates responses appropriately.  There are many, many layers this all passes through.\n\n\nBelow is an actual interaction.  This is NOT a glorified if-then loop.  This system uses NLTK and a large number of RexExps to detect patterns, rather than all possible permutations of words (which would require an enormous amount of if-thens to cover all the possible ways a person could phrase a conversational postulate, or ask for the time, etc.) \n\n\nI have the responses providing a large amount of background data from the core engine, such as commenting on the nature of the statement, not just providing an answer or an \"I don't know\".  This is what I call \"content-free\" responses.  As I build in knowledge bases that allow for more real \"answers\" or follow-up question generation by the system, the commentaries will be eliminated.\n\n\nThis interaction shows my inputs after the >, with the responses not preceded by a >\n\n\nNote that this version is a text-input version.  The actual input to the live system uses PocketSphinx and the Python SpeechRecognition wrapper for live input, and employs Cepstral Voices for the speech output.  I develop on the text version (faster, easier) and then load into the live version as I finish off new functionality.  You may see SSML code <emphasis>like this</emphasis> printed occasionally - this is interpreted by the SpeechCenter and influences the output speech.  Videos of the live system will be made available soon.\n\n\nA work in progress, but my goal of competent conversation is starting to feel reachable.  There's a ton more I could demo but I think I'm already pushing it by putting this much text in one post.... :)  Shout-out to other Westworld fans out there - notice the name of the system :)\n\n\nThanks for checking this out if you do.  I never expected this system to become as robust as it has.\n\n\n== RESTART: C:\\Users\\Dave Xanatos\\Desktop\\HOSTCORE\\TextLanguageProcessor.py ==\nGood evening! I'm Maeve,  And Language Processing is On Line.\nWaiting for utterance.\n\n\n> there are no dogs in my yard right now\n\n\nI think I heard you say, there are no dogs in my yard right now.  And, my response would be, No dogs are in your yard right now?\n\n\n> what's the difference between wisdom and knowledge\n\n\nI think I heard you ask me a question. The question was: what's the difference between wisdom and knowledge?  Knowledge is knowing that a tomato is a fruit; Wisdom is knowing you shouldn't include it in a fruit salad.\n\n\n> i'm wondering if you have the processing power for this\n\n\nI think I heard you ask me a question, but in the form of a conversational postulate.  Your original statement was: i'm wondering if you have the processing power for this.  The actual question you are asking is: Do you have the processing power for this?  And, the answer is, I don't know if I do have the processing power for this.\n\n\n> i'm wondering if you know the time\n\n\nI think I heard you ask me a question, but in the form of a conversational postulate.  Your original statement was: i'm wondering if you know the time.  The actual question you are asking is: Do you know the time? The answer is, Its 06 41 PM.\n\n\n> what is a fish\n\n\nI think I heard you ask me a question. The question was: what is a fish? The answer is, A fish is a cold-blooded aquatic vertebrate.\n\n\n> hello maeve\n\n\nI was wondering when you were going to say hello... hello to you too. You have me at a slight disadvantage; you know my name but I don't know yours?\n\n\nexpectResponse/check4name>> oh yeah i'm dave\n\n\nNice to meet you Dave\n\n\n> can you say hi dave this is a wonderful program\n\n\nI think I heard you ask me a question. The question was: can you say hi Dave this is a wonderful program?  And, Okay, hi Dave this is a wonderful program.\n\n\n> when the fuck will I finish this project\n\n\nI think I heard you say, when the fuck Will i finish this project. You also used a curse word.   Can you expand on that?\n\n\n> create a random sentence\n\n\nI think I heard you say, create a random sentence.  So, here is a random sentence: Umm, our smooth desk memorizes facts about many things.\n\n\n> create a random sentence\n\n\nI think I heard you say, create a random sentence.  So, here is a random sentence: Umm, the patient painter consumes ridiculous things..   I believe you said this <emphasis>exact</emphasis> same thing earlier.\n\n\n> who are you\n\n\nI think I heard you ask me a question. The question was: who are you? The answer is, My name is Maeve.\n\n\n> what's your name\n\n\nI think I heard you ask me a question. The question was: what is your name? The answer is, My name is Maeve.",
"date": "2019-03-17"
},
{
"vote": 7,
"title": "Natural Language Processing in Call Centres",
"text": null,
"date": "2019-03-17"
},
{
"vote": 11,
"title": "Python library to parse CoNLL-U files to nested python dicts",
"text": "https://github.com/EmilStenstrom/conllu/",
"date": "2019-03-17"
},
{
"vote": 14,
"title": "Deciding to go for Ph.D; Mid-career professional in Natural Language Processing",
"text": "I currently work at a research lab in a prestigious company doing machine learning and natural language processing.  I already have my masters in Machine Learning, plus keep current with the technology by getting certificates from MOOCs.  I would love to go to the next step and get my doctorate.  My question though is if this is practical?\n\n\nI already am getting positions in research labs where I work with Ph.D's as colleagues.  So I'm not sure if a doctorate would advance me further, unless it will open up new types of positions.  The only benefit I can see getting a doctorate is both to have a prestigious title (Dr.) and to get more money.  Is there a significant increase in salary though when you get a Ph.D as opposed to having a masters currently?  \n\n\nIf this is so, then do you think that the estimated 3-5 years absence from the work force would be worth it?  I'm 36 and I don't know if my age would also be a factor.  I would be 41 by the time I'd earn a doctorate.",
"date": "2019-03-15"
},
{
"vote": 4,
"title": "What is the esiest languge for NLP?",
"text": "Im just curious. As a native Spanish speaker,\n\n\nI think that English is a terrible language to start. Any noun can be a verb at the same time and viceversa.\n\n\nVerb stem terminations are just two for regular verbs and just two conjugations are not compound and differentiated: present, and simple past. Since imperfect is differentiated but compound. Even future is a compound!\n\n\nI'm testing the wikidata database using bash i fear, i tried to develop something for spanish but guess. Wikidata's lexical database is almost empty of spanish words so I switched for English. Spanish has many advantages for nlp: a stem termination for almost every grammatical feature, not many compound conjugations, and much less ambiguity in verbs.\n\n\nBTW may be Latin language is even better foe NLP though that of the five declinations.",
"date": "2019-03-15"
},
{
"vote": 5,
"title": "\"Context Resolution\" Task in NLP",
"text": "Hi!\n\n\nI'm looking for references to a standard(-ish) task/dataset in NLP that is close(-ish) to the following: we have a document with a list of references (sorry), for example, a scientific paper. For every reference, we have a sentence within the document and the corresponding text of another paper. The task is to find sentences/regions within the corresponding text, that \"support\" the sentence in the original text, given some metric. Basically, what anyone would do given cumbersome unexplained statement within some mind-boggling paper.\n\n\n&#x200B;\n\n\nApart from the main question, what are the possible/good/standard metrics one can use? Are there remarkable implementations/users of some in-house solutions/any other useful remarks?\n\n\n&#x200B;",
"date": "2019-03-15"
},
{
"vote": 1,
"title": "sklearn CountVectorizer help !!",
"text": "I am using countvectorizer in a pipeline, and have a max_df set,\n\n\nand I have a set of words (tagged tokens), which I want to make sure are not dropped whatsoever,\n\n\n&#x200B;\n\n\nTLDR; can I override the max_df for a specific set of tokens or boolean function like always_keep(token)\n\n\nso it should only drop the token if ( df >= max_df and not always_keep(token))",
"date": "2019-03-14"
},
{
"vote": 2,
"title": "Hard blocked with summarizers/NLP",
"text": "Iâ€™m in quite the bit of a rut, Iâ€™m trying to create an application that focuses primarily on summarizing verbose and needlessly wordy scientific textbooks. \n\n\nIâ€™ve fooled around with some boilerplate heap ranking algorithms with nltk as well as trying my hand at a sequence to sequence neural net. \n\n\nThe rankings techniques have more issues with uncanniness from large bodies of text. And the neural net... well itâ€™s hard to get training data in a clean way from hundreds of proprietary textbook.\n\n\nHow should I move from here? What sort of books and documents should I be reading? A lot of the papers Iâ€™ve read donâ€™t deal with text at such a scale, or they might have a solution but they never justify the reasoning for the hyper parameters. I think Iâ€™m having more trouble with understanding what is relevant on a linguistic scale rather than developing the application itself.",
"date": "2019-03-13"
},
{
"vote": 17,
"title": "An Idiotâ€™s Guide to Word2vec Natural Language Processing",
"text": null,
"date": "2019-03-13"
},
{
"vote": 3,
"title": "Looking for NLP narrative representation materials",
"text": "I am looking for any materials/suggestions for materials on representing narrative structure using NLP. I am familiar with OWL and Semantic web technology and am pretty familiar with NLP in general but I have had a hard time finding stuff about automatically extracting events or creating ontologies/representations of events from narratives. \n\n\nObviously it's a pretty complex problem and requires the use of various technologies but you would think somebody somewhere would have done substantial work on it. \n\n\nAny advice is appreciated! Thanks",
"date": "2019-03-13"
},
{
"vote": 0,
"title": "How Neural Networks Work- Simply Explained",
"text": null,
"date": "2019-03-11"
},
{
"vote": 11,
"title": "What is the name of this type of Natural Language Processing?",
"text": "When a computer reads a book or article, stores every possible fact in some type of database, and then you can query it later. \n\n\n\"Where did John go to look for work after he was fired in Chapter 4?\"",
"date": "2019-03-11"
},
{
"vote": 2,
"title": "Any models that can learn word and sentence embeddings simultaneously and keep them in a same space?",
"text": "Hello,\n\n\nI am looking to Doc2Vec to learn word embedding and sentence embeddings for information retrieval task, so I need to keep the words and sentences in the same space. For example, a movie review has the mention of Steven_Spielberg, and I want to use the word embedding to look up the sentence embedding, aka I must be able to take the cosine between the word vector of Steven_Spielberg against the sentence vectors to retrieve the reviews of his films. I have been able to get semi-decent performance with Doc2Vec, but are there any models that can help do this task better? Thank you.",
"date": "2019-03-11"
},
{
"vote": 0,
"title": "C Language Programming",
"text": null,
"date": "2019-03-11"
},
{
"vote": 4,
"title": "Meet Arckon Beta, conversational AI",
"text": "Hello everyone. I've been working on an interactive language processing AI for about 7 years, and recently made it available online on Mondays. I am looking for a little feedback to pinpoint the biggest remaining issues.\n\n\nI thought it might interest this sub as the program performs a lot of NLP and NLG by converting sentences to and from a knowledge database of fact triples, while the \"AI\" part lies in its ability to learn new facts and make inferences (GOFAI). The program was built from scratch, so unfortunately I can't direct you to any components that I've used. However I do explain some of its workings on my \nblog\n in layman's terms with links to relevant research. And just for this sub I've added a \"parse\" command.\n\n\nLink to the AI\n\n\nComponents:- spellcheck (typos)- syntax parsing- pronoun resolver / \nwinograd schemas\n- dialogue manager- sentence generation- learning- inference engine- opinion generation- sentiment analysis- \nsimple sarcasm detection",
"date": "2019-03-11"
},
{
"vote": 1,
"title": "Top 16 Machine Learning, Data Mining, and NLP Books",
"text": "[removed]",
"date": "2019-03-11"
},
{
"vote": 11,
"title": "Implementing Neural Networks from Scratch",
"text": null,
"date": "2019-03-10"
},
{
"vote": 20,
"title": "Listen Podcast on \"Challenges in Natural Language Processing\"",
"text": "Hi, \n\n\nI have introduced a new section to my Blog i.e Podcasts. In this i handpick interesting podcast and try to summarize the podcast in a paragraph or two for users to take the decision to listening to it fully or not. Check them at \nhttps://prakhartechviz.blogspot.com/p/podcasts.html\n\n\nDo share your feedback.",
"date": "2019-03-10"
},
{
"vote": 3,
"title": "Noun-Verb Association Help",
"text": "I have a list of actions in the form of \"[verb] x [preposition] y\".\nFor example, \"throw x at y\". \n\n\nI also have a list of objects and then a permutated list of those objects for every x and y combination.\n\n\nWhat I'm trying to do is to be able to select only combinations which make the most sense.\n\n\nFor example, \"throw house at rock\" doesn't make much sense but \"throw rock at house\" does.\n\n\nAre there any pretrained models or libraries that could help with this task?",
"date": "2019-03-09"
},
{
"vote": 11,
"title": "Sentiment analysis datasets... are there any that are more specific than \"negative\" or \"positive\"",
"text": "I was looking at the dataset here: \nhttps://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n\n\nand it seems that it is very broadly \"negative\" or \"positive\", which is fine, but I was wondering if there were any publically available datasets that were more targeted... say towards \"sexual harassment\" or \"fraud\"?\n\n\nIf not, any suggestions as to how would one go about tuning a dataset like that would be appreciated.\n\n\nI'm pretty new to this feel free to ELI5.",
"date": "2019-03-07"
},
{
"vote": 2,
"title": "[Ideas] Framework for studying code mixing",
"text": "Hi,\n\n\nI am trying to study how code mixing works for the past couple of months. During the process I realised a gap that exists in the present space for studying multilingual utterances in the same sentence. One of the major bottle-necks comes to having a large labelled dataset for the same.\n\n\nHaving said that, I am trying to brainstorm on different ideas of creating a framework that can help bridge this gap by some margin. I would love to get ideas on what could help.\n\n\nWhat I am envisioning is - A framework on top of spaCy or nltk that takes a raw dataset (eg: reddit comments) as the input and throws out a labelled dataset mentioning what rows are likely to have code-mixing.\n\n\nWould love to learn more from people who have already worked or working on code mixing. TIA",
"date": "2019-03-06"
},
{
"vote": 5,
"title": "NLP tools for beginner with experience in Java, Kotlin and Javascript?",
"text": "[deleted]",
"date": "2019-03-05"
},
{
"vote": 11,
"title": "Free datasets/corpora for Japanese language text",
"text": "[deleted]",
"date": "2019-03-05"
},
{
"vote": 1,
"title": "How To Ready Your Products Or Services for Worldwide Marketplace?",
"text": "[removed]",
"date": "2019-03-05"
},
{
"vote": 18,
"title": "How to compute sentence similarity using BERT",
"text": "I compute the sentence embedding as a mean of bert word embeddings. And then I would like to compute the sentence similarity or the distance between sentences. I tried using the cosines similarity but is very high. I saw this comment \nhttps://github.com/hanxiao/bert-as-service#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong\n so I guess the cos similarity is not a good measure. Which measure/distance should I use",
"date": "2019-03-04"
},
{
"vote": 0,
"title": "What is the absolute fastest way to get text from a webcam to a google search?",
"text": "[deleted]",
"date": "2019-03-04"
},
{
"vote": 3,
"title": "Tools for corpus stylistics",
"text": "[deleted]",
"date": "2019-03-03"
},
{
"vote": 6,
"title": "If I have a target word, what is the best way to find the nearest word2vec correlates?",
"text": "My word is 'ugly'. I want to find out the most related words based on word2vec. Is a python script more feasible than some app/website I am not aware of?",
"date": "2019-03-03"
},
{
"vote": 1,
"title": "LDC Access",
"text": "[deleted]",
"date": "2019-03-03"
},
{
"vote": 12,
"title": "NLP career opportunities outside academia?",
"text": "So I got interested in various NLP applications for text (sentiment analysis, text mining, information retrieval, you name it). Now I wonder, how are job prospects outside academia in this field? Is there even adequate demand for such skills in the market?",
"date": "2019-03-01"
},
{
"vote": 10,
"title": "What do you guys think about this new program at UBC: Master of Data Science program with Computational Linguistics Specialization",
"text": "I'm finishing up my bachelor's in computer science and linguistics in a few months and debating whether to pursue a master in CS with machine learning specialization or a master of data science program. I'm interning at an ML lab doing a small research project right now but I'm finding more and more that I want to do something that's more practical. I really want to do something related to NLP/computational linguistics and it looks like a lot of job posting require a minimum master's degree so I'm researching what kind of options are out there. I've applied to a CS master with ML specialization which is 2.5 year program but I just came across this new program at University of British Columbia which is just a 10 month program: \nhttps://masterdatascience.ubc.ca/programs/computational-linguistics\n\n\nWhat do you guys think about this? Does the curriculum look like it will prep me to step into NLP career? Any input is appreciated",
"date": "2019-03-01"
},
{
"vote": 5,
"title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition",
"text": null,
"date": "2019-02-28"
},
{
"vote": 1,
"title": "What are the research directions in NLP?",
"text": "English is not my first language and I have some problems with understanding it.\n\n\ntoday someone asked me what research directions look most interesting to me. I honestly don't know what does the \"research directions\" exactly means. \n\n\ndoes it mean for example \"text generation\" or \"semantic analysis\" or something else?\n\n\n&#x200B;\n\n\nthanks for helping.",
"date": "2019-02-28"
},
{
"vote": 1,
"title": "Recommend labels given old labeled datasets",
"text": "[deleted]",
"date": "2019-02-28"
},
{
"vote": 1,
"title": "When Does Arise Need of Portuguese Translation in the Business?",
"text": "[removed]",
"date": "2019-02-28"
},
{
"vote": 3,
"title": "How to improve the quality of a topic model",
"text": "Hello all,\n\n\nI'm trying to figure out some ways to improve the quality of topic models.\n\n\n\n\nIn the computation of topic models with Gensim, there are a lot of different factors that we could change: how alpha, eta hyperparameter, number of passes affect the results? I wonder if there exists a resource that provides us with general guidelines for improving the quality of our topic models.\n\n\n\n\nI have the feeling that removing stopwords is not necessary if we use a Tf-idf model, as this model already penalizes the most common terms. Is my assumption correct?\n\n\n\n\nCan the quality of topic modeling be enhanced by adding bigrams and trigrams? E.g., \"New_York_Times\" considered as a whole word\n\n\n\n\nCan the quality of topic modeling be enhanced by adding the POS tag to words? In this way, for example, the model can discriminate between light/NOUN and light/ADJ\n\n\n\n\nAre there some metrics to evaluate the quality of topic models? I know that Gensim has a topic coherence feature, could you spend a bit words about this?\n\n\n\n\n\n\nThank you everybody!",
"date": "2019-02-27"
},
{
"vote": 27,
"title": "Understanding BERT Transformer: Is Attention All You Need ?",
"text": null,
"date": "2019-02-27"
},
{
"vote": 5,
"title": "Question: What is the professional term of an NLP that can find a missing word?",
"text": "I'm very new to NLP. by \"professional term\" I mean terms like: \"Semantic analysis\" or \"Text generation\" that is used in NPL.\n\n\n&#x200B;\n\n\nI want to create a model that can find the missing words in a text like this:\n\n\nMax is 20yo. ___ is my brother.                                                               \nANS: he.\n\n\nNatural ___ processing (NLP) is a field of computer science.            \nANS: language.\n\n\nA tree is a tall plant with a trunk and ___ made of wood.                  \nANS: branches.\n\n\n&#x200B;\n\n\nWhat term is common for this type of work that I want to do? what do you call it?",
"date": "2019-02-26"
},
{
"vote": 2,
"title": "NLP Summarizer",
"text": "Hi all,\n\n\nWondering if an NLP platform exists that summarizes transcripts between two people? Thanks!",
"date": "2019-02-25"
},
{
"vote": 9,
"title": "Natural Language Understanding experts",
"text": "Who are considered world's top experts in Natural Language Understanding research? I can find many for Natural Language Processing in general, but trying to understand who are breaking new ground with respect to Natural Language Understanding?",
"date": "2019-02-25"
},
{
"vote": 5,
"title": "Results from BERT LM are not deterministic?",
"text": "[deleted]",
"date": "2019-02-24"
},
{
"vote": 3,
"title": "wrap-up of language and translation at Applied Machine Learning Days",
"text": null,
"date": "2019-02-21"
},
{
"vote": 6,
"title": "Judit Ãcs on BERT and ELMO for multilingual with subwords",
"text": null,
"date": "2019-02-20"
},
{
"vote": 4,
"title": "Hugging Face Releases Pytorch-BERT, Pretrained Models and More",
"text": null,
"date": "2019-02-20"
},
{
"vote": 2,
"title": "Classification of Web Page into home page, about-us page etc",
"text": "I have seen lot of web classification problems which classifies websites into categories like Agriculture, Forestry, Fishing, Mining, Construction, Manufacturing, Transportation & Public Utilities etc.  \n\n\nBut is there any way to classify a web page as\n\n\n\n\nHome page\n\n\nPricing page\n\n\nBlog page\n\n\nResources page\n\n\nContact us/ Enquiry page etc.\n\n\n\n\nIs there any ways to classify the same?",
"date": "2019-02-20"
},
{
"vote": 1,
"title": "Methods for pre-train embeddings for large text ?",
"text": "CBOW predicts the word given the context\n\n\nSkip-Gram predict the context C given and input word\n\n\nELMO pre-trained using bidirectional language models \n\n\nBERT pre-trained using masked words and next sentence classifications tasks\n\n\nAre there any other popular methods that are used today to generate embeddings ?\n\n\nWhat do you think would be the next task on which models will be pre-trained to generate embeddings ?\n\n\n&#x200B;\n\n\n&#x200B;",
"date": "2019-02-20"
},
{
"vote": 20,
"title": "What do you think the future of document embeddings is ?(2019 after BERT, ELMO ...)",
"text": "Do you think that we will be able to create more meaningful representations of \nlarge\n text documents ?",
"date": "2019-02-19"
},
{
"vote": 7,
"title": "What my first Silver Medal taught me about Text Classification and Kaggle in general?",
"text": null,
"date": "2019-02-19"
},
{
"vote": 5,
"title": "The Macroscope: A tool for examining the historical structure of language",
"text": null,
"date": "2019-02-19"
},
{
"vote": 1,
"title": "Multi-field document classification",
"text": "[deleted]",
"date": "2019-02-19"
},
{
"vote": 5,
"title": "NLP on a Forums thread with 200 comments",
"text": "Longtime listener, first-time caller. \n\n\nWhat's the simplest way to text analyze a discussion among multiple parties? Following a thread and I want to use natural language processing to get an idea of what the most popular themes are. \nhttps://sellercentral.amazon.com/forums/t/ask-questions-to-amazon-about-a-to-z-guarantee/443727",
"date": "2019-02-19"
},
{
"vote": 9,
"title": "Optimizing word embeddings for the web",
"text": null,
"date": "2019-02-18"
},
{
"vote": 8,
"title": "Simple Keras implementation for some sequence labelling models based",
"text": "A simple and easy to understand Keras implementation for some sequence labelling models based on neural networks which I read/write about a while a go:\n\n\nhttps://github.com/davidsbatista/SLANG\n\n\nhttp://www.davidsbatista.net/blog/2018/10/22/Neural-NER-Systems/",
"date": "2019-02-18"
},
{
"vote": 11,
"title": "How big tech is battling for the $49B voice market",
"text": null,
"date": "2019-02-17"
},
{
"vote": 1,
"title": "Lists of most common homographs in popular languages?",
"text": "I'm thinking of making a program that identifies words users don't already know, but I realized that due to some written words being the same, users might understand one meaning but not another unrelated sense. Are there resources listing several homographs (preferably the most common ones) for some common languages (English, Spanish, French, etc.; I'm looking for at least two) so that I can account for these cases?",
"date": "2019-02-17"
},
{
"vote": 8,
"title": "OpenAI's GPT-2 attains state-of-the-art metrics on Winograd Schema, reading comprehension, and compression progress of Wikipedia corpus.",
"text": null,
"date": "2019-02-16"
},
{
"vote": 0,
"title": "Understanding output of LSTM for regression",
"text": "[deleted]",
"date": "2019-02-16"
},
{
"vote": 0,
"title": "Here is my pytorch implementation of the model described in the paper DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs (https://arxiv.org/pdf/1606.00915.pdf) Source code: https://github.com/vietnguyen91/Deeplab-pytorch",
"text": null,
"date": "2019-02-16"
},
{
"vote": 1,
"title": "How do you clean this text?",
"text": "[removed]",
"date": "2019-02-15"
},
{
"vote": 1,
"title": "Are there any python modules or C++ libraries that can help me to extract keywords out of a text?",
"text": "We are doing a quick proof of concept so I would like to use pre-made packages or libraries. Given a group of texts about a specific topic, we would like to assign \"tags\" extracted from the text. I am using either C++ or python. I can give more information if needed. Thanks!",
"date": "2019-02-14"
},
{
"vote": 9,
"title": "Any pretrained machine translation systems?",
"text": "[deleted]",
"date": "2019-02-13"
},
{
"vote": 1,
"title": "What's the current state of the art in NLU?",
"text": "[removed]",
"date": "2019-02-13"
},
{
"vote": 2,
"title": "Is there a way to understand the type of a sentence?",
"text": "I am a beginner, just started studying around NLP, specifically various language models. So far, my understanding is that - the goal is to understand/produce natural language.\n\n\nSo far whatever methods I have studied, speak about correlation of words, using correct combination to make a meaningful sentence. Also I understand that the language modelling do not really care about the punctuation marks (or did I miss it?)\n\n\nThus I am curious is there a way they can classify sentence types such as Declarative, Imperative, Interrogative or Exclamatory?\n\n\nDoes this come under the NLP domain at all?\n\n\nIf yes, please point me to some resources. Thanks!",
"date": "2019-02-12"
},
{
"vote": 0,
"title": "Are there any special benefits to learning Korean?",
"text": "[deleted]",
"date": "2019-02-12"
},
{
"vote": 0,
"title": "First Steps in NLP â€” and with a non-mainstream language",
"text": "I have no background in NLP but I like working with Python and I know there are lots of tools built for Python. Iâ€™d like to do some projects with a language which wonâ€™t have much of a body of existing work. \n\n\nIâ€™ve noticed that some tools (most tools?) have a list of languages that they have models for already. (StanfordNLP is what I just read about). \n\n\nAny tips on a few bite-sized projects I could do on some non-traditional language? Or how hard would it be to build a new model for something like Stanford NLP?",
"date": "2019-02-12"
},
{
"vote": 5,
"title": "State of the art for key-phrase extraction?",
"text": "I have looked at a few conventional methods for this and also spacy to extract keyphrase. my ultimate goal is to capture a keyphrase that is relevant to a certain field (like Finance, Medicine, technology etc). Currently, I just use the result of the spacy keyphrase extraction as candidates and work on top of that to filter and choose which is similar to my field. what are the best approaches for the Key-Phrase extraction as the whole?",
"date": "2019-02-11"
},
{
"vote": 15,
"title": "My implementation of 3 NLP models for text classification in Pytorch and Tensorflow",
"text": null,
"date": "2019-02-10"
},
{
"vote": 1,
"title": "Determining Gender based on Title/Honorific",
"text": "I have some written data from the UK parliament in the form given here: \nhttps://api.parliament.uk/historic-hansard/lords/2005/mar/17/courts-act-2003-consequential-provisions\n\n\nI've extracted the text into author name, contribution, date, etc. and would like to determine the speaker's gender based on their title (where possible), since most titles are gendered.\n\n\nIs there a list, ideally as part of a Python library, that just contains a ton of titles for each gender? Currently, I have my own hard-coded, but obviously that's going to be less comprehensive and it'll look neater as part of a library.\n\n\nI've had a look online, but the only things that have popped up are rather short lists, and I'd like to be more comprehensive than that.\n\n\nCheers",
"date": "2019-02-10"
},
{
"vote": 7,
"title": "PyTorch Implementation of character based Convolutional Neural Network for text classification",
"text": null,
"date": "2019-02-10"
},
{
"vote": 5,
"title": "What is a good coherence score for an LDA model?",
"text": "I created a topic model and am evaluating it with a CV coherence score. I got a result of 0.36.\n\n\nI'm not sure what this means. Many tutorials for topic modeling online seem to have got a score in the range of 0.3 to 0.6, but what actually is a good coherence score?\n\n\nIs this score just a comparative score?",
"date": "2019-02-10"
},
{
"vote": 6,
"title": "Computational Linguistics research topics",
"text": "[deleted]",
"date": "2019-02-10"
},
{
"vote": 19,
"title": "Free datasets for chatbot training",
"text": null,
"date": "2019-02-09"
},
{
"vote": 6,
"title": "Conventional Methods for Text Classification",
"text": null,
"date": "2019-02-09"
},
{
"vote": 3,
"title": "Entity sentiment dataset?",
"text": "Does anyone know where a decent entity-level sentiment dataset is? Many of the ones I've found are not available or are very small (n=1000).",
"date": "2019-02-08"
},
{
"vote": 16,
"title": "Facebook Boosts Cross-Lingual Language Model Pretraining Performance",
"text": null,
"date": "2019-02-08"
},
{
"vote": 1,
"title": "Any tools/platform to rate and evaluate a clustering for a text corpus?",
"text": "Hi,\n\n\nI am looking desperately for a tool, a platform, anything really, that would help me rate and asses the quality of my clustering of a corpus.\n\n\nI just want something that can show like 10 documents in a cluster and put a rating on it like 7/10. Just human evaluation.\n\n\n&#x200B;\n\n\nI found nothing like that, I just need a simple tool, thanks !",
"date": "2019-02-08"
},
{
"vote": 3,
"title": "What are some of the best libraries/tools to scrub personally identifiable information?",
"text": null,
"date": "2019-02-07"
},
{
"vote": 2,
"title": "Why BERT keep only the first token for classification?",
"text": "Source: \nhttps://github.com/google-research/bert/blob/master/modeling.py#L228\n\n\nI understand you want to go from a matrix of embeddings to a vector, but why taking the first token and not summing all tokens or using an attention layer. Also, is it the same for the Transformer?",
"date": "2019-02-06"
},
{
"vote": 16,
"title": "Shannon.AIâ€™s Glyce Masters 13 Chinese NLP Tasks",
"text": null,
"date": "2019-02-04"
},
{
"vote": 2,
"title": "Building a Better Profanity Detection Library with scikit-learn",
"text": null,
"date": "2019-02-04"
},
{
"vote": 1,
"title": "NLP",
"text": "[deleted]",
"date": "2019-02-03"
},
{
"vote": 20,
"title": "What are some of the most hyped NLP papers in recent years?",
"text": "Recent years are 2017-19. I know BERT and ElMo were awesome, but what are other papers that deserve the spotlight.",
"date": "2019-02-02"
},
{
"vote": 4,
"title": "Beginner help: Is this a really stupid way to use Stanford NER/CRF? Can anyone suggest a better method? Would it even work?!",
"text": "[deleted]",
"date": "2019-02-01"
},
{
"vote": 1,
"title": "This device can translate like a pro. 40+ Language Instant Voice Translator is the Innovative, Fun &amp; Quintessential device that enables you to communicate in over 40 Languages! ALL Supported Languages are interchangeable. Replies can be understood with a simple change to the settings!",
"text": null,
"date": "2019-02-01"
},
{
"vote": 17,
"title": "A good site for finding NLP / AI jobs",
"text": null,
"date": "2019-02-01"
},
{
"vote": 9,
"title": "What kind of methods would you utilize if you were asked to cluster users by search query?",
"text": "I'm working on a side project where I'd like to take some data on previous search behavior and cluster users based on their search terms that occur within a single day. Hoping someone can at least point me in the direction of papers or resources that reference this!",
"date": "2019-01-31"
},
{
"vote": 0,
"title": "How NLP Neural Networks Work- Simply Explained",
"text": null,
"date": "2019-01-31"
},
{
"vote": 2,
"title": "Data extraction question",
"text": "Hi all, I have a question. I have a lot of text documents that are not in any particular format (they're just legal documents) and I want to try to extract certain data points, but I'm not familiar with all the NLP jargon so I don't know what to search for in trying to learn how to do this. It's basically \"person A sold X to person B for $Y\". I know I need NER (named entity recognition) for the people, but I need to also know which person is the buyer and which is the seller, how much it's sold for, and what is being sold.\n\n\n&#x200B;\n\n\nCan anyone help point me in the right direction for what, besides NER, I need to be reading about to accomplish this? There may be existing things like SpaCy that can do everything I need, I just can't tell without knowing what I'm looking for yet. Any help would be greatly appreciated! :)",
"date": "2019-01-30"
},
{
"vote": 1,
"title": "does there exist an audio data set, specifically a famous person's voice? similar to ImageNet or any kind of repo of voices?",
"text": "I'm new to both ML and NLP. Trying to synthesize speech of known persons. Thanks!",
"date": "2019-01-30"
},
{
"vote": 3,
"title": "Assigning weights to words in word vectors",
"text": "I'm currently working on a problem where I need find relevant scientific papers given a set of keywords. I'm still in the exploratory phase of this project and have been playing around with a bag of words representation and running some basic classifiers. I've been using python and sklearn. \n\n\n&#x200B;\n\n\n For the training set I'm working with now, I want to find papers on the topic of mechanically stimulated ATP release in cells. I have a set of thousands of papers from a past analysis queried from medical databases according to a keyword search.  The papers are marked as relevant or not relevant.  The keywords are \"ATP\", \"adenosine triphosphate\", and then dozens of words and bigrams related to mechanical stimulation, e.g. \"stimulate\",  \"perturb\", \"press\", etc. These keywords are used as the vocabulary for the model. \n\n\n&#x200B;\n\n\n**I was wondering if there is a way to assign more importance/weight to specific words/features in the vector?** I think ATP and adenosine triphosphate are relatively more important words, is there anyway to incorporate this in to the model? \n\n\n&#x200B;\n\n\nI hope this is a clear enough explanation of my problem. Any tips or advice is greatly appreciated.  If anyone has any suggestions for different ways to approach the problem entirely I would love to hear them, I'm very new at this. Thanks!",
"date": "2019-01-30"
},
{
"vote": 19,
"title": "Introduction to Deep Learning by Eugene Charniak",
"text": null,
"date": "2019-01-30"
},
{
"vote": 2,
"title": "Building a model to classify Text vs Non-Text",
"text": "I'm trying to build or find a pre-trained machine learning model that classifies a sentence into \"readable text\" or \"weird text\". For example:\n\n\nHi, my name is Petter. --> \nreadable text\n\n\n23 25 .. / jam38 [] --> \nweird text\n\n\nCan you help me with data sets or references? Thank you in advance.",
"date": "2019-01-29"
},
{
"vote": 0,
"title": "Contextualizing Word Clouds",
"text": null,
"date": "2019-01-28"
},
{
"vote": 8,
"title": "How much does batch training size affect text classification accuracy?",
"text": "I'm using BERT to fine-tune a sentence pair classification model and my GPUs only have 8GB memory. My batch training size is 12 to avoid memory issues, would my model be more accurate if batch training size can be 32 or more?",
"date": "2019-01-28"
},
{
"vote": 25,
"title": "Language Models and Contextualised Word Embeddings",
"text": "I've compiled notes I took in learning and understanding more about contextualised word embeddings. \n\n\nEssentially comparing 3 methods: ELMo, Flair Embeddings, BRET\n\n\nI also make a small and quick introduction on the classic/static embeddings methods (i.e., skip-gram, GloVe, fastText).\n\n\nEssentially it's information and explanations gathered from papers, tutorials and blog posts, and summarised in one post:\n\n\nhttp://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/\n\n\nHope you enjoy reading it :)",
"date": "2019-01-27"
},
{
"vote": 17,
"title": "What Kagglers are using for Text Classification",
"text": null,
"date": "2019-01-26"
},
{
"vote": 11,
"title": "What are some good Python libraries for text summarization?",
"text": null,
"date": "2019-01-23"
},
{
"vote": 5,
"title": "Building your first chatbot to search restaurants on Yelp and connecting it with Slack",
"text": "Follow this tutorial to learn how to build a chatbot integrated with Slack to search for restaurants on Yelp. Happy bot building! \n\n\nhttps://cai.tools.sap/blog/building-first-slack-chatbot-sap-conversational-ai/\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/7mp6dmz3n6c21.png?width=862&format=png&auto=webp&s=471ebd5cf226920a57513fccdeb42fe4df590a11",
"date": "2019-01-23"
},
{
"vote": 1,
"title": "Does terminology extraction belong to feature extraction or feature engineering in NLP?",
"text": "I want to extract some phrases in news article, but they are verbs and nouns, so name entity may not be appropriateã€‚Where to start from?",
"date": "2019-01-22"
},
{
"vote": 29,
"title": "LASER natural language processing toolkit - Facebook Code",
"text": null,
"date": "2019-01-22"
},
{
"vote": 8,
"title": "Advice on NER and IE with word embeddings",
"text": "So Iâ€™m fairly new to NLP. I have done plenty of doc classification and topic modelling at a basic level.  Recently I have been researching and running examples of Information Extraction and NER. From using oob spacy and coreNLP models to training my own. What I am confused about is how to incorporate word embeddings and sentence2vec etc. \n\n\nOne example of what I would like to do is some IE and use embedding so that (John, is father to, Alice) is treated the same as (John, is parent to, Daughter). \n\n\nDoes anyone have any tutorials or practical examples of using embeddings in these sort of tasks?\n\n\nThanks",
"date": "2019-01-22"
},
{
"vote": 12,
"title": "How NLP Neural Networks Work- Simply Explained",
"text": null,
"date": "2019-01-21"
},
{
"vote": 28,
"title": "NLP Learning Series: Text Preprocessing Methods for Deep Learning",
"text": null,
"date": "2019-01-19"
},
{
"vote": 3,
"title": "Looking for a good lemmatizer for biological sciences",
"text": "I want a lemmatizer for processing biomedical texts. I've tried the nltk WordNetLemmatizer but I'm not happy with the results. For example, I want it to trim \"stimulates\" to \"stimulate\", but it won't, it just keeps it as is. It's doing this for a lot of terms.  Does anyone know of any lemmatizer (that can be used in a python environment) that does a good job processing text related to the biological sciences?\n\n\nI'm also open to trying stemming if people think that's a better solution.\n\n\n&#x200B;\n\n\nAny suggestions greatly appreciated, thanks!",
"date": "2019-01-18"
},
{
"vote": 2,
"title": "Why are very long sentences not good for deep learning models?",
"text": "Is this just hearsay or is it true?  Why or why not?",
"date": "2019-01-18"
},
{
"vote": 4,
"title": "Bilingual Word Embeddings",
"text": "Hello,\n\n\nI am in search of bilingual word embeddings that were trained in English and German and are in same vector space. Same goes for Spanish and French also in same vector space.\n\n\nThank You\n\n\n&#x200B;",
"date": "2019-01-15"
},
{
"vote": 6,
"title": "Fresh evaluation of pre-trained MT (23 MT engines, 48 language pairs, January 2019)",
"text": null,
"date": "2019-01-15"
},
{
"vote": 3,
"title": "Right suite, Wrong suit : A neophyte's stumblings",
"text": "short version\n How to automatically re-introduce spaces to OCRed text \"thatendsup lookinglike this\"? ~2000 pages worth.\n\n\nI'm tackling a project that so far likely would be faster if I simply copied out all 500 pages of text by hand.\n\n\nUsing OCR (Tesseract 4.0.0), I've been trying to slice and dice old gaming manuals into a useable text document.\n\n\nThe document scans are not the highest quality (~72ppi), usually contain two columns of text per page, occasionally with pictures strewn about or tables.\n\n\nSo far I have:\n\n\n\n\nScanTailor Advanced (1.0.16) > auto-deskew the files (great!)\n\n\nBriss (2.0) >  trim the image borders without nicking the text\n\n\nScanTailor (again)  > convert the images to a clean B&W (Sauvola filter)\n\n\nBriss (again) > crop images for a set of left column & right columns per page\n\n\nPDFtk > shuffle the two piles of images back together so all columns are in the right order \n\n\nTesseract (4.0.0) > basic OCR to PDF, no fancy settings\n\n\n\n\nWhat's got me slumped is the output.  for the most part the text is 98% accurate (good enough for my needs), but more often than not words are stuck together \"likethis\", and I'd like to trust that words so formatted, found in a typical English dictionary (likely no words above a 10 grade reading level?), have the words properly spaced back out with, well, spaces.\n\n\nAutomatically identifying and removing all non-text from the document would also be a huge boon, but that's not nearly as much work to do manually.",
"date": "2019-01-14"
},
{
"vote": 2,
"title": "Help analyzing speech in audio and transcripts using R (or a tool in tandem with R)",
"text": "Hello everyone!\n\n\nI am doing a quantitative analysis of speech patterns and Iâ€™m looking for a tool or programming steps (in R) that would allow me to apply my methodology.\n\n\nTo be succinct hereâ€™s what I have and I want to do:\n\n\nWhat I have: \na)\ta series of audio interviews files; \nb)\tthese audio files respective transcripts in Word with i) manually identified timestamps for when subject X starts and stops speaking and ii) timestamps for when subject Y starts and stops speaking.\n\n\nWhat I want to do: \n1)\trepeated or integrated words by subject X and Y after one speaks the same words (to measure a sort of echo effect or mimicry);\n2)\tquantify the number and lengths of silences mid-speech by subject X and Y (trying to measure if thereâ€™s a relationship between emotional content and the time it takes a subject to say his/her words). I imagine I would have to set the sensitivity to a particular decibel in order to ignore chair movements or sighing or other non-verbal sounds.\n\n\nAny input will be tremendously valuable. Iâ€™ve been dabbling in R for a while now and I am familiar with the tidyverse. What seems to pose the biggest challenge for me is to think of which tools to use in tadem with R to analyze audio content.\n\n\nThank you so much.",
"date": "2019-01-14"
},
{
"vote": 1,
"title": "COCA corpus: number of hits",
"text": "Hello everyone!\n\n\nI have some trouble with COCA corpus. I am searching for the suffix -ness to compute its productivity. The problem is, I can't go further than 6590 hits while I think there could be more. Has it ever happened to anyone? I probably missed something but I can't figure what.\n\n\nThanks in advance!",
"date": "2019-01-14"
},
{
"vote": 8,
"title": "Anyone wants to help me with a seq2seq translation implementation?",
"text": "Hello\n\n\nI've been working on a seq2seq translation implementation in \nJulia\n (very similar to Python in terms of code readability) for some time now.\n\n\nAlthough my code runs and loss decreases a bit, the translations I get are quite rubbish.\n\n\nI've written a notebook with explanations about my code. \nhttps://nbviewer.jupyter.org/github/merckxiaan/flux-seq2seq/blob/master/seq2seq%20in%20flux.ipynb#The-Model\n\n\nFor the data preparation, I mainly follow \nthe official Pytorch tutorial on seq2seq\n. I'm really not sure what I'm doing wrong so any guidance would be appreciated.",
"date": "2019-01-13"
},
{
"vote": 4,
"title": "Is there any python library for generating sentences from keywords?",
"text": null,
"date": "2019-01-12"
},
{
"vote": 0,
"title": "Modeling Intent using Syntax, is it possible?",
"text": null,
"date": "2019-01-11"
},
{
"vote": 6,
"title": "I'm confused with what my lecturer said",
"text": "So I'm writing this thesis about speech recognition with CNN.\n\n\nSo, in my related work i referenced some paper with high accuracy already, one is even 100% (But i think its because the dataset isnt really high), and all those paper is not CNN or even inside Neural Network.\n\n\nBut my lecturer asked why would I choose something that already has a good accuracy in previous paper,\n\n\nI said because its entirely new method and then I talked about what AlexNet did, but she still somehow not sure.\n\n\nDo you guys can tell me something to convince her?? because I believe I am not in the wrong here. (the related work is all with MFCC and various classification methods like hmm and such)",
"date": "2019-01-10"
},
{
"vote": 1,
"title": "Meeting #45: Intrinsic Dimension Estimation with Marina Gomtsyan from HSE, Skoltech",
"text": "[deleted]",
"date": "2019-01-10"
},
{
"vote": 11,
"title": "Anyone looking for a NLP project to work on and a bit of direction/mentorship?",
"text": "Posted this on /r/machineslearn but decided to post here as well since these are NLP focused projects. I suggested some stuff related to research paper recommendation, since that's my focus and there are several ideas that I'll probably won't have the time to fully pursue.\n\n\nI got the idea to toss this offer out here too in case someone is looking for projects to work on / work with.\n\n\nSince it's an area I'm interested in, I would give whatever insights I have along the way and help problem-solve whenever you get stuck, including coding in Tensorflow and parsing datasets I'm familiar with. I'm not really a ML expert, but you learn much more and get a lot more done with you have the power of 2+ minds on a project. I have a pretty high interest in research paper search and recommendation, so if you ever was curious about that I can talk about that in quite a bit. \n\n\nThe arrangement would be pretty casual. Just hmu whenever you want, no time frames or strict goals anything like that from my end.",
"date": "2019-01-09"
},
{
"vote": 11,
"title": "When using tokenizers, why do we limit the number of words based on frequency?",
"text": null,
"date": "2019-01-08"
},
{
"vote": 4,
"title": "TF-IDF in Twitter",
"text": "Hi, I am trying to better my text classification model and my team was trying to do text classification  based on Donald Trump's tweets.\n\n\nI think one of the main issue with the task is due to the TF-IDF - due to Twitter's word limit, Trump's tweets are usually cut off, therefore, this affects the TF-IDF outcomes. For instance,\n\n\n\n\nCrooked Hillary! She has never been so dishonest, and she has the support of all the companies...\n\n\n... and voting for Crooked Hillary will mean that America will never break free of these companies.\n\n\n\n\nThe above will yield a different result as compared to:\n\n\n\n\nCrooked Hillary! She has never been so dishonest, and she has the support of all the companies and voting for Crooked Hillary will mean that America will never break free of these companies.\n\n\n\n\nAnd I feel the latter will be able to yield a TF-IDF that is better representative of what Trump was trying to say. \n\n\nPlease feel free to have a look and let me know if I am saying is correct. I am trying to find ways to merge all his tweets that are of similar content so that I can have a dataset that is more representative of what he is trying to say.",
"date": "2019-01-07"
},
{
"vote": 1,
"title": "How to model dialogue?",
"text": "Do you know any methods that does not require ML to model dialogue? I tried looking online and it was hard to find. I am looking for some context based generation of dialogue.",
"date": "2019-01-04"
},
{
"vote": 1,
"title": "Is there any model that can detect sentence error?",
"text": "[removed]",
"date": "2019-01-04"
},
{
"vote": 3,
"title": "Computer Speech question",
"text": "Apologies, there doesn't seem to be a subreddit for this...  Are there any speech engines that are more capable than just text-to-speech?  I need a system where emphasis, inflection, speed, etc., can be coded for...  basically, I'm needing to code for snark, the off-the-shelf stuff can't cope.  Thanks for direction/redirection/links.",
"date": "2019-01-02"
},
{
"vote": 1,
"title": "Why Need To Take Help From Best Source Of Kannada Translation?",
"text": "[removed]",
"date": "2019-01-02"
},
{
"vote": 6,
"title": "full example of how to use DataHandler in pytext",
"text": "Hi all -\n\n\n&#x200B;\n\n\nI know `pytext` is brand new, but the documentation here (\nhttps://pytext-pytext.readthedocs-hosted.com/en/latest/create_new_task.html\n) for creating a custom Task is pretty confusing.  I've been muddling my way through with moderate success, but I'm stuck on the DataHandler part.\n\n\n&#x200B;\n\n\nI have experience using `torchtext` (\nhttps://torchtext.readthedocs.io/en/latest/index.html\n) which is integrated with `pytext` but I still can't quite follow the example. \n\n\n&#x200B;\n\n\nDoes anyone have a full working example of how to use the `DataHandler`?  Note: I am *not* looking for a `config` file, but the actual `class`es that were overridden for a custom `Task`.\n\n\n&#x200B;\n\n\nThanks in advance!",
"date": "2018-12-31"
},
{
"vote": 1,
"title": "Need help decoding Morse code woke up to someone on the radio",
"text": null,
"date": "2018-12-30"
},
{
"vote": 10,
"title": "How NLP Neural Networks Work- Simply Explained",
"text": null,
"date": "2018-12-29"
},
{
"vote": 3,
"title": "Paragraph/ short text splitting",
"text": "Do you know of papers/ datasets dealing with the problem of splliting a given paragraph / short text into different  topics/ semantic segments?",
"date": "2018-12-29"
},
{
"vote": 7,
"title": "Low resource language dependency parser",
"text": "I'm looking to parse some specific relationships in a low resource language (Thai). From my research there's no tool available online. Also from what I've seen on ConLL shared Task the best parser there get very low accuracy (e.g 10%) on the thai dataset.\nDo you have any tip of how should I go about this?",
"date": "2018-12-27"
},
{
"vote": 3,
"title": "Replacing low-prob words in doc matrix with UNK for dealing with unseen words",
"text": "I successfully have a document-term matrix.\n\n\n\n\n&#x200B;\n\n\n\n\nI want to replace the words (columns, naturally) in the dtm that have extremely low incidence, with a universal \"UNK\" label for unknown, before training.  Then I'll train accordingly.  When I get test/production data for predictions, I'll make a dtm of it, match against the training dtm used for building the model, and for any unknown (new) word in the test/production data (as well as again for extremely low-incidence words), I'll similarly replace them with \"UNK.\"  Thus, I train a probability for \"UNK\" for each class, and don't just drop all new words every time.  I'd like to be able to compare the 2 performance of these 2 approaches.\n\n\n\n\n&#x200B;\n\n\nMy question is, after I take my sparse matrix and change the colnames to UNK for so many, do I need to consolidate all of my \"UNK\" columns into 1 summed column?  I think so.  The math is different (for naive bayes training), for 2 \"UNK\" columns of 1 entry (incidence of the word) each, versus for 1 column with 2 incidences of the word, isn't it?\n\n\n(sorry, it's not letting me fix the formatting at the top)\n(btw, no matter what I do, I can never post images.  it will preview the image in this text box, allow to enter a caption, and then never render the image.  would appreciate some advice there, it would help with this q).",
"date": "2018-12-25"
},
{
"vote": 1,
"title": "Are you interested in NLP and would like to learn more with tutorials? Check out this new youtube channel, Discover Artificial Intelligence. :)",
"text": null,
"date": "2018-12-25"
},
{
"vote": 2,
"title": "Any advices/guidances for an absolute beginner related to below scenario?",
"text": null,
"date": "2018-12-24"
},
{
"vote": 6,
"title": "How to go about multi-language sentence pair classification task",
"text": "I am hoping to classify sentence pairs in two different languages (English A and Chinese B). How would I go about this?\nAnother possible better classification task I am considering is classifying triplet-sentences. E.g English A, Chinese B, and machine-translation of Chinese B in English.\n\n\nOR\nIf the above is not recommended, I can also consider classifying English A and machine-translation of Chinese B in English, I feel this would be less data for the model to learn though.",
"date": "2018-12-24"
},
{
"vote": 18,
"title": "A potential approach to address the explosion of NLP paper submissions",
"text": null,
"date": "2018-12-24"
},
{
"vote": 18,
"title": "Wikipedia2Vec: An Optimized Implementation for Learning Embeddings from Wikipedia",
"text": "Abstract: We present Wikipedia2Vec, an open source tool for learning embeddings of words and entities from Wikipedia. This tool enables users to easily obtain high-quality embeddings of words and entities from a Wikipedia dump with a single command. The learned embeddings can be used as features in downstream natural language processing (NLP) models. The tool can be installed via PyPI. The source code, documentation, and pretrained embeddings for 12 major languages can be obtained at \nhttps://wikipedia2vec.github.io/\n.",
"date": "2018-12-23"
},
{
"vote": 0,
"title": "Novice Question",
"text": "Is NLP tech useful for chatbots that aren't just regurgitating from disparate sources? The chatbot I read about in a medium article (\nhttps://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e\n) seemed pretty useless for my purposes. Is there a chatbot that forms memories. For ex:\n\n\nme:\"hello, I am a traveller looking to get to Rome\"\n\n\nbot: limits possible responses to those useful to traveler to Rome \"The armies of Sulla have just sacked Rome, perhaps now is not the time to travel to Rome\"\n\n\nme: I seek my fortune; the chaos in Rome is why I travel there.\n\n\nbot: changes responses to be for adventuring types \"I overheard men in the tavern talk of a trade expedition to Rome. Perhaps they can help you.\"\n\n\nBasically, I want to make DnD world and NPCs. Do I have to program in that \"looking to get to\" and \"heading to\" is the same INTENT? Or has it already been done in the NLTK?\n\n\n&#x200B;\n\n\n&#x200B;",
"date": "2018-12-20"
},
{
"vote": 10,
"title": "Lingua: written natural language detection for both long and short text",
"text": "Hello dear NLP community,\n\n\nI know that Reddit is not primarily about self-promotion. However, I think that this community is always interested about new software solving NLP-related problems better than previous approaches. If I'm totally wrong with this assumption, then I'm sorry.\n\n\nI'm a computational linguist working as a Java software engineer. In my free time, I'm currently learning Kotlin.  For one of my personal needs, I've written a small library named \nLingua\n  that detects the language of written text, suitable for both long and  short texts. The code and detailed instructions can be found here:\n\n\nhttps://github.com/pemistahl/lingua\n\n\nI was trying to analyze Twitter messages and other pretty short documents  but other language detection libraries only work reliably with long texts. So I thought I could practice my Kotlin skills and write my own  library that tries to remedy detection problems for short text fragments. It is in a very early stage of development but I'm planning  to continuously keep moving it forward. In \ncomparison with Apache Tika\n, \nLingua\n already outperforms it in accuracy for short paragraphs of text by up to 15%.\n\n\nAny constructive comments on the idea and my approach to it are very much welcome. Also, if you find this library useful I would be very happy to hear about your experience with it. Direct contributions through issues or pull requests are appreciated as well. However, if it's the totally wrong place here for my matter, then feel free to delete this post.\n\n\nThanks a lot and regards from Germany, Peter",
"date": "2018-12-20"
},
{
"vote": 7,
"title": "Where and how should a beginner start with natural language processing?",
"text": "I've had experience with Python, and I know the basics of machine learning, deep learning and stats. I've been meaning to delve into NLP given how interesting the possibilities are. \n\n\nI've already looked through reddit for resources but they all suggest Dan Jurafskys online course, which sadly isn't available anymore.\n\n\nSo are there any alternatives or should I just continue  with what my professor has suggested i.e going through the book 'Speech and Language Processing' along with 'Handbook of Natural Language Processing'. But seeing as it's all theory, can someone suggest a good hands on resource or a completely different approach? Thank you.",
"date": "2018-12-19"
},
{
"vote": 7,
"title": "Q&amp;A with",
"text": "/u/vgratian (@vgratian) recently shared this very readable doc \nhttps://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/view\n that was put together at Deep Learning Indaba.\n\n\nThe questions are:\n\n\n\n\nWhat do you think are the three biggest open problems in NLP at the moment?\n\n\n\n\nWhat would you say is the most influential work in NLP in the last decade, if you had to pick just one?\n\n\n\n\nWhat, if anything, has led the field in the wrong direction?\n\n\n\n\nWhat advice would you give a postgraduate student in NLP starting their project now?\n\n\n\n\n\n\nThe answers are from an incomplete but really formidable array of people in NLP and DL, the word that this doc brings to mind is \ndensity\n:\n\n\nAlta de Waal\t\nAnders SÃ¸gaard\t\nAnnie Louis\t\nBarbara Plank\t\nBernardt Duvenhage\t \nBrink van der Merwe\t \nChris Dyer\t\nChristine de Kock\t\nDirk Hovy\t \nFelix Hill\t \nGeorge Dahl\t\nIsabelle Augenstein\t\nJan Buys\t\nKaren Livescu\t\nKevin Gimpel\t\nKevin Knight\t\nKyunghyun Cho\t \nLea Frermann\t\nMaletÅ¡abisa Molapo\t\nManaal Faruqui\t \nMichael Roth\t\nMiguel Ballesteros\t\nMikel Artetxe\t\nRichard Socher\t\nSebastian Riedel\t\nYoshua Bengio\t\n\n\nI sense this sub is full of PhD students with similar questions, and I also get many questions like this, better to get a diverse set of opinions from people with far more output on the research side.",
"date": "2018-12-18"
},
{
"vote": 9,
"title": "NLP newbie: finding close matches in a store database",
"text": "Dear redditors,\n\n\nI have a database with names of stores and their address. I'm looking to unify almost-duplicates rows such as:\n\n\n\n\nstore name: \"Danny's Liqeurs\", address: \"Washington blvd. 122\"\n\n\n\n\nand\n\n\n\n\nstore name: \"Dani's Liqeur store\", address: \"Washingtaon Boulevard east 122\".\n\n\n\n\n&#x200B;\n\n\nMy naive solution was to use \nLevinshtein\n distance (word mover) between words and add hard coded rules (e.g. blvd=boulevard).\n\n\nBut what is the NLP area that deals with such topics?\n\n\n&#x200B;\n\n\n&#x200B;\n\n\nThank you.",
"date": "2018-12-16"
},
{
"vote": 25,
"title": "Open-sourcing PyText for faster NLP development",
"text": null,
"date": "2018-12-15"
},
{
"vote": 3,
"title": "Moving towards Understanding words - Resolution of comparing and superlative words",
"text": "What can be done if I want to create a chatbot which can talk to a user and when it returns a list of results, it can query based on what the user says?\n\n\nFor example, \n\n\nUser: Show me running shoes\n\n\nBot: Here are some running shoes  \n\n\n\n\nABC $123  \n\n\nXYZ $80  \n\n\nPQR $90\n\n\n\n\nUser: Show me details about the cheapest one\n\n\n&#x200B;\n\n\nUnless I write rules where I say if you see \"cheapest\" order in ascending order and pick the first element, I don't see how this could work. Is there a smarter way to do this? I can't seem to find any papers either since I don't know what to search for (haha XD)",
"date": "2018-12-15"
},
{
"vote": 1,
"title": "Getting error on running the trained Machine Learning model",
"text": "I have a dataset containing columns 'studentDetails' and 'studentId'. I trained my model on this dataset and saved it. When I am training the model and saving the trained model, then loading the trained model to predict, it successfully giving me the output. But when I am loading the saved model standalone and predicting using that, it is giving me an error \"CountVectorizer - Vocabulary wasn't fitted\"\n\n\nHere is the code I am using:\n\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport pickle \nfrom sklearn.svm import LinearSVC\nX_train, X_test, y_train, y_test = train_test_split(df[&#039;studentDetails&#039;], df[&#039;studentId&#039;], random_state = 0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer() \nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nclassificationModel = LinearSVC().fit(X_train_tfidf, y_train) \nfilename = &#039;finalized_model.sav&#039;\npickle.dump(classificationModel, open(filename, &#039;wb&#039;))\n\n\n\nNow loading the model and predicting:\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer \ndata_to_be_predicted=&quot;Alicia Scott is from United States&quot;\nfilename = &#039;finalized_model.sav&#039;\nloaded_model = pickle.load(open(filename, &#039;rb&#039;))\ncount_vect = CountVectorizer()\nresult = loaded_model.predict(count_vect.transform([data_to_be_predicted])) \nprint(result)\n\n\n\noutput:\n\n\n94120\n\n\n\nWhen I am running just the second code snippet standalone, it is giving me an error\n\n\nerror:\n\n\nCountVectorizer - Vocabulary wasn&#039;t fitted\n\n\n\nI am just wondering, how come I am getting an error in the second case because I am not redefining the count_vect = CountVectorizer() anywhere in the first case when I am getting the correct result.",
"date": "2018-12-14"
},
{
"vote": 30,
"title": "PyText: A natural language modeling framework based on PyTorch",
"text": null,
"date": "2018-12-14"
},
{
"vote": 1,
"title": "No such thing as a text-trained model that can make predictions on new data with unseen words?",
"text": "[deleted]",
"date": "2018-12-14"
},
{
"vote": 11,
"title": "What is the best model for keyphrase extraction from super long text?",
"text": "Hi, everyone. Iâ€™m working on a keyphrase extraction task. The biggest difficulty of this task is that the text is very long (5000-20000 words). Iâ€™ve tried several unsupervised algorithms such as Tf-idf and TextRank which didnâ€™t result in a good performance.\n\n\nNow, Iâ€™m seeking supervised algorithms to improve the performance. One of my choices is Seq2seq model with attention and copy mechanism which is proposed in \nhttps://arxiv.org/pdf/1704.06879.pdf\n. My concern is that the f1 score of this model is just 20-30%. Besides, I deeply doubt whether the Seq2seq model can handle such a long input.\n\n\nCan somebody give me some advice on the selection of models for this task? Any other methods that is helpful to this task is also welcome.\n\n\nThanks in advance!",
"date": "2018-12-13"
},
{
"vote": 2,
"title": "NeurIPS 2018 Recap by Forge.AI",
"text": null,
"date": "2018-12-12"
},
{
"vote": 1,
"title": "Has anyone ever worked on trying to get a computer to remember things based off of event triggers?",
"text": null,
"date": "2018-12-11"
},
{
"vote": 25,
"title": "[P] awesome sentence embedding: A curated list of pretrained sentence embedding models (x-post /r/MachineLearning)",
"text": "Hey guys,\n\n\nI have assembled a huge list of word and sentence embeddings with pretrained models\n\n\nI've tried my best to include everything from the past four years and made sure to check whether they have official code releases or not\n\n\nhope you guys enjoy it and more importantly use it somehow! :D\n\n\nhere is the link",
"date": "2018-12-11"
},
{
"vote": 7,
"title": "Python - Context Free Grammar using Shift Reduce Parser",
"text": "I am trying to use the Shift Reduce Parser for the following sentence:\n\n\n&#x200B;\n\n\n\\\nHe eats pasta with some anchovies in the restaurant``\n\n\n&#x200B;\n\n\nI have created some code and grammar BUT the grammar only works up to:\n\n\n&#x200B;\n\n\n\\\nHe eats pasta with some anchovies``\n\n\n&#x200B;\n\n\n    import nltk\n    from nltk.parse import ShiftReduceParser\n    \n    \n    grammar = nltk.CFG.fromstring(&quot;&quot;&quot;\n    \n    S -&gt; NP VP\n    NP -&gt; PropN | N PP | Det N\n    VP -&gt; V NP | V S | VP PP | NP PP\n    PP -&gt; P NP \n    PropN -&gt; &#039;He&#039;\n    Det -&gt; &#039;some&#039; | &#039;the&#039;\n    N -&gt; &#039;pasta&#039; | &#039;anchovies&#039; | &#039;restaurant&#039;\n    V -&gt; &#039;eats&#039;\n    P -&gt; &#039;with&#039; | &#039;in&#039;\n    \n    &quot;&quot;&quot;)\n    \n    \n    sr = ShiftReduceParser(grammar)\n    \n    sentence1 = &#039;He eats pasta with some anchovies&#039;\n    # He eats pasta with some anchovies in the restaurant\n    \n    tokens = nltk.word_tokenize(sentence1)\n    \n    \n    \n    print(&quot;--------------------------- Sentence 1 ---------------------------&quot;)\n    \n    for x in sr.parse(tokens):\n        print(x)\n\n\n\n&#x200B;\n\n\nNow my attempt was adding `Det NP` to `NP ->`\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/ctbuiy3wog321.png?width=1106&format=png&auto=webp&s=9789094839ab4056f1a989caccb4652a26549d3c\n\n\n&#x200B;\n\n\nBut apparently \n\\\nDet NP`` is incorrect grammar. Which area is my mistake and how would I make the shift parser fully parse my sentence.",
"date": "2018-12-10"
},
{
"vote": 0,
"title": "What is Pre-processing and what's the best pre-processing method for speech recognition with CNN?",
"text": null,
"date": "2018-12-10"
},
{
"vote": 3,
"title": "Machine-readable subset of natural languages?",
"text": "I'm working on a syntax parser for a university course and I got to wondering whether something like this has been proposed or already exists, and what utility it would have:\n\n\nThe idea is a standardized subset of English or any other natural language, defined with the strictness and precision of a programming language, meant to be as expansive as possible to approach the fullness of the natural language while remaining unambiguously machine readable and semantically analyzable. A compiler could then be built to translate texts written in this stricter variety of the language into an abstract semantic representation, and vice versa. \n\n\nI realize that such a system would be very challenging to create and probably limited to a strict domain, but I can imagine that applications would include the ability to simultaneously publish laws, government documents, or scientific papers in a variety of languages at once. For example, in a country with multiple national or regional languages, laws and notices could be essentially written with the abstract semantic representation and available in any of the languages without the potentially time-consuming and contentious process of manual adaptation.\n\n\nIs there any existing or past project which sounds like this?",
"date": "2018-12-10"
},
{
"vote": 2,
"title": "Who is investing in startups doing natural language processing",
"text": null,
"date": "2018-12-09"
},
{
"vote": 0,
"title": "Automatic Text Generation",
"text": "Have  any worked on Automatic text generation using generative adversarial  networks.I need some guidance for that I want to extract wikipedia page  on my website and apply algorithm on that for question answer.If anyone  worked on that?",
"date": "2018-12-07"
},
{
"vote": 1,
"title": "Classification of cryptic crossword clue types / Solving cryptic crossword clues",
"text": "[deleted]",
"date": "2018-12-06"
},
{
"vote": 2,
"title": "NLTK Shift Parser Doesn't Print",
"text": "I'm trying to use the Shift Parser in NLTK on a sentence provided. When I use the example grammar on the documentation it works and prints as the example shows, when i try my own grammar it does not work and print's nothing. Can anyone see where my issue is?\n\n\n&#x200B;\n\n\nWhen I try my grammar on the ChartParser it does work so I'm not sure where the issue is.\n\n\n&#x200B;\n\n\n    grammar = nltk.CFG.fromstring(&quot;&quot;&quot;\n    S -&gt; NP VP\n    NP -&gt; PRP | NN | NP PP | DT NNS | DT NN\n    VP -&gt; VBZ NP\n    PP -&gt; IN NP\n    PRP -&gt; &#039;He&#039;\n    VBZ -&gt; &#039;eats&#039;\n    NN -&gt; &#039;pasta&#039; | &#039;restaurant&#039;\n    IN -&gt; &#039;with&#039; | &#039;in&#039;\n    DT -&gt; &#039;some&#039; | &#039;the&#039;\n    NNS -&gt; &#039;anchovies&#039;\n    &quot;&quot;&quot;)\n\n    shift_prse = ShiftReduceParser(grammar)\n    sent = &#039;He eats pasta with some anchovies in the restaurant&#039;.split()\n    for x in shift_prse.parse(sent):\n        print(x)\n\n\n\n&#x200B;",
"date": "2018-12-05"
},
{
"vote": 14,
"title": "The best books on conversation analysis",
"text": null,
"date": "2018-12-04"
},
{
"vote": 5,
"title": "Out of vocabulary word embedding in a pretrained model",
"text": "I have a pretrained fasttext  model and I wish to add some nouns from my existing domain data. \n\n\nI have a method to solve this problem. I wish to know the flaws of this method. \n\n\nThe method:\n\n\n\n\nGet a separate fasttext model for the domain data. \n\n\n\n\nGet the most similar 5-10 words from the domain vocab which are present in the existing model. \n\n\n\n\nAverage the word embedding vector from \npreexisting model\n for those selected words and assign it to the new vocab from the domain data.\n\n\n\n\n\n\nIt seems naive but I feel it could work.\n\n\nEdit: I rephrased the question a bit as it was a bit unclear previously. \n\n\nThanks for all your suggestions. Really appreciate it.",
"date": "2018-12-03"
},
{
"vote": 3,
"title": "Does anyone have experience with the LIWC API?",
"text": "I have been given access to LIWC, but not the application that looks all nice. I'm trying to follow \nthis user manual\n as best I can, and also \nthis sample code\n, but I cannot get this to run. I get all kinds of errors. (I can specify if necessary for those who are interested.)\n\n\nI would really appreciate if anyone who has worked with the API before DMs me!",
"date": "2018-12-03"
},
{
"vote": 1,
"title": "How AI is challenging traditional translators",
"text": "[deleted]",
"date": "2018-11-29"
},
{
"vote": 8,
"title": "Is there a speech recognition API which matches the input to the closest of a limited vocabulary?",
"text": "I'm creating a web application which prompts the user to say a command. I'm currently using the chrome web speech API, but I need to restrict the vocabulary to improve accuracy. Is there another speech API I can use or is there a way of converting one word into a matching word from a smaller vocabulary? Also, this is not in English.",
"date": "2018-11-26"
},
{
"vote": 1,
"title": "How to build a human-like chatbot",
"text": "[deleted]",
"date": "2018-11-25"
},
{
"vote": 2,
"title": "remove punctuations like apostrophe, hyphen, and percent for creating word embeddings using gensim",
"text": "I am studying creating word embeddings, I followed this \nlink\n. It just shows processed sentences split into word tokens as a list of lists. I have a raw custom text doc, looks somewhat like below:\n\n\n \nToday's date is Nov 24, 2018. It is a sunny day today in USA. The temperature is 1.23% more than the last year's record.\n\n\nIf the punctuations are removed, then Today's will become Todays, and I am not sure what to do the percentages or fractions. Also, should the case be lower case? If so, then USA will become usa which won't make any sense. Thanks!\n\n\n&#x200B;",
"date": "2018-11-24"
},
{
"vote": 14,
"title": "Is there any web demo that returns the k nearest words given a word using GloVes (English)?",
"text": "http://bionlp-www.utu.fi/wv_demo\n is great but only has word2vec.",
"date": "2018-11-24"
},
{
"vote": 9,
"title": "Punctuation Restoration with sequence to sequence models.",
"text": null,
"date": "2018-11-24"
},
{
"vote": 1,
"title": "Concept Frequencyâ€”Inverse Document Frequency (does the make sense?)",
"text": "[deleted]",
"date": "2018-11-23"
},
{
"vote": 4,
"title": "Question: is there a single place where I can learn about what various inputs are taken by NLP models?",
"text": "I'm in a Data Science Immersive program in Denver and, for a capstone, I want to build and compare performances between RNNs, GANs, DCNNs, and whatever mainstream frameworks exist for text generation. \n\n\nSo I want to feed an algorithm the works of Shakespeare or \"Atlas Shrugged\" and have it spit out semantically similar text. \n\n\nI'm just struggling to get clear what inputs these algorithms take, which is necessary for building a data-cleaning pipeline. Some, like Andrej Karpathy's RNNs, just take raw text. But are there others that require TFIDFs, or topic clusters built with LDA? \n\n\nWhile I've thoroughly enjoyed just plunging into the literature I have less than a month to get this done and would like to shorten at least this part of the search process. \n\n\nThank you in advance.",
"date": "2018-11-23"
},
{
"vote": 8,
"title": "Is it ok to use an classifier/ML algorithm from a library without knowing anything about what it is or how it works?",
"text": "I'm doing NER modeling using StanfordCoreNLP's CRFClassifier model. I'm basically feeding my own labeled data into a CRFCLassifier. I have no idea what CRF is. I know it stands for \"conditional random field\", and I know that it's effective for named entity recognition (That's why I'm using it). But that is the full extent of my knowledge on CRF. I don't know what a conditional random field is, I don't know what \"random\" or \"field\" refers to at all in this, I don't know how it works, I don't know how it classifies, how it structures features, what features it uses, what equations are programmed into it, I know nothing about any of this. If I were asked in an interview to explain CRF, I would not be able to. All I would know how to say is that it stands for conditional random field, and it's effective for the task of NER. \n\n\nI know how to use a pre-programmed CRF model to train NER and achieve very good results, but I don't know what CRF IS or DOES. Is that ok? I feel like I'm fine just using it as a black box and knowing nothing about it. However when I go down the rabbit hole of interview questions, It will go like this:\n\n\nMe: I've trained an NER model in StanfordCoreNLP\n\n\nInterviewer: What kind of classifier was it?\n\n\nMe: I used stanford's CRFClassifier model and put my data through that.\n\n\nInterviewer: How does a CRF classifier work?\n\n\nMe: .....\n\n\nTHen I'd be dead in the water in an interview. How important is this? Do I absolutely NEED TO KNOW everything about the computations and inner workings of the classifier models I'm using? I know how to explain the basics of logistic regression, naive bayes, and support vector machines, even simple neural nets. But there are plenty of ML models I've used for my tasks which I have no idea of, but I've used them completely black-box because they work. Is this a bad thing to do that will cause me to get caught out in interviews?",
"date": "2018-11-23"
},
{
"vote": 5,
"title": "ðŸ”¥ðŸ“ŠSix Domain-Adaptive Neural MÐ°chine Translation systems evaluated",
"text": null,
"date": "2018-11-21"
},
{
"vote": 3,
"title": "Adding custom labels for NER in Spacy",
"text": "I am working on an NLP problem in which from a sentence some certain search parameters for a search engine of employee database are to be picked up. So for this I don't have textual data  to train only a database of employees containing job titles, work location etc. I was hoping to use Spacy, and looking for how to add  custom labels like these through dictionary or something, can anyone suggest something?",
"date": "2018-11-21"
},
{
"vote": 0,
"title": "Beating the state-of-the-art in NLP with HMTL",
"text": "https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf",
"date": "2018-11-21"
},
{
"vote": 1,
"title": "Beating the state-of-the-art in NLP with HMTL",
"text": "[deleted]",
"date": "2018-11-21"
},
{
"vote": 17,
"title": "Where to start learning about Natural Language Understanding?",
"text": "Hey guys, I'm a PMATH student at the University of Waterloo, and for the past few years my interests have mainly been in general machine learning concepts (training neural networks, statistical learning theory, etc.). However, recently I started working on a project that required NLP.\n\n\nNamely, in a general sense the application would need to convert human text to a (\"small\" but \"dense\") set of computer-readable commands. By \"small\" I mean the space of interaction of the commands is quite small (confined to the environment of the program I'm working on), and by \"dense\" I mean able to represent a high percentage of the total set of programs that can be executed in this environment.\n\n\nWhere are some good resources to start learning about this sort of stuff? I'm trying, currently, to not overcomplicate things and start with a simple idea like \"I have N commands with generic formats, what is the best way of determining which of these commands a given input is closest too semantically?\". I've started with the bare minimum reading \nwww.nltk.org/book\n, but I bet there are better resources out there.\n\n\nAlso, I've been told by my CS friends that this subfield of NLP is relatively emergent, and so most of the groundbreaking work will be in papers and not in textbooks or online courses. Of course, I'm looking to start out small, but I would eventually like to break out into harder problems in language understanding and knowledge representation for this project.\n\n\nWhat do you guys think?",
"date": "2018-11-20"
},
{
"vote": 1,
"title": "Any recent follow up studies to Banko et al's 2001 paper on scaling to very large datasets for NLP?",
"text": "Their result is basically that the machine learning model used doesn't matter as much as the size of the text corpus. The figure I have in mind is this one:\n\nfigure 1\n\n\nI'm aware of Peter Norvig's \"The Unreasonable Effectiveness of Data\" from 2009 but is there anything even more recent than that? I see some papers addressing the concept of \"more data > better models\" -- \nhere, for example\n -- but they are talking about image classification rather than text classification.\n\n\nAny suggested follow up papers with respect to NLP would be much appreciated. Thanks!",
"date": "2018-11-20"
},
{
"vote": 1,
"title": "[D] Inverting embeddings",
"text": null,
"date": "2018-11-20"
},
{
"vote": 5,
"title": "Finding medical words from a document",
"text": "I have a document from which I have to find the medical words, for example:\n\n\n&#x200B;\n\n\nCristy has failed conservative treatment of his carpal tunnel syndrome and wishes to proceed with carpal tunnel release.\n\n\n&#x200B;\n\n\nJohn is a good candidate for total shoulder arthroplasty as he has been suffering from this problem from a very long time.\n\n\n&#x200B;\n\n\nI have to find from the above sentences:\n\n\n\n\ncarpal tunnel release.\n\n\n\n\ntotal shoulder arthroplasty\n\n\n\n\n\n\nMay I know how it could be done, I tried to use Spacy but I am not getting the results.",
"date": "2018-11-19"
},
{
"vote": 1,
"title": "Resoomer | RÃ©sumeur pour faire un rÃ©sumÃ© de texte automatique en ligne",
"text": null,
"date": "2018-11-18"
},
{
"vote": 13,
"title": "Need datasets for abstractive BioMedical text summarization.",
"text": "I am working on an NLP project for which I need datasets for biomedical text summarization. Most of the paper that I have read have made their own datasets and they are not available opensource. I would love if anyone can point me to an opensource dataset for the same. It can even be a dataset that is available only for research purposes.",
"date": "2018-11-18"
},
{
"vote": 0,
"title": "Have You Ever Been so Broke You Could Not Even Afford to Pay Attention?",
"text": "[removed]",
"date": "2018-11-17"
},
{
"vote": 5,
"title": "Create a working chatbot in 10 mins",
"text": null,
"date": "2018-11-17"
},
{
"vote": 1,
"title": "[P] A simple LSTM based Statement of Purpose Generator for grad school.",
"text": "[deleted]",
"date": "2018-11-16"
},
{
"vote": 1,
"title": "Do we have Q&amp;A machine reading comprehension dataset for Spanish language?",
"text": "Hi guys,\n\n\nLooking for Spanish language dataset similar to SQUAD dataset.\n\n\nDo we have any such ?\n\n\nThanks.",
"date": "2018-11-16"
},
{
"vote": 0,
"title": "Dual conversationalism",
"text": "Person 1 explains a question\nPerson 2 says a question at the same time\n\n\nBoth respond answers to others question at the same time after above\n\n\nHuman talk time reduction 50%",
"date": "2018-11-15"
},
{
"vote": 3,
"title": "I want to extract phrases from sentences. Is there any good way of doing this with any nlp library?",
"text": "For example, let say the sentence is \"Which are the top sellers on Amazon?\", and I want to extract the phrases \"which are\", \"top sellers\", \"on Amazon\". Or let's say the sentence is \"New York is a big city\". I want to extract the phrases - \"New York\", \"big city\". Is this something that can be done with NLP? If yes, what should I google?\n\n\nEdit: Essentially, I want to extract all the phrases in a sentence that makes sense.",
"date": "2018-11-14"
},
{
"vote": 19,
"title": "Why is CV (Computer Vision) more popular than NLP ? [Soft Question]",
"text": "I don't know if anyone else have noticed, but I find there are fewer people into NLP than CV. Some of my observations\n\n\n\n\nMost posts in r/ML are concerned about CV (Imagenet, CNNs for vision) topics than NLP\n\n\n\n\nAmong my friends, I have seen a lot of people interested in CV.\n\n\n\n\nPresence of more research labs focusing on CV \n\n\n\n\nAbundant resources for CV (blogs, courses, books) than NLP.\n\n\n\n\n\n\n&#x200B;\n\n\nWhat are your thoughts, do you guys observe the same  or am I wrong or it doesn't really matter ?",
"date": "2018-11-14"
},
{
"vote": 3,
"title": "Using just words to extract relations from a corpus",
"text": "I have a list of words. How do I extract the relations between the words from a given corpus?",
"date": "2018-11-13"
},
{
"vote": 17,
"title": "Dear NLP Friends, help a physicist out",
"text": "[deleted]",
"date": "2018-11-13"
},
{
"vote": 13,
"title": "Wanna steal one of Linguistic Data Consortium datasets",
"text": "I'm just wondering if someone who has an access to Linguistic Data Consortium website could send me the \nACE 2005\n dataset, so I could save $4,000 and buy a few hundreds pizzas instead.\n\n\nThere are a lot of universities whose students have the access, but mine is not one of them. Since SOTA is established on this dataset I need it for a reference while doing my research.\n\n\nThank you in advance!",
"date": "2018-11-12"
},
{
"vote": 3,
"title": "A simple NLP pipeline to calculate running sentiment polarity of longer text",
"text": "Had some fun this weekend building a sentiment polarity pipeline and observing the \"running sentiment\" of fictional works in the public domain.  Algo is pretty naive/rules-based, just coded for fun.\n\n\nGitHub Repo:\n\n\nhttps://github.com/wesdoyle/simple-sentiment\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/p9w2bf01iyx11.png?width=889&format=png&auto=webp&s=9958254860fc2dbc526f23842618dcb2352fb80b",
"date": "2018-11-12"
},
{
"vote": 2,
"title": "Bigger training set/model causing StanfordCoreNLP to hit heap space error. Any solution?",
"text": "I'm using IntelliJ IDE and know how ot increase heap size for a specific Java program. The problem is, when I added more to my training set, it ran into a heap space error. So I increased heap size from 1g to 1.5g. The model fully trained and worked. However, now I've just added even more to the training set, and will want to add a lot more data soon. This addition has led to a heap space error even with 1.5g allocated. I can't really allocate any more space than this. If I try 2g then IntelliJ refuses to start the program, and tells me that it cannot initialize an object heap of this size.\n\n\nSo it seems right now that the model is too big to accommodate a 1.5gb heap size, and any higher than a 1.5gb heap size and IntelliJ will refuse to allocate this memory. I don't know how to get around this. Is there anyone in here with first-hand experience in building StanfordCoreNLP models, and specific experience in dealing with the heap space issue?",
"date": "2018-11-11"
},
{
"vote": 1,
"title": "Please share: Data segmentation shown in Google I/O related research",
"text": "[removed]",
"date": "2018-11-10"
},
{
"vote": 6,
"title": "Confusion matrix NER on the entity level instead of token level",
"text": "Hi, I've been working on an NER problem recently. I'm trying to construct a confusion matrix to inspect the mistakes my model is making. Thus far, I've only been able to find per token confusion implementations like \nsklearn\n which expects a flat list of labels per token (it can't take the span of entire entities into account). What is the way to make a confusion matrix per entity i.e. the whole span of the sub-entities, what would be the logic behind this? \n\n\nGiven the gold standard sentence:\nMichael B-PER Jordan I-PER lives 0 in 0 the 0 United B-LOC States I-LOC\n\n\nPredicted sentence: \nMichael B-PER Jordan 0 lives 0 in 0 the 0 United B-ORG States B-LOC \n\n\nWhat would an entity-level confusion matrix look like? \n\n\nEither the logic or a link to a python implementation would be great.",
"date": "2018-11-09"
},
{
"vote": 1,
"title": "Chinese president gives hint on where AI can make transformative changes in China",
"text": "[deleted]",
"date": "2018-11-08"
},
{
"vote": 46,
"title": "Recent Research and Trends in NLP",
"text": null,
"date": "2018-11-06"
},
{
"vote": 2,
"title": "MultipleAttribute text rewriting - style transfer for text",
"text": null,
"date": "2018-11-06"
},
{
"vote": 2,
"title": "How do I generate an ARPA file from my own training file with KenLM?",
"text": "Here\n is KenLM. I was just given a HackerRank challenge by some company, and they basically linked me this github repo, told me to learn this package and how it works, and then use a text file they provided to me to train a 4-gram language model and then tell them what is on certain lines of the resulting ARPA file, and also to tell them the perplexity values of certain words or sentences. All in a 2-hour time limit in HackerRank.\n\n\nI looked into it and found the installation instructions at the bottom, so I installed it. Then I tried to run the lines of python code to build a language model. Didn't work, said that \"lm/test.arpa\" was not found. Ok, so then I looked in the file example.py and tried to run that locally. It worked. The language model trained on the test.arpa file already provided in the repo, and ran its LM on the strings. I tried to put the LM through all lines of the trainng file I was given, and it gave me a file full of perplexity results.\n\n\nBut this was useless, because the code was making use of a pre-made ARPA file and was just evaluating perplexity based on that. I also couldn't tell them what was in \"the resulting ARPA file\" because I HAD no resulting ARPA file. I know that to build the LM with this package, you need to feed it an ARPA file that's alreayd been made. But all I have is this guy's pre-made ARPA. I have no idea how to generate my own ARPA using the training file I was given. There are absolutely no clear instructions in the README for how to do this, and I was just completely stuck haing no idea what to do if I didn't know how to generate an ARPA from my own training file.\n\n\nIt was 1:15 into the 2 hours, and I had tried to learn the package as best I could, and trying to read over and over through the README to find a place where he explains how to generate an ARPA. I couldn't find any clear explanation. There might be an explanation somewhere in there, but there are so many technical terms in there that completely make no sense to me. I thought that the instruction to generate an ARPA from a text file was by running the command:\n\n\nbin/lmplz -o 5 <text >text.arpa\n\n\nBut this didn't work. It said it was invalid syntax when I typed this into my iPython console. I also tried it in Cygwin, it just said that bin was not recognized. So I don't know what to do to generate an ARPA. I thought that maybe I have to \"compile\" first (I don't know why, if this is python), so the instructions to compile are to run the commands:\n\n\nCompiling\nUse cmake, see BUILDING for more detail.\n\n\nmkdir -p build\ncd build\ncmake ..\nmake -j 4\n\n\nI tried this and it didn't work either, it says cmake isn't recognized. It also says stuff like \n\n\n\"If you want to compile with your own build system (Makefile etc) or to use as a library, there are a number of macros you can set on the g++ command line or in util/have.hh .\"\n\n\nAnd\n\n\n\"The bjam build system will auto-detect bzip2 and xz support. After compiling with bjam, run bin/lmplz -o 5 <text >text.arpa\"\n\n\nAgain, I have no idea what any of these things mean, and the creator doesn't even bother to EXPLAIN the meaning of any of this. I don't know what the hell \"bjam\" is and how to compile with it.  Am I supposed to go google what this is, install it, learn to use it with some sample files, and then go back to this to try to use this? All within a 2-hour Hackerrank time limit?\n\n\nAll of this took 1:15, and I was stuck and extremely frustrated and had no idea what to do. What I did know was that if I could get an ARPA file out of the training file they gave me, I'd be able to do what they were asking. But without any clue how to get the ARPA, I was stuck and completely lost. So I entered in the text box in the Hackerrank challenge exactly that explanation, and just left it. I had 45min remaining, but I know I would've just sat there and stared at it for that 45min, and gotten more frustrated and pissed off.\n\n\nSo what do you think? Can YOU see a clear instruction on how to use this repo and generate an ARPA from your own training file? Should I have been able to recognize an instruction on how to do it from the README? Everything I tried that I thought might generate an ARPA, did not work. And I had only 2 hours to figure out how to use this package, and couldn't. This is the kind of thing that requires a few days for several hours a day to actually look at and experiment with, before I'm comfortable with actually working with it. Can you guys figure out what I couldn't within 2 hours? If you can, then am I just stupid? I feel fucking stupid right now.",
"date": "2018-11-04"
},
{
"vote": 3,
"title": "Medical NER",
"text": "Hi,\n\n\nI am in need of a NER recogniser / corpus for different medical terms e.g. drugs, body parts etc. After much googling I haven't really been able to find one so I wondered, is there a method of creating a corpus through Apaches OpenNLP toolkit as I have thousands of article abstracts available?",
"date": "2018-11-02"
},
{
"vote": 3,
"title": "[HIRING] NLP Analyst at Personetics",
"text": null,
"date": "2018-11-01"
},
{
"vote": 3,
"title": "Intento launched an app to try many Machine Translation services at once (all of them)",
"text": null,
"date": "2018-11-01"
},
{
"vote": 7,
"title": "NLP for digesting historic documents?",
"text": "I'm a programmer and I'm interested in history, especially the digitization of history. I was wondering if someone could point me in the right direction for the type of NLP that would be best at digesting historic documents and extracting basic units of historic information.",
"date": "2018-10-31"
},
{
"vote": 2,
"title": "[P] Real-time digital forensics for public PCs using ML and NLP",
"text": null,
"date": "2018-10-31"
},
{
"vote": 3,
"title": "Where is NLG Most Successful Commercially? An Overview",
"text": null,
"date": "2018-10-31"
},
{
"vote": 3,
"title": "Time expressions extraction",
"text": "Do you know of a good open source projects for time expression extraction? (Including dates of course) The most important issue for me is relative time expression (\"two days after....\")\n\n\nPython packages are the most welcome ðŸ˜ƒ",
"date": "2018-10-30"
},
{
"vote": 22,
"title": "BERT-keras: BERT in keras with OpenAI's pertained transformer network for weight initialization",
"text": "&#x200B;\n\n\nhttps://i.redd.it/5v6rvypx4gv11.png\n\n\nHere's my full implementation of BERT in keras with both fine tuning and pre training code, because the pertained weights are not out yet, I used OpenAI's transformer pertained model for initialization: \nhttps://github.com/Separius/BERT-keras\n\n\nUPDATE: you can now import the pretrained weights and use them",
"date": "2018-10-29"
},
{
"vote": 3,
"title": "Best CNN architecture for Spectogram?",
"text": "what's the best CNN architecture for speech recognition?",
"date": "2018-10-29"
},
{
"vote": 12,
"title": "Text Classification Using NLP",
"text": "[removed]",
"date": "2018-10-28"
},
{
"vote": 12,
"title": "interview with FranÃ§oise Beaufays of Google Research @ I/O '17 on building language-agnostic systems",
"text": null,
"date": "2018-10-28"
},
{
"vote": 1,
"title": "What tools are used in MT?",
"text": "Hello,\n\n\nI am required to give a presentation on tools used in machine translation. \nTo give you an idea of what I mean by tools: \nPOS-Tagging, Tokenization/End of sentence recognition, Parsing\n etc. \n\n\nI would like to know more about what tools are used in MT myself as well as give a good presentation. \n\n\nTherefore I'd like to ask you guys if you could suggest more topics (than the three I've mentioned) so a) I get an idea of what tools are used and b) I can give a good presentation. \n\n\nThanks in advance.",
"date": "2018-10-27"
},
{
"vote": 5,
"title": "Natural Language Processing Tools",
"text": null,
"date": "2018-10-26"
},
{
"vote": 2,
"title": "Survey on Chatbot Creation. I'd love your input.",
"text": null,
"date": "2018-10-24"
},
{
"vote": 35,
"title": "Tencent AI Lab Open-Sources 8M Word Chinese NLP Vector Dataset",
"text": null,
"date": "2018-10-24"
},
{
"vote": 6,
"title": "r/machinetranslation reborn",
"text": "r/machinetranslation\n was moribund and modless for months.  I successfully applied to take it over and have seeded it with relevant posts.\n\n\nThere is overlap so cross-posting is encouraged, but it will naturally have a different focus than this sub - less for beginners, and more on machine translation research, engineering, products and industry that would probably be a bit noisy for the wider r/LanguageTechnology audience.",
"date": "2018-10-24"
},
{
"vote": 2,
"title": "language technology project - gathering data",
"text": "[removed]",
"date": "2018-10-23"
},
{
"vote": 9,
"title": "Cosine Similarity â€“ Understanding the math and how it works (with python codes)",
"text": null,
"date": "2018-10-22"
},
{
"vote": 14,
"title": "Looking for NLP research topic",
"text": "I'm looking for a research topic for master's degree related to text summarization or question answering. I'm not quite experienced in NLP and at a first look it seems that everything that is possible has already been solved, and any problem that has not been solved yet can only be done by a large company, like Facebook or such. Can you give me advise on what research can be done with only hard work and Google Colaboratory? Thank you in advance!",
"date": "2018-10-22"
},
{
"vote": 7,
"title": "A Simple Chatbot Using RL3 and Python",
"text": null,
"date": "2018-10-22"
},
{
"vote": 17,
"title": "Dynamic Meta-Embeddings improve AI language understanding",
"text": null,
"date": "2018-10-19"
},
{
"vote": 0,
"title": "How Natural Language Processing can boost your Net Promoter Score",
"text": null,
"date": "2018-10-18"
},
{
"vote": 6,
"title": "Why Lexical Problems are the Key to NLP: An Interview with Researcher Vered Shwartz",
"text": null,
"date": "2018-10-17"
},
{
"vote": 1,
"title": "Methods/tools for extracting training corpus from email archives?",
"text": "Hi everyone, I'm building a training corpus for NLP and have access to a bunch of email archives, but am trying to figure out an easy way to extract the relevant information from those in an automated way. Does anyone have any experience doing this? Are there any tools or methods you'd recommend?",
"date": "2018-10-17"
},
{
"vote": 14,
"title": "Extracting data from PDFs",
"text": "[deleted]",
"date": "2018-10-17"
},
{
"vote": 1,
"title": "NLP/NER Library - Contributors Wanted",
"text": null,
"date": "2018-10-17"
},
{
"vote": 6,
"title": "Gensim Tutorial - A Complete Guide to NLP for Beginners",
"text": "Hello guys,\n\n\nFor a fantastic NLP package it is, Gensim is not receiving the attention it deserves. May be the native tutorials aren't as easy to grasp compared to other NLP packages. So I wrote a \ngensim tutorial \nfor beginners and as single point cheatsheet reference for users.\n\n\nThanks",
"date": "2018-10-17"
},
{
"vote": 35,
"title": "Best NLP Model Ever? Google BERT Sets New Standards in 11 Language Tasks",
"text": null,
"date": "2018-10-16"
},
{
"vote": 1,
"title": "SmartReader: Train an unsupervised text classification model",
"text": null,
"date": "2018-10-16"
},
{
"vote": 8,
"title": "NLTK Python",
"text": "Is there any video lectures to learn this book ? OR what are the efficient ways to complete all chapters.\n\n\nNatural Language Processing with Python\n\n\nâ€“ Analyzing Text with the Natural Language Toolkit\n\n\nSteven Bird, Ewan Klein, and Edward Loper",
"date": "2018-10-16"
},
{
"vote": 3,
"title": "Training Big Neural Nets like OpenAI's GPT/BERT on Larger Batches: Practical Tips",
"text": null,
"date": "2018-10-16"
},
{
"vote": 3,
"title": "classify intents for non-seen words (SVM text classifier)",
"text": "I have a SVM text classifier that uses word bag encoding.\nNow as I understand in the encoding the vector space would be the complete vocabulary of my training data\nAnd I will get a very sparse encoding vector for my input sentence to, where each dimension will be a word of the vocabulary \n\n\nNow I want to understand how can I handle new words, my vocabulary hasn't seen before , and how can control which intent to classify it to.  \n\n\nDetails :\n\n\nI am using sklearn pipeline , steps : CountVectorizer -> TfIdfTransform -> SGDClassifier (l2 penalty, perceptron loss)\nwhile writing this\n\n\nI got an Idea, since it is a new word the encoding would be [0,0,....0] the zero vector\nso if a train a couple a cases with empty sentences to the intent I want, will that work ?",
"date": "2018-10-15"
},
{
"vote": 8,
"title": "Sentiment Analysis Beyond Binary Polarity?",
"text": "I'm interested in a project that can classify text along a continuous dimension of sentiment rather in contrast to the typical two or three state classification of most literature. For example, being able to say that a given post is more X than 54% of other posts.\n\n\nI've been thinking that maybe a continuous dimension is too ambitious and that maybe the best way forward is to layer binary analyses on top of each other to create bins with higher resolution.\n\n\nDoes anyone have any thoughts about how to best accomplish this or point me in the direction of interesting/relevant papers?\n\n\nAny insight/discussion would be greatly appreciated!",
"date": "2018-10-15"
},
{
"vote": 8,
"title": "How do I extract just entities from a highly domain specific corpus?",
"text": "The existing NER methods do not work well as the entities are highly specific to a domain.\n\n\nIs Tagging the only way? How much data would I need to annotate?\n\n\nWould just extracting Nouns be good enough?",
"date": "2018-10-14"
},
{
"vote": 2,
"title": "Subject-aware sentiment analysis?",
"text": "[deleted]",
"date": "2018-10-13"
},
{
"vote": 4,
"title": "Search Suggestions using Python and Postgres",
"text": "[deleted]",
"date": "2018-10-13"
},
{
"vote": 1,
"title": "NER to identify name types",
"text": "This may be a better question for /MLQuestions but I was down a rabbit hole of nltk/spacy & beyond thinking that tagged words would be helpful.\n\n\nMost of the stuff I see with NER seems to be based around sentences or larger so there is context to work with.  Is there anything I can do at the phrase/name level?\n\n\nI have US property owner names I'd like to categorize as [family/individual] or [everything else].\nBoth \"Bob & Mary Smith\" & \"Smith Trust\" would be categorized as individual/family and everything else wouldn't.\n\n\nSome labeled examples\n\n\nName; Label\nMos Eisley School District; 0 \nGotham Water District; 0\nBruce Wayne; 1\nWayne Family Trust 1910; 1\nWatto Junk Service, Inc; 0\nBespin Mining llc; 0\nBruce Banner; 1\nCounty of Bellerophon; 0 \nStark Industries; 0 \nMJ Watson; 1\nWick, J; 1\nLannister tr; 1\nLandfall Property Management; 0\nTheed Estates Home Owners Assn; 0\nAM Weasley; 1\n\n\n\nI'm not limited to strictly owner names, I have other property fields also that may be helpful such as type, zoning ... but I was wondering if there's anything I can do directly on the names that would assist me in tagging.",
"date": "2018-10-12"
},
{
"vote": 14,
"title": "Fun with NLP",
"text": "I am in the process of learning Machine Learning.\n\n\nFor this purpose, I have gathered a big clean dataset, made of thousands of books from the Gutenberg project. And I'm looking for NLP-related things to do with that dataset.\n\n\nSo far I have :\n\n\n\n\nextracted base stats for each book (words & sentences counts)\n\n\nimplemented language detection models\n\n\nplayed with unsupervized classification (KNN)\n\n\nstarted playing with Word Embedding (word2vec)\n\n\n\n\nWhat other insightful/fun/interesting/unexpected things can I try with that dataset, to learn more about NLP and ML in general ?\n\n\nThanks in advance !",
"date": "2018-10-12"
},
{
"vote": 3,
"title": "How do I evaluate an NER model in spacy? Solution I found on stackoverflow doesn't work.",
"text": "I got some code from stackoverflow that basically told me to use the Scorer and GoldParse modules in spacy:\n\n\ndef evaluate(ner_model, examples):\n\nscorer = Scorer()\nfor sents, ents in examples:\n    doc_gold = ner_model.make_doc(sents)\n    gold = GoldParse(doc_gold, entities=ents[&#039;entities&#039;])\n    pred_value = ner_model(sents)\n    scorer.score(pred_value, gold)\nreturn scorer.scores\n\n#...gather training data, put it into correct format for spacy NER...\n\nnlp = spacy.blank(&#039;en&#039;)\n    if &#039;ner&#039; not in nlp.pipe_names:\n        ner = nlp.create_pipe(&#039;ner&#039;)\n        nlp.add_pipe(ner)\n    \n    for label in ner_labels:\n        ner.add_label(label)\n\n    optimizer = nlp.begin_training()\n    for i in range(20):\n        random.shuffle(training_data)\n        for sentence, entities in training_data:\n            nlp.update([sentence], [entities], sgd=optimizer)\n\nresults = evaluate(nlp, training_data)\n    print(results)\n\n\n\nThis doesn't work. It's telling my my entity accuracies are 100%. They're not 100%. i can see when I do the web visualization of the NER model that there's so much incorrect tagging in it. How come this code doesn't seem to pick up any of my incorrect tagging? I'm now having to manually look over the text and tally up the correct tags and incorrect tags to get an accuracy. What's wrong here? Why isn't this function getting my gold standard tags and simply evaluating whether the full string has the correct tag as per the gold standard?\n\n\nedit: Correction, they're not actually showing 100% when I print the results. My printed results are showing:\n\n\n{&#039;uas&#039;: 0.0, &#039;las&#039;: 0.0, &#039;ents_p&#039;: 91.07142857142857, &#039;ents_r&#039;: 91.8918918918919, &#039;ents_f&#039;: 91.4798206278027, &#039;tags_acc&#039;: 0.0, &#039;token_acc&#039;: 100.0}\n\n\n\nThis is still very wrong though. I know my tagger isn't operating at 91%. I manually tallied the results and it was around 55%. So why are the results showing this and did I do something wrong with the code in how to evaluate the NER?",
"date": "2018-10-11"
},
{
"vote": 2,
"title": "Can't use OLLIE method in Standford Core NLP OpenIE for open information extraction",
"text": "I am facing trying to extract triples using OLLIE with Standford Core NLP's OpenIE tools.\n\n\nI've installed both \nstanford-corenlp-3.9.1\n as well \nstanford-corenlp-3.9.2\n to try to extract triples from text.\n\n\nFor \nstanford-corenlp-3.9.1\n:\n\n\n\n\nCan only extract information using default method, despite adding the flag \"\n-format ollie\n\" or \"\n-openie.format ollie\n\"\n\n\nI've tested it with this sentence\n\n\n\n\n>Some people say Barack Obama was not born in the United States.\n\n\nWhich should yield this:\n\n\n>(Barack Obama; was not born in; the United States)[attrib=Some people say]\n\n\nThis is the example to test if the OpenIE methids is indeed OLLIE. But I get no triples instead. It does work for other sentences however, but the output is that of the default method.\n\n\n\n\nThis is the example to test if the OpenIE methids is indeed ollie. But I get no triples instead. It does work for other sentences however, but the output is that of the default method.\n\n\n\n\nFor \nstanford-corenlp-3.9.2\n:\n\n\n\n\nI was unable to extract any triples at all, but get this error instead.\n\n\n\n\n&#x200B;\n\n\njava.lang.IllegalArgumentException: annotator &quot;openie&quot; requires annotation &quot;CorefChainAnnotation&quot;. The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,depparse,natlog\n\n\n\nClick here\n to be directed to the post that I've initially posted on Stack Overflow.",
"date": "2018-10-11"
},
{
"vote": 5,
"title": "OCR several webpages automatically",
"text": "Hello,\n\n\nI bought an ebook but found out that unfortunately it can only be read online in their website. It's not compatible with kindle or kobo. It can also be read on the phone but the xhtml is all encrypted.\n\n\nIf I go to the beach or want to read it on my bed, for example, is not adequate.\n\n\nI'd like to at least convert it to pdf so i can read it on my kindle device.\n\n\nIs there a way where i can flip all the pages on the book and have an OCR program recognize all the written text?\n\n\n&#x200B;\n\n\nThank you",
"date": "2018-10-11"
},
{
"vote": 10,
"title": "Where can I learn unsupervised learning for NLP?",
"text": "Textbooks usually only have one chapter dedicated to unsupervised learning, this is an exaggeration of course.\n\n\nI want to learn unsupervised learning techniques specifically for NLP and proper text cleaning procedures, etc\n\n\nAny recommendations on textbooks, resources, etc?",
"date": "2018-10-10"
},
{
"vote": 0,
"title": "These two chatbots will help you on your day-to-day work",
"text": null,
"date": "2018-10-10"
},
{
"vote": 6,
"title": "Discourse Representation Theory: 1993 text is all there is? Nothing since of major importance? How can this be true?",
"text": "[deleted]",
"date": "2018-10-10"
},
{
"vote": 1,
"title": "Sarcasm Detection Dataset",
"text": "Please suggest dataset for detecting  sarcasm for Hindi language for deep learning model? I am working on this project for which I am searching dataset but 'm unable to get that in good amount. Thanks in advance.",
"date": "2018-10-10"
},
{
"vote": 6,
"title": "How do I answer scenario-based interview question for NLP problems?",
"text": "I need to know how to answer scenario-based interview questions in by talking through the steps of exactly what I'd do, what kind of problems I might run into, how to overcome problems, etc.\n\n\nBasically, things like: I want you to make me a morphological parser. Take me through the steps to do that. I want you to make me a dependency parser in an unknown language. How would you do that? A company wants a chatbot that a user can ask a question like \"Which power tools do you have that are cheap and good for house work?\" And the company wants the computer to respond with the 3 cheapest power tools that are good for housework. How would you design a system that can do that?\n\n\nI don't really have much of a clue on how to begin to answer scenario-based interview questions like this on NLP problems. I would stare like a deer in the headlights if I'm asked things like this. But I need to know how to do it because this is what they ask in interviews for NLP positions and I KNOW I'm going to be asked things like this. I'm looking for ideas on how to answer these, how to take the interviewer through a step-by-step process for how I would design these types of systems, from start to finish, what resources I'd use along the way, how I'd ensure quality control, what benchmarks and timelines I would have, etc. Could I please get help on how to do that? Thanks.",
"date": "2018-10-09"
},
{
"vote": 2,
"title": "[NEWBIE] Can Discourse Representation Structures be considered a Meaning Representation Language?",
"text": "I'm preparing my first presentation for a seminar and don't want to make a serious type error on my first slide.\nIn particular I want to briefly describe two approaches: FOL and DRT, and I'm wondering what common denominator I can use other than \"formalisms\".",
"date": "2018-10-09"
},
{
"vote": 1,
"title": "My NER model isn't tagging anything, only trained on a small set, is that the reason or is there something more wrong?",
"text": "I have NER-tagged just one news article and have converted it into this format, appropriate for spacy:\n\n\nTRAIN_DATA = [\n (&quot;Uber blew through $1 million a week&quot;, {&#039;entities&#039;: [(0, 4, &#039;ORG&#039;)]}),\n (&quot;Google rebrands its business apps&quot;, {&#039;entities&#039;: [(0, 6, &quot;ORG&quot;)]})]\n\n\n\nI'm using this code to train the model:\n\n\nnlp = spacy.blank(&#039;en&#039;)\n    optimizer = nlp.begin_training()\n    for i in range(20):\n        random.shuffle(training_data)\n        for sentence, entities in training_data:\n            nlp.update([sentence], [entities], sgd=optimizer)\n\n\n\nThen I take a sentence called 'sample' and try to test the model on sample like so:\n\n\ndoc = nlp(sample)\ndisplacy.serve(doc, style=&#039;ent&#039;)\n\n\n\nThe model doesn't tag anything in my sample sentence. It gives me the message \"No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the \ndoc.ents\n property manually if necessary.\"\n\n\nI'm putting in sentences that are in my (very small) training set. It tags nothing. I'm wondering if the training set just isn't big enough and that's why it's doing it, or if the code is wrong as the error message suggests, and somehow this model doesn't support NER. What's going on here? Should I just label more data and re-train and it'll work better?",
"date": "2018-10-08"
},
{
"vote": 1,
"title": "I'm looking for an annotation tool for FREE where I can put in a sentence or short piece of text, highlight over a portion, and it shows me start and end indexes",
"text": "I want to do named entity annotation. I want to eventually convert it into spacy's acceptable format, which looks like this:\n\n\n(&quot;Uber blew through $1 million a week&quot;, {&#039;entities&#039;: [(0, 4, &#039;ORG&#039;)]}),\n (&quot;Google rebrands its business apps&quot;, {&#039;entities&#039;: [(0, 6, &quot;ORG&quot;)]})]\n\n\n\nFor this, I need to look at sentences or short text snippets, and identify the named entity by start index, end index, and the type of named entity it is. I could do this manually by physically counting out the indexes, but this would get frustrating. So I'm looking for a tool, whether that's a web app or something downloadable, where I can input some short text, and highlight over a portion, and it'll tell me the start and end indexes right away. Is there a tool for that available? Thanks.",
"date": "2018-10-08"
},
{
"vote": 16,
"title": "Deep Contextualized Word Representations With ELMo from AI2",
"text": "Nice blog post describing the impressive recent work by the Allen Institute for AI: \nhttps://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/",
"date": "2018-10-08"
},
{
"vote": 2,
"title": "Detecting references to tables/images/lists in text",
"text": "I am building a domain specific Q&A system. I wanted to detect whether a paragraph contains reference to a table or image or list in that section, or some other section, basically detect phrases similar to 'refer to table below...', 'see image...', 'to switch on the monitor, follow steps....' and so on.",
"date": "2018-10-08"
},
{
"vote": 4,
"title": "Question about the features of Dependency parser",
"text": "`Link of the paper: \nhttps://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf\n\n\nhttps://preview.redd.it/nfmgvue5clq11.png?width=1120&format=png&auto=webp&s=fc1953a264be1cc5e17a15d83c2c3261ee63c10e\n\n\n&#x200B;\n\n\nhttps://preview.redd.it/jj3wiu9aclq11.png?width=1148&format=png&auto=webp&s=6efccec8be4fda6c4b032f99718b5c9f89d8609b\n\n\nThese are two screenshots that I think is relevant to my question. So my question is what does s1.wt stands for? I know s1.w is the word on the top of the stack and s1.t is the POS tag of it. Can not understand what 'wt' means",
"date": "2018-10-06"
},
{
"vote": 5,
"title": "Twitch Chat Sentiment Analysis (survey)",
"text": "Hi,\n\n\nI'm currently writing a paper on Sentiment Analysis using Twitch chat messages. Since Twitch chat often consists of Emotes only, we need to know which emotes express which sentiment (positive, negative, neutral). \n\n\nIf you have the time and are a bit familiar with Twitch chat it would help us a lot if you participated in this survey: \nhttps://goo.gl/forms/Dm1581DyqlsOSQW73\n\n\nIf you're interested you can PM me and I'll send you a pdf of the paper once it's published which should be around the end of November. :)\n\n\nAbstract:\n> The continuing rise of esports and gaming in general opens up new opportunities for gamers and gaming related companies to gather feedback about their products. In this work, we explore the possibility of getting feedback for Twitch streams by analyzing the sentiment of live text comments made by stream viewers. The language used in comments, which is very different from common English, is a challenging task which we solve by using emotes as weak labels. We show in two case studies that our methods produce sentiment predictions of commenting users during Twitch chats that correlate with events happening in the given stream. This allows a new kind of implicit feedback gathering for Twitch streamers and companies producing games or streaming content on Twitch.",
"date": "2018-10-05"
},
{
"vote": 1,
"title": "Python nltk.CFG random terminals",
"text": "I am trying to create a CFG using the nltk.CFG\n package and then generate random strings with it using the generate\n method. The CFG.fromstring\n method creates a grammar with a pre-defined set of productions with static terminals. What I want, is to incorporate random terminals into the grammar. That is, one of the non-terminals should be a randomly generated number. Obviously, I cannot add a random number into an immutable string.\n\n\nWhat I first tried, is to build the grammar string using string concatenation, which works OK. But if I do it that way, I have to recreate the grammar for each generated string, which is not efficient. Is there any other way/library in Python where I can simulate randomness in the grammar?",
"date": "2018-10-05"
},
{
"vote": 4,
"title": "Failed in sentence correction using LSTM",
"text": "I want to correct a sentence of 20 words. \nThe dataset has 10k sentence. I uses a seq2seq model \nhttps://github.com/IBM/pytorch-seq2seq\n, but the result is terrible (the result is more like sentence prediction...).\nWhat are the possible reasons?  I chose the wrong model or the dataset is too small?",
"date": "2018-10-05"
},
{
"vote": 8,
"title": "Parallel Text Datasets for Machine Translation",
"text": null,
"date": "2018-10-05"
},
{
"vote": 6,
"title": "College Courses to Take for a Career in Language Tech/NLP",
"text": "Hi! I'm a fourth year undergrad student, wondering if there are any courses you would consider essential or really helpful in pursuing a career in language tech/NLP/comp ling, especially courses that may not seem intuitive to take for someone with no actual experience in industry. I'm getting a B.A. in linguistics with a minor in computer science, and these are the courses I've already taken or am currently taking that are at least remotely relevant:  \n\n\nLinguistics:\nPhonetics & Phonology\nSyntax\nSemantics & Pragmatics\nMorphology\nHistorical Linguistics\nSociophonetics\nA class on how languages encode spatial relationships  \n\n\nComputer Science/Math/Stat:\n\n\nMy school's intro sequence: basic data structures, algorithms and systems stuff\nScientific Visualization\nComputational Linguistics\nIntro to Statistics\nDiscrete Math\nAlgorithms\nI plan on taking a probability course next quarter, but other than that, I'm not sure what I should be looking for. Any suggestions are really appreciated!",
"date": "2018-10-04"
},
{
"vote": 20,
"title": "OCR &amp; Handwriting Datasets for Machine Learning",
"text": null,
"date": "2018-10-04"
},
{
"vote": 3,
"title": "Simple Named Entity Recognition Example",
"text": null,
"date": "2018-10-03"
},
{
"vote": 15,
"title": "A Review of the Recent History of Natural Language Processing",
"text": null,
"date": "2018-10-02"
},
{
"vote": 3,
"title": "List of (very) negative nouns",
"text": "Can anyone tell me where I can find a list of extremely negative nouns or noun phrases, such as \"holocaust\", \"nuclear holocaust\", \"genocide\", \"civilization-ending asteroid impact\"?\n\n\n&#x200B;\n\n\nIf no such list is available, perhaps someone could advise a strategy for constructing such a list?",
"date": "2018-10-01"
},
{
"vote": 16,
"title": "\"Perplexity is ubiquitous in the evaluation of language models. But where did it come from? Jelinek et al. (1977) invented the measure and give a justification for its use.\"",
"text": null,
"date": "2018-10-01"
},
{
"vote": 3,
"title": "new book \"Quality Estimation for Machine Translation\"",
"text": null,
"date": "2018-09-29"
},
{
"vote": 2,
"title": "Punctuation Restoration With Recurrent Neural Networks",
"text": null,
"date": "2018-09-28"
},
{
"vote": 1,
"title": "Attention Models: Amplifying Machine Learning Benefits for Enterprise",
"text": null,
"date": "2018-09-27"
},
{
"vote": 28,
"title": "Free Chatbot Datasets for Machine Learning",
"text": null,
"date": "2018-09-27"
},
{
"vote": 4,
"title": "Rule-based information extraction engine",
"text": null,
"date": "2018-09-26"
},
{
"vote": 2,
"title": "CoNLL 2018 programme",
"text": null,
"date": "2018-09-26"
},
{
"vote": 21,
"title": "[1809.08510] Towards Language Agnostic Universal Representations",
"text": null,
"date": "2018-09-26"
},
{
"vote": 1,
"title": "QuAC - Question Answering in Context - dataset for modeling, understanding, and participating in information seeking dialog",
"text": null,
"date": "2018-09-26"
},
{
"vote": 6,
"title": "NLP data supervision/labeling tool",
"text": "What are the best sentence pair alignment tools out there?",
"date": "2018-09-25"
},
{
"vote": 18,
"title": "The essential guide to how NLP works",
"text": null,
"date": "2018-09-25"
},
{
"vote": 4,
"title": "Drawing a blank on what NLP technique this is called.",
"text": "Within a piece of text, I'm trying to detect who did what to whom.\n\n\nFor instance, in the following sentences:\n\n\n> CV hit IV. CV was hit by IV\n\n\nI'd like to know who hit who.\n\n\nI can't remember what this technique is called. Thanks for any help!",
"date": "2018-09-24"
},
{
"vote": 2,
"title": "Question about Training phase in Convolutional Neural Network",
"text": "I'm really a newbie about this thing. So please if someone can answer my idiotic question.\n\n\n\n\nin training phase, what we do is doing Gradient Descent with backpropogation to all of our training data to find Datamodel/feature that we gonna use as a filter in convolution layer in our main program?? yes or no???",
"date": "2018-09-23"
},
{
"vote": 5,
"title": "Machine-learning system tackles speech and object recognition, all at once",
"text": null,
"date": "2018-09-22"
},
{
"vote": 9,
"title": "[P] Keras implementation of Guo et al.'s (2017) End-to-End Multi-View Networks for Text Classification",
"text": null,
"date": "2018-09-21"
},
{
"vote": 7,
"title": "[D] Any open repository or literature which has rules concerning the basic human commonsense?",
"text": null,
"date": "2018-09-21"
},
{
"vote": 5,
"title": "EMNLP schedule and accepted papers",
"text": null,
"date": "2018-09-20"
},
{
"vote": 23,
"title": "NLU Is Not NLP++",
"text": null,
"date": "2018-09-20"
},
{
"vote": 1,
"title": "Making Technology Available in Vernacular Languages to Help Boost Growth: Nitin Gadkari",
"text": null,
"date": "2018-09-19"
},
{
"vote": 1,
"title": "Terminology of beings",
"text": null,
"date": "2018-09-18"
},
{
"vote": 16,
"title": "Complexity of transformer attention network",
"text": null,
"date": "2018-09-18"
},
{
"vote": 2,
"title": "I'm thinking of doing Speech Recognition with CNN for my thesis, help.",
"text": "Its about verification of pronunciation of Arabic letter.\n\n\nCan you guys oversimplify step by step what i must do?\n\n\nYou dont have to be spesific, thank you so much",
"date": "2018-09-18"
},
{
"vote": 3,
"title": "Best way to display large number of words (~20,000) in 2D XY-plane",
"text": "[deleted]",
"date": "2018-09-17"
},
{
"vote": 3,
"title": "Python language libraries: ability to summarize a text?",
"text": "I've been experimenting with various Python natural language libraries. They seem to work, and they give very simple results. I'm looking for something more complex -- the ability of a script to process a text and summarize that text with, for instance, three main points. Does such a thing exist in language processing technology? If so, what is the official term for this kind of technology?",
"date": "2018-09-17"
},
{
"vote": 19,
"title": "\"The Gale--Church algorithm (Gale and Church 1993) provides a method, still used today, for aligning *sentences* (not words) in a parallel corpus. In an age before github, they released their code in an 11-page appendix!\"",
"text": null,
"date": "2018-09-17"
},
{
"vote": 4,
"title": "Similar document recommender",
"text": "I'm trying to learn similar documents in an unsupervised fashion using embedding representations (gensim doc2vec). Problem is that embeddings are sensitive to size and window, type of training which leads to model working well for some documents and performing poorly for the rest and vice versa for different settings. I am yet to find a setting that works well for all 1M documents.  \n\n\n\n\nMaybe I'm expecting too much ? If so, what techniques can I use that have worked for you?\n\n\nI do not like LSI/LDA way of asking for the number of topics to extract beforehand.\n\n\nAny suggestions on ways for preprocessing in a way that'll help learn important stuff. Atm I am splitting in sentences followed by word tokenizing -> only keeping nouns, stemming and lowercasing. (I have not stripped numbers/dates yet and neither entities like location etc)\n\n\nThe label is just a unique number for each corresponding document, end goal is to get similar documents. (Should I pick nouns from document and use it as label to force clustering of docs by bag of words?)\n\n\nThe model is not research only, is expected to go into production. So in production what should be done if I have low scoring similar samples for given document?\n\n\n\n\nThank you r/LanguageTechnology for teaching me so much and keeping positive vibes.",
"date": "2018-09-15"
},
{
"vote": 10,
"title": "[P] Hey /r/LT, I made a research paper recommender for Machine Learning /Computer Science, and I would love for you to try it out! Uses embedding representation for each paper: you can get recommendations of a combo of several papers, and TSNE maps of the recommendations. Easily to run, Google Colab",
"text": "I made a research paper recommender for Machine Learning and Computer Science in general, try it out! It uses embeddings to represent each paper, so you can get TSNE maps of the recommended papers, recommendations of a combo of several papers, and TSNE maps of the recommendations for that combo of several papers.\n\n\n###What is it?\n\n\nThe dataset used is Semantic Scholar's corpus of research paper (\nhttps://labs.semanticscholar.org/corpus/\n ), and was trained by a Word2Vec-based algorithm to develop an embedding for each paper. The database contains 1,666,577 papers, mostly in the computer science field. You can put 1 or more (as many as you want) papers and the recommender will return the most similar papers to those papers. You can also make TSNE maps of those recommendations. \n\n\nhttps://i.imgur.com/B4qdoCC.jpg\n\n\nhttps://i.imgur.com/OCgp0MV.jpg\n\n\n###Where is it?\n\n\nGithub\n\n\nhttps://github.com/Santosh-Gupta/Research2Vec/blob/master/Research2VecPublicPlayGround.ipynb\n\n\nOr direct look to Google Colab\n\n\nhttps://drive.google.com/open?id=1-0ggLs2r-5nWDWb-TNWqR2osaiXqNEsL\n\n\n###What can you do with it ?\n\n\nYou can input a paper, and see what are the most similar papers to it, though the first 30-80 will most likely be papers it has cited or was cited by. I've set it to return 300 papers but it ranks all 1,666,577 papers so you can set it to return whatever number of papers you want without any change in performance (except when it comes to developing the TSNE maps)\n\n\nNow, the fun part: utilization the embedding properties:\n\n\nYou can see a TSNE map of how those similar papers are related to each other. The TSNE takes a while to process for 500 points (10-20 minutes). You can decrease the number of papers for a speedup, or increase the number of papers but that'll take more time.\n\n\nYou can input several papers by adding the embeddings, and get recommendations for combined papers, just add the embeddings for all the papers (you don't have to average them since the embeddings are normalized ). \n\n\nFinally, my favorite part, you can get TSNE maps of the recommendations for the combined papers are well. \n\n\nA great use case would be if you're writing a paper, or plan to do some research and would like to check if someone has already done something similar. You can input all the papers you cited or would like to cite, and look over the recommendations.\n \n\n\nHow important is this ?\n\n\nWhen I was in R&D, we spent a lot of time reinventing the wheel; a lot of techniques, methods, and processes that we developed were already pioneered or likely pioneered. But we weren't able to look for them, mainly due to not hitting the right keyword/phrasing in our queries.\n\n\nThere's a lot of variation in terms which can make finding papers for a particular concept very tricky at times.\n\n\nI've seen a few times someone release a paper, and someone else point out someone has implemented very similar concepts in a previous paper.\n\n\nEven the Google Brain team has trouble looking up all instances of previous work for a particular topic. A few months ago they released a paper of Swish activation function and people pointed out others have published stuff very similar to it.\n\n\n> \"As has been pointed out, we missed prior works that proposed the same activation function. The fault lies entirely with me for not conducting a thorough >enough literature search. My sincere apologies. We will revise our paper and give credit where credit is due.\"\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/773epu/r_swish_a_selfgated_activation_function_google/dojjag2/\n\n\nSo if this is something that happens to the Google Brain team, not being able to find all papers on a particular topic is something all people are prone too.\n\n\nHere's an example of two papers whose authors didn't know about each other until they saw each other on twitter, and they posted papers on nearly the exact same idea, which afaik are the only two papers on that concept.\n\n\nWord2Bits - Quantized Word Vectors\n\n\nhttps://arxiv.org/abs/1803.05651\n\n\nBinary Latent Representations for Efficient Ranking: Empirical Assessment\n\n\nhttps://arxiv.org/abs/1706.07479\n\n\nExact same concept, but two very different ways of descriptions and terminology.\n\n\n###How do I use it ?\n\n\nHere's a quick video demonstration:\n\n\nhttps://youtu.be/tlutFm1meMs\n\n\nI tried to make this user friendly and as fast to figure out and run as possible, but there's probably stuff I didn't take into account. Let me know of you have any questions on how to run it or any feedback. If you want, you can just give me what papers you want to analyze and I'll do it for you (look up the papers on \nhttps://www.semanticscholar.org/\n first )\n\n\nHere's a step by step guide to help people get started\n\n\nStep 1:\n \n\n\nRun the Section 1 of code in the Colab notebook. This will download the model and the dictionaries for the titles, Ids, and links. \n\n\nhttps://snag.gy/rmoCXO.jpg\n\n\nStep 2:\n\n\nFind the papers want to find similar papers for at Semantic Scholar \nhttps://www.semanticscholar.org\n\n\nGet either the title or Semantic Scholar's paperID, which is the last section of numbers/letters in the link. For example, in this link\n\n\nhttps://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/9abbd40510ef4b9f1b6a77701491ff4f7f0fdfb3\n\n\nThe Semantic Scholar paper ID is '9abbd40510ef4b9f1b6a77701491ff4f7f0fdfb3'\n\n\nUse the title(s) and/or Semantic Scholar's paperID(s) with Section 2 and Section 3 to get the EmbedID from the model. EmbedIDs are how the model keeps track of each paper (not the paperID). If using the title to search, don't forget to use only lower case letters only. \n\n\nhttps://snag.gy/3yjx2o.jpg\n\n\nThe EmbedID is what each dictionary first returns. \n\n\nStep 3:\n\n\nInsert the EmbedID(s) as the values of paper1EmbedID, paper2EmbedID, paper3EmbedID, paper4EmbedID, etc. \n\n\nhttps://snag.gy/AzeP91.jpg\n\n\nIf you have less than or more than 4 papers you want to analyze, change this line \n\n\nextracted_v = paper1 + paper2 + paper3 + paper4\n\n\nand create or eliminate the lines of code for vector extraction\n\n\npaper1 = np.take(final_embeddings, paper1EmbedID , axis=0)   \npaper2 = np.take(final_embeddings, paper2EmbedID , axis=0) \npaper3 = np.take(final_embeddings, paper3EmbedID , axis=0)   \npaper4 = np.take(final_embeddings, paper4EmbedID , axis=0) \n\n\n\nFinally, run Section 4 to get a TSNE map of the recomendations. With 300 papers, it takes 15-18 minutes for the map to be produced. \n\n\nAsk any question you have no matter how minor, I want people to use this as quickly as possible with as little time as possible figuring out what to do.\n \n\n\nOther details\n\n\nSo it probably doesn't have any papers released in the last 5 months; I think the corpus was last updated in May 2018. Due to the limitation on my computational resources (Google Colab) I had to filter towards more papers with more connections to other papers in the database. A connection is either a citation to another paper in the database, or cited by another paper in the database. I filtered to only include papers with 20 or more connections because Colab would crash if I tried to include more. \n\n\nAs of right now, the recommender has 1,666,577 papers. I hope to make future versions with more many more papers, including papers from other fields. \n\n\nFeedback greatly appreciated !\n\n\nI am hoping to get as much feedback as possible. I am specifically looking for cases where you feel that the recommender should have given a particular paper in the top results, but didn't. I am hoping to make an evaluation toolkit (kinda like Facebook SentEval \nhttps://github.com/facebookresearch/SentEval\n ) that I can use to tune the hyperparameters. \n\n\nThis feedback will also be helpful in my future plans, where I am planning on incorporating several other measures of similarity, and then use a attention mechanism to weight them for a final similarity. One method of content analysis I would really like to use is Contextual Salience \nhttps://arxiv.org/abs/1803.08493\n. Another was something another Redditor just pointed out is cite2vec \nhttps://matthewberger.github.io/papers/cite2vec.pdf\n\n\nUsing a combo like this would help in one of the reoccurring hard search cases I encountered in R&D, which was trying to look up parameters of a particular method, when the method itself is not the focus of the paper. I was just actually encountering this issue when doing my project. I wanting to know more about what optimal hyperparameters others have found when working with embedding representations. This may not be the main topic of the paper, but it may have been described in the methods section of the paper. But this is hard to search for since paper searches mainly focus on the main subjects of the paper.\n\n\nOf course, I would very much appreciate whatever feedback, questions, comments, thoughts you have on this project. \n\n\nEdit:\n\n\nIf the language of this post seems familiar, I'm the guy who did the paper find / paper help threads a few months ago. I will actually use the stuff from the paper help discussions in evaluating my recommender. \n\n\nhttps://www.reddit.com/r/MachineLearning/comments/8bwuyg/d_anyone_having_trouble_finding_papers_on_a/\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/8elmd8/d_anyone_having_trouble_reading_a_particular/\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/8b4vi0/d_anyone_having_trouble_reading_a_particular/?utm_content=comments&utm_medium=front&utm_source=reddit&utm_name=MachineLearning",
"date": "2018-09-15"
},
{
"vote": 1,
"title": "Let's go to MALL",
"text": "[removed]",
"date": "2018-09-14"
},
{
"vote": 23,
"title": "What are some good NLP projects to contribute to?",
"text": "[deleted]",
"date": "2018-09-14"
},
{
"vote": 5,
"title": "Learning to Summarize Radiology Findings",
"text": null,
"date": "2018-09-13"
},
{
"vote": 15,
"title": "A computational approach to politeness",
"text": null,
"date": "2018-09-11"
},
{
"vote": 2,
"title": "Mapping natural language commands to web elements",
"text": null,
"date": "2018-09-10"
},
{
"vote": 3,
"title": "[R] Document-Level Neural Machine Translation with Hierarchical Attention Networks",
"text": null,
"date": "2018-09-10"
},
{
"vote": 6,
"title": "Best way to learn NLP specifically for Text Summarization and Topic Classification`",
"text": "I am looking to develop my skills in NLP specifically in the areas of Text Summarization and Classification. I am currently enrolled in Applied Text Mining in Python and it seems to be insufficient for my needs. Also the NLP courses in Coursera seem to have been deprecated. Can anyone suggest a good primer text/lectures for someone with a decent knowledge of Python?",
"date": "2018-09-09"
},
{
"vote": 0,
"title": "Join r/MachinesLearn!",
"text": "With the permission from moderators, let me invite you to join the new AI subreddit: \nr/MachinesLearn\n.\n\n\nThe community is oriented on practitioners in the AI field, so tutorials, reviews, and news on practically useful machine learning algorithms, tools, frameworks, libraries and datasets are welcome.\n\n\nSince it's r/LanguageTechnology, here are a couple of our recent language-related posts:\n\n\nâ€” \nespnet: End-to-End Speech Processing Toolkit\n\n\nâ€” \nMoving Beyond Translation with the Universal Transformer\n\n\nJoin us!\n\n\n(Thanks to mods for allowing this post!)",
"date": "2018-09-08"
},
{
"vote": 1,
"title": "Text Analytics APIs 2018: A Consumer Guide",
"text": "Text Analytics APIs 2018: A Consumer Guide (\nhttps://www.language-technology.com/textapiguide\n) is a deep-dive technical review of 26 software-as-a-service text analysis services, created by Dr. Robert Dale, an internationally-recognized expert in Natural Language Processing.\n\n\nDr. Dale's guide will be invaluable for any organization seeking to add NLP capabilities to its solution set -- for developers, data scientists, and managers, and for anyone who would like to understand cloud text analytics possibilities.\n\n\nThe guide covers ten key Text Analytics capabilities: entity recognition, sentiment analysis, classification, language detection, keyphrase extraction, linguistic analysis, relation extraction, text extraction, summarization, and concept tagging.\n\n\nVisit \nhttps://www.language-technology.com/textapiguide\n for a full description, pricing, and to obtain a sample excerpt of the full 299-page document.",
"date": "2018-09-08"
},
{
"vote": 7,
"title": "[1809.00066] Indicatements that character language models learn English morpho-syntactic units and regularities",
"text": null,
"date": "2018-09-07"
},
{
"vote": 4,
"title": "Collaboration on OpenNLP project",
"text": "I am currently working on a natural language processing tool utilizing opennlp and other Java libraries to change how the job market works. Resume & job posting creation, parsing, and analysis (Business intelligence). I'm looking for people who are interested in learning or collaborating on this project.\n\n\nPM me for more details.",
"date": "2018-09-07"
},
{
"vote": 1,
"title": "Marian NMT",
"text": null,
"date": "2018-09-07"
},
{
"vote": 14,
"title": "Fast word vectors with little memory usage in Python",
"text": "[deleted]",
"date": "2018-09-06"
},
{
"vote": 0,
"title": "Collaborative text filtering",
"text": "What are great publicly available collaborative text filtering datasets with published results.",
"date": "2018-09-06"
},
{
"vote": 5,
"title": "State of the Art Language Models",
"text": "I ran the salesforce awd lstm(\nhttps://github.com/salesforce/awd-lstm-lm\n) model on PTB and wikitext-2 with the parameters suggested in the readme and the paper. In neither of the four models, I got the mentioned perplexities. My perplexity figure differs by 4 points.\n\n\n&#x200B;\n\n\nWhat can I be doing possibly wrong?  Or is there a possibility that of some mistake in the paper?",
"date": "2018-09-06"
},
{
"vote": 3,
"title": "Finding answer span - issues in SQuAD dataset",
"text": "[removed]",
"date": "2018-09-05"
},
{
"vote": 3,
"title": "Word for word translation system using sentence context?",
"text": "I did a linguistics degree but then moved into programming, but unfortunately never did any NLP, so I don't really know where to start. I have a project that needs to get the best translation for the individual words in sentences (Chinese -> English to start with). I have looked around but can't find anything that does this. It's either individual words with zero context, or whole phrases with no way to map and no guarantee a given word will even get a translation.\n\n\nI have started playing around with the Stanford parser and the models they produce are exactly the kind of thing I'm looking for - I just need context-sensitive translations. What I would really love is to be able to plug that in to a system that will enable me to get translations for each leaf node in the tree. I don't need or want translations of phrases.\n\n\nDoes this sort of thing exist? Any pointers?",
"date": "2018-09-04"
},
{
"vote": 31,
"title": "A quick summary of the books I consider useful for anyone in NLP/LanguageTechnology",
"text": null,
"date": "2018-09-04"
},
{
"vote": 1,
"title": "Python-Spacy- Summarize a sentence",
"text": "I am trying to summarize a sentence using spacy python NLP library, In such a form even after summarizing it should convey the meaning and should be grammatically correct. Below are the few examples what I am trying to figure out.\n\n\n\n\nKnowledge is a constructed element resulting from the learning process.\n\n\n\n\nSol: Knowledge: constructed element from the learning process.\n\n\n\n\nTeachers must be able to mesh their life experiences with the curriculum.\n\n\n\n\nSol: Teachers mesh their life experiences with the curriculum.\n\n\n\n\nNo aspect of life goes untouched by social class\n\n\n\n\nSol: Life: no aspect goes untouched by social class.\n\n\n4)The twin babies Liza and Lira cried and thrashed around.\n\n\nSol: Liza and Lira, twin babies: cried and thrashed around.\n\n\nâ€‹\n\n\nAny suggestions on how to achieve this are appreciated.  Thank you in advance",
"date": "2018-09-03"
},
{
"vote": 5,
"title": "Intuition on the trick of reversing input sequences for seq2seq? Has it been tried for tasks beyond machine translation?",
"text": "The 2014 \nSequence to Sequence Learning with Neural Networks\n from Ilya Sutskever, Oriol Vinyals, Quoc Le introduced a trick that is now often used in translation:\n\n\n> we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\n\n\nRecently \na comment\n in a conversation between Yoav Goldberg and Graham Neubig hinted at something similar.\n\n\n> one of my unproven intuitions is that making life hard for the decoder improves the overall model accuracy. i wonder if thats also at play here.\n\n\nWhere can I read more?  And - coming to my actual question - has it been tried for tasks beyond machine translation?\n\n\nBackground: I recently \nmodified Fairseq's seq2seq impl to char-level to train it on a transliteration dataset\n. \n Transliteration has a smaller vocabulary and fewer long-distance dependencies than translation.  I am wondering whether reversing the input sequence helps or hurt and how it varies by dataset size.",
"date": "2018-09-03"
},
{
"vote": 14,
"title": "Storage and retrieval of Word Embeddings in various databases",
"text": null,
"date": "2018-09-02"
},
{
"vote": 1,
"title": "How do machine translators detect languages?",
"text": "[deleted]",
"date": "2018-08-29"
},
{
"vote": 6,
"title": "Sentiment Analysis via Self-Attention with MXNet Gluon - x-post r/mxnet",
"text": null,
"date": "2018-08-28"
},
{
"vote": 2,
"title": "Pytorch for start!",
"text": "Hi guys!\n\n\nI've decided start NLP by Pytorch.\n\n\nActually I want to design a translator.\n\n\nIs it suitable for this?\n\n\nWould u tell me which source is suitable for learning?",
"date": "2018-08-28"
},
{
"vote": 5,
"title": "2nd Conversational AI workshop at NIPS",
"text": null,
"date": "2018-08-28"
},
{
"vote": 1,
"title": "@nberpubs: Introduction of a machine-learning language-translation program on eBay increased U.S. exports facilitated by eBay to Spanish-speaking Latin American countries by 17.5â€“20.9 percent",
"text": null,
"date": "2018-08-28"
},
{
"vote": 2,
"title": "Need a paragraph tokenizer from nltk similar to nltk's sent_tokenize function",
"text": "I know that sent_tokenize exists and have used it. But I can't find a function that works exactly like this, except it tokenizes paragraphs instead of sentences. TextTilingTokenizer() doesn't work, it says this is for paragraphs, but all I get returned from this is a list of length 1, i.e. the full text as the single entry in a list. This doesn't help, I need a list returned where each entry is a paragraph of the original text. What's the function to do this?",
"date": "2018-08-28"
},
{
"vote": 4,
"title": "NLP Task: Finding Top 5 Customer issues mentioned in a customer reviews dataset",
"text": "Hello,\n\n\nI have been assigned an NLP task to compute the top 5 issues customers have complained about in a customer review dataset. The dataset is not big (less than 50,000 words). I used the LDA and NMF topic models and have failed to interpret the results. I adjusted the models to output 3,5, and 7 topics with 3 or 5 words in each topic. Is there any other way to compute the top 5 issues other than using topic modeling with LDA and NMF.\n\n\n&#x200B;\n\n\nI appreciate any help!",
"date": "2018-08-26"
},
{
"vote": 22,
"title": "COVFEFE: COre Variable Feature Extraction Feature Extractor",
"text": "Weâ€™re announcing the availability of COVFEFE, the COre Variable Feature Extraction Feature Extractor.\n\n\nhttps://github.com/SPOClab-ca/COVFEFE\n, under the Apache License 2.0.\n\n\nCOVFEFE is a fast, multi-threaded tool for running various feature extraction pipelines. A pipeline is a directed acyclic graph where each node is a processing task that sends its output to the next node in the graph. Features include lexicosyntactic (including lexical norms and grammatical complexity), semantic (including information content), pragmatic (including topic modeling and rhetorical structure theory), and acoustic.\n\n\nThis tool has been run in a variety of contexts, from assessing speech for signs of neurodegeneration, to analysis of quarterly business reports.",
"date": "2018-08-25"
},
{
"vote": 2,
"title": "Comparing Google's Cloud Natural Language API to open source alternatives",
"text": "https://cloud.google.com/natural-language/\n\n\nThey offer these services at various prices:\nDetects sentiment in the text.\nDetects entities in the text.\nDetects syntax in the text.\nDetects entity sentiment in the provided text.\nClassifies content categories of the provided text.\n\n\nHow much better are they compared to open source libraries like spaCy and nltk? Did they fully made use their advantages such as the access to vast information on the Internet to their advantage?\n\n\n&#x200B;",
"date": "2018-08-25"
},
{
"vote": 6,
"title": "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
"text": null,
"date": "2018-08-25"
},
{
"vote": 3,
"title": "Help with my research on natural language generator (online survey)",
"text": "[deleted]",
"date": "2018-08-24"
},
{
"vote": 2,
"title": "Help: Generating sentences from a graph of words",
"text": "Suppose I have a directed acyclic graph like the one in the image.\n\n\nWhat is the best approach to generate acceptable sentences? (\n\"she is nice\"\n, for example)\n\n\nI could assign weights to the edges based on a word n-gram language model, and then treated it as if it were a shortest (or longest) path problem.\n\n\nI'm not sure how to implement this (since it's not really a graph of words, but of sets of words that are only similar in their pronunciation -- think DoubleMetaphone).\n\n\nAny resources on this specific problem? (papers, articles, open source projects, anything)\n\n\nâ€‹\n\n\nhttps://preview.redd.it/iyb1zqgyrwh11.png?width=860&format=png&auto=webp&s=e6a177c058defea9eb8ac561a17f08433bd4814e",
"date": "2018-08-23"
},
{
"vote": 4,
"title": "Resources for implementing TTS from scratch?",
"text": "Iâ€™m looking to learn about speech synthesis so as to implement a basic text-to-speech system for a school project.\n\n\nThere donâ€™t seem to be as many resources on this, and most donâ€™t seem to be beginner friendly. I know a little bit about machine learning but am largely a beginner to NLP itself. Does anyone know of a good way to learn how to do this?\n\n\nThank you!",
"date": "2018-08-23"
},
{
"vote": 25,
"title": "More Effective Transfer Learning for NLP",
"text": null,
"date": "2018-08-23"
},
{
"vote": 1,
"title": "Abstractive summarizer demo?",
"text": "Is their a website/API I can use to try out an abstractive summarizer, or is the only way to download some of the pretrained models I see on Github?",
"date": "2018-08-23"
},
{
"vote": 11,
"title": "[1808.03840] Fake Sentence Detection as a Training Task for Sentence Encoding",
"text": null,
"date": "2018-08-21"
},
{
"vote": 6,
"title": "Language Models (SoTA)",
"text": "I am looking for some state of the art Language Model code.  Could anyone guide me where  to look for it ? \n\n\nPreferably this paper:  \nhttps://arxiv.org/pdf/1707.05589.pdf",
"date": "2018-08-20"
},
{
"vote": 1,
"title": "Translation Services in India",
"text": "[removed]",
"date": "2018-08-20"
},
{
"vote": 0,
"title": "Chat bot : confused to build chat bot intent (500 intents) classifier with less data (10 utterances for each intent) having lots of domain specific words.",
"text": "Hi,\n\n\nI have to build a intent classifier for chat bot with less data having lot of domain specific keywords, here is how my data is\n\n\n2 intents data from my actual data set:\n\n\n\n\nintent : create ECCI -- domain specific word is ECCI  \n\n\n\n\nexample utterances:  \n\n\n\n\ncreate ecci\n\n\ngenerate ecci\n\n\nproduce ecci, etc..\n\n\n\n\n\n\n\n\n\n\nIntent: create E-bill -- domain specific word is E-bill  \n\n\n\n\nExample utterances:  \n\n\n\n\ncreate e-bill\n\n\ncreation of ebill\n\n\nproduction of ebill, etc...\n\n\n\n\n\n\n\n\n\n\n\n\nlike this similarly i have 500 intents and following are the approaches i had thought:\n\n\n\n\ntf - idf : as we can't write all possible utterances it's difficult if users asks (make ecci ..) to identify so this concept will not work.\n\n\n\n\nWord embeddings : here problem is with domain specific words, as we don't have any vector representation of this in the pre - trainined models, we can either do following approaches.  \n\n\n\n\nuse the average of word embeddins but as i have similar intents as above 1 and 2 model is not able to identify exact intent -- this method is also not useful\n\n\ntraining own word embeddings as I read from lot of articles, huge data should be required to get good word representation for each word -- so confused to go with this one\n\n\n\n\n\n\n\n\nSo this is my ambiguity, kindly share some insights on above approaches if my understanding is wrong and any thoughts on how to solve this problem, links for blog posts are much appreciated.\n\n\nThanks.",
"date": "2018-08-18"
},
{
"vote": 1,
"title": "Match similar sentences",
"text": "What are the best deep learning models to find if two sentences are similar.\n\n\nI have tried the following:\n\n\n\n\nword2vec + cosine similarity\n\n\nMaLSTM",
"date": "2018-08-17"
},
{
"vote": 12,
"title": "UnsupervisedMT released by FAIR",
"text": null,
"date": "2018-08-16"
},
{
"vote": 3,
"title": "Event Extraction",
"text": "[deleted]",
"date": "2018-08-15"
},
{
"vote": 2,
"title": "Rapid Adaptation of Neural Machine Translation to New Languages",
"text": null,
"date": "2018-08-15"
},
{
"vote": 22,
"title": "An Attempt to Chart the History of NLP in 5 Papers: Part I",
"text": null,
"date": "2018-08-14"
},
{
"vote": 5,
"title": "Generate a text description through a list of subjects and their relations",
"text": "Hello r/LanguageTechnology,\n\n\nI've recently started approaching NLP for a project in my university which from an image it provides a textual description of it based on the elements within and their relationship. The first part (obtaining the objects and their relationship) of the project has already been made, but for what concerns the text description generation I'm having hard times due to the lack of cover from the course I'm doing this project for. Reading through some papers about text description from an image, I've found they use a CRF most of the times to predict what the next word is based on the previous ones. This seems clear to me, but some other questions came up to my mind about how to accomplish this:\n\n\n\n\nFirst of all, since I already have a list of triples in the form (Noun1, Noun2, Relationship), is it really necessary to use something like that?\n\n\nAssuming I want to go for CRF, does the fact I already have a list of triples, from a*finite relationship set, affect my training phase (and what datasets would suit my goal)?\n\n\nDo you have some books / examples that already cover this problem? I haven't found a specific solution, just lot of papers which talks about without providing any snippet at all.\n\n\n\n\nI'm sorry for this newbie-request question, but as you could tell I'm currently in a situation where I've found lot of theory about what I want to accomplish without knowing which framework / strategy / algorithm works best.\n\n\nThank you!",
"date": "2018-08-14"
},
{
"vote": 13,
"title": "CMU CS 11-731: Machine Translation and Sequence-to-Sequence Models",
"text": null,
"date": "2018-08-13"
},
{
"vote": 2,
"title": "What are the disadvantages of SPARQL based databases",
"text": "SPARQL query language with RDF databases are widely used in Question answering like system where they used structured data to answer user queries. But what are some queries that can not be answered by SPARQL based systems.",
"date": "2018-08-13"
},
{
"vote": 1,
"title": "Translation Services in Delhi",
"text": "[removed]",
"date": "2018-08-13"
},
{
"vote": 3,
"title": "How to evaluate an answer to a question given the original answer?",
"text": "Assume the question could be both objective and subjective.\n\n\nI don't have a dataset. Suggest me one. And also, similar work available online.\n\n\nAt least, suggest how to approach this problem.",
"date": "2018-08-12"
},
{
"vote": 0,
"title": "MFCC+CTC",
"text": "i want to do Thesis about Verification Recitation of Quran Verse, basically arabic word. Most of the paper i read is using MFCC+HMM, \n\n\ni read that CTC is better than HMM-DNN, so what do you think about that? Will it work?\n\n\nI'm sorry if that is a stupid question, i just finished NLP last semester",
"date": "2018-08-11"
},
{
"vote": 5,
"title": "Proceedings | COLING 2018",
"text": null,
"date": "2018-08-09"
},
{
"vote": 3,
"title": "Database for Question Answering type system",
"text": "I am working on a Question Answering based system for that I need to decide the databases. I see Stack overflow and Quora uses SQL based databases but the industry is moving towards NoSQL, Graph based databases. Can anybody help me modelling database for this system. Thank you",
"date": "2018-08-09"
},
{
"vote": 2,
"title": "Course selection: Phonetics, Syntax, or Semantics?",
"text": "I'm currently working on an applied math undergrad with an emphasis and minor in linguistics. Most of the classes I can take for the minor are specifically related to computational linguistics (NLP, text analysis, speech synthesis, etc.), however I need to take one class from the following (brief course descriptions are in the links): \n\n\n\n\nSounds of Language\n (phonetics)\n\n\nStructure in Language\n (syntax)\n\n\nMeaning in Language\n (semantics/pragmatics)\n\n\n\n\nMy question is this: In your experience, which would you say might be most useful in pursuing education in computational linguistics? \n\n\nI'm leaning mostly toward either Structure or Meaning. I also understand that it may depend on what I specifically intend to do as a career, so I would be more than happy to hear input as to what the various applications of each might be.",
"date": "2018-08-08"
},
{
"vote": 26,
"title": "â›µ Learning Meaning and Semantics in Natural Language Processing",
"text": null,
"date": "2018-08-08"
},
{
"vote": 3,
"title": "When is it appropriate to remove words according to their document frequency, versus their total frequency across documents?",
"text": "Different libraries perform the removal of low-frequency words from the vocabulary using different procedures. For example, \ngensim\n for its doc2vec method removes words based on their total frequency across documents (e.g. if it occurs in one document 15 times, and another 30 times, and no other documents, and the threshold is 40, then the total document frequency is 45 and the term would not be removed), while \nsklearn\n for its more general method removes terms based on their document frequency (e.g. if it occurs in one document 5 times, and another 15 times, but in no other documents, and the threshold is 3, then the document frequency is 2 and the term would be removed).\n\n\nI'm just curious if there is a particular justification for this - it seems to me that the document frequency would be more stable across different document lengths, and makes sense in the particular case of removing all terms that occur in only one document (as there is little similarity information). Is there some niche in the doc2vec application?",
"date": "2018-08-07"
},
{
"vote": 15,
"title": "What is the best book to get started with Natural language processing?",
"text": "Any good blog will do . I am looking for reading books or blogs not watching videos",
"date": "2018-08-07"
},
{
"vote": 1,
"title": "Language Interpretation in Delhi",
"text": "[removed]",
"date": "2018-08-07"
},
{
"vote": 7,
"title": "Fuzzy TFIDF/Vectoriser",
"text": "I am currently vectorising messy text strings using TFIDF and then a classifier, but my model isn't handling spelling errors very well. \n\n\nIs there a nice way to use fuzzy matching with TFDIF to capture strong features, or am I going to have to come up with some sort of spellchecking preprocess?",
"date": "2018-08-06"
},
{
"vote": 3,
"title": "A parallel corpus for English-Azerbaijani and Azerbaijani-English translation tasks.",
"text": null,
"date": "2018-08-06"
},
{
"vote": 5,
"title": "To check if a string of words is a sentence",
"text": "I have a text file from which I have to eliminate all the statements which do not make any meaning or in other words, I have to check for a statement that if it is a sentence or not.\n\n\nFor example:\n\n\n1. John is a heart patient. 2. Dr. Green, Sean is the referring doctor for the patient. 3. Jacob Thomas, M.D. works at Boston hospital  4. Xray Shoulder PA, Oblique, TRUE Lateral, 18Â° FOSSA LAT LT; Status: Complete;\n\n\n\nThe sentence 1,2, ad 3 makes some meaning but sentence 4 does not make any meaning, so I want to eliminate it.\n\n\nMay I know how it could be done?",
"date": "2018-08-06"
},
{
"vote": 4,
"title": "How are review highlights in Google Play generated?",
"text": null,
"date": "2018-08-03"
},
{
"vote": 2,
"title": "FutureError in python",
"text": "I am trying to compute the accuracy of a Word2Vec model. This is my code:\n\n\nimport gensim\n\n\nvectors = gensim.models.KeyedVectors.load(\"cbow_vectors.kv\", mmap='r')\n\n\nquestions = \"questions-words.txt\"\n\n\nanalogy_scores = vectors.accuracy(questions, restrict_vocab=30000, case_insensitive=True)\n\n\nI get the following error:\n\n\nC:\\Users\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from int to np.signedinteger is deprecated. In future, it will be treated as np.int32 == np.dtype(int).type. if np.issubdtype(vec.dtype, np.int):\n\n\nand then nothing happens :( Anyone know how to fix this?",
"date": "2018-08-02"
},
{
"vote": 30,
"title": "28-part Natural Language Processing Course / Book (with Python): 16 Free Tutorials. Paid Quizzes, Assignments, Projects and Certificate.",
"text": "The course / book was created by myself (MIT alum) and 3 other experts, including another MIT alumni and a NLP instructor from Bangalore, India. We've been working on this course for more than a year, and it is constantly improving.\n\n\nThe course covers all standard NLP concepts, and includes workflows, examples and projects. Thereâ€™s also a related course on Deep NLP, which covers Deep Learning methods for NLP.Â \n\n\nThe tutorials and end-to-end examples are available for free. Hands-on projects require Pro version (ranges from $5-$9/month) which gives unrestricted access to ALL courses. Almost all the user reviews say that this is a \"real steal\" / \"no brainer\".\n\n\nLinks\n\n\n\n\nNatural Language Processing Course\n\n\nDeep Learning for NLP Course\n\n\nData Science with Python Course\n\n\nMachine Learning Course\n\n\nDeep Learning Course\n\n\n\n\nHope you all like it. Do let me know if you have any questions.\n\n\nP.S.: We collect ratings and reviews from students. The course has an average rating of 4.8/5.0",
"date": "2018-08-02"
},
{
"vote": 3,
"title": "Effective Parallel Corpus Mining using Bilingual Sentence Embeddings",
"text": null,
"date": "2018-08-02"
},
{
"vote": 5,
"title": "Paper / Resources guide for NLP prediction?",
"text": "What are some good research papers or resources recommendations on using NLP to predict consumer behavior based on texts? (Besides RNN & Sentiment Analysis.)\n\n\nApplications could be in fields like finance, marketing, journalism, sales etc.",
"date": "2018-07-31"
},
{
"vote": 5,
"title": "Natural Language Interface In Enterprise",
"text": null,
"date": "2018-07-29"
},
{
"vote": 4,
"title": "Feedback on ML/NLP tool?",
"text": "Hi, a small team and myself built a web app for easily running ML/NLP on text - primarily to help people new to AI/not ML experts run complex models. You can do classification, and we're working on getting summarization and entity tagging up too. I've included here a 30-sec video for how it works: \nhttp://tolstoy.ai/contact/\n\n\nHere's the app: \nhttps://tools.tolstoy.ai/\n\n\nYour feedback would be appreciated! Whether you like/don't like the tool, I'm around via chat to hear your thoughts.\n\n\n***If you want a sample dataset, here's one from NASA: \n50,000 documents and their classifications.\n If you run Classify --> Multilabel --> Logistic Regression, it should return ~81% accuracy - in less than a minute.\n\n\n---\n\n\nPS. If you don't want to log in via email, here's a login without email: \nhttps://tools.tolstoy.ai/?t=YXJ0Lm9wM25vMkBnbWFpbC5jb20uRGo0aVFBLnRRSEFqbWZNRUgxVk5hb1F5SlFKSjlsSzFaOA\n It'll include other people's saved models.",
"date": "2018-07-27"
},
{
"vote": 2,
"title": "[Question] Technique for extracting sections of an entity",
"text": "Suppose I have the following sentences:\n\n\n\n\nsection 7  from article. 5 of Law 6194/74\n\n\nConstitution (article 226, section 8)\n\n\narticle 195, section I, subsection \"a\", and section II of the Constitution\n\n\n\n\nI'd like to obtain the referred section:\n\n\n\n\nsection 7, which belongs to the article 5 of Law 6194/74\n\n\nsection 8, which belongs to the article 226 from the Constitution\n\n\nboth subsection \"a\", which belongs to section 1 of article 195 from the Constitution; and section II which belongs to the same article\n\n\n\n\nI don't think I can solve the problem with a rule based approach, due to the number of possible combinations. Is there a technique or tool that could help me? Any suggestion would be greatly appreciated.",
"date": "2018-07-27"
},
{
"vote": 5,
"title": "Can OpenIE do this?",
"text": "I'm not up to date on I.E.. I do know relation extraction is a branch of information extraction. \n\n\nEx. &quot;The dog wagged it&#039;s tail.&quot; -&gt; (dog, wagged, tail) \n\n\n\nDoes I.E. have standards for handling other types of extraction, like adj-noun, adv. clause?\n\n\n&quot;The big dog wagged ...&quot; -&gt; (dog, is, big)\n&quot;After eating, the dog ...&quot; -&gt; (dog, was, eating)",
"date": "2018-07-27"
},
{
"vote": 3,
"title": "[Question] What tool/algorithm for generalizing/shortening long phrases to short key phrases",
"text": "Hi,\nCurrently I have a dataset full of long food names and I want to be able to do some processing such that the names can be shortened/generalized.\n\n\nExample:\nQuick-Braised Chicken with Caramelized Fennel and Endive --changes into--> Braised Chicken\nPan-Seared Steak with Onion and Worcestershire --changes into--> Steak\n\n\nSo far I've tried using entity extraction and topic modelling but they weren't all that helpful so I was wondering if there are any off the shelf tools out there that can be used.\n\n\nAny other suggestion/advice is also appreciated!",
"date": "2018-07-27"
},
{
"vote": 3,
"title": "Looking for complex text comparison between two pdfs with or without NLP",
"text": "I am looking for python implementations to compare two PDFs where paragraphs might be jumbled or missing in one of them. There might be difference in capitals or fonts etc. What is a good way to compare two PDFs.\n\n\nI have basic understanding of diff match patch by Google.\nAre there better methods to compare text on PDFs where sections can be jumbled and/or missing in one of the PDFs that I wish to compare with the other.  Are there good NLP implementations in python for this. Or simple text compare might be enough? Also might have various different languages apart from English German etc like Chinese Korean and stuff\n\n\nAny leads with code or papers would be helpful",
"date": "2018-07-26"
},
{
"vote": 9,
"title": "Socher: \"AIâ€™s Next Great Challenge: Understanding the Nuances of Language\"",
"text": null,
"date": "2018-07-26"
},
{
"vote": 3,
"title": "Linguistics student trying to break into the NLP field looking for some guidance!",
"text": "I graduated last year with a BA in Linguistics and have since landed work at 2 Big 4 companies doing tangentially NLP related work. But recently I interviewed with facebook for an ontology position and the next step is a technical coding interview. I'm not a good coder but I'd like to be better. Most resources out there are not NLP specific but I'm looking for projects to work on that deal with script writing, ontology architecture and things that deal with opening and processing large textfiles and corpora.",
"date": "2018-07-25"
},
{
"vote": 1,
"title": "Are you interested in NLP and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",
"text": null,
"date": "2018-07-24"
},
{
"vote": 12,
"title": "Google Developers: Text Classification Guide",
"text": null,
"date": "2018-07-24"
},
{
"vote": 6,
"title": "[Question] Which keyword extraction scoring methods to use?",
"text": "Hi,\n\n\nI've implemented the RAKE algorithm and ran some tests for single document keyword extraction.\n\n\nAlthough the keyword splitting side of RAKE works relatively well, I have to say that I'm a bit disappointed by the scoring function for multi-word keywords.\n\n\nThe whitepaper advises to sum the degree of a word divided by it's frequency.\n\n\nThis seems to be fine for single-word keywords but counter intuitive for multi-word keywords.\n\n\nLonger versions of a keyword will always score higher even though it's shorter version is more frequent.\n\n\ne.g: If a text contains 15 times \"foo bar\" and 1 time \"foo bar baz bat\",  RAKE will score \"foo bar baz bat\" higher.\n\n\nAlso, order isn't taken into account while scoring multi-word keywords.\n\n\ne.g: \"foo bar baz bat\" might score high even though the text doesn't contain any variation of \"foo bar\", \"foo bar baz\", \"bar baz\", \"bar baz bat\", etc.\n\n\nOutside of introducing some bad candidates it also seems to lower keyword diversity since many selected multi-word keywords will be similar (but different, even after text normalisation has been applied) due to them containing certain identical high scoring words.\n\n\nHave better scoring algorithms been developed since?",
"date": "2018-07-23"
},
{
"vote": 1,
"title": "[QUESTION] What are the real-life uses of Natural Language Interface *outside* of chatbots?",
"text": null,
"date": "2018-07-23"
},
{
"vote": 5,
"title": "Introduction Into Semantic Modelling for Natural Language Processing",
"text": null,
"date": "2018-07-20"
},
{
"vote": 1,
"title": "Pre-processing for Word2Vec",
"text": "So it's my first time using Word2Vec and Im using a wikipedia dump with WikiCorpus to pre-process the file before training my Word2Vec model. I want to use the following pre-processing techniques:\n\n\n\n\nConvert all letters to lowercase (I think WikiCorpus does this already).\n\n\nRemove all punctuation (Done by WikiCorpus).\n\n\nConsider word pairs/phrases as a single word, for example 'Big Apple' -> 'big_apple', not 'big', 'apple'.\n\n\nConvert all digits to their word forms, so '3' -> 'three'\n\n\n\n\nAt the moment I have no idea how to do the last two. I know about num2text but not sure how to incorporate with WikiCorpus or Word2vec. Can anyone help?",
"date": "2018-07-20"
},
{
"vote": 1,
"title": "Pretrained Stanford.NLP.POSTagger Models?",
"text": "[deleted]",
"date": "2018-07-20"
},
{
"vote": 1,
"title": "\"Learning Noise-Invariant Representations for Robust Speech Recognition\"",
"text": null,
"date": "2018-07-20"
},
{
"vote": 1,
"title": "\"On the Impact of Various Types of Noise on Neural Machine Translation\" - Khayrallah and Koehn",
"text": null,
"date": "2018-07-20"
},
{
"vote": 1,
"title": "Hire Malay Translation services and win over target customers",
"text": null,
"date": "2018-07-20"
},
{
"vote": 2,
"title": "SVM vs RNN for named-entity recognition",
"text": "For some time I work on \nMindForger\n, which is a knowledge management tool. Users work with (personal) remarks, research papers/logs, how-tos, documentations etc. My goal is to make it \n\"smarter\"\n than usual apps in this domain.\n\n\nI \nuse Support Vector Machine\n to perform named-entity recognition (via \nMITIE\n/\ndlib\n). However, a default model for recognition of person/location/organization names is relatively big (~350MB) and it loads ~10s. Recognition itself is fast, but it has relatively high number of false positives.\n\n\nI'm looking for an advice based on real use of NER: Should I go with \nSVMs\n (tune model, eliminate false positives using pre/post processing, etc.) or should I rather use \nRNNs\n (recurrent neural networks @ LSTM/GRU/...)?\n\n\nI have a number of use cases for NER in MindForger. In case of common entities (person/organization/... names) big data sets are available, however, in case of other entity types I may have as less as <10k examples.\n\n\nWhat's your \nNER suggestion\n for a desktop application? I look forward a method which has better results in general (and it's worth to invest time in tuning its model) and is reasonably fast (initialization and recognition).",
"date": "2018-07-20"
},
{
"vote": 6,
"title": "Is it possible to create Question out of a given text using NLP?",
"text": "I have some text which contain facts, definitions , name etc . Is it possible using NLP to process this text and automatically create Questions based on this text ?. And If yes what kind of questions possible like just basic questions asking for one word answer or some deep question can be possible. Consider me newbie to NLP , I just used python sumy to summarize result based on TEXT-RANK algo and these summaries are quite good.",
"date": "2018-07-20"
},
{
"vote": 2,
"title": "Ways to sentiment analyse large documents",
"text": "Large Documents: 100k docs of ~5k words each, can be split into sentences too.\nObject: Positive / Negative polarity\n\n\nWhat I've tried:\n\n\n\n\nBaseline: tf-idf on 1,2,3 N-grams \n\n\nSplit docs into sentences, train word2vec window size of 10. -> Map each sentence with the label of the corresponding document that sentence originated from and then train 3 layer LSTM on top. This approach does not even come close to baseline for no reason. \n\n\nDirectly train doc2vec and pass features to an MLP, again not as good as baseline.\n\n\n\n\nLoss is binary crossentropy with embedding size of 300, and 50 both tried.\n\n\nAny thoughts? and how to go about doing sentiment analysis on documents that large.\n\n\nThanks",
"date": "2018-07-20"
},
{
"vote": 2,
"title": "I wrote an API for working with people's names.",
"text": "I got fed up seeing all the first/last name fields on various sites, because it excludes a pretty substantial group of people. So I tried to create a more universal alternative.\n\n\nhttps://www.alphanym.com/",
"date": "2018-07-19"
},
{
"vote": 5,
"title": "How to Make a Chatbot: AWS Lex Weather Bot for Slack Tutorial",
"text": null,
"date": "2018-07-19"
},
{
"vote": 2,
"title": "Doubt in a NLP task",
"text": "I am a noob in Natural Language processing. I have to implement a search engine optimization task for a e-commerce website. Let it be Amazon for example. If I want to search for an **Addidas T-shirt**, I can put any search query:\n\n\n\n\nAddidas T-shirt\n\n\n\n\nSports T-shirt\n\n\n\n\nT-shirt Addidas\n\n\n\n\nAddidas sports\n\n\n\n\n\n\nand so on.\n\n\nHow can I implement this task using NLP and LSTMs? I am sorry if I am unclear. I am a newbie to this field.",
"date": "2018-07-19"
},
{
"vote": 1,
"title": "How to train a pretrained binary file on my own corpus using gensim?",
"text": "Hey guys I have a pretrained binary file and I want to train it on my corpus.\n\n\nApproach I tried :\n\n\nI tried to extract the txt file from the bin file I had and use this as a word2vec file at time of loading and further trained it on my own corpus and saved the model but the model is performing badly for the words which are there in the pre-trained bin file (I used intersect_word2vec_format command for this.)\n\n\nHere\n is the script I used.\n\n\nWhat should be my approach for my model to perform well on words from both the pre-trained file and my corpus?",
"date": "2018-07-19"
},
{
"vote": 7,
"title": "d-lemma - a learning approach to lemmatisation",
"text": null,
"date": "2018-07-19"
},
{
"vote": 1,
"title": "Translation Company in India",
"text": null,
"date": "2018-07-18"
},
{
"vote": 1,
"title": "Newbie question : How can I tokenize words in a sentence into custom categories?",
"text": "I'm fairly new to NLP and I'm not sure how to get started with this side project I'm working on. \n\n\nThe idea is that given a user entered sentence, I'd like to mark/identify/tag certain words based on a list I provide to my tokenizer/API. Consider the following sentences.\n\n\n\"Give John an orange\"\n\n\n\"Pass the butter to Mary\"\n\n\n\"Pick up the fork\"\n\n\nI'd provide the tokenizer a bag of words like this:\n\n\nfood: \"orange\", \"banana\", \"rabbit\"\n\n\nutensils: \"plate\", \"glass\", \"fork\"\n\n\nFor a sentence sent to the tokenizer/API, I'd like it to return a list like this (assume the first sentence):\n\n\nNoun: John\n\n\nVerb: Give\n\n\nFood: Orange\n\n\nI'm aware of NLTK and after looking at some samples online, it seems that NLTK can tokenize words and classify words in a sentence as nouns, verbs, etc. However, I can't figure out if NLTK (or maybe another library) can do the custom tokenization I'm looking for.\n\n\nHow would I go about approaching the solution to this problem?\n\n\nthanks!",
"date": "2018-07-17"
},
{
"vote": 6,
"title": "Anybody Aware of a audio based emotion detection project",
"text": "like the one linked, but in English? \n\n\nhttps://github.com/MarioRuggieri/Emotion-Recognition-from-Speech\n\n\nReally, just trying to leverage that second to last layer for audio embeddings associated with speech.",
"date": "2018-07-17"
},
{
"vote": 4,
"title": "[Question] - Speech-to-Text | Punctuation for Classification",
"text": "Hello everyone!\n\n\nI'm trying to build an application which will classify some text, given a certain audio file.\n\n\nInitially, I'm not trying to develop anything from the scratch because it's a conceptual idea. The goal is very simple:\n\n\nGet audio -> convert to text -> classify\n\n\nTo convert audio and classify, I would use any API that google / azure / ibm provides.\n\n\nWhat should be a simple task became a nightmare. I can not find any API that transcribes audio into text with punctuation. Google offers a punctuation feature but it's only in english and all my audios are in another language. Since punctuation is essential to text classification, i guess i'm in trouble here.\n\n\nWhat I'm trying to do is split audio files when there's silence using a simple algorithm in python (not A.I), and send them to some API to transcribe and classify.\n\n\nAll my audio files are quite the same (a simple one person conversation with almost no external noise), and I achieved a reasonable result. The issue is that I can not get rid of the breathing noises, and to some old audios, the split algorithm needs to be tweaked to get a regular result.\n\n\nI've seen that NLP is a trend nowdays and I really could not find anything related to this. Seems like it's a simple task, but internet lacks this kind of subject. Does anybody knows any service / api which can transcribe audio with punctuation ? Or, is there any AI algorithm that can learn to split those audio files and achieve a good result?\n\n\nThanks for your help!",
"date": "2018-07-17"
},
{
"vote": 2,
"title": "Are you interested in NLP and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",
"text": null,
"date": "2018-07-16"
},
{
"vote": 3,
"title": "[Question] Document metrics in addition to sentiment?",
"text": "[deleted]",
"date": "2018-07-16"
},
{
"vote": 1,
"title": "Website Translation Services in Delhi",
"text": "[removed]",
"date": "2018-07-16"
},
{
"vote": 23,
"title": "[Question] What are some good NLP blogs?",
"text": null,
"date": "2018-07-13"
},
{
"vote": 1,
"title": "Indian languages translation company",
"text": null,
"date": "2018-07-13"
},
{
"vote": 22,
"title": "New Gensim Release 3.5.0",
"text": "Gensim just released a new version focusing mostly on better documentation. But it also brings a few improvements and bug fixes.\n\n\nNew version is available for download in their github, pip and anaconda.\n\n\nhttps://github.com/RaRe-Technologies/gensim/releases/tag/3.5.0",
"date": "2018-07-12"
},
{
"vote": 1,
"title": "Topic Modelling on documents containing code snippets and latex",
"text": "We are trying to do topic modelling and EDA on blogs from a popular programming competitions website called \ncodeforces\n. To get a better idea of how the corpus looks like, you can take a look at \nthis blog\nÂ | \nformat we get from API\nÂ (it's encoded in unicode) | \nformatted HTML\n. As you can see most of the blogs contain heavy use of markdown (which is eventually converted to HTML). I guess for better results we need to somehow parse the html tags for example all content inside <code> </code> should be replaced with just <code> .Â Â Similarly all latex can be normalized. But we felt that we are loosing a lot of valuable information by doing so.\n\n\nWe would like to know a better way of going about this. Could you please point us to the common ways of handling documents with such varied contents.",
"date": "2018-07-12"
},
{
"vote": 4,
"title": "NLP chatbot help",
"text": "I'm trying to develop a Q&A bot that takes in queries about a certain topic area and gives an answer. \nThe corpus is a section of web pages, with text pulled using beautiful soup. \n\n\nIs this project feasible for someone with mild programming experience but little NLP experience? \nI'm using python and NLTK\n\n\nI'm doing this for a school project and, basically, I'm stuck and a bit directionless. \nAny ideas/inspiration/nudges in the right direction would be very much appreciated.",
"date": "2018-07-10"
},
{
"vote": 24,
"title": "NLP's ImageNet moment has arrived",
"text": null,
"date": "2018-07-09"
},
{
"vote": 1,
"title": "How can I generate popular or probable sentences from seeds?",
"text": "[deleted]",
"date": "2018-07-07"
},
{
"vote": 0,
"title": "Keyword Extraction",
"text": "I have a list of words of all ngrams where n ranges from 1 to 8. I have to find which words appear alone most of the time and which words appear together most frequently. How do I approach and what do I do to solve this problem?\n\n\nPS- Please help, I've got to complete this as soon as possible.",
"date": "2018-07-06"
},
{
"vote": 1,
"title": "WhatsApp Chatbot",
"text": "[removed]",
"date": "2018-07-05"
},
{
"vote": 3,
"title": "Remove date and time from text",
"text": "Hello,\n\n\nI'm looking for a package that'll help me remove data and time(both written in various formats) mentioned in the text.\nAny suggestions?",
"date": "2018-07-04"
},
{
"vote": 4,
"title": "Best Research Journal for article/paper publication.",
"text": "Which journal specifically caters to Natural Language Processing papers? I have built a solution which dynamically creates questions and answers from given text. I have tweaked the model little bit.\n\n\nI used pytorch and SQUaD dataset.\n\n\nI am still undergrad. \n\n\nThanking in anticipation of positive answers.",
"date": "2018-07-03"
},
{
"vote": 1,
"title": "Help - Target of Sentiment with Multiple Entities",
"text": "Hey, everyone! I'm a little stuck with a current analysis and would appreciate some guidance!\n\n\nI've been given a dictionary of 1k toys/electronic products, and I'm trying to identify the subject/target of the reviews and return a sentiment score. The score isn't giving me any trouble, but i'm not sure how to handle references to multiple named entities in a single record (I.e a review may say, \"the GameCube has a better game library than Xbox, but the graphics are nothing to write home about.\" )\n\n\nWith my dictionary, I can return a vector with all the named items from my dictionary (Xbox, gamecube), so i'm thinking some sort of decision tree logic should be able to use the part of speech and grammar rules to identify which is the subject of the previous segment. If that's sound, are there any good starting points for this? Do I need a large training dataset? I've previously only used bag-of-words analysis, so I want to make sure I'm not missing anything.\n\n\nAnother approach would be to split that records into two weight-reduced records to capture sentiment for each item (seems more tedious than its worth). \n\n\nEdit: grammar",
"date": "2018-07-03"
},
{
"vote": 2,
"title": "Splitting of multiple merged PDF documents",
"text": "PROBLEM\n: I have a few PDF documents(few 100's), each of these documents are essentially created by merging multiple smaller PDF files. \n\n\n\n\nHow do I split each of these PDF's into their original files? \n\n\nplease suggest any approaches/Ideas/libraries to do this\n\n\nIs there any CV/ML/NLP approaches to do this?\n\n\n\n\nso far I've been thinking of extracting the text (and page numbers, if present) from the PDF's and do some kind of clustering, assuming each of these smaller PDF's get clustered together according to their respective content, but obviously this wont work, most likely.\n\n\nnote:\n \n\n\n\n\nI do not have the original smaller pdf's\n\n\nMost of these smaller pdf's are scanned pdf's and  their quality could be bad/low resolution and as such I intend to use something like\n \ntesseract\n \nto do OCR and extract the text.",
"date": "2018-07-03"
},
{
"vote": 10,
"title": "Writing a reasearch paper on NLP",
"text": "I'm a computer science undergrad  and having worked on NLP in this summer internship, I'm interested in writing a research paper on NLP. How should I start? And what are the topics that I should look forward to for research purpose",
"date": "2018-07-03"
},
{
"vote": 4,
"title": "Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems",
"text": null,
"date": "2018-07-02"
},
{
"vote": 1,
"title": "Anyone heard of Arria?",
"text": "Hey\n\n\nMy company wants to use an online tool called Arria to write reports, however, I played around with the demo which didn't seem to be that impressive. \n\n\nWas just wondering if anyone has experimented with it and what they thought.",
"date": "2018-06-29"
},
{
"vote": 21,
"title": "Deep-learning-free Text and Sentence Embedding, Part 2",
"text": null,
"date": "2018-06-29"
},
{
"vote": 3,
"title": "Using NLG to generate unique facts about a given topic from heterogeneous data",
"text": "Hey,\n\n\nI am looking for information, scientific articles, platforms, etc. about fluent text generation of unique interesting facts which could be based on various data: datasets, webpages, etc. A perfect example of the technology would be the insights Google generates for world cup matches (also updating them real time during the game), which can be found by googling \"world cup\", clicking more, and scrolling down a bit on the Google webpage.\n\n\nI suppose the main difficulty is for a model to decide if the generated facts will be interesting for the people.\n\n\nAny help would be appreciated!\n\n\nThanks",
"date": "2018-06-29"
},
{
"vote": 6,
"title": "How likely is a job in NLP job after data science bootcamp if you do not have CS or Linguistics degree?",
"text": "I have a background in philosophy, but i specialized in philosophy of language and linguistics (particularly formal semantics and pragmatics). I recently completed my masters and took three graduate courses in computational linguistics, two of the courses focused on NLP (speech recognition and machine translation and one in corpus analysis). I am very interested in developing my interest in NLP into a carrier, and my barriers have always felt on the math and computer science side and less on the linguistics side. However, I was recently accepted into NYC Data Science Academy's full-time bootcamp. I would like to apply for NLP jobs or at least data science jobs that deal with NLP systems after completion. However, I do not know how realistic that is, given I have taken classes in NLP, have a lot of training in linguistics, and would have a data science foundation, but not actually a degree in computational linguistics or computer science. Any pointers on what to do over the next 6 months to help make this a more likely prospect. Also any materials that will give me a better idea of what the NLP job market looks like and what i can expect given my experience would be greatly appreciated.",
"date": "2018-06-27"
},
{
"vote": 2,
"title": "Grammar correction",
"text": "[deleted]",
"date": "2018-06-27"
},
{
"vote": 2,
"title": "Help with classifier for subject by context",
"text": "[deleted]",
"date": "2018-06-26"
},
{
"vote": 2,
"title": "In-Context LQA",
"text": "Learn how to radically improve your Linguistic QA for agile software. Make your reviews and changes in-context without screenshots or hunting for files. This webinar is free to all registrants: \nhttps://lingoport.com/incontext-linguistic-qa-webinar/\n \n\n\nI hope you all would find value in such a tool.",
"date": "2018-06-25"
},
{
"vote": 2,
"title": "Anyone aware of an English word-word pointwise mutual information dataset?",
"text": "I need one for a project, and I could construct one on my own, but it'd save me time if I can use a pre-compiled one!\n\n\nPreferably, this would be built from as-large-and-diverse a corpus as possible.",
"date": "2018-06-25"
},
{
"vote": 2,
"title": "Is the core of NLP libraries like NLTK and SpaCy based on advanced methods of string manipulation?",
"text": "I was wondering if NLTK and SpaCy all are just based on advanced way of dealing with strings?",
"date": "2018-06-25"
},
{
"vote": 3,
"title": "Registering a transcript and its summary",
"text": "I have a bullet point summary and the transcript of a conversation between two person. Any idea on how find out which set of lines in the transcript that corresponds to a point in the summary?\n\n\nIdea 1: Convert the point in the summary to a 'question' and use comprehension models. \n\n\nIdea 2: Find matching key words in the transcript and use co-reference resolution and word matching to find other lines in the transcript to find all the relevant conversations.\n\n\nCons: Both comprehension and co-reference models/data sets do not work well in dialog transcripts.\n\n\nAny new ideas or comments on my approaches?",
"date": "2018-06-24"
},
{
"vote": 3,
"title": "Tf-idf and only cosine similarity ?",
"text": "Hi everyone, \n\n\nI'm currently looking into text similarity techniques for my research, explained in this \npost\n.\n\n\nIn order to match two texts, my current set up borrows from this 3 part \nblog post\n. \n\n\nA â€œterm frequency-inverse document frequencyâ€ (tf-idf) matrix is obtained and cosine similarity is used to calculate the similarity between different documents. \n\n\nFor those who know, my question is whether tf-idf and cosine similarity go hand in hand? Or whether any other similarity techniques could be applied once the tf-idf matrix is obtained.\n\n\nEssentially, for the sake of my paper, should I treat tf-idf and cosine similarity as one and the same, contained within the same chapter? \n\n\nFollowing which, I can look separately into other similarity measures seperately, Levenshtein distance, etc. Thanks a lot!",
"date": "2018-06-23"
},
{
"vote": 2,
"title": "What would be the approach to know if a content is associated with its title?",
"text": "Like Quora question and answer, news headline/title and article, recipe and steps. Assume there is no training data for a Machine/Deep Learning model. May use NLP.",
"date": "2018-06-21"
},
{
"vote": 2,
"title": "NLP Yelp tourist classifier dataset problems [Help wanted]",
"text": "Hey all,\n\n\nI took a NLP class a few years back and I have decided to optimize correct some mistakes in a text classifier I made as a project. The objective function is to predict whether a Yelp restaurant review was written by a tourist. I constructed a labeled corpus out of a  the data from Yelp's dataset challenge. However I think there is some problems with my dataset.\n\n\nThe idea was to create a unigram language model trained with naive bayes. Right now if I only use features from the unigram model I can get a score of 62&#37;. Also, training the model with \nsklearn.linear_model.LogisticRegression()\n, which is a classifier, I currently get a higher score of 67&#37;. However, when I drop the unigram features and only use the metadata from the Yelp review's I get a score of .85 (with \nLogisticRegression()\n).\n\n\nI have figured out this is due to the \ncity\n feature, which is a categorical variable representing the city where the review is written. Even though my training and test set is split evenly between reviews labeled as \"tourist\" and  \"local\",  I think that my training and test data include too many reviews from cities that have a disproportionate amount of local reviews. For example, there are too many instances of reviews of businesses in cities like Paris, Texas that do not have a lot of tourist reviews. So it is really accurate when categorizing reviews from such cities, and therefore skews the score.\n\n\nMy idea is to construct a new training and test set that only include cities from the top 25 to 50 travel destinations in the US, which i think will eliminate this problem. Is this the right course of action? This is hard to do using Yelp's dataset because it is tries to represent over a thousand cities. So I am open to any ideas on how to fix this problem. Any insight would be greatly appreciated. References to educational material would also be appreciated (I am sure I could have articulated this all better)!",
"date": "2018-06-20"
},
{
"vote": 0,
"title": "Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems",
"text": "[deleted]",
"date": "2018-06-20"
},
{
"vote": 3,
"title": "What datasets are used to train category prediction models like these?",
"text": null,
"date": "2018-06-19"
},
{
"vote": 1,
"title": "Amazon Launches Alexa For Hotels, Partners With Marriott For Launch",
"text": null,
"date": "2018-06-19"
},
{
"vote": 2,
"title": "How to make multilingual glossary?",
"text": "Hi.I would like to create a mutilanguage dictionary of terms of my expertise area so I can publish it online.How to start? Is there some specialized software to do this? I own a Mac. Thanks in advance for any replies.",
"date": "2018-06-19"
},
{
"vote": 4,
"title": "How do i extract custom entities(text) with less training data",
"text": "Hey,\nI need to extract cusotm entities from the user quires.\nex: extracting custom text (like, projects, leave types, reasons).\nshow me all short term projects.\nshow me all sick leave requests\nlist of user who took work from home\n\n\nI have tried with spacy but it's not working as i have very less data and not clear defined key words to extract each.\n\n\nAny thoughts around this.",
"date": "2018-06-18"
},
{
"vote": 1,
"title": "Language Interpretation in Delhi",
"text": "[removed]",
"date": "2018-06-18"
},
{
"vote": 4,
"title": "Detecting sentence Fragmentation",
"text": "[deleted]",
"date": "2018-06-18"
},
{
"vote": 3,
"title": "Are you interested in AI and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",
"text": null,
"date": "2018-06-18"
},
{
"vote": 1,
"title": "part148 Learn \"SHALL NOT BE + ADJ\" in filipino Sentence",
"text": null,
"date": "2018-06-17"
},
{
"vote": 1,
"title": "Curating financial and business news using NLP to benefit the masses | Amazon Web Services",
"text": null,
"date": "2018-06-15"
},
{
"vote": 2,
"title": "Thinking notebook ideas",
"text": "For some time I work on a \nnote taking app\n. My goal is to make it \n\"smarter\"\n than usual apps in this domain.\n\n\nIts name is \nMindForger\n and it is inspired by \nhuman mind\n. I explore concepts like \nlearning , recalling, thinking/dreaming, associations\n and \nforgetting\n. For instance I use (basic) NLP methods to provide associations as you read or write the notes (~ recommender system). In general I aim to extract knowledge from my notes (created over years) to make them more useful. \n\n\nI just released an initial imperfect implementation which is a foundation for much more interesting features to come.  \n\n\nI would be very interested in any \nideas, suggestions\n and \nconstructive critics\n related to \"thinking notebook\" concept in general and/or MindForger.",
"date": "2018-06-14"
},
{
"vote": 1,
"title": "Thinking notebook ideas",
"text": "[deleted]",
"date": "2018-06-14"
},
{
"vote": 1,
"title": "The Basic Principles of Language",
"text": null,
"date": "2018-06-13"
},
{
"vote": 14,
"title": "Overview and benchmark of traditional and deep learning models in text classification",
"text": null,
"date": "2018-06-13"
},
{
"vote": 1,
"title": "[D] Authors from 2 ICML-accepted papers agreed for a Q &amp; A on a new QnA paper platform, \"Learning Longer-term Dependencies in RNNs with Auxiliary Losses\" and \"Towards Fast Computation of Certified Robustness for ReLU Networks\"",
"text": null,
"date": "2018-06-13"
},
{
"vote": 8,
"title": "Best papers of ACL 2018",
"text": null,
"date": "2018-06-12"
},
{
"vote": 1,
"title": "The Difference Between an Interpreter and a Translator",
"text": "[removed]",
"date": "2018-06-12"
},
{
"vote": 1,
"title": "Translation Services in India",
"text": "[removed]",
"date": "2018-06-12"
},
{
"vote": 31,
"title": "ðŸš€100 Times Faster NLP in Python using spaCy's internals and a bit of Cython",
"text": null,
"date": "2018-06-12"
},
{
"vote": 19,
"title": "The Best 25 Datasets for Natural Language Processing",
"text": null,
"date": "2018-06-12"
},
{
"vote": 13,
"title": "How hard is to get paper accepted in Computational Linguistics Journal(ACL/MIT Press)?",
"text": null,
"date": "2018-06-10"
},
{
"vote": 2,
"title": "Help me find a evaluation metric.",
"text": "Hi! I am currently working on an open domain relation extractor. I am stuck at the phase where I evaluate how good it is. Can you help me how should I evaluate my relation extractor? (Any suggestions except the SLOT FILLING CHALLENGE)",
"date": "2018-06-09"
},
{
"vote": 2,
"title": "Finding Undisclosed Relationships in Large Text Documents.",
"text": "Hi, I am undertaking a project that aims to construct relationships between concepts in different pieces of text. I am currently in the planning stage so am wondering if you can provide me with different concepts that I should look into to hopefully improve this project.\n\n\nI am currently becoming more familiar with Christopher Mannings work on information retrieval with the goal of understanding how actually taking the useful data from the document but the actual relationship forming is something I am struggling to get my head around so if anyone can point me in the right direction that would be great.",
"date": "2018-06-09"
},
{
"vote": 3,
"title": "Repositories for Sanskrit/Sumerian words",
"text": "I am interested in developing a natural language generation system for old languages, and I am looking for lists of Sanskrit and Sumerian words. I was wondering if there exists a readily available comprehensive list(s) of Sanskrit or Sumerian on the Internet, perhaps similar to the /usr/share/dict/words that exists on Unix computers.",
"date": "2018-06-08"
},
{
"vote": 2,
"title": "Science news sensationalism survey | UofW | (x-post)",
"text": null,
"date": "2018-06-07"
},
{
"vote": 0,
"title": "Research Paper on Text Summarization using Statistical and Semantic Models in a mix, Reviews and critical remarks are highly appreciated.",
"text": "[deleted]",
"date": "2018-06-05"
},
{
"vote": 2,
"title": "Asking and remembering chatbot in the wild?",
"text": "Is there a chatbot that is able to retrieve information of it's user by \n\n\nasking,\nlogic,\n\n\nand are able to give this information back if asked?",
"date": "2018-06-03"
},
{
"vote": 5,
"title": "NER on Legal documents",
"text": "[deleted]",
"date": "2018-06-03"
},
{
"vote": 6,
"title": "Rewrite sentences in 3rd person?",
"text": "Given short sentences like: \n\"I'm loving it!\" \nIf the quest is to rewrite this in 3rd person, e.g. John is loving it. How should one go about accomplishing this? Are there tools out there that can do this out of the box? \n\n\nConsider more complex cases too: \n\"It'll be great if you can make me coffee in the morning\", with \"me\" being Jane, and \"you\" being John, and get the result: \nJane would like it if John made her coffee in the morning. \n\n\nAny pointers / general direction appreciated.",
"date": "2018-06-02"
},
{
"vote": 1,
"title": "Confused on use Minimum Edit Distance for a Spell Correction System.",
"text": "Hey, I've been creating a Spell correction system (not for English, but for Sinhala language). I've gathered a corpus. What I'm thinking about doing is suggest the most suitable word using Permutation Generation Method and Minimum Edit Distance method. I have an understanding of how to do the select suitable words with Permutation Generation, but I don't have a clear idea on how to use Minimum Edit Distance.  \n\n\nI know what Minimum Edit Distance Means and how to do that ( I can build an algorithm ) . What I'm trying to understand is how can I select words which should be considered for this.  \n\n\neg : If the wrongly identified word is \nABCDE\n and suggestions for MED are \nABCD , ABCDEF, ABDE\n (assuming these three are the lowest MED ) how can I select these correct three words ( which can be \nABCD\n \nor ABC\nDEF \nor A\nBDE ) which should be considered for MED . I have a dataset for million of words and 6000+ unique words. So, I'm confused how can I select the words with Minimum Edit Distance for the given (wrongly spelled) word ? Should I consider calculating MED for the whole dataset (corpus) ?  \n\n\nAny help would be appreciated. Thanks",
"date": "2018-06-02"
},
{
"vote": 1,
"title": "How to detect tweets regarding a specific topic?",
"text": "Hi,\n\n\nI'm new to this subreddit (and new to Comp. Ling. as well.) I was working on a project and I was wondering what topics and general areas I would have to understand to develop some sort of algorithm to detect tweets that were sympathetic to a tragic event (e.g. acts of terrorism, etc.) I read up about some stuff with sentiment analysis, but I'm pretty lost when it comes to machine learning and AI since there's so much and I don't really know where to start.\n\n\nThanks!",
"date": "2018-06-01"
},
{
"vote": 1,
"title": "How to measure variance in natural language datasets?",
"text": "When comparing models for bias-variance tradeoff we often want to tune according to a use case. However, how to we actually define variance when it comes to free text?\n\n\nOne of the ways I'm thinking is measure N-gram distribution but then again if I were to use Deep Learning methods, this won't be helpful as we would want to use information from much farther in the text than just tri-quadgrams.\n\n\nAny suggestions would be helpful?\n\n\nThank You !!",
"date": "2018-06-01"
},
{
"vote": 3,
"title": "Js sentiment analysis without node?",
"text": "Hi,\nI'm desperately looking for a way to Analyse the sentiment of short German texts containing emoji. I'm trying to do this client side in the browser, so node modules won't work.\nI'm happy about any pointers.",
"date": "2018-05-31"
},
{
"vote": 5,
"title": "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces",
"text": null,
"date": "2018-05-31"
},
{
"vote": 8,
"title": "Alternative to one-hot encoding for output to a model when vocabulary size is very large",
"text": "I was following \nthis blog\n. In it he talks about how to build a language model in keras. He shows how to build a simple model in keras.\n\n\n> After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.\n\n\n>This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n\n\n>Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n\n\nHe uses the following:\n\n\ny = to_categorical(y, num_classes=vocab_size)\n\n\nIn his case, the vocabulary size is manageable. I am working with vocabulary having size > 100 million. I guess I should not use a one-hot encoding for the output \ny\n as done by him. Is there any alternative?",
"date": "2018-05-31"
},
{
"vote": 30,
"title": "Magnitude: a python package for quickly loading/using vector embeddings",
"text": null,
"date": "2018-05-30"
},
{
"vote": 6,
"title": "Master of NLP, but know nothing about it",
"text": "[deleted]",
"date": "2018-05-29"
},
{
"vote": 6,
"title": "Word frequency Vocabulary",
"text": "Hi, i'm creating spell checker using ngrams approach, but i have a problem with Word frequency Vocab. \n\n\nI found only \"big.txt\" file from \nhttps://norvig.com/spell-correct.html\n  where I can draw that vocab which have 40,000 tokens, but it is not enough for me. Do you know where can i find some a big Word frequency vocabulary?\n\n\nRegards",
"date": "2018-05-27"
},
{
"vote": 10,
"title": "On-line study group for cs224n",
"text": "Hello everyone,\n\n\nI am starting with the archived version of cs224n (taught in year 2017). Anyone out there willing to form a study group to facilitate discussion.",
"date": "2018-05-26"
},
{
"vote": 6,
"title": "Question answering system for *multiple* possible answers",
"text": "[deleted]",
"date": "2018-05-26"
},
{
"vote": 1,
"title": "Bhasha Bharati Arts Translation Services - Launch of Language Studio S...",
"text": null,
"date": "2018-05-25"
},
{
"vote": 1,
"title": "Question answering on unstructured documents",
"text": "[deleted]",
"date": "2018-05-24"
},
{
"vote": 8,
"title": "Please help me understand word embeddings",
"text": "My basic question is how much of the embedding matrix is saved when a model is deployed? \n\n\nSo let's say I train an LSTM model to assign a respondent a score between 1 and 5 on an essay from a training set of 500 responses. Now my understanding of the way word embeddings work is that if the word is within the embedding matrix it utilizes the tensor from said word. If it is not present it is assigned as unknown. This makes sense when training is happening. \n\n\nBut once you move into production and respondents start providing words the trained model hasn't seen my understanding is those words become unknown words and not only does the model not have parameters for these words, but it can't even leverage the word embedding matrix for similarity context. Is this accurate or is my understanding off? If it is accurate have any of you devised strategies to utilize the embedding matrix for more words that may be seen when the model is in production? One thing I was wondering is if you provided a single response with say the top 10k most frequently used words. This way the majority of words a respondent uses would almost certainly be known. You could then assign it a training value of the median, so the words don't necessarily impact the score in either a positive or negative way. But I'm sure there is a more sophisticated way to leverage all of the information from the embeddings on new responses. \n\n\nAm I way off on this? Would love some help from the experts.",
"date": "2018-05-24"
},
{
"vote": 1,
"title": "How to assign a meaningful sentence to big 5 score of a piece of text",
"text": "Suppose we have a text representing speech given by some person and the big5 score for the text. Now we need to generate some meaningful sentence that describes the text on the basis of calculated big 5 score. How can we do that, any suggestion would be appreciated. Thanks in advance",
"date": "2018-05-24"
},
{
"vote": 7,
"title": "Twitter is treating Bulgarians tweeting in Cyrillic like Russian bots",
"text": null,
"date": "2018-05-23"
},
{
"vote": 12,
"title": "\"Microsoft acquires Semantic Machines, advancing the state of conversational AI\"",
"text": null,
"date": "2018-05-22"
},
{
"vote": 1,
"title": "What is a good approach to map arguments?",
"text": "As described in this video lecture at minute 10:00,\n and \non Wikipedia,\n arguments can be mapped to a tree structure. What tools and methods exist today which I can use to parse text into \na complex tree like this?",
"date": "2018-05-21"
},
{
"vote": 1,
"title": "Translation services company in Noida",
"text": "[removed]",
"date": "2018-05-21"
},
{
"vote": 1,
"title": "à®Žà®©à¯à®©à®¤à¯ à®‡à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à¯",
"text": "[removed]",
"date": "2018-05-20"
},
{
"vote": 1,
"title": "Flutter : Can Be A amazing Platform for Developers",
"text": null,
"date": "2018-05-17"
},
{
"vote": 22,
"title": "Intro to neural text generation and conditional language models (Keras)",
"text": null,
"date": "2018-05-16"
},
{
"vote": 0,
"title": "Document-Summarizing Chatbots : SummarizeBot",
"text": null,
"date": "2018-05-15"
},
{
"vote": 6,
"title": "Text classification on a small dataset",
"text": "[deleted]",
"date": "2018-05-14"
},
{
"vote": 3,
"title": "Bitext renewed NLP API platform",
"text": null,
"date": "2018-05-14"
},
{
"vote": 5,
"title": "Papers combine rule based and machine learning methods in NLP",
"text": "What are some good papers that combine rule based methods and machine learning methods in NLP problems?",
"date": "2018-05-14"
},
{
"vote": 10,
"title": "Fujitsu's NLP programming Challenge, $20,000 prize",
"text": "[deleted]",
"date": "2018-05-11"
},
{
"vote": 2,
"title": "BioNLP best results",
"text": "Hi, does anyone know what the best results are for BioNLP'09 or 2011 are in the literature? any pointers would be very useful.\nThanks!",
"date": "2018-05-10"
},
{
"vote": 10,
"title": "New Google AI Can Have Real Life Conversations With Strangers",
"text": null,
"date": "2018-05-09"
},
{
"vote": 4,
"title": "South Korean company Naver's Papago defies Google Translate",
"text": "According to \nNaver\n's claim, \nPapago\n correctly translate \"Moon\" in \nEnglish\n political news article into ë¬¸ìž¬ì¸ (\nMoon Jae-in\n, \nSouth Korean\n \npresident\n's name) in \nKorean\n, but \nGoogle\n wrongly \ntranslate\n it into ë‹¬ (\ndal\n, meaning \nthe earth's natural satellite\n). Naver said it improved Papago using the \nNMT\n technology.\n\n\n\n\nRelated Article\n (Written in Korean)\n\n\n\n\nNotes: Naver is the biggest portal site in South Korea. \nPapago\n means \"parrot\" in \nEsperanto\n, but currently Naver Papago does not yet embed Esperanto.",
"date": "2018-05-09"
},
{
"vote": 5,
"title": "South Korea is trying to translate its historical records written in an old written language to its modern language using AI with NMT technogoy",
"text": "In pre-modern East Asia, many documents were written in Classical Chinese, a written language based on Old Chinese. It was Latin in the area. However, Classical Chinese is now replaced by modern local languages today. Thus, the translation of old documents are important to be understood by modern readers.\n\n\nSouth Korea has many old documents in Classical Chinese. The \nJournal of the Royal Secretariat\n is a historical record, which is too massive to be translated by humans. If all professional translators in the country try to translate this journal (without AI), it'll take 80 years.\n\n\nThe \nInstitute for the Translation of Korean Classics\n (í•œêµ­ê³ ì „ë²ˆì—­ì›, éŸ“åœ‹å¤å…¸é£œè­¯é™¢) is a state-run organization which is translating Classical Chinese documents in Korea into modern Korean language. It wants to use artificial intelligence to shorten the term of translation of the Journal of the Royal Secretariat. The process is simple: if AI yield the translated texts, professional translators check and correct them.\n\n\nOf course, the AI wasn't made yet, however it'll embed the Neural Machine Translation (NMT) technology.\n\n\n\n\nRelated article\n (written in Korean)",
"date": "2018-05-09"
},
{
"vote": 3,
"title": "HD Voice Playback with Deep Learning",
"text": null,
"date": "2018-05-08"
},
{
"vote": 12,
"title": "How to Train your Own Model with NLTK and Stanford NER Tagger? (for English, French, Germanâ€¦)",
"text": null,
"date": "2018-05-07"
},
{
"vote": 1,
"title": "Stanford CoreNLP error",
"text": "I'm using Windows 10 with JDK 10.0.1 for Windows x64 installed.\nAt Stanford CoreNLP website,\n(\nhttps://stanfordnlp.github.io/CoreNLP/download.html\n)\n\n\nI did the following\n\n\n\n\ndownloaded\n'Download CoreNLP 3.9.1' (stanford-corenlp-full-2018-02-27.zip), \n'English' (stanford-english-corenlp-2018-02-27-models.jar), and\n'English (KBP)' (stanford-english-kbp-corenlp-2018-02-27-models.jar)\n\n\nunzipped the first file and put the second and third files at the unzipped folder \n(C:\\Users\\JohnSmith\\Documents\\Stanford_CoreNLP\\stanford-corenlp-full-2018-02-27)\n\n\nadded the above folder at CLASSPATH\n\n\n\n\nAll I did below was done at the above folder where all .jar files are located.\n\n\n\n\nTrying CoreNLP server\nIn the command prompt, I entered\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\nand ran it on Chrome.\nAt the window \"Text to annotate\", I entered an English sentence \"The quick brown fox jumped over the lazy dog\" and clicked \"Submit\"\n\n\n\n\nThen the error message occurred.\nedu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl\n\n\n\n\nTrying command line interface\n\n\n\n\nI ran the following commands at the command prompt as in the official website\n\n\necho \"the quick brown fox jumped over the lazy dog\" > input.txt\njava -mx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat json -file input.txt\n\n\nI checked that 'input.txt' file has been successfully created.\nBut the second command gives me an error message\n\"Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLP\"\n\n\nI think I solved this problem by inserting \"-cp \"\n\"\" before \"-mx3g\", but now I have other error messages. \n3) Trying interactive mode\nI entered\njava -mx4g -cp \"\n\" edu.stanford.nlp.pipeline.StanfordCoreNLP\nbut got the error messages. Please see the screenshots. I think they are almost the same as I got at trying command line interface\n\n\nI guess there are common causes for these problems. Could anybody tell me what I should do?\n\n\nThanks",
"date": "2018-05-04"
},
{
"vote": 3,
"title": "Converting statements to questions.",
"text": "I have no real experience with NLP techniques so I'm mostly wondering what the state of the art is for the following problem:\n\n\nGiven some statement like \"The color of the sky is blue\". Generate one or more questions like \"What color is the sky?\", \"What has the color blue?\". Etc.",
"date": "2018-05-04"
},
{
"vote": 4,
"title": "Here's how you can create the best chatbot for your needs and budget",
"text": null,
"date": "2018-05-04"
},
{
"vote": 1,
"title": "Log Files Analysis with Natural Language Processing API",
"text": null,
"date": "2018-05-02"
},
{
"vote": 1,
"title": "Get different tenses of a word.",
"text": "Are there any Python libraries (Python 3.x) using which I can get different tenses of a word? Something other than NodeBox (which is for Python 2.x) and Pattern.",
"date": "2018-05-01"
},
{
"vote": 14,
"title": "Fountain - Natural Language Data Augmentation Tool",
"text": null,
"date": "2018-05-01"
},
{
"vote": 7,
"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
"text": null,
"date": "2018-04-27"
},
{
"vote": 2,
"title": "[p] pytorch implementation of Get To The Point: Summarization with Pointer-Generator Networks",
"text": null,
"date": "2018-04-27"
},
{
"vote": 6,
"title": "Dealing with conjunctions in semantic parsing",
"text": "[deleted]",
"date": "2018-04-27"
},
{
"vote": 14,
"title": "Get a spacy model in your native language",
"text": null,
"date": "2018-04-27"
},
{
"vote": 1,
"title": "Topic modeling on small pieces of text?",
"text": "I'm starting a project where I'm mining trivia questions.  The sample size is around 700.  I've read up quite a bit on topic modeling and LDA, but it seems like most methods are aimed at documents like news articles and books.  There's also the issue of traditional metrics like tf-idf picking out tokens based on \"this is common in this document and uncommon in others\", but with trivia questions, you can have many questions about presidents or a particular TV show.  It's also unlikely to see repeated words that aren't stopwords.\n\n\nAre there any good resources for topic modeling on small pieces of text?  I've found a few specifically on tweets, but I'm not sure how useful they'll be with only 700 data points.  I also wasn't sure if there's anything more general for small pieces of text.",
"date": "2018-04-26"
},
{
"vote": 1,
"title": "NLP resources request",
"text": "Hi all! My manager for my internship informed me that I will be developing a NLP based chatbot for employees to analyze internal mutual fund data. A sample input could be \"show me x fund compared to y fund from m date to n date (and then a cadence such as 'biyearly')\".\n\n\nI worked on this project last summer only as a proof of concept, we used Apache OpenNLP for common NLP tasks and models. The results were deemed okay for a POC, the findings were printed to a console and not too accurate, this was possibly due to our lack of understanding of how to create our own corpus to analyze our internal data. Being that I understand the basics of NLP, I am hoping to gain some guidance in choosing the appropriate resources tailored to the task I am given.",
"date": "2018-04-26"
},
{
"vote": 0,
"title": "Free books to uses",
"text": "Hi,\nI am writing a spell checker generating errors myself. However, I need free books , but I do not know how to characterize them, anyway, when I searching \"free python books\", I get python learning books. I am new to this and needs help. Can someone recommend a source to me?\nRegards :)",
"date": "2018-04-25"
},
{
"vote": 5,
"title": "Learning Semantic Textual Similarity from Conversations",
"text": null,
"date": "2018-04-25"
},
{
"vote": 1,
"title": "[D] Anyone having trouble reading a particular paper ? Post it here and we'll help figure out any parts you are stuck on | Anyone having trouble finding papers on a particular concept ? Post it here and we'll help you find papers on that topic [ROUND 2]",
"text": null,
"date": "2018-04-24"
},
{
"vote": 8,
"title": "ngram graph NLP library in python. Request for feedback",
"text": "So I wrote this library : \nhttps://pypi.org/project/ngram-graphs/\n based on this paper\n\n\n> Giannakopoulos, George & Karkaletsis, Vangelis. (2009). N-gram graphs: Representing documents and document sets in summary system evaluation. \n\n\nI am requesting feedback, making sure that I understood everything from the paper and that the implementation seems solid enough.\n\n\nThe library provides igraph based graph creation from text documents, using the update function from the paper to create a model of a set of graphs as well as means of comparison between graphs using the proposed functions from the paper\n\n\nCode can be found here : \nhttps://github.com/loginn/ngrams_graphs\n\n\nThanks",
"date": "2018-04-24"
},
{
"vote": 5,
"title": "Noob Question: How does TextSummarizer API (http://textsummarization.net/text-summarizer) work?",
"text": "http://textsummarization.net/text-summarizer\n\nThe API works great for any domain text. I am trying to implement something locally. \n\n\nI read few papers like,\nUnsupervised Text Summarization Using Sentence Embeddings(\nhttps://www.cs.utexas.edu/~asaran/reports/summarization.pdf\n) . This explains some unsupervised techniques using sentence vectors to summarize text. I want to know, if there are other ways to implement such API.",
"date": "2018-04-23"
},
{
"vote": 1,
"title": "[6 hours left at this price! - 5.0 stars course] \"Hands On Natural Language Processing (NLP) using Python\" Course for FREE",
"text": null,
"date": "2018-04-22"
},
{
"vote": 1,
"title": "Text Rank Update order",
"text": "I was implementing \nText Rank\n and I had a question about when to update parameters.\n\n\nBasically I have the equation on page two in two different ways, they both end up with pretty similar results but they are slightly different\n\n\nfor vertex in vertices:\n    update = 0\n    for v_j in vertex.in:\n        edge = v_j.find(vertex)\n        update += edge.weight / norm * WS[v_j]\n    WS[v_j] = (1 - d) + d * update\n\n\n\nAnd the second is\n\n\nupdates = []\nfor vertex in vertices:\n    update = 0\n    for v_j in vertex.in:\n        edge = v_j.find(vertex)\n        update += edge.weight / norm * WS[v_j]\n    updates.append(update)\nfor i in range(len(vetrices)):\n    WS[i] = (1 - d) + d * updates[i]\n\n\n\nThe details aren't too important, the main different is that the first way updates the parameters right away so that a vertex visited later is computed with the new scores of earlier while the second one computes the values for all vertices then updates.\n\n\nDoes anyone know which way is it supposed to be implemented?",
"date": "2018-04-22"
},
{
"vote": 7,
"title": "rake-nltk 1.0.3 released. Comes with the flexibility to choose metric for ranking algorithm.",
"text": null,
"date": "2018-04-21"
},
{
"vote": 16,
"title": "Predicting Sports Outcomes Using Python and Machine Learning",
"text": null,
"date": "2018-04-21"
},
{
"vote": 10,
"title": "How to mine newsfeed data and extract interactive insights in Python",
"text": null,
"date": "2018-04-20"
},
{
"vote": 3,
"title": "A python package to parse time expressions from natural language",
"text": "A pure python package to parse time expressions from natural language, a mixture of a rule and regular expression based system + some probabilistic modelling\n\n\nhttps://github.com/comtravo/ctparse",
"date": "2018-04-19"
},
{
"vote": 1,
"title": "A pure python package to parse time expressions from natural language, a mixture of a rule and regular expression based system + some probabilistic modelling.",
"text": "[deleted]",
"date": "2018-04-19"
},
{
"vote": 10,
"title": "Crash course in NLP?",
"text": "[removed]",
"date": "2018-04-19"
},
{
"vote": 2,
"title": "State of the art for NL to logic?",
"text": "What is the current state-of-the-art for techniques of converting natural language into some kind of formal logic? In particular, I'm trying to figure out whether there's any particularly significant tech out there that DOESN'T use machine learning to learn from pairs of NL:logic (I don't expect the domain that I'm working in to really have enough pairs of examples to be able to do this kind of learning easily).\n\n\nA lot of NLP software out there (eg. Stanford parser) provides functionality to generate dependency parses - does anything leverage these kinds of dependency grammars to convert NL into logic? Is this kind of conversion considered trivial (though perhaps tedious to write all the rules)?",
"date": "2018-04-18"
},
{
"vote": 9,
"title": "Text Embedding Models Contain Bias. Here's Why That Matters.",
"text": null,
"date": "2018-04-17"
},
{
"vote": 9,
"title": "Does anyone have an opinion on SpaCy's \"noun chunk\" vs gensim's Phraser?",
"text": "I'm trying to build a general pipeline for processing given text and I want to deal with common bigrams. In the past, I found gensims Phraser object better for my use case but given that I'm not \nsuper\n experienced and also that the new SpaCy version is pretty packed with new features, I wanted to see if anyone had any opinions on the matter. \n\n\nthanks",
"date": "2018-04-16"
},
{
"vote": 4,
"title": "Web App Demo for text summarization and Named-entity recognition",
"text": null,
"date": "2018-04-15"
},
{
"vote": 8,
"title": "pretrained Doc2vec on clinical text",
"text": "Hi,\n\n\nAre there any pretrained doc2vecs on clinical datasets such as Pubmed or Merc along with some extra dataset merged in? Perhaps a doc2vec on top bio.nlplab of \nhttp://bio.nlplab.org/\n ?",
"date": "2018-04-15"
},
{
"vote": 2,
"title": "Word sense classification in Japanese with Python",
"text": "Iâ€™m wondering if anyone knows of a convenient tool in Python for getting word senses in Japanese.\n\n\nIâ€™m doing a cross-lingual Parsing task between Japanese and Korean, which requires mapping between particles.  For example ã® > ì˜, ãŒ > ê°€/ì´, ãŒ > ì¹˜ë§Œ, etc.  The last two demonstrate the problem; the particle ãŒ can be either a subject marker or a conjunctive particle meaning â€œbutâ€, and so would map to two different Korean particles,\n\n\nI need to delexicalise my Korean and Japanese conllu files \nexcept\n for the particles, which I need to map to a concatenation of the Japanese and Korean equivalents (e.g. ãŒ/ê°€/ì´).  So Iâ€™m looking for a word sense classifier to make sure that these get mapped correctly (i.e. â€œif the sense is \nsubject marking particle\n, then transform to ãŒ/ê°€/ì´).",
"date": "2018-04-14"
},
{
"vote": 2,
"title": "Finding mathematical formulas in spoken text",
"text": "I'm trying to find mathematical formulas given a word for word transcript of a lecture. \n\n\nFor example: The input \"This can be written as f of x equals x squared when using a quadratic model\" to my program should return \"f of x equals x squared\".\n\n\nI don't have any experience with NLP. \nHow should I go about doing this?",
"date": "2018-04-12"
},
{
"vote": 8,
"title": "What are good ways to filter out noise when using TF-IDF for keyword selection in documents?",
"text": "I have some applications that have a large number of documents, and I've stored the document frequencies in a database. I have dozens of regular expression filtering rules for candidates and stopword lists, but there are still quite a lot of phrases that end up in the database that do not really provide much useful information about a document (maybe if I was using a semi-supervised clustering algorithm, but that's not my goal here).\n\n\nIn general, I've put a \"lower limit\" on document frequency in order to filter out phrases that only appear in a small fraction of the documents.\n\n\nI should note that my \"keywords\" are 1-grams, 2-grams, and 3-grams.\n\n\nI've also put limits on the effect of term-frequency in a document (e.g., capped it at a certain value, square-root transformation, a combination of both of those), although that is more for using the now-modified TF-IDF value as a feature in classification.\n\n\nAre there any general or specific approaches any of you has taken that reduce noise from this algorithm?",
"date": "2018-04-12"
},
{
"vote": 1,
"title": "Speed up your Python with C: a guide to Ctypes, Cython and CFFI",
"text": null,
"date": "2018-04-12"
},
{
"vote": 1,
"title": "Natural Language Processing Using Python or NodeJS",
"text": null,
"date": "2018-04-12"
},
{
"vote": 1,
"title": "How We're Using Natural Language Generation to Scale at Forge.AI",
"text": "[deleted]",
"date": "2018-04-11"
},
{
"vote": 12,
"title": "Is this a toxic comment? - Notes from a NLP focused Kaggle competition",
"text": null,
"date": "2018-04-11"
},
{
"vote": 3,
"title": "twitter data analysis?",
"text": "I am involved in twitter analysis data. I want to find trending topics in tweets  with some hashtags, like #finance or #technology. I have a hugh data set of tweets and now I need to analyze them. Are there common techniques or libraries in tweets analysis?",
"date": "2018-04-10"
},
{
"vote": 14,
"title": "For what tasks would you use a LSTM over a CNN in language classification?",
"text": "Edit: I mean sentence classification. Sorry\n\n\nI think I understand what these structures do, but I don't really understand how they correspond to tasks. When would you choose an LSTM over a CNN? Which would be better, e.g., for sentiment classification, or classifying the 20 newsgroups dataset? (document classification on topics)",
"date": "2018-04-10"
},
{
"vote": 24,
"title": "Supporting rapid prototyping for research, I am LAUNCHING PyTorch-NLP, a deep learning NLP toolkit!",
"text": null,
"date": "2018-04-09"
},
{
"vote": 1,
"title": "10,000 topics with each topic's key words and phrases",
"text": null,
"date": "2018-04-08"
},
{
"vote": 4,
"title": "What's a good API where I can send it 2 random entities and it returns how related the two topics are on some objective scale?",
"text": "Wasn't sure where to ask but figured you guys might know. Edit: my -particular use case is predicting if someone has some knowledge on a different topic if they prove they have knowledge on related topic, and I was gonna use relatedness as a feature in the algo.",
"date": "2018-04-08"
},
{
"vote": 20,
"title": "Hiring Two NLP Data Science Positions at Wells Fargo",
"text": "Hi,\n\n\nI'm hiring two data scientists at Wells Fargo (jobs just posted) and I think a lot of people that follow this subreddit would make good candidates.\n\n\nI'm in a specialized NLP / Machine Learning group at Wells Fargo where we are looking to build out our capabilities around things like text classification, sentiment analysis, topic modeling and other NLP use cases. There is a huge amount of untapped data that is in the form of documents or other natural language formats in financial services, and it's the job of the group I'm in to generate insights and value from that data.\n\n\nWe work in Python and PySpark primarily, and tend to make heavy use of word embeddings. Past projects have included things like automatic complaint detection in customer communications using word embeddings and an ensemble of machine learning models, as well as a deep autoencoder to identify anomalous complaints.\n\n\nIf you are interested, you can look at the job descriptions at \nWellsFargo.com/careers\n and search for job opening IDs 5398649 (for the senior position) or 5398760 (for a mid-level data scientist position). Both jobs are open for any U.S. location (including remote / work from home).\n\n\nIdeal candidates would be strong Python/PySpark coders who have an NLP / ML background and big data experience. The best thing to do if you're interested is just to apply. But, if anyone wants to reach out to me directly with questions, LinkedIn is probably the best way. Thanks! \nhttps://www.linkedin.com/in/nathan-susanj-35856824/",
"date": "2018-04-06"
},
{
"vote": 4,
"title": "Generating glove vectors with lstm (seq2seq)?",
"text": null,
"date": "2018-04-06"
},
{
"vote": 5,
"title": "Input/Output encoding of RNN/LSTM",
"text": "Hello,\n\n\nI want to build a LSTM model to predict an answer given a question.\nI have a training set more or less in that form:\n\n\n\"What is your name?\" -> \"John\"\n\n\n\"What is your e-mail address?\" -> \"\njohn@gmail.com\n\"\n\n\n...\n\n\nSo I was seeing two different approaches to handle this problem:\n\n\n\n\nUsing a word model based, with word embedding like pretrained word2vec from Google News corpus (vectors of size 300).\n\n\nUsing a char model based, the number of different characters would not exceed 50.\n\n\n\n\nThe problem with word embedding I think, is that if one of my output is not contained in the word2vec vocabulary (like '\njohn@gmail.com\n'), the network would not be able to generate it.\nTo handle this, maybe I could consider to retrain my own word embedding on the top of the word2vec pre-trained vectors (not sure how to do it, I think need to retrain the whole corpus (old+new words)).\nAnother option would be to have in input 300 neurons for the word2vec vectors and in output a one-hot encoding for my output vocabulary. But is it a good practice to have large one hot encoding output (in case the vocabulary is quite large)?\n\n\nThe char approach would be more computationally expensive because of more hidden neurons to let the LSTM understand relations betweens chars. I guess it would be longer to train then. I don't know if it could be able to produce good output without missing some characters.\nWhat I like from the word based approach is the ability for the network to understand similarity between words with word2vec, so I guess that it would generalize better to slightly different questions.\n\n\nWhat approach would you consider to be better? Any ideas or help is welcome! :D\n\n\nThanks",
"date": "2018-04-06"
},
{
"vote": 1,
"title": "RNN-CharCNN for sequence tagging: convolution best practices?",
"text": "Hi all,\nI've got an LSTM sequence tagger - say, for NER in BIO-tag format, but it might be used for different purposes in the future.\nI'm extending it with a Char-CNN for better OOVs processing and better overall generalization to the new data. My approach is to concatenate the Char-CNN and LSTM outputs after each token and pass them into an MLP for the final prediction.\nBut here's the question: what's the best practice in passing the input to the CharCNN: is it the entire string up to the current token, or a fixed-length window? I currently use the former, but it's dead slow in training compared to just an LSTM. Also, is it necessary to use character embeddings? Could you please share your experience?",
"date": "2018-04-05"
},
{
"vote": 1,
"title": "XiaoIce, Microsoft's AI-powered social chatbot in China, makes breakthrough in \"full duplex\" conversationâ€”like talking to a friend on the phone",
"text": null,
"date": "2018-04-04"
},
{
"vote": 2,
"title": "Online news classification",
"text": "I am performing an online news classification. The idea is to recognize group of news of the same topic.\nMy algorithm has these steps:\n\n\n\n\nI go through a group of feeds from news sites and I recognize news links.\n\n\n\n\nFor each new link, I extract the content using \ndragnet\n, and then I tokenize it.\n\n\n\n\nI find the vector representation of all the \nold news\n and the \nlast one\n using \nTfidfVectorizer\n from sklearn.\n\n\n\n\nI find the nearest neighbor in my dataset computing euclidean distance from the \nlast news\n vector representation and all the vector representations of the \nold news\n.\n\n\n\n\n\n\nThis algorithm is not so efficient, because I have to vectorize all the news each time a new one is coming (because it can contain another words: another dimensions in the vector representation) and this is expensive.\n\n\nAlso, I have a problem using TfidfVectorizer because it weights more the special words that only appear in a few news, like \nApple\n, and news that talk about \nAple\n are grouped together even when they deal with different topics.\n\n\nSo, Is there a common approach more efficient than the one I am using?",
"date": "2018-04-03"
},
{
"vote": 12,
"title": "The hands-on NLTK tutorial: A tutorial for the popular NLP Python library",
"text": null,
"date": "2018-04-01"
},
{
"vote": 9,
"title": "Dl-text - Quick text processing library to prepare datasets for training deep learning models (Keras, tensorflow).",
"text": null,
"date": "2018-03-31"
},
{
"vote": 1,
"title": "Natural Language Processing for Law",
"text": "How do I automate the process of separating the information related to the two parties (the plaintiff and the defendant) in a court case using Machine Learning and Natural Language Processing? The inputs will be court cases. An example case is \nhttp://caselaw.findlaw.com/us-supreme-court/15-1194.html\n.",
"date": "2018-03-28"
},
{
"vote": 1,
"title": "Phone call classification questions",
"text": "I want to figure out how to classify calls based on transcripts of them. Classifications like voicemail, robodialer, calls related to specific subjects etc. In order to do something like this, where should I start and what kind of software do I need? I've been looking at h2o + python to do something like this. Any resources you'd recommend for learning?",
"date": "2018-03-28"
},
{
"vote": 2,
"title": "Comparing articles based off similarity?",
"text": "Hi Guys,\n\n\nI have a set of strings where each string represents an article on a certain topic. I want to be able to group the strings based off similarity to each other. So, to give an example, I might have multiple articles on President Obama (where each article gives some type of overview of him and each article is represented as a string with all the text). I want to be able to group articles that are similar in terms of meaning. So, if multiple articles are mostly talking about his childhood, I'd like to be able to identify that and group that. Or, if possible, if half the articles are written by democrats and the other half are written by republicans (democrats talk about obama in a very favorable view while republicans negative), I'd like to be able to group that. How would you guys recommend going about this? I've looked at Princeton's WordNet library and I think that might be useful in terms of trying to find words that mean similar things but I'm still not sure how to transfer that into articles. Thanks a lot.",
"date": "2018-03-28"
},
{
"vote": 3,
"title": "Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision [pdf]",
"text": null,
"date": "2018-03-27"
},
{
"vote": 6,
"title": "Show Reddit: my POS and Dependencies training app",
"text": null,
"date": "2018-03-26"
},
{
"vote": 1,
"title": "What is the SOTA in text classification right now?",
"text": "[removed]",
"date": "2018-03-25"
},
{
"vote": 5,
"title": "Abbreviation expansion for clinical terms",
"text": "I'm currently working on medical health records data and trying to clean it up. I've tried looking online for libraries on medical data and could only find one on GitHub and have heard about one on pubmed but couldn't find it. Are there any recommended libraries for expansion of medical/clinical abbreviations?",
"date": "2018-03-24"
},
{
"vote": 9,
"title": "My first NLP / ML project: wikimark computes similarity with wikipedia vital articles",
"text": "This is my first project ever using ML and NLP. I try to compute the semantic relatedness (or similarity) between an URL and \nwikipedia vital articles\n.\n\n\nFor doing that I use gensim's doc2vec to create regression models using scikit learn SVR for each of the 80 subcategories and then predict the score a new documents against each regression models.\n\n\nThe output looks like that:\n\n\n$ ./wikimark.py guess build/ https://en.wikipedia.org/wiki/Personal_knowledge_base\nsimilarity\n +-- Technology ~ 0.17140869732263997\n |   +-- Computing and information technology ~ 0.17140869732263997\n +-- Science ~ 0.15282761155955393\n |   +-- Physics ~ 0.15282761155955393\n +-- Society and social sciences ~ 0.1509537790911352\n |   +-- Media ~ 0.18972681220299564\n |   +-- Language ~ 0.13639047880005872\n |   +-- Social issues ~ 0.12674404627035118\n +-- People ~ 0.13834670265555754\n |   +-- Politicians and leaders ~ 0.14120826187649851\n |   +-- Inventors and scientists ~ 0.1354851434346166\n +-- Everyday life ~ 0.13576029816193244\n |   +-- Recreation and entertainment ~ 0.13576029816193244\n +-- Arts and culture ~ 0.12927331950583784\n |   +-- Literature ~ 0.12927331950583784\n +-- History ~ 0.12506135377632388\n     +-- General ~ 0.12506135377632388\n$ ./wikimark.py guess build/ https://en.wikipedia.org/wiki/Age_of_Enlightenment\nsimilarity\n +-- People ~ 0.15687219491239904\n |   +-- Philosophers and social scientists ~ 0.16457852199543446\n |   +-- Politicians and leaders ~ 0.16347889387078907\n |   +-- Writers ~ 0.1517315759601271\n |   +-- Inventors and scientists ~ 0.14769978782324555\n +-- Arts and culture ~ 0.15212829109952172\n |   +-- Artistic movements ~ 0.15212829109952172\n +-- Society and social sciences ~ 0.13352365637215666\n |   +-- General ~ 0.13973604120782718\n |   +-- Politics and government ~ 0.13498417545798852\n |   +-- Social issues ~ 0.12585075245065422\n +-- History ~ 0.1228191269614678\n     +-- History by region ~ 0.12432311591982381\n     +-- History by subject matter ~ 0.12131513800311178\n\n\n\nThis is my first project ever and I am not a datascientist.\n\n\nLet me know whatever your think, in particular how I can tell whether the current model works good enough and how to improve it.\n\n\nhttps://github.com/amirouche/wikimark\n\n\nFeedback welcome!!\n\n\nedit: changed the example output\n\n\nedit2: fixed a bug in the 'process' comamnd, now you can train the algorithm.",
"date": "2018-03-24"
},
{
"vote": 3,
"title": "Japanese Lemmatisation",
"text": "I'm a little bit uncertain if it still worth doing Japanese lemmatisation since only verbs in Japanese words have overt and extremely regular inflection according to Wikipedia and also based on my own knowledge of Japanese. It seems that so far rule-based methods dominate the task of Japanese lemmatisation and the results are satisfactory (sorry for missing some references here but I'm quite sure about that). Do you think it's still meaningful to try out neural methods such as sequence-to-sequence models to do Japanese lemmatisation? Thanks for any opinions or advice!",
"date": "2018-03-22"
},
{
"vote": 7,
"title": "Papers on how prod NMT systems implement HTML translation?",
"text": "Google, Yandex, Microsoft... translation APIs support not only text but HTML, in order to translate web pages.\n\n\nAre there any papers that give a clue as to how different prod systems are currently doing it?",
"date": "2018-03-20"
},
{
"vote": 4,
"title": "Doubts in question answering , NER , classification , corenlp and semantic parsing .",
"text": "Hey guys I'm working on a question answering project for my internship. It's kind of like a natural language interface to a structured data source (a API) . \n\n\nIt seems \nsemantic parsing\n is usually used for this task , but since the number of intents/categories of my questions are small (2) I thought of not using semantic parsing for the initial trial. But this can be potentially be used in future.\n\n\nI am using Stanford's \ncorenlp\n for NER and classifier. \n\n\nThis is how the architecture is , I run the questions through an NER and identify the required entities (PERSON, ORGANIZATION, LOCATION, TITLE) then I replace these entities to their canonical form . So a question like \"who knows XYZ from New York\" becomes \"who knows PERSON from LOCATION\" . This is now sent to a classifier which identifies the intent and then accordingly neccasary API calls are made.\n\n\nRight now I have manually created around 200 base questions (has placeholder for entities). \n\n\nI have a few doubts regarding the NER and classifier\n\n\nNER\n\n\n\n\nI trained the NER on multiple copies of my dataset . I take my 200 base questions and fill it with different names in the place holders . This is giving me somewhat ok results but its highly overfitting . The main problem is that I can't find a dataset which has TITLE (eg. VP, CEO , manager, trader, etc.) tag . I found that \nOKE\n has TITLE tags , but their dataset in github is very small. So I am planning to take some articles from \nBloomberg financial news corpus\n and manually annotate. Is this a good idea or should I stick to training on the questions itself ?\n\n\nI have a basic idea of how discriminative models work , but I wasn't able to understand the documentation of feature templates in \nNERFeatureFactory\n . I wasn't able to understand a few features such as useDisjunctive, useSymTags .\n\n\nIs there an option to evaluate the NER only if it gets all the words of the named entity right , i.e. marking \"New York\" as \"New LOCATION\" should give a accuracy of 0% instead of 50% .\n\n\n\n\nClassifier\n\n\n\n\nI am using the \nColumnDataClassifier\n . The problem of a small dataset haunts here also . The model is highly overfitting and exploiting patterns that I have unconciously introduced to the dataset. eg. I had the word \"people\" in a lot of sentances in class 1 , now when I was testing it I used \"people\" in a question of class 2 but the classifier made a error. When I printed the features I found that it had given high weights to the presence of \"people\" for class 1.  I tried to use the \"l1reg\" option , but I was getting an error that one of the QN optimization classes wasn't available \n(I can't recollect it as of now , I'll add it later)\n (edu.stanford.nlp.optimization.OWLQNMinimizer)\n\n\nFor tackling vairations and synonyms , I tried using GloVe . There is an option in the classifier \"useSplitWordVectors\" which just takes the average of the words in the senatance . Is there a better way to handle this . Since it's not regularized , the classifier is assigning very low weights to the word vector features . This  is causing the classifier to completely ignore those features.\n\n\n\n\n  â€‹\n\n\nTL;DR\n\n\n\n\nGetting dataset for training NER to detect TITLE tags \n\n\nHandling variations of different questions using GloVe\n\n\nRegularization for the classifier\n\n\nIs semantic parsing more suitable here ?",
"date": "2018-03-18"
},
{
"vote": 8,
"title": "LDA with seed words?",
"text": "Is there any implementation out there that takes a list of seed words to build topic clusters in LDA?\n\n\nSay, I would like to cluster all words that are about a family. For this I would give it a seed list of the following words as an example: [\"mother\", \"father\", \"family\", \"children\"] and would expect to get everything surrounding those words.\n\n\nAll I could find was the \"eta\" argument in gensim's LDA: \nhttps://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel\n, but I can't find a single example on how to actually use it!\n\n\nAny help appreciated.",
"date": "2018-03-18"
},
{
"vote": 3,
"title": "[D] Advice needed for Product Title Compression?",
"text": "The task i want to do is given any fashion related product title from an ecommerce site, i want to compress the title in a smaller set of words. I have thought of building a custom NER model which will able to detect the brand, attributes, type, product from a product title.\nFor example : Say the product name --> \"Gant Solid Men's Polo Neck Dark Blue T-Shirt\", it should extract the entities like {Brand: Gant, Product: T-Shirt, Type: Polo}\n\n\nWhat is the right way for approaching this problem? Please help, I am newbie to nlp and eager to learn more...",
"date": "2018-03-17"
},
{
"vote": 3,
"title": "Where to download a MultiNet Semantic Network for english or french?",
"text": "I discovered multinet by reading the book \"Knowledge Representation and\nthe Semantics of Natural Language\". It's very interesting. I can't find an example MultiNet database for english or french on the net. Is there such a thing? I can't even find a the German version.\n\n\nIs there similar work for english (or french) available online?\n\n\nref: \nhttp://pi7.fernuni-hagen.de/forschung/multinet/multinet_en.html",
"date": "2018-03-17"
},
{
"vote": 1,
"title": "Fixing the reference standard",
"text": "Got a reference standard, got validation resultsâ€” found Annotator errors that account for all false positives. My numbers look bad but I know my system is good. Is changing the reference standard the right thing to do or is it cheating?",
"date": "2018-03-15"
},
{
"vote": 0,
"title": "GraphGrail Ai and its Market Positioning",
"text": null,
"date": "2018-03-15"
},
{
"vote": 2,
"title": "Easy-to-use, good-enough-performance keyword extraction package in Python?",
"text": "I have 50K news articles from 9 different websites, and for each article I'd like to extract keywords. Is RAKE my best bet here? I can parallelize on my university's cluster if the sloth of NLTK might be an issue (as I've read elsewhere).",
"date": "2018-03-15"
},
{
"vote": 1,
"title": "Noise Cancellation: State of the Art â€“ 2Hz",
"text": null,
"date": "2018-03-15"
},
{
"vote": 7,
"title": "Uses for Part-of-speech tagging?",
"text": "Hello, \n\n\nI'm currently writing a paper about POS-Tagging for Uni and I am struggling to find articles on where POS-Tagging is used. I'm using google scholar but the results have been rather slim. \n\n\nI'll be straightforward: Do you guys know any papers/sources that describe where POS-Tagging is used? Or even, do you know papers that highlight the importance of POS-Tagging?\n\n\nThanks in advance",
"date": "2018-03-15"
},
{
"vote": 1,
"title": "Most common POS reorderings between english/spanish",
"text": "Does anyone know which are the most common Part of Speech Reorderings between english and spanish?\n\n\nThere are some that are rather obvious, like [ADJ, NN] ('red car') -> [NN, ADJ] ('coche rojo'), but I've been trying to look for data, but I can't seem to find any clear numbers. \n\n\nAny help appreciated.",
"date": "2018-03-14"
},
{
"vote": 3,
"title": "Code Galaxies Visualisation of Word2Vec Model",
"text": null,
"date": "2018-03-14"
},
{
"vote": 1,
"title": "Stochastic Conversational Workflows",
"text": null,
"date": "2018-03-14"
},
{
"vote": 2,
"title": "TechCrunchâ€™s Messenger bot gets smarter and more conversational",
"text": null,
"date": "2018-03-13"
},
{
"vote": 4,
"title": "What do you use for text-processing tasks, e.g. removing punctuation?",
"text": "I've been using Python and gensim, but recently I've started wanting slightly more complexity outside of the default and now I'm using regex.",
"date": "2018-03-12"
},
{
"vote": 2,
"title": "Generic coreNLP algorithm(s) or framework(s).",
"text": "What are some generic coreNLP algorithms/frameworks which can be applied to multiple languages (e.g porter stemmer) just by tweaking the rules/processes specific to that language?",
"date": "2018-03-12"
},
{
"vote": 2,
"title": "Help with standard practice on Stanford Sentiment Treebank dataset",
"text": "I'm trying to properly prepare the binary subset of the \nStanford Sentiment Treebank\n (SST) dataset. I've looked at some recent papers, and they stated the following:\n\n\n>In the binary case, we use the given splits of 6920 training, 872 development and 1821 test sentences. Likewise, in the fine-grained case, we use the standard 8544/1101/2210 splits. \nLabelled phrases that occur as subparts of the training sentences are treated as independent training instances.\n\n\nThis is an example sentence, with what I think is a phrase inside of it:\n\n\n>Despite its shortcomings, Girls Can't Swim represents an engaging and intimate first feature by a talented director to watch, and it's a worthy entry in the French \ncoming-of-age\n genre.\n\n\nIs \"coming-of-age\" what is meant by a phrase? Should this phrase be removed and added as another training instance, what is the advantage of that?",
"date": "2018-03-10"
},
{
"vote": 6,
"title": "Open Source SQuAD dataset",
"text": "What are some of the things you would want ensured if someone was to collect and label another question answering dataset like \nSQuAD\n?\n\n\nFor myself I wanted these things:\n\n\n\n\nLonger contexts\n\n\nMultiple ways of asking the same question\n\n\nSignal saying if the question is even answerable given the context.\n\n\n\n\nCurrent SQuAD leaderboard is difficult to trust for me because of 2 main reasons. Firstly the higher level models are hard to reproduce thus reducing the usability of the leading models. Second, since the test set is a fixed set and has been for a long time, it is inevitable that people will have overfit to the leaderboard. A new release of SQuAD would solve this but that's not coming as far as I can see.\n\n\nI was thinking of putting in effort to build a similar dataset but without those constrains. What are the things you would want to see in such a dataset?",
"date": "2018-03-10"
},
{
"vote": 10,
"title": "Stanford NLP Videos taken down?",
"text": "Does anyone know where/how to view the old stanford nlp videos from professors Jurafsky and Manning? There used to be a few playlists that had all the videos (about 100) but they are all deleted now.",
"date": "2018-03-09"
},
{
"vote": 0,
"title": "What are the following, and can I get an example of an NLP problem that deals with each of these and how to solve them?",
"text": "propositional meaning representations\n\n\nrobust parsing\n\n\ndiscourse function classification\n\n\n dialog state modeling\n\n\nintent classification\n\n\nslot-filler based systems\n\n\ndescription logics\n\n\npartially observable Markov processes (POMDP)\n\n\nI have a vague idea of what \"intent classification\" and \"slot-filler based systems\" are, because I have taken several courses in machine learning and know about classification algorithms and have done some of my own simple text processing with nltk datasets and scikit-learn. As for the rest of these things, I have no idea what any of them mean. I need to know exactly what all these things are, in detail, and I need to know examples of NLP problems for each of these, and how they are approached and resolved.\n\n\nThanks.\n\n\nedit: why don't you try to help instead of downvoting? I know it was the same person who downvoted all 3.",
"date": "2018-03-08"
},
{
"vote": 4,
"title": "Using gensims LDAMulticore vs LDAModel",
"text": "For a project, I am using gensims LDAMulticore implementation and I was wondering if there are any differences in the results, compared to the \"normal\" LDA implementation.\nI have a set of 1,8mio documents in a 30 year range. I read somewhere that when using batch learning, the later documents will not influence the model as much as the ones at the beginning. Therefore it is assumed, that no Topic drifting is occurring. Since my documents are spread across 30 years, I actually expect Topic drifting to occur. Does anyone have some further information regarding this? Should I shuffle my documents before running MulticoreLDA or can I read time (time-)linearly?\nI also have the problem that my documents are not uniformly distributed among the years (later years are much more present).\nIt would be nice if anyone had some more information regarding the results of LDAMulticore vs LDAModel. I couldn't find anything regarding that matter.",
"date": "2018-03-08"
},
{
"vote": 0,
"title": "Anyone going to Train AI this year? I've got 30% discount codes. PM me for details.",
"text": null,
"date": "2018-03-06"
},
{
"vote": 4,
"title": "Summarization, sentiment analysis and web scraping API that supports almost any language",
"text": null,
"date": "2018-03-06"
},
{
"vote": 3,
"title": "Can you usefully expand NatLang datasets by modifying sentences in a way that maintains semantics?",
"text": "Say you have a sentence: \n\n\n> I like the colours blue, green, red, and black.\n\n\nWe could rearrange the listed words to form the semantically very similar\n\n\n> I like the colours red, green, blue, and black. \n\n\nConceivably we could write some rule that could do this for any sentence for which it is appropriate. We could also probably write many other transforming rules to act on other types of sentences to create semantically identical 'siblings'.\n\n\nIs this actually conceivable/feasible? If so, would it be useful to identify these kinds of rules and use them to take natural language datasets and 'bulk them up' by generating new data with a known semantic meaning and with a known ground truth label?",
"date": "2018-03-06"
},
{
"vote": 11,
"title": "What work is there in understanding, visualizing or explaining Language Models?",
"text": "The state-of-the-art for Language Models has increased rapidly, potentially owing to its readily available datasets, open-sourced code, and pre-published results [[SOTA]] (\nhttps://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems\n). \n\n\nHowever, it seems largely unclear exactly what these models have learned, despite the furor around explainability etc. \n\n\nI know about [this] (\nhttps://colinmorris.github.io/lm-sentences/#/billion_words\n) method by /u/halfeatenscone for visualizing language models, but what other work has been done? Has anyone tried other methods for interpretability \n1\n \n2\n that have worked for example, on explaining LSTM's that classify sentiment, but applied to this particular task?",
"date": "2018-03-05"
},
{
"vote": 7,
"title": "A classic in NLP: what is the difference between stemming and lemmatization?",
"text": null,
"date": "2018-03-05"
},
{
"vote": 1,
"title": "Context and Sequentiality in Conversational Applications",
"text": null,
"date": "2018-03-05"
},
{
"vote": 7,
"title": "Overview of an NLP workflow",
"text": null,
"date": "2018-03-05"
},
{
"vote": 2,
"title": "Guide to Visual Question Answering: Datasets, Approaches and Evaluation",
"text": null,
"date": "2018-03-05"
},
{
"vote": 3,
"title": "Using simple effect filters to correct overfitting",
"text": "I am training an NLP module for use in beginning English pronunciation practice software. I am in the process of building small corpora for 10 colors. Each corpus has 150-200 different samples of the same word. Overfitting is not horrible, but I'd like to improve the module. \n\n\nCan anyone speak to the effectiveness of using simple effect filters, specifically pitch and speed, to increase sample size? I suppose this could be considered a kind of noise injection technique, but I'm not too clear on that either. \n\n\nThanks in advance for any thoughts.",
"date": "2018-03-05"
},
{
"vote": 0,
"title": "Building word clouds from Reddit comment threads | Machine Love Us",
"text": null,
"date": "2018-03-04"
},
{
"vote": 1,
"title": "Information Retrieval problem - Searching huge archive of news",
"text": "So I'm making a search engine for large collection of newspaper archives(few decades probabaly). \nWhat would be a good approach for this?\nI wrote some code for retrieving results for a query by using tf-idf from sklearn library for small sample of articles but that may not scale well.\nSome people suggested elasticsearch. I have no idea what that is.. Should I invest my time and learn about it?",
"date": "2018-03-04"
},
{
"vote": 3,
"title": "Semantic similarity: How to define context in LSA?",
"text": "Hi,\nI am a beginner here so elaborate answers are greatly appreciated!!\n\n\nProblem statement:\nI have a recorded conversation between two people where one person is asking a series of questions. My intention is to identify the question (or similar sentences) in the recorded text. \n\n\nA little bit of 'googling' told me that I am trying to do semantic similarity. I read up LSA - so my question is how do you define context?\n\n\nAlso, on a related note, how to use word vectors (pre-trained glove vectors) for text similarity?",
"date": "2018-03-04"
},
{
"vote": 3,
"title": "Train Embeddings model on Programming book",
"text": "I want to train embeddings model on programming book such as \"Data Structures using C++ by D.S. Malik\"\n\nhttp://bu.edu.eg/portal/uploads/Computers%20and%20Informatics/Computer%20Science/1266/crs-10600/Files/Esam%20Halim%20Houssein%20Abd%20El-Halim_4-%20Data-Structure%20Using%20C++%20Malik.pdf\n\"\n\n\nI have converted this pdf copy to .txt file, Any idea, what kind of pre-processing should I do before feeding to the network?\n\n\nBecause when I convert book to .txt file, equations and tables are not displayed properly. Should I just remove them?",
"date": "2018-03-03"
},
{
"vote": 0,
"title": "Woebot raises 8 million, Tensorflow 1.6, Project Alexandria, Google ML course, Deep learning notations,â€¦",
"text": null,
"date": "2018-03-03"
},
{
"vote": 11,
"title": "Hierarchical Classification at Forge.AI",
"text": null,
"date": "2018-03-02"
},
{
"vote": 7,
"title": "Noob questions about NLP",
"text": "I had a couple of questions regrading POS Tagging. It would be great if anyone could answer it. \n\n\n\n\nHow would we train a model to identify whether the word â€œAppleâ€ in a sentence belongs to a fruit or the company?\n\n\nI am building a POS tagger from scratch given and have a corpus of annotated sentences. How should I deal with unknown words?\n\n\nWhat would be the simplest approach to build a POS tagger?\nThanks",
"date": "2018-03-01"
},
{
"vote": 0,
"title": "Just a simple example on how to use scikit-learn TfidfVectorizer on text which is already tokenized",
"text": null,
"date": "2018-02-28"
},
{
"vote": 14,
"title": "New technique to find answers within a large body of text",
"text": null,
"date": "2018-02-27"
},
{
"vote": 2,
"title": "Building a language model using a corpus of large number of short documents",
"text": "I am looking to train a language model (predicting the token at position n using the previous tokens n-1, n-2, ...) using a corpus consisting of many short documents, reddit comments to be precise. I am wondering what might be the best preprocessing strategy would be. I have been searching high and low for best practices but haven't found any. The approach I have tried so far is to first concatenate all commens with a special [NEW COMMENT] token inserted between two comments. Intuitively this feels like a bad approach as the network would have to learn to forget everything once it sees a token like this. \n\n\nIf I fed the network pieces of single comments as separate training examples, how would I deal the different length of the comments?",
"date": "2018-02-26"
},
{
"vote": 5,
"title": "Fast custom entities extraction from text",
"text": "I have the following problem: I have some lists of custom entities (e.g. mammal animal names, fish names, plant names) and I need to extract them from a [potentially] large text. And I need that to be as fast as possible. \nStatistical methods are not useful, at least I think so, as I have no annotated phrases and most matchs can be done by matching lemmas/stems/spell-corrected tokens. \nDo you know good libraries for Python that does that? I need it to be performant so that I can use it in production environment in real time. Are Elastic Search Percolate queries any good for that?\nThank you!",
"date": "2018-02-26"
},
{
"vote": 3,
"title": "Building a code switching corpus in Python",
"text": "Hello, \n\n\nI have a corpus-based code switching analysis project coming up. I need to build a preliminary corpus for Hindi-English code switching and I am looking for any ideas/tips/resources on how to go about implementing this. \n\n\nAlso, any papers or additional resources that might aid in this project later on would also be helpful.\n\n\nThanks in advance!",
"date": "2018-02-23"
},
{
"vote": 1,
"title": "Letâ€™s start with the beginning. What is NLP Therapy? This is a misleading question. We can not give it a single definition",
"text": null,
"date": "2018-02-22"
},
{
"vote": 3,
"title": "Snips built its on-device Voice AI using the Rust programming language",
"text": null,
"date": "2018-02-22"
},
{
"vote": 4,
"title": "Forge.AI: Fueling Machine Intelligence",
"text": null,
"date": "2018-02-21"
},
{
"vote": 3,
"title": "Where can I find CoreNLP dataset ?",
"text": "I am trying to find the CoreNLP dataset, \nhttps://stanfordnlp.github.io/CoreNLP/\n. I could only access to the model. I am interested in their NER dataset.",
"date": "2018-02-21"
},
{
"vote": 6,
"title": "NLP Side Project?",
"text": "Hi all, I am an undergraduate student at a university and am double majoring in computer science and linguistics. I'm on my reading break right now for a week and have just been playing with NLTK and tokenization. \n\n\nI'm looking for a compling related side project that I can do over this week. I couldn't really find any compling side projects online that I can work on for the next week. Do any of you have any suggestions? Thanks so much!",
"date": "2018-02-19"
},
{
"vote": 3,
"title": "Trucks and Beer: Textual analysis of 12,446 country songs [OC]",
"text": null,
"date": "2018-02-19"
},
{
"vote": 6,
"title": "Predicting Musical Genre Using Lyrics (with d3.js dashboard)",
"text": null,
"date": "2018-02-18"
},
{
"vote": 2,
"title": "Has anyone used TextEvaluator's models to run an analysis?",
"text": "Either that or, though I know it's a bit older, Coh-Metrix?",
"date": "2018-02-18"
},
{
"vote": 2,
"title": "[Request] Open source toolkit that can identify and classify deontic statements (e.g., obligations, permissions, prohibitions) from unstructured text",
"text": "I'm looking for an open source toolkit that can help with legal text mining. For example, after parsing a contract into sentences, it will tag statements as obligations (\"The home owner must...\"), permissions (\"The home owner may...\"), or prohibitions. I'm hoping there is something more sophisticated than a tool that searches for modal verbs. Some sentences could be tricky, e.g.,: \"The home owner \nmay\n decide that the agent \nmust\n provide proof of insurance.\"",
"date": "2018-02-17"
},
{
"vote": 16,
"title": "[R] [1802.05365] Deep contextualized word representations",
"text": null,
"date": "2018-02-17"
},
{
"vote": 5,
"title": "What aligners are used for compiling parallel corpora?",
"text": "I'm looking for an aligner for creating a small parallel corpus aligned at the sentence level. So far, I've tried \nLF Aligner\n (based on the Gale-Church algorithm); I can't say if it's good or bad because I haven't tried another aligner, but its output usually has a number of misalignments that have to be corrected manually.\n\n\nNow I wonder: what aligners are used in the industry, for example, in machine translation development? Back in the days when Google Translate used the statistical method, their corpora were aligned at the phrase level. How was this possible? Did they have people manually correcting the alignments, or were they already precise?\n\n\nTo make my question clearer: are there any aligners so good that their output wouldn't require much manual editing?",
"date": "2018-02-16"
},
{
"vote": 8,
"title": "Adding voice control to your projects â€“ Hackers at Cambridge",
"text": "[deleted]",
"date": "2018-02-15"
},
{
"vote": 7,
"title": "Can't understand dependency parsing",
"text": "Hello\nGoing through \nLecture 6\n and \nmany\n \nother\n for an understanding on how basic transition based parsers work.\n\n\nI can gather the core, which is words are put in stack and buffer and on them shift, reduce, left arc, right arc of various categories are performed. I can clearly understand this. Along with what features to consider as input for training.\nWhat I can't be clear and struggling is in how the order of this operation is then to be determined.\n\n\nManning says in his lecture a ML classifier is used, SVM, Avg Perceptron, etc and a dataset of Conll-x is fed which has columns of ID, word, POS, relation-id, dependency label.\n\n\nNow how are those rows of column trained to arrive at a weight, and how is that applied to new data for a label? I say this because unlike text classification, where a series of connected words can be vectorized or simply one-hot encoded to represent feature set, in this task how are words in stack and buffer needs to be represented with features like POS in stack,  buffer, adjoining words ,etc.\n\n\nBasically can someone explain how in a simple MALT parser features are to be represented, the weights derived and then applied for parsing?",
"date": "2018-02-14"
},
{
"vote": 6,
"title": "Question/Answering datasets for Virtual Assistant/Chatbots",
"text": "Hi, \n\n\nI am looking for Question/Answering Dataset that I can use to train Virtual assistant/Chatbots models. In case you have any idea, what are some of the good sources of dataset that I can use to train a virtual assistant bot. \nSomething like these ones but more for assistants \nhttps://github.com/dapurv5/awesome-question-answering#datasets\n\n\nLet's say that I want to train the model for intent (send email). \n\n\nSome possible \n\n\nsend email to (somebody) ?\n(can u please) send email to (somebody) ?\n(would u please) send email to (somebody) ?\n.. etc",
"date": "2018-02-13"
},
{
"vote": 3,
"title": "Topic to write a paper about",
"text": "Hello all, \n\n\nthis may sound a little unorthodox but I am having difficulty picking a topic to write a paper about.\nIt shouldn't be anything special - 3200 words - and I'm a Bachelor student.\nI had an extensive presentation on POS Tagging (rule-based, statistical and Brill Tagger) and if possible I'd like to write a paper on that but I can be other fields of computational linguistics as well. \n\n\nI am not asking to give me a topic directly. What I am asking is that maybe you could give me ideas for a paper - and more importantly - if and where I can find a large repository of literature on such topics. So far, I've worked with Daniel Jurafsky's book and it'd be nice if I could find more. \n\n\nNot asking you to do my paper, of course, just asking if you have ideas (and know good literature on that).  \n\n\nHelp is much appreciated because I my sick body can't think of any ideas right now",
"date": "2018-02-13"
},
{
"vote": 5,
"title": "Word alignment task vs dictionary induction",
"text": "I've been reading up on multi-lingual word embedding methods, and couldn't quite grasp the difference between two of the evaluation methods used - word alignment and dictionary induction.\nMy curiosity was heightened by looking at Table 1 from \nthis paper\n, where the Bilingual Autoencoders methods overperforms Inverted index for the Word Alignment task, but it's the other way around for Dictionary Induction.\nThanks for the help!",
"date": "2018-02-12"
},
{
"vote": 3,
"title": "Named Entity Recognition: whatâ€™s more important, perfect training data or volume of training data? Also, what is a good starting point for number of observations for a good NER model?",
"text": "Objective: Iï¸ (have long been a user of pretrained NER models but) have been tasked with building one for a specific domain. \n\n\nSetup: Iï¸ can generate more training data by using a rules based system and then feed it into a neural model but the training data might not be perfect as Iï¸ wouldnâ€™t intend on reviewing every single observation. OR Iï¸ can hand score every observation and have perfect training data but much less of it.  \n\n\nQuestion: based on your experience, what creates a better NER model?  Big volume with some imperfection or small and perfect data. \n\n\nAlso, if you could provide an idea of the volume of data needed in the case of your suggestion Iï¸ would appreciate it. Iï¸ know this changes by use case but knowing how much was required elsewhere at least gives some idea.",
"date": "2018-02-11"
},
{
"vote": 3,
"title": "Sequence Labeling / Tagging Tasks in Natural Language Processing (NLP)",
"text": "Hello community,\n\n\ni am searching for \nsequence labeling / tagging\n tasks in natural language processing (NLP). \nRight now we are developing a system to solve a bunch (all?) of them and evaluated our current general architecture on \npart-of-speech tagging\n, \nnamed-entity recognition and classification\n tasks for English and German data.\nWe are thinking about adding \nchunking\n, \nanaphora detection\n (coreference mention detection) and \npredicate detection\n. Opinions on the sense of that are welcome.\nBesides the tasks themselves, the appropriate data from shared-tasks/workshops are definitely of interest.\n\n\nTo summarize, this are the current tasks + data sources:\n\n\n1. Part-of-Speech Tagging\n\n\n\n\nEnglish: \nTreebank-3\n\n\n\n\nGerman: \nTIGER Corpus\n\n\n\n\n\n\n2. Named-Entity Recognition and Classification\n\n\n\n\nEnglish: \nW-NUT 2016\n\n\n\n\nEnglish: \nKaggle\n\n\n\n\nGerman: \nGermEval 2014\n\n\n\n\n\n\nAdditional data sets to the report ones are welcome.\n\n\nMany Thanks in advance! :blush:",
"date": "2018-02-11"
},
{
"vote": 1,
"title": "Python NLP with NLTK introduction [Video]",
"text": null,
"date": "2018-02-10"
},
{
"vote": 3,
"title": "Help: Text classification",
"text": "I have a collection of documents, which fall into 4 classes, and each of these classes can further be divided into 3 subtypes, and these sub-types can further be broken down into 2 categories each.  \n\n\n\n\nwhat I would like to do, is basically classify these docs into their respective classes/sub-types/categories.    \n\n\nwhat is the best approach to accomplish this?  \n\n\nwould a multinomial naive Bayes be a good pick?    \n\n\nIf MNB, then what is the best strategy to implement this? like should I first classify them into 4 classes, and then implement the algorithm to classify within these classes into further steps and as such?\n\n\n\n\nany help would be really helpful(I'm new for the document classification type of problems)    \n\n\n(also, any resources/examples doing this kind of thing would be highly appreciated)",
"date": "2018-02-10"
},
{
"vote": 2,
"title": "Does anyone know a good Python library/ code snippet to query a Wikidata dump?",
"text": "I have downloaded the \nWikidata JSON dump\n, which is a 20GB+ .bz2 file. \n\n\nIs there a Python library that would help me query this dump?\nFor example, if I search for \"Douglas Adams\", it would give me the json entry for the id \nQ42\n. If I search for \nQ5\n, it'd return the json entry for it.",
"date": "2018-02-09"
},
{
"vote": 4,
"title": "Dealing with (near) duplicate documents... what is best way to identify which are templates?",
"text": "Hey guys,\n\n\nI'm analyzing a bunch of (1-2 page) comments/letters in Python, and for the project I'm working on, there are a ton of duplicates where a letter might be copied from a template, then signed with a different name.\n\n\nWhile I'm tempted to go guns loaded and try to use gensim for doc similarity scores using WMD, there has to be a better simple solution. Was thinking about fuzzywuzzy, but corpus is too large to do that effectively. \n\n\nSo my question is... what is the elegant solution? \n\n\nDealing with ~2000 different letters/comments.",
"date": "2018-02-09"
},
{
"vote": 3,
"title": "Non-Projective Dependency Parsing via Latent Heads Representation (LHR)",
"text": null,
"date": "2018-02-07"
},
{
"vote": 1,
"title": "Interesting Facts About \"To Kill a Mockingbird\" By Harper Lee",
"text": null,
"date": "2018-02-07"
},
{
"vote": 5,
"title": "YouTube Murals: Painting Topic Change Over Time in Videos",
"text": "[deleted]",
"date": "2018-02-06"
},
{
"vote": 3,
"title": "Research progress on decoding the Voynich manuscript",
"text": null,
"date": "2018-02-04"
},
{
"vote": 11,
"title": "Which NLP method Dialogflow used?",
"text": "I am very curious about Google's Dialogflow platform. What would be the technique they used in intent matching and entity identification?",
"date": "2018-02-03"
},
{
"vote": 0,
"title": "The Shallowness of Google Translate",
"text": null,
"date": "2018-02-02"
},
{
"vote": 1,
"title": "How do you do SMOTE on text classification and how do you visualize it?",
"text": "Pardon me if this is too beginner's question, but I'm kind of stuck now, and I guess you guys can help.\n\n\nI've been trying to learn SMOTE, that I aim to use on text classification. I am using \nthis scikit learn imbalanced library\n. And I have a few questions.\n\n\n1. Which part of the text that you need to into oversample? In normal, structured dataset, I basically did the following\n\n\nX_sm, y_sm = smote.fit_sample(X, y.values.ravel())\n\n\n\nwhere X is the data (features) and y is the target (class). In text classification, I figure it'd be more complicated? I figure you feed the bag of words to the SMOTE?\n\n\n2. This is probably an unnecessary step, but I kind of want to see how it works. I can use PCA with mathplotlib to visualize the data before and after the SMOTE on a scatter plot. But when the data is text, how do you visualize it? Can you still use PCA on this one?\n\n\nEDIT: formatting and details",
"date": "2018-02-01"
},
{
"vote": 2,
"title": "Noob question about the paper 'Machine Comprehension by Text-to-Text Neural Question Generation'",
"text": "Can someone explain to me in simple terms the formula to obtain the initial state of the decoder?\n\n\nThis is my first time reading a proper research paper so some help is much appreciated!",
"date": "2018-02-01"
},
{
"vote": 2,
"title": "dimensionality reduction of word2vec using autoencoder for Text clustering",
"text": "I have multi-million short sentences which i want to cluster. I did doc2vec on the sentences and the resultant matrix is high dimensional. I understand that tf-idf output can be put through dimensionality reduction and then processed for clustering. My question is can we also do dimensionality reduction for doc2vec output and then use clustering? I believe that we can, can anyone suggest if it is otherwise?",
"date": "2018-01-31"
},
{
"vote": 16,
"title": "12 Natural Language Processing and Machine Learning educational resources",
"text": null,
"date": "2018-01-31"
},
{
"vote": 8,
"title": "Analyzing Rap Lyrics Using Word Vectors",
"text": null,
"date": "2018-01-29"
},
{
"vote": 22,
"title": "What are the best papers to read to learn about Transfer learning techniques as applied to NLP?",
"text": null,
"date": "2018-01-25"
},
{
"vote": 8,
"title": "How we (almost) completed a natural language recipe recommendation engine using Twitter in a 24-hour hackathon",
"text": null,
"date": "2018-01-24"
},
{
"vote": 4,
"title": "Where should I look to research the topic of producing tests/multiple choice questions based on text automatically?",
"text": "I've found one paper on the topic however I was wondering whether there was more out there.",
"date": "2018-01-24"
},
{
"vote": 6,
"title": "How to solve 90% of NLP problems: a step-by-step guide",
"text": null,
"date": "2018-01-24"
},
{
"vote": 1,
"title": "natural language processing",
"text": "any where i can find and expert, to discuss the topic because there is processes in my office that could be solved by nlp i think but i dont know. \n\n\nWhere can i find these an expert to consult?",
"date": "2018-01-24"
},
{
"vote": 5,
"title": "What is Natural Language Processing?",
"text": null,
"date": "2018-01-23"
},
{
"vote": 0,
"title": "Where to find a NLP supervisor?",
"text": "[deleted]",
"date": "2018-01-23"
},
{
"vote": 5,
"title": "Advanced document tagging problems",
"text": "[deleted]",
"date": "2018-01-23"
},
{
"vote": 1,
"title": "A search engine for municipal land use codes using a graph of word embeddings",
"text": "[deleted]",
"date": "2018-01-22"
},
{
"vote": 1,
"title": "Semantic Role Labeling at the Paragraph Level",
"text": "Is there any analysis of SRL performed at the paragraph or multi-sentence level? \n\n\nAlthough a sentence-level predicate selects the arguments, those arguments can be pushed beyond the sentence boundary, typically through light verbs. For example, \"Bob TRAVELED to Italy by plane. He went for business from Monday until Friday.\" Here, the roles for Purpose and Start/End Date are pushed into a follow-on sentence. You could argue that those additional roles are arguments of \"went\", (which is fine) - but then you have roughly the same problem: merging synonymous predicate structures. Either way, any guidance on the topic is appreciated.",
"date": "2018-01-22"
},
{
"vote": 2,
"title": "What are standard accuracy/F1 scores when classifying documents from the 20 newsgroups dataset?",
"text": "I am currently using a Decision Tree to classify the documents from the 20 newsgroups dataset (\nhttp://qwone.com/~jason/20Newsgroups/\n) and I am having trouble finding some similar accuracy/F1-score to my own results, and I'm also having trouble finding papers that report the same metrics. Does anyone know a \"standard\" reference point for classifying documents on this dataset that I can use as a sanity check? For the record, my accuracy is currently 0.97 and my F1 score is 0.71. I believe these are very good results and they seem too good to be true.\n\n\nThanks for anyone who can help.",
"date": "2018-01-21"
},
{
"vote": 6,
"title": "Is there any algorithm or approach to get the most representative sentence from a group of sentences?",
"text": "Hi everyone,\nI hope this is the right place to ask.\n\n\nI want to accompany my visualization with an actual sentence (from the set of sentences in my data), but not any random sentence or a made up sentence, rather a sentence which is most representative.\n\n\nFor example If a question is asked via reddit or twitter: How do you feel about Donald Trump calling African countries \"shit-hole\" countries?\n\n\nThe response should probably be negative, since that's the general sentiment I get when I read though the comments. \n\n\nI was thinking maybe word2vec then get the mean vector, but is their a president for this type of task? A better way to handle this?",
"date": "2018-01-20"
},
{
"vote": 1,
"title": "Context Aware NLU",
"text": "What does it mean to have a context aware NLU system? Anyone have intuitive examples?",
"date": "2018-01-19"
},
{
"vote": 4,
"title": "r/StarWars spoilers model",
"text": "I built a model that infers whether a submission to r/StarWars contains spoilers are not, using character level deep learning. \n\n\nAnyone have feedback?\n\nGitHub Repo\n, \nSummary",
"date": "2018-01-19"
},
{
"vote": 11,
"title": "Introduced today: \"Amazon SageMaker BlazingText: Parallelizing Word2Vec on Multiple CPUs or GPUs\"",
"text": null,
"date": "2018-01-19"
},
{
"vote": 10,
"title": "[1801.06146] Fine-tuned Language Models for Text Classification",
"text": null,
"date": "2018-01-19"
},
{
"vote": 1,
"title": "20 Forgotten English Words That Are Just As Useful Today",
"text": null,
"date": "2018-01-18"
},
{
"vote": 12,
"title": "At the intersection of natural language processing and computer vision, Microsoft researchers build a bot that draws what you tell it to",
"text": null,
"date": "2018-01-18"
},
{
"vote": 2,
"title": "Advice needed - extracting names of known musicians and songs from text",
"text": "I'm an experienced programmer but [currently] have only basic understanding of NLP/ML and I'm looking to solve the following problem:\n\n\nGiven a short piece of text (a title, sentence or short paragraph) that may (or not) contain the name of [known] musician (singer, band) and the name of a musical composition, I'd like to extract the artist/song names or an indication that the text doesn't contain any.\n\n\nObviously, there are no set patterns for the location of this information in the text, the names may be slightly misspelled, contain inconsistent symbols (double/single quotes, numeric symbols vs number names, etc).\n\n\nI downloaded and normalized an extensive database of artists and track names (from the \ndiscogs data dump\n ) and can manipulate it to any structure needed but I'm not sure how to proceed from here. The naive approach I first thought of is to break the text into all possible permutations of adjacent words and then searching them against the artists/tracks lists and coming up with some logic to determine the best option from the results. But this seems very costly and will probably yield many false positives.\n\n\nIs this achievable with reasonable accuracy? What would be a good approach to tackle this? What algorithms, libraries, tools, additional data sets I should be looking into?\n\n\nThanks",
"date": "2018-01-18"
},
{
"vote": 10,
"title": "Python NLP tutorial: Using NLTK for natural language processing - HyperionDev Blog",
"text": null,
"date": "2018-01-17"
},
{
"vote": 2,
"title": "RASA and Preprocessing",
"text": "Hello!\n\n\nSimple question. Does rasa use some forms of text preprocessing like tokenization, stemming or lemmatization? Or does it take the text 'as-it-is' without further changing?\nI couldn't get any definte information on this in the docs, but maybe I just overlooked something.\n\n\nI guess from the docs (\nhttps://nlu.rasa.ai/languages.html\n) (\nhttps://nlu.rasa.ai/entities.html\n) that it is indeed using some preprocessing, depending on the backend and pipeline (\nhttps://nlu.rasa.ai/pipeline.html\n). But it seems to be only tokenization and nothing else. Is my assumption correct?",
"date": "2018-01-16"
},
{
"vote": 1,
"title": "Music and Music Idioms | English Vocabulary",
"text": null,
"date": "2018-01-16"
},
{
"vote": 2,
"title": "LanguageCrunch NLP Service docker image",
"text": null,
"date": "2018-01-15"
},
{
"vote": 3,
"title": "Question about inverse document frequency",
"text": "I was wondering what is the correct way to count idf for summarization purpose. \n\n\nSay that I have a set of gold standard documents comprising of N documents with one summary of those 5 docs. I want to use tf-idf as the feature vector for those documents as part of my learning and prediction process. The way I see it, there are two ways of doing it:\n\n\n\n\nCount the idf of the gold standard documents and reuse it during prediction for test document. For unknown words, I put the number log(N). \n\n\nDuring the prediction, I recount the idf using the test set documents.\n\n\n\n\nWhich approach is correct here, and why?",
"date": "2018-01-14"
},
{
"vote": 0,
"title": "Want to learn how to make your own NLP-Based Neural Network using TENSORFLOW? Check this video out, and if you enjoy it, make sure to subscribe. :)",
"text": null,
"date": "2018-01-13"
},
{
"vote": 7,
"title": "Agreeing/Disagreeing opinions",
"text": "[deleted]",
"date": "2018-01-09"
},
{
"vote": 3,
"title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition [PDF]",
"text": null,
"date": "2018-01-09"
},
{
"vote": 14,
"title": "What are some quality open source text corpus used in the field of NLP",
"text": "[deleted]",
"date": "2018-01-06"
},
{
"vote": 2,
"title": "A useful wiki-resource I never knew existed! Great for ontology building and many other things.",
"text": null,
"date": "2018-01-06"
},
{
"vote": 1,
"title": "Explore Language Learning Solutions with Babbel | Studica Blog",
"text": null,
"date": "2018-01-05"
},
{
"vote": 16,
"title": "Where to start in NLP?",
"text": "I am a computer science student with some fundamentals of programming now I am very much interested in Natural Language Processing but I am quite unsure where to begin with. I have understood Machine learning at the heart of it yet confused where to begin with, kindly help.",
"date": "2018-01-03"
},
{
"vote": 10,
"title": "Over 100 Linguistic Features to Determining Quality of Articles in Polish Wikipedia",
"text": null,
"date": "2018-01-03"
},
{
"vote": 3,
"title": "Are there certain algorithms or common methods for ranking content based on quality metrics?",
"text": "Things like information density in an article, author attributes, references, structure, readability, how unique/original the content is, etc. I am referring here to attributes that measure \"quality\" and not virality or popularity.\n\n\nA related project which has started recently is \"News Quality Scoring\"\n\nhttps://knightfoundation.org/grants/8015",
"date": "2018-01-03"
},
{
"vote": 4,
"title": "I had some fun using word vectors to provide clues for the board game Codenames. Feedback welcome!",
"text": null,
"date": "2018-01-03"
},
{
"vote": 4,
"title": "Generating inspirational quotes with Markov chains",
"text": null,
"date": "2018-01-01"
},
{
"vote": 13,
"title": "Anyone applied capsule networks to language yet?",
"text": "Seems like many of the putative advantages of capsule networks to the visual domain could apply to language. The richer detail and what is called 'pose' information in the visual might allow better understanding of ordering and syntax effects in a linguistic context.",
"date": "2017-12-31"
},
{
"vote": 5,
"title": "How can I detect if an article is \"News\"",
"text": "I want to be able to differentiate between \"News\" articles talking about an update or time-bounded event and a \"Knowledge\" article talking about evergreen timeless content. Is it a matter of training binary classifier over hundreds of thousands of documents tagged manually as \"News\" or \"Non-News\"? Is it a deep learning problem?",
"date": "2017-12-29"
},
{
"vote": 5,
"title": "How can I detect if 2 documents are exactly the same?",
"text": "I am not talking here about document clustering or normal similarity. I want to be able to tell that these 2 articles are covering the same topics with the same depth and hence, I need to read only one of them.",
"date": "2017-12-29"
},
{
"vote": 1,
"title": "NLTK Extracting information from text",
"text": "I wanna to make presentation about the chapter \"Extracting information from text\" from NLTK --> \nhttp://www.nltk.org/book/ch07.html\n\n\nThere is an example where the author extract information about companies and locations. I wanna to present this example as well as the steps to reach this result (Relations between company and location, e.g. Apple --> Silicon Valley). \n\n\nDo you know any other example for this topic or some project where this kind of information extraction technique is used?",
"date": "2017-12-28"
},
{
"vote": 7,
"title": "KotlinNLP - NeuralParser | http://www.dependencyparsing.com/",
"text": "Are you familiar with Google SyntaxNet? For those who do not know, it is a great artificial intelligence tool developed by Google for natural language analysis. It is based on TensorFlow and runs under C++/Python.\n\n\nToday I am pleased to inform you that a similar tool is under development, entirely written in Kotlin (a JVM language by JetBrains).\n\n\nThe project is called KotlinNLP, and it has been designed to support an entire text analysis process, ranging from tokenization to categorization, from parsing to entities recognition. One of the aim of KotlinNLP is trying to maintain a clear separation among the natural language processing techniques and the underlying deep learning algorithms so that you don't need to be a Machine Learning expert to work with it.\n\n\nYou can follow this link (\nhttp://www.dependencyparsing.com/\n) to try the current results of some KotlinNLP modules: LanguageDetector, NeuralTokenizer, and NeuralParser.\n\n\nPlease note that the project is in a very early stage of development. Soon the algorithms will be strengthened, and the number of supported languages will be increased. In the meantime, have fun with it and if you like, do not forget to add a star to the project on GitHub (\nhttps://github.com/KotlinNLP/NeuralParser\n)!\n\n\nThank you",
"date": "2017-12-28"
},
{
"vote": 3,
"title": "[Watch] Taking Advantage of Conversational Context to Improve NLP Models",
"text": null,
"date": "2017-12-27"
},
{
"vote": 11,
"title": "NLG: How to generate sentences from lists of facts.",
"text": "Lets pretend we had a list of facts (similar to prolog tuples) that define some knowledge about some entities. e.g.\n\n\ndoing(clean, data)\ndone(collect, data)\ntodo(train, model)\ntodo(write, paper)\n\n\nWhat methods could I use to generate sentences like:\n\n\nYou should be cleaning the data you collected, then you need to train your model and write your paper.\n\n\nAny suggestions or links to relevant papers would be amazing!",
"date": "2017-12-26"
},
{
"vote": 6,
"title": "Topic Extraction/Modeling with Small Text Samples",
"text": "If I have a goal of extracting a latent topic from conversational language, say from a scene in a movie, should I use something like TextRank or use a seq2seq RNN or something else entirely?",
"date": "2017-12-20"
},
{
"vote": 2,
"title": "I recently code for chunking data techniques in named entity recognition(NER) using NLP libraries and algorithm. Can you guys give me feedback on it?",
"text": null,
"date": "2017-12-20"
},
{
"vote": 15,
"title": "Tacotron 2: Generating Human-like Speech from Text",
"text": null,
"date": "2017-12-19"
},
{
"vote": 4,
"title": "Coupling top-down and bottom-up approaches to Natural Language Processing",
"text": null,
"date": "2017-12-18"
},
{
"vote": 5,
"title": "What concepts have you had the most trouble with incorporating into your intuition ?",
"text": null,
"date": "2017-12-18"
},
{
"vote": 6,
"title": "Datasets with question and corresponding knowledge base tuples â€¢ r/textdatamining",
"text": null,
"date": "2017-12-16"
},
{
"vote": 2,
"title": "Programming for laughs: A.I. tries its hand at humor at YSEAS",
"text": null,
"date": "2017-12-15"
},
{
"vote": 8,
"title": "Classifying and visualizing with fastText and tSNE",
"text": null,
"date": "2017-12-13"
},
{
"vote": 2,
"title": "Has anyone had a chance to implement pointer sentinel-LSTM models? If so, how did it go?",
"text": null,
"date": "2017-12-13"
},
{
"vote": 10,
"title": "Is there anywhere I can learn how search engines (Google, Bing, etc.) have been incorporating NLP into their search algorithms?",
"text": null,
"date": "2017-12-10"
},
{
"vote": 4,
"title": "Predicting Stock Performance with Natural Language Deep Learning",
"text": null,
"date": "2017-12-08"
},
{
"vote": 1,
"title": "Introduction to Natural Language Processing",
"text": null,
"date": "2017-12-08"
},
{
"vote": 5,
"title": "Add GloVe from scratch in Golang for Word Embeddings",
"text": "[deleted]",
"date": "2017-12-07"
},
{
"vote": 5,
"title": "Where to get started?",
"text": "As a software developer and linguistics fan, how can I start learning more about doing meaningful work with language technology? I've worked on some very small personal projects like a simple n-gram analysis in matlab and simple text to phoneme conversion in python. However as an amateur, I'm not really sure where to start learning how to do more proper NLP, particularly in a way that would be useful for computational or research-based work",
"date": "2017-12-06"
},
{
"vote": 1,
"title": "Portuguese Translation Services",
"text": "[removed]",
"date": "2017-12-06"
},
{
"vote": 1,
"title": "Anyone using the new Twitter premium API to analyze a large volume of tweets?",
"text": "[deleted]",
"date": "2017-12-05"
},
{
"vote": 8,
"title": "Amazon Comprehend â€“ Continuously Trained Natural Language Processing | Amazon Web Services",
"text": null,
"date": "2017-12-04"
},
{
"vote": 1,
"title": "Language Translation Services India",
"text": null,
"date": "2017-12-03"
},
{
"vote": 2,
"title": "[Advice] How to program my Cognitive Science thesis project?",
"text": "[deleted]",
"date": "2017-12-02"
},
{
"vote": 2,
"title": "Need help classifying sentences as a question",
"text": "I'm pretty new to NLP and am trying to create a model that will take in a sentence (without a question mark at the end) and classify it as a question or not a question. I've done some looking into question classification and have mostly only found examples on classifying input questions as certain types of questions, which is not what I'm trying to do. I have a basic model right now where I look to see if there are commonly used 'question words' (e.g. who/what/when/where/why/how) in a given sentence, but it is obviously very imprecise. \n\n\nAs an example I'd like to do: \"What is the weather\" -> Question, \"I like dogs\" -> Not a question. \n\n\n Are there any known models or available code out there that I wasn't able to find? Thanks!",
"date": "2017-12-01"
},
{
"vote": 5,
"title": "Transform articles via natural language processing into a revolutionary reading format that allows you to read smarter and faster.",
"text": "[deleted]",
"date": "2017-11-30"
},
{
"vote": 11,
"title": "Artificial intelligence goes bilingual - without a dictionary",
"text": null,
"date": "2017-11-29"
},
{
"vote": 5,
"title": "What are 'head words' and 'lexical head' in parse trees?",
"text": "In the JurafskyMartin NLP textbook, a head tag in parse trees are mentioned that in lexicalized grammar, non-terminal in the tree is annotated with its lexical head.\n\n\nI don't actually get what lexical heads are. \n\n\nIn the image attached (\nhttps://imgur.com/a/ks4cz\n), the word inside the parenthesis is the head word. What exactly are these and how do we determine them?",
"date": "2017-11-28"
},
{
"vote": 2,
"title": "Looking for direction - trying to parse e-mails using NLP",
"text": "Hi everyone - hoping someone here can offer some assistance...\n\n\nA project I've been working on for the past while involves parsing incoming e-mails to determine whether any of an extensive (but finite) set of actions are required.  I've rolled my own algorithm that does some of this, but I feel that I should really start looking into the \"right\" way to do this sort of thing and read up on NLP, and perhaps machine learning...  I think?  I'm not sure if those concepts are what I really need.  I'm looking into some direction on where I should start.\n\n\nAs I mentioned, the tool I built already kinda works.  I'll briefly describe what it's meant to do and what I've done so far:\n\n\nWhen you get a text message message on an iphone it recognizes things like \"tomorrow night\" and asks if you want to add an event to your calendar.  My project is similar, except that that it's reads e-mails (as opposed to SMS), there are many more possible resulting actions, and I'd like it to be a lot smarter than just recognizing keywords.\n\n\nTo give an idea of the complexity I'd like to achieve, imagine that an e-mail said \"Dinner at your place tomorrow night?  Don't forget to bring wine.\"  The app would automatically create the appointment in the calendar, and also recognize that the user needs to bring wine, so will add a reminder half an hour before the meeting to get wine.\n\n\nI've built a version of this that works, to a degree.  I used Stanford NLP (NER, POS, SUTime) to recognize entities and identify keywords as part of a sentence.  I've also built in a capability to determine keywords' locations relative to one another (between sentences, paragraphs...).  Users need to \"teach\" the app by highlighting words and then logging actions through a custom generic interface.  A user can highlight a keyword and log multiple actions against it, incorporating nearby elements as content.  The next time a similar e-mail comes in \"Dinner at my place next week.  Don't forget to bring beer.\" the program will center on the word \"dinner\", and use the relative positioning and sentence structure to apply the same actions as before, this time with different parameters.\n\n\nWhat I have so far is finicky, and I feel that users will need to log many different variations of saying the same thing... things will get out of control.  \"Don't forget to bring wine to dinner tomorrow\" is so similar to the previous example, but my program won't know that, because \"don't forget\" is in the same sentence as \"dinner\" this time.  The user would need to log another action against this new structure.  I can throw band-aids at the process to clean things up here and there, but at it's core, this doesn't scale well.  My algorithm can't be the right way to do this.  \n\n\nSo, if you're still reading, thank you.  My question is, what should I be learning in order to deal with this problem?  I've played around with python beginner tutorials and Stanford NLP tools...  these are good at parsing the sentences but I don't see anything there about \"getting the bigger picture\".  I've looked into machine learning a bit, but that seems more to do with taking large datasets and deriving certain conclusions...\n\n\nWhat should I be looking into here?  Any chance that I'm on the right track and just need to make minor tweaks to the algorithm?\n\n\nAny other advice would be welcome as well.\n\n\nThank you so much for your time.\n\n\nTl;dr - What tools/concepts should I look into that can parse incoming e-mails into required actions?",
"date": "2017-11-28"
},
{
"vote": 1,
"title": "[Discussion] Tokenization before POS-tagging. It is absolutely necessary? Why do you think so?",
"text": "[deleted]",
"date": "2017-11-27"
},
{
"vote": 3,
"title": "Question about Max Margin Loss equation (from Stanford's deep learning NLP course)",
"text": "Here's the specific equation in question\n\n\nhttps://youtu.be/uc2_iwVqrRI?t=50m6s\n\n\nThis is where some of those terms are defined\n\n\nhttps://youtu.be/uc2_iwVqrRI?t=49m10s\n\n\nMy question, is how is there 2 s's in the loss equation? Do you only take one sample at a time, so that one of the s's is always 0? Or is it summed over several sentences?",
"date": "2017-11-26"
},
{
"vote": 4,
"title": "Thomas Friedman Op/Ed Generator",
"text": null,
"date": "2017-11-25"
},
{
"vote": 2,
"title": "[Advice needed] Loss function for a copy-augmented seq2seq",
"text": null,
"date": "2017-11-25"
},
{
"vote": 8,
"title": "Natural Language Interface for Web Scripting",
"text": null,
"date": "2017-11-24"
},
{
"vote": 1,
"title": "More than a Million Pro-Repeal Net Neutrality Comments were Likely Faked",
"text": "[deleted]",
"date": "2017-11-24"
},
{
"vote": 5,
"title": "Use sentence for knowledge base lookup",
"text": "I've a set of claims from politicians and I'd like to make something that does the following\n\n\nA) Take a claim like: \"Is not true that inflation rates in 2017 are 5% higher than in 2016\".\n\n\nB) Extract key terms: not | inflation rate | 2017 >= 2016*1.05\n\n\nC) claim is true or false\n\n\nThis is a simple sentence, but there are a lot of possibilities so there's no way to hard-code this using simple regex patterns.\n\n\nMy current approach is applying PoS to make some syntactic analysis in order to select only the useful words but after a first run I don't really know how to make something scalable with this.\n\n\nI've been trying to find some information about how to do it properly, looking at Dan Jurafsky new book and some other courses but I can't seem to find something similar to my problem. The most similar problem is the Question Answering one, but I think that QA is simpler because you don't have to make a final true|false claim.\n\n\nDo you know what could be a good approach to this problem or where can I find useful examples/theory?",
"date": "2017-11-23"
},
{
"vote": 5,
"title": "Predicting if website URL is human made or computer",
"text": "So DGA (domain generation algorithms) exist that can generate domain names on the fly, some of which are even pronounceable. A malicious use of the same is done by botnets. \n\n\nSo what ML/NLP method can be used in order to determine with a certain success, if a domain if actually a legitimate one and not a DGA generated one? \n\n\nMy dataset would just be set of domains. The challenge is to segregate DGA generated domains form authentic domains.\n\n\nTechniques like dictionary word overlap matching etc. don't work as modern day malicious DGA are very sophisticated.\n\n\nAny ideas, pointers would be great!\n\n\nBtw, by a domain I mean an URL like \nwww.example.com",
"date": "2017-11-22"
},
{
"vote": 1,
"title": "Best embedding technique for paraphrases detection?",
"text": "Hey guys, I'm working on the quora question pair dataset, I have a good net (I guess) but I feel like I lack in the embedding phase\n\n\nAt the moment I'm using spacy's sentence.vector to convert sentences in vectors.\n\n\nSince I could turn it into thesis work for university, I don't feel like copying directly from other's code, that's why I am asking:\nDo you have any reference, video, paper or else that I could use to improve my embedding?",
"date": "2017-11-20"
},
{
"vote": 1,
"title": "Best embedding technique for paraphrases detention",
"text": "[deleted]",
"date": "2017-11-20"
},
{
"vote": 6,
"title": "A basic overview comparing techniques for document/text similarity in Python.",
"text": null,
"date": "2017-11-19"
},
{
"vote": 2,
"title": "Document term weighing visualization - Using NIPS 2017 poster titles",
"text": null,
"date": "2017-11-19"
},
{
"vote": 10,
"title": "Approaching (Almost) Any NLP Problem on Kaggle",
"text": "[deleted]",
"date": "2017-11-18"
},
{
"vote": 15,
"title": "Sequence prediction: comparison between Hidden Markov Models and Maximum Entropy Markov Models",
"text": "I've just wrote two simple blog posts related with sequence prediction in NLP problems:\n\n\nThe first one analysis the Hidden Markov Model, and it's relationship with the Naive Bayes Model:\n\n\nhttp://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/\n\n\nThe second one, explains the limitations of the previous model, and introduces the Maximum Entropy Markov Model and it's relationship with Logistic Regression:\n\n\nhttp://www.davidsbatista.net/blog/2017/11/12/Maximum_Entropy_Markov_Model/\n\n\nI'm preparing a third one introducing Conditional Random Fields, to finish the loop =)\n\n\nAll the posts contain after the explanations references to code implementations of the models presented and also links to more in-depth tutorials and papers. Hope you enjoy reading it, as I did reviewing and writing about these models in detail :)",
"date": "2017-11-18"
},
{
"vote": 2,
"title": "ELI5: Viterbi Algorithm: Pseudocode",
"text": null,
"date": "2017-11-17"
},
{
"vote": 2,
"title": "Natural Language Processing Tool Dialogflow V2 API and Enterprise Beta",
"text": "[deleted]",
"date": "2017-11-17"
},
{
"vote": 2,
"title": "Using Word2vec for Concept Search",
"text": null,
"date": "2017-11-16"
},
{
"vote": 1,
"title": "Seq2Seq: Kaggle Notebook of Basic Conversational Model - Trained on Twitter Customer Support Data",
"text": "[deleted]",
"date": "2017-11-16"
},
{
"vote": 19,
"title": "Last Words: Computational Linguistics and Deep Learning",
"text": null,
"date": "2017-11-16"
},
{
"vote": 8,
"title": "Applying NLP and Entity Extraction To The Russian Twitter Troll Tweets In Neo4j",
"text": null,
"date": "2017-11-16"
},
{
"vote": 1,
"title": "Tracking Hackers with NLP and Machine Learning â€“ Center for Data Science",
"text": null,
"date": "2017-11-16"
},
{
"vote": 8,
"title": "Cornell Natural Language Visual Reasoning Dataset",
"text": null,
"date": "2017-11-15"
},
{
"vote": 1,
"title": "Thinc: Practical Machine Learning for NLP in Python",
"text": null,
"date": "2017-11-15"
},
{
"vote": 2,
"title": "Is there a favoured data structure for storing ambiguous parse trees?",
"text": null,
"date": "2017-11-15"
},
{
"vote": 1,
"title": "Choosing number of feature maps in a CNN for extreme multi-label text classification",
"text": "Hi there!\n\n\nA few months ago, a \nnice paper that uses a CNN for extreme multi-label text classification\n was released. \n\n\nI would like to draw inspiration from this paper for my own project. However, there is one detail in the choice of hyperparameters which I don't quite grasp.\nThe authors differentiate between small datasets that have less than 30k labels, and large datasets that have 500k labels or more.\nNow, intuitively, I would think that the large datasets are more complex and therefore require a model with higher capacity than the smaller ones. However, the authors set the number of feature maps to 32 for the large datasets and to 128 for the small datasets, without explaining why.\n\n\nDo you have an explanation why they do this? Maybe there is a fundamental misunderstanding on my side? Or has any previous studies indicated that it's better to do it this way?",
"date": "2017-11-14"
},
{
"vote": 3,
"title": "Does anyone know a good way to programmatically add contractions?",
"text": null,
"date": "2017-11-13"
},
{
"vote": 4,
"title": "I made a Markov text generator specifically to impersonate Donald Trump. What do you think? GitHub link in the comments",
"text": null,
"date": "2017-11-13"
},
{
"vote": 7,
"title": "Paragraph detection",
"text": "Has there been any research done on detecting a cohesive paragraph from a collection of sentences or even a sentence fragment? The idea here is if I take a paragraph and choose to randomly segment it, I could then recover the original paragraph or even discover if multiple lines of text can form a cohesive paragraph",
"date": "2017-11-11"
},
{
"vote": 1,
"title": "Can somebody point me to a good tutorial of fasttext.",
"text": "I do not seem to understand the code released by Facebook.",
"date": "2017-11-10"
},
{
"vote": 1,
"title": "ModernMT/MMT",
"text": null,
"date": "2017-11-09"
},
{
"vote": 4,
"title": "NLG for Reports Tutorial",
"text": "Does anyone know of a guide or tutorial that is specific to generating written interpretations of data? Ideally, I'd like to input some data (# of visitors to a website, change in bounce rate, etc.) and get back a written summary (\"the bounce rate increased 50% from last month\").",
"date": "2017-11-09"
},
{
"vote": 3,
"title": "Understanding Context Free Grammar sentence generation",
"text": "I am currently learning Context Free Grammar sentence generation, but I am still not clear how it exactly works.\n\n\nI understand the following rules generates the sentence \"The dog laughs\"\n\n\nS -> NP VP\nNP -> DT N\nVP -> VB\nDP -> \"the\"\nN -> \"dog\"\nVB -> \"laughs:\n\n\nBut I don't understand how we should generate rules based on a sentence. Is there like a basic step to follow when we generate a grammar rule? For example, what are the steps of generating the sentence \"They operate ships and banks.\" ?",
"date": "2017-11-09"
},
{
"vote": 26,
"title": "spaCy v2.0 released!",
"text": null,
"date": "2017-11-09"
},
{
"vote": 2,
"title": "Extract data using OCR and identify values",
"text": "I am wondering what is involved here. There are faxes and there could some handwritten content  in the fax. Using OCR a team here extracts the data.\nNow we have to identify what the content is. The content could be individual dates or something else.\nHow does ML or NLP help in this case ?\nIs there a way to train a model to learn what the text means by feeding it faxes ? I am told that there are some identifiers like 'Name' or 'DOB' that could give us an indication of what the following text is.\nAny papers deal with this ?",
"date": "2017-11-08"
},
{
"vote": 1,
"title": "Expected word count question",
"text": "I'm reading \nthis\n paper by David Blei and Sean Gerrish. I was trying to understand the first paragraph in page 14 where they describe a \ntest\n statistic. This statistic is calculated like so: \n(Observed word count in the corpus - Expected word count in corpus)/\nsqrt(Expected word count)\n\n\nDoes anyone know what Expected Word Count is? Or how to calculate it?",
"date": "2017-11-07"
},
{
"vote": 7,
"title": "inten.to comparison of translation prices @ 100M chars / month",
"text": null,
"date": "2017-11-07"
},
{
"vote": 1,
"title": "Can someone give me a tsv file for mood classification?",
"text": "I want a TSV file that I can train my classifier on so it can detect happy and sad sentences. Also, some code advice would be much appreciated I'm new to NLP :p Thanks!",
"date": "2017-11-07"
},
{
"vote": 4,
"title": "Language Technology M.Sc. programme at University of Saarland: a couple of questions",
"text": "Hi,\n\n\nDo somebody aware of \nLanguage Science and Technology Master's programme\n at University of Saarland? Or do you know somebody who studied there? I'm seeking for a Master's programme in a field of NLP, and this programme looks nice, but I have some questions about it.",
"date": "2017-11-06"
},
{
"vote": 6,
"title": "Some Natural Language Processing: Using Trigram Hidden Markov Models and Viterbi Decoding to Tag Genes in Biological Text in Python",
"text": null,
"date": "2017-11-06"
},
{
"vote": 8,
"title": "Advice on how to find more information about Google's rank brain and how it interprets text.",
"text": "Hi,\n\n\nI'm looking for academic papers or anything that gets into the nitty gritty of how google interprets users search queries and evaluates pages. \n\n\nI have a fairly solid foundational understanding of how pagerank works but I want to focus more on the NLP behind google. \n\n\nLooking at some of the things google puts out, it looks like they're using tfidf, word2vec, and LSI/LDA in some kind of combination but I'm looking to get a more detailed treatment on the subject. \n\n\nAny leads would be greatly appreciated! Thanks",
"date": "2017-11-04"
},
{
"vote": 1,
"title": "Berlin's Ada raises $47M to become the Alexa of healthcare",
"text": null,
"date": "2017-11-04"
},
{
"vote": 0,
"title": "fasttext.js",
"text": null,
"date": "2017-11-03"
},
{
"vote": 12,
"title": "This Startup's Artificial Voice Sounds Almost Indistinguishable From A Human's",
"text": null,
"date": "2017-11-03"
},
{
"vote": 5,
"title": "RFC: NLPGuide/2017 - the year in review",
"text": null,
"date": "2017-11-01"
},
{
"vote": 9,
"title": "Topic Modeling for Humans with Phrase Detection",
"text": null,
"date": "2017-11-01"
},
{
"vote": 11,
"title": "Word vectors with tidy data principles, word counts, and matrix factorization",
"text": null,
"date": "2017-10-30"
},
{
"vote": 1,
"title": "Massage Therapy Toronto",
"text": null,
"date": "2017-10-30"
},
{
"vote": 12,
"title": "Google no longer lets you change domains to search different countries",
"text": null,
"date": "2017-10-29"
},
{
"vote": 1,
"title": "South Jersey Electricians",
"text": null,
"date": "2017-10-28"
},
{
"vote": 9,
"title": "What interesting NLP applications can you imagine creating from the JFK files?",
"text": null,
"date": "2017-10-27"
},
{
"vote": 2,
"title": "Contracting creation of a NLP text generator",
"text": "Hello,\n\n\nI'm looking to contract someone to create a NLP algorithm that generates text based on (or heavily influenced by) a ~175,000 word corpus from a specific writer. I've played around with intro-level sample algorithms and things like Botnik Writer without much success (this isn't anything close to my area of expertise), so I was hoping that I could find someone in this community that would be interested.\n\n\nIdeally this would be something that I could plug a seed into and get X words back, but like I said, I'm not an expert, so I'm flexible on what exactly the end product would look like. Also, I'm more than happy to compensate for the work!\n\n\nPlease send me a DM if you're interested or have any questions.",
"date": "2017-10-26"
},
{
"vote": 1,
"title": "How To Get Working Capital For Small Business",
"text": null,
"date": "2017-10-26"
},
{
"vote": 1,
"title": "Current standard for text prediction dataset?",
"text": "Hey, I've got a new recurrent NN model I want to test on problems outside of my specific field. Sorry for the noob question, I'm not really in the NLP space - could you suggest a dataset for \"next word or sequence of words\" prediction that is publicly available and has current benchmarks to compare my model to?",
"date": "2017-10-25"
},
{
"vote": 1,
"title": "Satyabrata Chatterjee",
"text": null,
"date": "2017-10-24"
},
{
"vote": 3,
"title": "Sentic.net has anyone used it",
"text": "Hi, I came across \nhttp://sentic.net\n and found the approach very elegant, did anyone used it yet?\n\n\nIs it as powerful as it seems?",
"date": "2017-10-24"
},
{
"vote": 0,
"title": "Question regarding different language",
"text": "[removed]",
"date": "2017-10-23"
},
{
"vote": 20,
"title": "Word embeddings in 2017: Trends and future directions",
"text": null,
"date": "2017-10-21"
},
{
"vote": 1,
"title": "LSA or any association between cue-target index given a priming concept?",
"text": "Hey guys, \nI am currently looking for any database of associations between target- cue words (backwards association would be nice as well) if given a priming concept. For example: \"how much does the color red make you think of a fire truck? \" versus  \"how much does the red make you think of a fire truck? \"The research ive seen shows this changes the semantic associations (albeit slightly). Any help would be appreciated.",
"date": "2017-10-21"
},
{
"vote": 3,
"title": "Need some advice on compiling a bilingual corpus",
"text": "I want to compile a parallel corpus (sorry that I didn't specify that in the title) of English and Lithuanian abstracts from medical papers that are available online as PDFs. A typical paper looks like \nthis\n, if you're wondering (although in some cases the English abstract is placed at the end of a paper).\n\n\nI'm a translator and would use this corpus for looking up medical terms in the context, so, as far as functionality  goes, paragraph-level alignment would be enough (sentence-level would be more functional, but wouldn't it also be as twice as complicated?)\n\n\nThis looks complicated to me, so I need your advice on some of the steps of corpus compilation.\n\n\n\n\nExtracting abstracts in both languages from PDFsâ€”how should I do it? The obvious procedure would be just to copy and paste those abstracts into TXT files, but, I guess, this wouldn't be the most effective way to extract the text. I've heard about such things as web scraping/mining, but I only got a basic knowledge of Python. I don't what would be more of a drudgeryâ€”copy-pasting those texts or trying to learn an advanced method to extract them. \n\n\n\n\nHow should I store those texts before putting them into an alignerâ€”should I store each abstract in a separate file or just have one big file for English abstracts and another for Lithuanian?\n\n\n\n\nAs for free alignment tools, I guess I don't have much choice. I'm going to use LF Aligner.\n\n\n\n\n\n\nGenerally, is there any any guide for compiling a parallel corpus that you could recommend? \nThis\n is the most practical one I've found so far, but it doesn't address the problem of acquiring texts for a corpus.",
"date": "2017-10-21"
},
{
"vote": 1,
"title": "How to classify text in unsupervised methodï¼Ÿ",
"text": "I have some chat blog data about travelling. And I want to classify which type of travelling the chater likes.\nWithout the labels, I surppose to use unsupervised classifier. After that maybe supervised.\nI think of using bag of words to vectorized the document, the clustering.\nIs there any other method to dealing it.\nIs there any articles or open source about unsupervised classifer in \nNLP?",
"date": "2017-10-19"
},
{
"vote": 4,
"title": "Looking for interesting projects with Github data.",
"text": "Hi. I'm looking for an NLP project on Github data. It contains a lot of potentially interesting text fields like commit messages, issues, pull requests (title/body) etc. Any ideas would be appreciated.",
"date": "2017-10-16"
},
{
"vote": 3,
"title": "What type of dataset am I looking for?",
"text": "I'm looking to perform contextual analysis on the evolution of the word \"intelligence\". Specifically, I'm curious to see what type of nouns this adjective has modified historically (around ~60-100 years).\n\n\nBut, I am not sure what type of dataset to hunt for â€“ news articles, books? I'm looking for something generally representative of the English-speaking public's use of this word.",
"date": "2017-10-15"
},
{
"vote": 8,
"title": "Sequence Pair Classification using Sequence-Semantic-Embeddings (SSE), an encoder toolkit for NLP.",
"text": null,
"date": "2017-10-07"
},
{
"vote": 21,
"title": "New models for spaCy 2 alpha -- now near state-of-the-art (NER: 86.4 F on OntoNotes; parsing: 94.4 UAS on WSJ)",
"text": null,
"date": "2017-10-05"
},
{
"vote": 10,
"title": "Rasa Core kicks up the context for chatbots",
"text": null,
"date": "2017-10-04"
},
{
"vote": 7,
"title": "Almost anything you want, in one demo.",
"text": null,
"date": "2017-10-04"
},
{
"vote": 7,
"title": "Flask app using LDA to suggest GitHub repositories based on what you have starred.",
"text": null,
"date": "2017-10-02"
},
{
"vote": 3,
"title": "How to display dependency tree in spacy using program?",
"text": "I can check dependency tree using displaCy : \nhttps://demos.explosion.ai/displacy/\n\n\nBut I want to display using program in spacy, is it possible? \n\n\nI can check individual components such as ROOT, nsub etc. from sentence using spacy, but i want to check complete dependency tree.",
"date": "2017-10-01"
},
{
"vote": 1,
"title": "How Syntactic Dependencies is useful in word alignment for textual similarity?",
"text": "I am reading research paper \"Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence\" , and author is performing syntactic dependencies, but I am unable to find reason how it is useful for alignment of words",
"date": "2017-10-01"
},
{
"vote": 9,
"title": "Beautiful soup? justtext? dragnet? Not super happy with any of these. Maybe I'm using them wrong? Advice on the best way to reliably pull text from a large number of varied websites. Beginner ooking for advice from the veteran data miners.",
"text": "I'm working on a project that pulls text from sites. At one point or another, I've used all of these packages and I feel like I find instances where they let me down. \n\n\nIs there any advice you guys can give me on a strong way pull text from pages?\n\n\nMaybe running them all and having a way to dynamically choose among their outputs???\n\n\nNot really sure tho",
"date": "2017-09-28"
},
{
"vote": 3,
"title": "Explainable document classification",
"text": "Hi,\nI am currently working on identifying vulnerable customers (ill, learning challenges, addiction, etc.) from communications with companies. It is quite important that the models created are explainable.\nOther than decision trees what are my options for explainable models? I was thinking about trying a neural model with an attention mechanism and returning an interpretation of the attention mechanism as a justification of the decision.",
"date": "2017-09-28"
},
{
"vote": 10,
"title": "A Random Walk Through EMNLP 2017",
"text": null,
"date": "2017-09-28"
},
{
"vote": 15,
"title": "Is there a place where I can find problem sets for basic NLP ?",
"text": "I am taking an NLP course in my college, and I would like to find some problem sources related to basic n-gram probability, smoothing, POS tagging, and some other intro-level NLP topics.\n\n\nAny kind of source would be appreciated. Thanks!",
"date": "2017-09-27"
},
{
"vote": 7,
"title": "This text viz helps you see top issues being discussed in any topic from hundreds of google news results!",
"text": "This has been created using \nforce-directed graphs\n and Google News. It's updated ~ realtime. We created it because you can search Google Trends for hadoop, but you won't get results when you drill down into eg: manufacturing. See top issues being discussed in a topic of interest at senseihub.com! Here are a few more examples: \n\n\n\n\n[human capital](\nhttps://www.senseihub.com/human\n capital)\n\n\nbiohacking\n\n\nexoskeleton",
"date": "2017-09-26"
},
{
"vote": 1,
"title": "The 05 Most Spoken Languages in The World",
"text": null,
"date": "2017-09-23"
},
{
"vote": 5,
"title": "Starspace - Learning embeddings for classification, retrieval and ranking.",
"text": null,
"date": "2017-09-21"
},
{
"vote": 10,
"title": "[Project] A PyTorch implementation of Paragraph Vectors (doc2vec)",
"text": "I'm implementing a library for training paragraph vector models as proposed by Q. V. Le et al. (Distributed Representations of Sentences and Documents).\n\n\nThe code is available on GitHub: \nhttps://github.com/inejc/paragraph-vectors\n\n\nI would appreciate any kind of feedback. Contributions in any form are also more than welcome (I have already opened some issues regarding future work).",
"date": "2017-09-20"
},
{
"vote": 6,
"title": "Question: Cluster conversational text based on topics",
"text": "Hi guys,\nhow would one go about clustering conversational text by topic? The text is unlabeled but some have named entites.\n\n\nI've tried an average weighted TF-IDF with w2v and got ok results. My next step would be to try Doc2Vec. For now I've ignored the entities, didn't do anything special with them.\n\n\nBut I have one problem with it, let's say I have a long group chat and last week we've talked about the birthday of Tim and today we're talking about the birthday of Mike. With Doc2Vec and my TF-IDF approach I would put these two topics in the same cluster.\n\n\nIs there a way to separate my previous example? I was thinking about including a time factor, but I'm not sure this would be a good approach and how to go about it.",
"date": "2017-09-19"
},
{
"vote": 15,
"title": "Highlights of EMNLP 2017 - Exciting Datasets, Return of the Clusters, and more!",
"text": null,
"date": "2017-09-18"
},
{
"vote": 1,
"title": "Cooking Fever Gems Hack",
"text": null,
"date": "2017-09-18"
},
{
"vote": 5,
"title": "What is 'featuresets'? Can someone ELI5 please?",
"text": "I am trying to learn NLP with python on my own and now I feel I have been stuck and frustrated. Wherever I go, I will read the code about featuresets, from blogs or youtube tutorials on NLP. But they never really explain what it is and why it's important. What is it really? I don't usually use it and there hasn't been any problem so far. Usually I do the preprocessing stuff with NLTK then put the data straight to TfIdfVectorizer, then put it in a classifier. It works fine and I get my accuracy just fine. But now I bump into a case where my accuracy is really low and I wonder if not using this 'featuresets' is what causes it. What is the difference between having 'featuresets' and not? \n\n\nI hope this is clear enough, but if it is not, please tell me and I will edit the question. Thanks!",
"date": "2017-09-17"
},
{
"vote": 8,
"title": "Question matching via deep learning",
"text": "Say you have two questions\nâ€œWhat are the best ways to lose weight?â€,\nâ€œWhat are effective weight loss plans?â€\n\n\nAnd you want to determine if these are in fact the same question just with different phrasing. What are some good papers / github repos that tackle this problem?\n\n\nI stumbled across this post by quora (quora engineering):\n\nhttps://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n\n\nHowever I'm finding that their answer is not all that detailed.",
"date": "2017-09-14"
},
{
"vote": 7,
"title": "Question about doing an undergrad-level NLP research.",
"text": "I am a senior in college and I got to do an NLP research with a professor. But he wants us to find the project by ourselves. I was wondering what could be a good NLP project to work on as an undergrad and a non-NLP expert. \n\n\nTwo specific questions I have are:\n\n\n\n\nHow do we know what kinds of available projects are out there?\n\n\n\n\nHow do we know if the project(or task) still has a space to improve?\n\n\n\n\nHow do we know if it's an workable project for an undergrad student?\n\n\n\n\n\n\nAny kind of advice would be great. Thanks!",
"date": "2017-09-13"
},
{
"vote": 2,
"title": "Using NLP to build a trustworthy Twitter bot",
"text": null,
"date": "2017-09-12"
},
{
"vote": 14,
"title": "State of the art named entity recognition?",
"text": "What is the state of the art in named entity recogntion currently? I am looking in particular for time, date extraction and monetary value extraction through numbers and currencies. Without a large labelled data set how could you evaluate named entity recognition system? Just looking at papers?",
"date": "2017-09-12"
},
{
"vote": 1,
"title": "Hybrid Code Networks",
"text": "Yes, like the rest of the world I'm looking to implement a (potentially rather complicated) chat bot. Sue me. \n\n\nI suffer from extreme analysis paralysis so I'm looking to feed my addiction to thinking too much with some academic inspiration.\n\n\nI recently read Williams' paper on \nhybrid code networks\n and thought it was really good, and I'm just wondering if anyone can recommend any similar work in the space of dialogue management.\n\n\nSpecifically as it relates to:\n\n\n\n\ndialogue state tracking\n\n\nreinforcement learning with imitation\n\n\nwizard of oz simulations\n\n\npretty much any method of reducing the necessary amount of training data\n\n\n\n\nI'm particularly interested in ways one might start as a hand-coded expert system and transition into an RL/ML based end to end system as seamlessly as possible.\n\n\nObviously I don't expect any papers that address exactly all of these things, but some nudges in the right direction would be greatly appreciated.",
"date": "2017-09-11"
},
{
"vote": 15,
"title": "AllenNLP - An open-source NLP research library, built on PyTorch",
"text": null,
"date": "2017-09-09"
},
{
"vote": 5,
"title": "Anyone here familiar with Document Frequency Thresholding?",
"text": "I think this might be the right sub, but if it's not I am sorry. It's still about NLP anyway. \n\n\nI'm still new to the topic and still trying to familiarize myself with a lot of things. Recently I got myself to learn about Tf-Idf, but apparently it's not enough for my research. I read about this feature selection technique called Df Thresholding. Basically, you use some kind of thresholds to decide whether certain features (terms) really contribute to the classification or not by looking at its document frequency (Df). I've tried to google it but every search result only points to Tf-Idf article and not the Df thresholding I am trying to learn. I wonder if you guys could link me to some articles or shed me some light on it. Thanks.",
"date": "2017-09-06"
},
{
"vote": 0,
"title": "Knowledge",
"text": "[removed]",
"date": "2017-09-06"
},
{
"vote": 4,
"title": "Parsing multiple questions contained in one single user query",
"text": "If a single query from the user contains multiple questions belonging to different categories, how can they be identified, split and parsed?\n\n\nEg -   \n\n\nUser - what is the weather now and tell me my next meeting  \nParser - {:weather =&gt; &quot;what is the weather&quot;, :schedule =&gt; &quot;tell me my next meeting&quot;}  \n\n\n\nParser identifies the parts of sentences where the question belongs to two different categories\n\n\nUser - show me hotels in san francisco for tomorrow that are less than $300 but not less than $200 are pet friendly have a gym and a pool with 3 or 4 stars staying for 2 nights and dont include anything that doesnt have wifi  \nParser - {:hotels =&gt; [&quot;show me hotels in san francisco&quot;,  \n          &quot;for tomorrow&quot;, &quot;less than $300 but not less than $200&quot;,\n          &quot;pet friendly have a gym and a pool&quot;,\n          &quot;with 3 or 4 stars&quot;, &quot;staying for 2 nights&quot;, &quot;with wifi&quot;]}\n\n\n\nParser identifies the question belonging to only one category but has additional steps for fine tuning the answer and created an array ordered according to the steps to take\n\n\nFrom what I can understand this requires a \nsentence segmenter\n, \nmulti-label classifier\n and \nco-reference resolution\n\n\nBut the \nsentence segementer\n I have come across depend heavily on grammar, punctuations.   \n\n\nMulti-label classifiers\n, like a good trained naive bayes classifier works in most cases but since they are multi-label, most times output multiple categories for sentences which clearly belong to one class. Depending solely on the array outputs to check the labels present would fail.  \n\n\nIf used a multi-class classifier, that is also good to check the array output of probable categories but obviously they dont tell the different parts of the sentence much accurately, much less in what fashion to proceed with the next step.\n\n\nAs a first step, how can I tune sentence segmenter to correctly split the sentence without any strict grammar rules.\n Good accuracy of this would help a lot in classification.\n\n\nPlease pardon me if this question comes across naive. It would be helpful if you could point out where to be correct. I have only started using reddit and trying to making sure I conform the the guidelines",
"date": "2017-09-04"
},
{
"vote": 1,
"title": "Side Effects of Chemo!",
"text": null,
"date": "2017-09-03"
},
{
"vote": 3,
"title": "I have a basic NLP script using gensim but I think it's written pretty poorly. Can someone advise me from the 10,000' view of how this should be designed? Ive never done this before so I'm really just taking shots in the dark here and would really appreciate some guidance.",
"text": "So the purpose of the program is to get 15 documents from 15 different urls. Compute tf-idf + LDA for them, then grab a new document and compare it to the original 15 and display the results.\n\n\nTwo of the biggest requirements as I see them..\n\n\n\n\nI want to build a website interface for this program. I'd also like to be able to experiment with different stop word lists and lemmatizers as well add word2vec at some point. I want to be able to somewhat easily compare the results and performance of different NLP methods/libraries without needing to go out and get the original 15 documents again. \n\n\n\n\nI want to have a 'user login' function so that different users can pick up where they left off and see the all the results of their personal previous queries.\n\n\n\n\n\n\nThis indicates to me that I'll need some type of database - so I guess the first question would be, what database to do you guys prefer for storing raw text, preprocessed text, dictionaries, corpora and word vectors???\n\n\n(to be clear, if it was just raw text, i'd go with elasticsearch but with the addition of dictionaries and corpora, I'm not really sure how that changes the problem)\n\n\nGensim has a function that helps with saving corpora and dictionaries but I'm not sure how this should best be integrated with a database??\n\n\nThe other big question is about object oriented best practices.\n\n\nIn my head, I'm imagining I should make a class object like '15_document_corpus', and then give it attributes like 'raw_txt', 'preproccessed_text', etc, etc. \n\n\nAs well at methods like 'make_dict', 'make_corpus', 'compute_tfidt', etc.\n\n\nI feel like the benefit of that is that I can save and load the whole object when I want to visualize the data. I imagine this will also help with the user login feature when someone wants to load a previously processed corpus + data visulization.\n\n\nAgain though, I've never done anything like this before so these are all just wild guesses from someone totally inexperienced. \n\n\nAny help here is greatly appreciated!",
"date": "2017-09-02"
},
{
"vote": 11,
"title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
"text": null,
"date": "2017-09-02"
},
{
"vote": 1,
"title": "FrameNet parser in Java?",
"text": "Does anyone know of any good FrameNet parser \nwritten java\n?",
"date": "2017-09-01"
},
{
"vote": 7,
"title": "RFC on language Python package",
"text": "I'm opening up some very basic low-level building blocks that I use for working with text data.\n\n\nIt's intended to be used by code that processes text at scale and across many languages, so it doesn't pull in dependencies or large data files or do things that only work for English.  Often it's simple, but it is not something that should be implemented inside a machine learning pipeline.\n\n\nFor example, extracting n-grams from a string.  The usefulness of char-level n-grams is appreciated more and more, especially for morphologically rich languages, which is most of them, especially for languages with fewer data, which is most of them, for all types of applications.\n\n\nAnd self-contained code for extracting char-level n-grams is easy to hack together:\n\n\n> ng = lambda s, n: list(zip(*[s[i:] for i in range(n)]))\n> ng('This is not a test.', 3)\n\n\nBut in practice we often only want n-grams within words, not across words.  And it makes sense for that kind of code to be open and common, to have more features and more eyes on it, and fewer names and conventions.\n\n\nSo I've put out the n-grams code in an initial release of the \nlanguage\n Python package (\nGitHub\n | \nPyPI\n).\n\n\n> pip install language\n\n\nI am seeking feedback on what belongs in there, what does not, what the structure should be and any other tips on developing and maintaining an open-source NLP library.\n\n\nFor now the basic structure is:  \n\n\nlanguage.languages\n (coming soon)\nBasic information on languages, for example equivalent and related ISO codes, and which chars are in which language.\n\n\nlanguage.chars\nMostly I want to expose and build on the unicodedata categories.  For example, finer-grained categories for things like quotation marks, and some conservative canonicalisations.\n\n\nlanguage.tokens\nDumb but robust language-agnostic tokenisation.  It should also take good guesses about so-called shapes - \n&quot;3d&quot;\n, \n&quot;1st&quot;\n, \n&quot;https://youtube.com&quot;\n, \n&quot;don&#039;t&quot;\n, \n&quot;NamedEnt&quot;\n, \n&quot;ACRNYM&quot;\n.\n\n\nlanguage.ngrams\nWord- and char-level n-grams and associated operations like diffing and matching.\n\n\nEach submodule depends only on the previous ones.",
"date": "2017-09-01"
},
{
"vote": 2,
"title": "DeepL schools other online translators with clever machine learning",
"text": "[deleted]",
"date": "2017-08-31"
},
{
"vote": 6,
"title": "What's the best way to compare several corpora in natural language? (x-post r/machinelearning)",
"text": "Hi everyone,\n\n\nI've been doing LDA topic models of narrative reports in natural language for a research project. I have several smallish corpora (from 1400 to 200 docs each â€“ I know, that's tiny!) that I'd like to compare, but I don't know how to do that beyond looking at each LDA model (for instance with pyLDAviz). My academic background is not in CS, and I'm still a bit new to NLP.\n\n\nWhat are some good ways to compare topics across corpora/topic models? For instance, is it possible to estimate how much two LDA models overlap? Or are there other ways to assess the topic similarity of several corpora?\n\n\nThanks in advance for your help!",
"date": "2017-08-31"
},
{
"vote": 12,
"title": "DeepL Translator",
"text": null,
"date": "2017-08-30"
},
{
"vote": 6,
"title": "Word embedding demo",
"text": null,
"date": "2017-08-30"
},
{
"vote": 1,
"title": "What is the best way of generating sentences with similar semantics?",
"text": "Given a seed sentence, how to effectively generate a bunch of sentences having equivalent or similar semantics as the original one? Any paper or established method for this task?",
"date": "2017-08-28"
},
{
"vote": 7,
"title": "[Chatbot] How to break down FAQs into intents and entities?",
"text": "So for work we are starting to design a chatbot for an FAQ system.  I have all of the questions and answers organized into general categories, but I wanted to get some ideas/second opinions on how to process everything to break down intents and entities.  \n\n\nMy first step was going to be taking each category and looking for the main verb in each question.  Then, once I see the main verb in the question, extract the most significant nouns out of it and get a term frequency.  \n\n\nI haven't done a whole lot of NLP so I don't know anything else I should do to it.  I mean I know that if the same  verb is extracted from multiple questions in one section and different nouns are pulled out (so verb1 in sentence 1 extracts a,b, and c while verb1 in sentence2 extracts b,d, and e), I could do some matching or something with a tf-idf vector, but I don't know what else I could do.  Would anyone here have any ideas?",
"date": "2017-08-28"
},
{
"vote": 1,
"title": "Texts with the most important phrases or sentences highlighted / tagged in some way",
"text": "I am looking for texts with the most important phrases or sentences highlighted / tagged in some way. I am mostly interested in English, and any type of document.",
"date": "2017-08-27"
},
{
"vote": 0,
"title": "Text Visualization in Python",
"text": "[deleted]",
"date": "2017-08-27"
},
{
"vote": 7,
"title": "Rust library for natural language recognition",
"text": null,
"date": "2017-08-25"
},
{
"vote": 6,
"title": "Where do you find topical text corpora?",
"text": "Anybody know of a good resource to search for a domain-specific text corpus? \n\n\nI'm attempting to train my own word embeddings but my current corpus is rather small and if I could supplement it with an applicable corpus (rather than the general common crawl/Wikipedia trained embeddings) that would be helpful. Thanks!",
"date": "2017-08-23"
},
{
"vote": 2,
"title": "Named Entity Recognizer for products",
"text": "Are there any trained models for detecting named entities which are products, such as Iphone or Android?",
"date": "2017-08-20"
},
{
"vote": 2,
"title": "Figuring out the scope of an English sentence syntax parser for a bachelor's essay",
"text": "(If this is the wrong place to ask this, please just lmk.)\n\n\nFor a bachelor's essay, I have been wanting to create an english sentence syntax tree parser using a combination of Stanford's part of speech tagger and ANTLR.\n\n\nThe generative part of the project would be to write the grammar rules and, if I make it that far, some kind of weighting system to resolve ambiguity, both of which would obviously be rudimentary.\n\n\nMy college is lenient on essay topics (your \"essay\" can be a program; one of my inspirations is a student who wrote her own Spanish to English machine translator), but I'm unfamiliar enough with the topic that I'm not sure if \"only\" writing my own english grammar is a wide enough scope.\n\n\nIt's also entirely possible that there's some fundamental element to writing this parser that I've completely missed and will be blindsided by later.\n\n\nEssentially: Does writing the rules in ANTLR and tying that to other people's libraries sound like a realistic bachelor's essay topic? There seems to be plenty to learn about and experiment with, but I'm also worried I'm either undershooting or completely in over my head.",
"date": "2017-08-19"
},
{
"vote": 2,
"title": "State-of-the-art in humour generation?",
"text": null,
"date": "2017-08-17"
},
{
"vote": 4,
"title": "[Question] NLP Platform choice for chatbots",
"text": "Hi Forum,\n\n\nI am rather new to the world of NLP. I have a question on which technology to use. Here is my use-case.  \n\n\nI have a chatbot manages help desk tickets. user logs into slack, user needs to create an incident ticket, user needs to update incident ticket and close incident ticket.   \n\n\nCurrently the chatbot processes text base don Reg-ex and they have to be exactly what the bot expects. I want to leverage NLP so ,  \n\n\n\"get incident 123\", \"show me incident 123\", \"let me see incident 123\" for example map to the same intent. I know there are technologies such as API.AI where you can do all this, but google gets your data and if security is a concern it is a no-go.   \n\n\nSo with something like Stanford NLP, can i do this? does it have a functionality where it lets me map intent? How is intent processing different than NLP ?",
"date": "2017-08-16"
},
{
"vote": 3,
"title": "Anyone have a trained lemmatizer model for OpenNLP?",
"text": "I am using the standard dictionary that they use as an example. I am wanting a trained model for the lemmatizer, because this dictionary has many holes.",
"date": "2017-08-15"
},
{
"vote": 0,
"title": "What is Googleâ€™s Ability to Understand a Language?",
"text": null,
"date": "2017-08-15"
},
{
"vote": 6,
"title": "Cornell Movie Dialogs Corpus",
"text": null,
"date": "2017-08-15"
},
{
"vote": 3,
"title": "How advanced are plagiarism detection algorithms? Are there useful datasets online?",
"text": "[deleted]",
"date": "2017-08-14"
},
{
"vote": 1,
"title": "Professional Translation Services",
"text": null,
"date": "2017-08-14"
},
{
"vote": 11,
"title": "Get into NLP and Data Processing from humanities background",
"text": "[deleted]",
"date": "2017-08-13"
},
{
"vote": 3,
"title": "Looking for technique for fixing English grammar",
"text": "I have an ordered bag of words which I would like to transform it into a fluent language by fixing the grammar (adding the preposition, fixing word form and etc). What is the best way to do it?",
"date": "2017-08-12"
},
{
"vote": 2,
"title": "Interpolated Knesser Ney smoothing problem",
"text": "Can someone explain how to handle with lambda weight returning zero in highest order equition for interpolated Kneser-Ney method for language modelling? As all interpolated lower interpolated will be multiplied on zero, the total probability will be also zero",
"date": "2017-08-11"
},
{
"vote": 3,
"title": "Zero counts and as a consequence - zero probabilities in Kneser-Ney smoothing",
"text": null,
"date": "2017-08-10"
},
{
"vote": 1,
"title": "Would using lojban to create a language processing knowledgebase bea good idea?",
"text": "[deleted]",
"date": "2017-08-10"
},
{
"vote": 6,
"title": "State of the art mention-detection",
"text": null,
"date": "2017-08-10"
},
{
"vote": 4,
"title": "[P] Python module to easily generate text using a pretrained char-rnn â€¢ r/MachineLearning",
"text": null,
"date": "2017-08-10"
},
{
"vote": 3,
"title": "How to extract comments from websites?",
"text": "[deleted]",
"date": "2017-08-06"
},
{
"vote": 15,
"title": "Natural Language Processing with Small Feed-Forward Networks",
"text": null,
"date": "2017-08-04"
},
{
"vote": 6,
"title": "Dataset for chat bot / email analysis",
"text": "Hi guys,\n\n\nI'm working on a prototype for a service that going analysis for emails / chatbots between Customer Service / Sales and the actual customers.\n\n\nFor the prototype, in order to train my NLP and ML models, I want to find a dataset of emails / chats  between Customer service / sales and customers.\n\n\nDo you know of such DS? (I've tried working with Enron DS, but it does not suit my needs)\n\n\nthanks!",
"date": "2017-08-03"
},
{
"vote": 18,
"title": "Leipzig Corpora Collection - 271 Corpus-Based Monolingual Dictionaries for 236 Languages",
"text": "The Leipzig Corpora Collection presents corpora in different languages using the same format and comparable sources. All data are available as plain text files. They are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.\nThe corpora are identical in format and similar in size and content. They contain randomly selected sentences in the language of the corpus and are available in sizes from 10,000 sentences up to 1 million sentences. The sources are either newspaper texts or texts randomly collected from the web. The texts are split into sentences.",
"date": "2017-07-30"
},
{
"vote": 7,
"title": "Anyone know where I can read about AI programs that can write original content?",
"text": "I've seen a few articles out there about programs that write original content but I have no idea how they work. I'm just \n\n\nI want to try and learn more about them and see how they construct such coherent articles\n\n\nAny help is appreciated!",
"date": "2017-07-28"
},
{
"vote": 3,
"title": "Machine learning in C#",
"text": "Hi guys, im trying to use tokens in a document to classify by document category. I have plenty of data but with thousands of features (token frequency relative to corpus) I keep getting wierd errors in Accord C#. Anybody know any better c# machine learning libraries?",
"date": "2017-07-28"
},
{
"vote": 5,
"title": "Looking for parellel english/mandarin corpus data and your thoughts on cn-&gt;en translation and english correction",
"text": "[deleted]",
"date": "2017-07-27"
},
{
"vote": 1,
"title": "Looking for a parallel english/mandarin corpus and your thoughts on cn-&gt;en translation and english text correction",
"text": "[deleted]",
"date": "2017-07-26"
},
{
"vote": 4,
"title": "NLP in healthcare",
"text": "Hi, looking to find commonly used formats or patterns used in nlp concepts and or free text searches in healthcare. \nSpecifically in radiology. Do users look fir diseases? Procedures? \nThanks\nDave",
"date": "2017-07-26"
},
{
"vote": 4,
"title": "Are there any publicly available cross-lingual embeddings?",
"text": "[deleted]",
"date": "2017-07-25"
},
{
"vote": 11,
"title": "Extracting Keywords From Short Text",
"text": null,
"date": "2017-07-25"
},
{
"vote": 6,
"title": "The Benefits of Attention for Document Classification",
"text": null,
"date": "2017-07-25"
},
{
"vote": 10,
"title": "Looking for an introductory book on Natural Language Processing",
"text": "Hey guys, newbie here. I recently became interested in Natural Language Processing, and I was wondering if anyone could recommend a good book to get me started. I have a bachelors degree in English language and literature, and no actual knowledge about coding or programming.\nAny help is welcome, and thanks!",
"date": "2017-07-23"
},
{
"vote": 10,
"title": "Any good python book on NLP with spacy?",
"text": "I don't like nltk. Spacy API seems cleaner and modern.\nI see nltk as focusing on the small picture and requiring going through any task as a step by step process.\n\n\nSo any books using nlp with spacy?\nif not, any good python book on nlp?\n\n\n(There's one from o'reilly that was written centuries ago. There's any other from apress that looks promising but many reviewers are automated and the book gets an F from fakespot.com)",
"date": "2017-07-23"
},
{
"vote": 2,
"title": "Heavy Metal and Natural Language Processing - Part 3",
"text": null,
"date": "2017-07-22"
},
{
"vote": 6,
"title": "facebookresearch/fairseq (FB's answer to seq2seq)",
"text": null,
"date": "2017-07-21"
},
{
"vote": 7,
"title": "Iris: A Conversational Agent for Complex Tasks",
"text": null,
"date": "2017-07-21"
},
{
"vote": 3,
"title": "Open source tool for editing audio transcript",
"text": "[deleted]",
"date": "2017-07-20"
},
{
"vote": 2,
"title": "CHILDES text extraction",
"text": "[deleted]",
"date": "2017-07-19"
},
{
"vote": 3,
"title": "Classification of English Words Data Based on Class [Noun, Verb, Adjective, Preposition, etc]",
"text": "def Interpretation_be(sentence):\n\n     be = ( &#039;be&#039;, &#039;been&#039;, &#039;is&#039;, &#039;was&#039;, &#039;are&#039;, &#039;were&#039; )\n     place = (&#039;there&#039;, &#039;here&#039;)\n     gender_identity = (&#039;he&#039;, &#039;she&#039;)\n     indicator =(&#039;it&#039;, &#039;that&#039;, &#039;this&#039;, &#039;these&#039;, &#039;those&#039;)\n     exist =(&#039;exist&#039;)\n     equal=(&#039;equal&#039;)\n     condition=(&#039;condition&#039;, &#039;status&#039;, &#039;quality&#039;, &#039;state&#039;, &#039;situation&#039; &#039;circumstance&#039;) \n     number =(&#039;1&#039;, &#039;2&#039;, &#039;3&#039;, &#039;4&#039;, &#039;5&#039;)\n     consequence=(&#039;consequence&#039;, &#039;result&#039;)\n     to = (&#039;to&#039;) \n\n\n     if sentence.split()[1] in be:\n          if sentence.split()[0] in place:\n               print(sentence.replace(sentence.split()[1],&#039;exists&#039;))\n          if sentence.split()[0] in gender_identity:\n               print(sentence.replace(sentence.split()[1],&#039;equal to&#039;))\n          if sentence.split()[0] in indicator:\n               print(sentence.replace(sentence.split()[1],&#039;equal to&#039;))\n\n     if sentence.split()[1] in exist:\n           if sentence.split()[0] in place:\n               print(sentence.replace(sentence.split()[1],&#039;is&#039;))\n     \n     if sentence.split()[1] in equal:\n          if sentence.split()[0] in condition:\n                  print(sentence.replace(sentence.split()[1],&#039;is&#039;))\n          if sentence.split()[0] in number:\n                  print(sentence.replace(sentence.split()[1],&#039;is&#039;))\n          if sentence.split()[0] in consequence:\n                  print(sentence.replace(sentence.split()[1],&#039;is&#039;))\n\n     if sentence.split()[2] in to:\n           print(sentence.replace(sentence.split()[2],&#039;toward&#039;))\n           ##need to discern between preoposition to and infinitive to##\n                  \n##result_check##\nInterpretation_be(&#039;i have been to Spain&#039;)\nInterpretation_be(&#039;there was a cat&#039;)\nInterpretation_be(&#039;there is a woman&#039;)\nInterpretation_be(&#039;he is a boy&#039;)\nInterpretation_be(&#039;it is a cat&#039;)\nInterpretation_be(&#039;there exist a hat&#039;)\nInterpretation_be(&#039;situation equal to bad&#039;)\nInterpretation_be(&#039;1 equal one&#039;)\nInterpretation_be(&#039;consequence eqaul bad&#039;)\n              \n\n\n\nI am trying to make up English rephraser as a most basic start of natural language processing.\n\n\nWhile building up like above code, I had wanted to get some already-classified text data which criteria of classification is word-class, such as Noun, Verb, Preposition etc.\n\n\nAny good text data that I can obtain? \n\n\nPlease let me know.",
"date": "2017-07-19"
},
{
"vote": 9,
"title": "Are hard-coded grammar rules still in use / useful?",
"text": "I have been learning Japanese for a few years and also have a background in computer science.\n\n\nLike any diligent student, I study grammar rules.  It seems somewhat necessary as an adult human to learn the rules or at least practice a bunch of example sentences pared down to some specific grammar usage without too much extraneous stuff.\n\n\n\n\nMy question:\n\n\nI wonder to what extent are individual natural languages' grammar rules\n \nhard-coded\n \ninto parsers and translation engines these days?\n\n\n\n\nI'm interested to make my own translation app or extend something existing.\n\n\nI figured that as a starting point I'll code in dozens / hundreds of grammar rules....\n\n\n... with the side effect that I'll teach myself a bit more Japanese grammar just by being focused on these rules.\n\n\nBut I wonder how much the big boys like Google Translate and any small-fry translation/parsing engines (even open source...?) use these kinds of hand coded rules?\n\n\nCertainly I can imagine scenarios where the neural net is a black box and is given only example sentences and left to infer everything...\n\n\nOr scenarios where there is significant hand coding. In English we have grammar like \"the more... the more...\" Eg. The more I exercise, the more happy I am. Or (a variation) The more I exercise, the happier I am. Japanese likewise has a grammatical form for this kind of sentence. I have about a hundred grammar points I'm trying to learn as a human to get from an upper-beginner to an intermediate level of Japanese.  So I'd be thinking about hard coding these 100 grammar rules.\n\n\nNow I know there could be a bit of a combination going on, and that in casual speech rules get bent, broken, rewritten for dramatic effect... and that machine learning might get some of this flexibility better than hand coding.\n\n\nBut still I wonder, is it reasonable to hand code hundreds of grammar rules these days?\n\n\nI get the feeling that the consensus is no, but my gut feeling is it's still useful, and in some cases Google translate etc is actually stupider and more intractable than it should be.\n\n\nWhat do you think?",
"date": "2017-07-19"
},
{
"vote": 3,
"title": "Does anyone know a public chat database (like whatsapp chats) which I can use?",
"text": "I want to perform some analysis on chat conversations (in english) but I can't find the relevant corpus (as usual)",
"date": "2017-07-19"
},
{
"vote": 5,
"title": "Splitting a complex sentence (in the context of text simplification)",
"text": "Given an English text, I need to (1) identify sentences that are good candidates for split into several simpler sentences, and (2) perform the split. What is the current state-of-the-art in this field? Are there any tools I can use, or approaches that can be (relatively) easily implemented?\n\n\nThanks a lot in advance!",
"date": "2017-07-18"
},
{
"vote": 1,
"title": "Free Microsoft e-book giveaway with thousands of books. Grab 'em.",
"text": "[deleted]",
"date": "2017-07-17"
},
{
"vote": 15,
"title": "Ravings of One Angry AI and Aftermath of Two Scientific Conferences",
"text": null,
"date": "2017-07-17"
},
{
"vote": 12,
"title": "Information Extraction using Dependency Parse of a sentence.",
"text": "Hey guys, \nSo I was working on an assignment during an interview. They asked me to make a information extraction system which can extract some information from a bunch of pdf documents. \n\n\nExample Sentence 1 - The delivery and return amount will be rounded up to 10000 USD. \n\n\nAfter Extraction - I should extract that both delivery and return amount will be rounded up to 10000 USD.\n\n\nExample Sentence 2 - The delivery amount will be rounded up by 10000 units of base currency and the return amount will be rounded down up to the nearest integral multiple of 10000 units of base currecny.\n\n\nAfter Extraction - I should extract that delivery amount will be rounded up to 10000 units and return amount will rounded down to nearest 10000 units. \n\n\nI did a simple approach of getting a dependency parse tree of the sentence and then extracted tokens with text \"rounded\" with pos tag = VERB . Then I traversed the parse tree using children of these token and checked if I can extract everything. \n\n\nBut I was using custom rules like dependency should be 'prep', 'nsubj' and several other rules. I am sure that this rule based approach won't generalise. For any new unseen sentence these rules\n will certainly fail. \n\n\nAny learning based approaches on how to train a model for this kind of extraction preferably with lesser training data are appreciated. Thanks !\n\n\nEdit: Sorry if this is not the right subreddit to post this. I will delete the post if asked to.",
"date": "2017-07-16"
},
{
"vote": 2,
"title": "Easy Semantic Similarities (Java)",
"text": null,
"date": "2017-07-15"
},
{
"vote": 1,
"title": "Do-It-Yourself NLP for bot developers by Rasa.ai",
"text": null,
"date": "2017-07-13"
},
{
"vote": 12,
"title": "Text classifier algorithms: overview of main approaches with tutorials",
"text": null,
"date": "2017-07-12"
},
{
"vote": 5,
"title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
"text": null,
"date": "2017-07-11"
},
{
"vote": 6,
"title": "Downloading more than 20 years of The New York Times",
"text": null,
"date": "2017-07-11"
},
{
"vote": 2,
"title": "Downloading all English books from gutenberg.org with Python",
"text": null,
"date": "2017-07-10"
},
{
"vote": 2,
"title": "Text Classification on Imbalanced Medical Data",
"text": "[deleted]",
"date": "2017-07-09"
},
{
"vote": 8,
"title": "Does Spacy provide built-in Levenshtein distance, hamming distance?",
"text": "Hey, Is there any built-in function that Spacy provides for string similarity, such as Levenshtein distance, Jaccard Coefficient, hamming distance etcetra?",
"date": "2017-07-08"
},
{
"vote": 10,
"title": "If I want to do a job related to NLP, do I need a Masters Degree?",
"text": "I am currently an Undergrad majoring in CS. I became interested in Machine Learning, especially in NLP, recently. I was wondering if I want to do a job related to NLP, do I need a Masters degree?\n\n\nIf so, do I need a research experience on NLP?\n\n\nI would love any advice regarding my question.\n\n\nThanks!",
"date": "2017-07-08"
},
{
"vote": 12,
"title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation - Summary of 2017 shared task on computing semantic textual similarity.",
"text": null,
"date": "2017-07-06"
},
{
"vote": 3,
"title": "Natural Language Processing in iOS",
"text": null,
"date": "2017-07-05"
},
{
"vote": 6,
"title": "Efficient \"Dictionary-Based\" Text Rewriting using Subsequential Transducers",
"text": null,
"date": "2017-07-05"
},
{
"vote": 3,
"title": "I was reading about text classification and logistic regression. What does the right side mean ?",
"text": null,
"date": "2017-07-04"
},
{
"vote": 7,
"title": "Tools for data labelling (text)? (X-POST /r/MachineLearning)",
"text": "Before I go off and roll my own, are there any (open-source) frameworks available for labelling textual datasets?\nReally just looking for something simple that churns through files in a specified directory, displays the document on the left, a set of labels on the right, and saves the tags that each user applies to each document. Ideally multi-user (so it could be used on Mechanical Turk in future) but that's not a dealbreaker at this stage.",
"date": "2017-07-03"
},
{
"vote": 11,
"title": "InspiroBot - machine-generated quotes",
"text": null,
"date": "2017-07-01"
},
{
"vote": 12,
"title": "Interpreting Deep Neural Networks using Cognitive Psychology | DeepMind",
"text": null,
"date": "2017-06-29"
},
{
"vote": 5,
"title": "Need advice for matching a work experience description to job descriptions",
"text": "Hello, I'm completely new to the area of NLP and machine learning so please forgive my ignorance.\n\n\nI'm looking to work on a project that allows users to input a description of work experience (ie. bullet points describing what the person did at that role) as well as a set of N job descriptions, and then retrieve the K most relevant job descriptions based on the work experience provided.\n\n\nFor instance using the following (made up) work experience description:\n\n\nSoftware Developer at Company XYZ:\n\n\n\n\nImplemented a RESTful service to allow users to make payments using the mobile app using Java and the Play framework.\n\n\nDeveloped feature on mobile app to allow users to create and customize profiles using ReactJS.\n\n\nUsed tensorflow to train a model to predict user's spending habits.\n\n\n\n\nThen maybe we would find job descriptions that mention similar technologies (Java, Play, REST, ReactJS, tensorflow) or any \njob descriptions with similar tasks/domains such as mobile development, training models, machine learning, or working with payments.\n\n\nI've searched the web for information about how to accomplish this and NLP and machine-learning pop up a lot so it leads me to think\nI need to learn about them and figure out how to apply it to my problem. However, the fields are so vast that I'm not sure where to begin.\n\n\nI think in another thread I've seen things like \ntfidf, ngrams, and doc2vec\n pop up. I've done a bit of reading on what these things are,\nbut am not sure how to apply them to my problem yet.\n\n\nFrom poking around, it seems like some approaches try to match the work experience description to job descriptions purely based on \nword frequency similarity (how often words in work experience appear in job descriptions.) and others look for similarity in the meaning/intent (semantic similarity?). I'm still sure which approach I should go for, or both.\nI'm not actually sure if I\"m using those words correctly, so again, apologies if it doesn't make sense.\n\n\nOne thing I'm concerned about is that training a good model in machine learning seems to require a lot of \"training data\" (1000s to millions) and I don't\nreally have access to that many work experience descriptions or job descriptions at the moment.\n\n\nAny guidance on how to get started would be much appreciated!",
"date": "2017-06-26"
},
{
"vote": 9,
"title": "Kazakh Stop Words List",
"text": null,
"date": "2017-06-24"
},
{
"vote": 6,
"title": "Searching for text normalization libraries/tools",
"text": "Hi there,\n\n\nNot so long ago I presented here \ncucco.io\n (still work to do on the page and the API is not public yet), my little project for text normalization in Python.\n\n\nI'm using this project as part of a master degree project and one of the requisites for the documentation is to present alternatives. The thing is that when I started the project I didn't find anything related with the normalization of texts and today I've been actively searching and I still haven't found anything. I mean, NLKT is there but it's more oriented to classification, and there is a Python library out there with a bit of support for this, but can't find anything else.\n\n\nDo you know, as experts on the topic, about any alternative?\n\n\nThank you in advance,\nDavid",
"date": "2017-06-23"
},
{
"vote": 3,
"title": "Jenny Starchat: An Open Source, scalable conversational engine for virtual agents",
"text": null,
"date": "2017-06-23"
},
{
"vote": 8,
"title": "Improving text classification with vectors of reduced precision",
"text": null,
"date": "2017-06-21"
},
{
"vote": 9,
"title": "Classification of measures for text similarity",
"text": "I have doubt about classification of measures for text similarity. Some papers state that \"similarity measures are classified into three major types:\n\n\n\n\nString Similarity\n\n\nKnowledge-based Similarity\n\n\nCorpus-based Similarity\n\n\n\n\nReference : \n\n\nA Survey of Text Similarity Approaches by Wael H.Gomaa \n\n\nSome papers state that \"Similarity measures are classified into four types:\n4. Word Vector Representation \n\n\nReference:  Vector Based Techniques for Short Answer Grading\n\n\nDoes Word Vector Representation also come at the same level as other three categories are?",
"date": "2017-06-19"
},
{
"vote": 3,
"title": "Recognizing (Extracting) Document Sentences from Text?",
"text": "Hi there,\n\n\nI have some text content extracted from structured and formatted documents, so there are section headers, table of contents, and other sorts of things. I'm looking to extract just the sentence content from the document, and am curious what techniques may be recommended. Trying tools like the stanford/corenlp sentence matching matches the headers, and tends to get confused with the formatting artifacts.\n\n\nThanks for any advice!",
"date": "2017-06-17"
},
{
"vote": 0,
"title": "*Updated Excellent file with PDF's on Mind Control, Good Manuals and Survival.",
"text": "[deleted]",
"date": "2017-06-17"
},
{
"vote": 4,
"title": "Anyone know of any good work on or software for screening for depression based on language?",
"text": "I'm currently building a chat bot and the user's data (with their consent) will be screened for signs of depression and other indicators of mental health.",
"date": "2017-06-15"
},
{
"vote": 2,
"title": "Book for Textual similarity in Natural language processing",
"text": "I am looking book for textual similarity in natural language processing, I have found book \"Speech and Language Processing\" but it is mostly concerned with speech processing, that is not helping me much for textual similarity. If you have any idea, please let me know\n\n\nThanks",
"date": "2017-06-15"
},
{
"vote": 3,
"title": "Best place to contribute code and data for parsing, tagging, lemmatisation etc for more languages?",
"text": "As of 2017, what is the best place, in terms of impact, for a research group to contribute code and data for more languages?\n\n\nUniversal Dependencies\nNLTK\nspaCy\nStanford NLP\n...\nsomewhere else?",
"date": "2017-06-15"
},
{
"vote": 1,
"title": "[AI] Snips is a private-by-design Voice Assistant Platform that you can use to build 100% on-device assistants in any language!",
"text": null,
"date": "2017-06-14"
},
{
"vote": 3,
"title": "Regular Expressions(Regex) in Text similarity",
"text": "I read the topic for Regex in Natural Language Processing. My question is that how can I use Regex in Text similarity? From my point my view, it is not required, because tokenization will be necessary rather than Regex. Please answer my question.\n\n\nThanks",
"date": "2017-06-10"
},
{
"vote": 7,
"title": "A general Sequence to sequence learning package using TensorFlow (ver 1.2.rc1)",
"text": null,
"date": "2017-06-09"
},
{
"vote": 21,
"title": "An Adversarial Review of â€œAdversarial Generation of Natural Languageâ€",
"text": null,
"date": "2017-06-09"
},
{
"vote": 10,
"title": "Approaches for sentence-level word vector similarity?",
"text": "As you know, traditional string metrics like Levenshtein, Jaccard and so on are brittle.  And they are often length-normalised.\n\n\nSimilarity metrics based on word vectors have inherent tolerance for synonyms, free word order and so on.\n\n\nThe default vector cosine similarity works great for imprecise tasks like topic clustering, and for single words.\n\n\n(Example: spacy.io/docs/usage/word-vectors-similarities)\n\n\nBut for sentences they are too tolerant of critical errors like missing negation or incorrect pronouns, and they are not length-normalised.\n\n\nAre there some approaches with refinements for sentences?  Ideally without coming up with a better sentence representation.\n\n\nFor example, taking the min across all dimensions is not obviously worse than the default.\n\n\nThe results will be subtly different for fastText vs gloVe.",
"date": "2017-06-08"
},
{
"vote": 0,
"title": "Named entity CRF++ - adding category of sentence as a feature.",
"text": "How can I represent category of sentence predicted from a text classifier as a feature in CRF++ or Wapiti?\n\n\nFor instance, if the sentence, \nTumblr merges with Yahoo.\n, is classified as \nBusiness\n, then while composing the training file for crf, where can I indicate the label \nBusiness\n as a feature? And how should then the template be modeled?\n\n\nShould the train file be like this\n\n\nTumblr    business    ORG\nmerges    business    O\nwith     business    O\nYahoo    business    ORG\n\n\n\nOr only include the category with the \nORG\n label? How so? And the template file?",
"date": "2017-06-07"
},
{
"vote": 5,
"title": "api.ai sunsets their ASR- and TTS-Tool",
"text": null,
"date": "2017-06-07"
},
{
"vote": 11,
"title": "Some NLP: Spelling Correction with Noisy Channel Model in Python",
"text": "[deleted]",
"date": "2017-06-06"
},
{
"vote": 5,
"title": "Cucco has reached version 2.0 [thanks][self-promoting]",
"text": "Hi guys,\n\n\nI'm coming back to thank you for your nice comments about cucco, the text normalization library I presented weeks ago.\n\n\nToday I'm releasing version 2.0 with some of your suggestions (CLI, better defaults, etc.). You can check it out here \nhttps://github.com/davidmogar/cucco/releases/tag/v2.0.0\n\n\nAnd again, thank you for your help. Your input was a great aid for improving this tiny personal project.\n\n\nLooking forward to more useful comments ;)",
"date": "2017-06-04"
},
{
"vote": 2,
"title": "Semantic Textual Similarity in NLP",
"text": "I have read that, semantic similarity is to compare the sentences that are conceptually similar, so what if sentences are exactly similar(contain same words), is it also included into semantic similarity? Because when two sentences contain same words is considered as String based similarity, I think String based similarity and semantic similarity are different? Please make my doubts clear. \n\n\nThanks",
"date": "2017-06-03"
},
{
"vote": 2,
"title": "Meaning Extraction Helper | A program to help with text analysis, topic modeling, and language exploration",
"text": null,
"date": "2017-06-03"
},
{
"vote": 2,
"title": "Description about knowledge based similarity measures",
"text": "I am looking for details about knowledge based similarity measures, what are they are how they work in general?",
"date": "2017-06-01"
},
{
"vote": 5,
"title": "EMail summarizer",
"text": "Hi I am trying to write an email summarizer. I have tried Text Rank and LSA. These are good for emails that have verbose text but not good for emails that have bullet points and when the responses for the original email is inline. After doing some search, I came across Clue Word Summarizer, MEAD and RIPPER algorithms. Can anyone point me to any implementations of these algo?",
"date": "2017-06-01"
},
{
"vote": 5,
"title": "Why the seq2seq model trained with small data set failed to generate any meaningful samples?",
"text": "[deleted]",
"date": "2017-05-31"
},
{
"vote": 0,
"title": "http://lingosolutions.blogspot.in/2017/05/german-to-english-translation.html",
"text": "[removed]",
"date": "2017-05-30"
},
{
"vote": 24,
"title": "Best NLP blogs to follow?",
"text": "What are some of the best NLP and Computational Linguistics blogs to follow on the internet? \n\n\nFor example: Chris Olah's github \nblog\n on neural networks.",
"date": "2017-05-28"
},
{
"vote": 1,
"title": "June 1: NLP conference on CHATBOT technology at the Sportsmen's Lodge Events Center in Los Angeles.",
"text": null,
"date": "2017-05-28"
},
{
"vote": 2,
"title": "Good discourse parser?",
"text": "Anyone has experience using existing discourse parsers out there? (JVM-languages preferably)",
"date": "2017-05-28"
},
{
"vote": 11,
"title": "A tool to suggest github repositories based on the repositories you have shown interest in.",
"text": null,
"date": "2017-05-26"
},
{
"vote": 12,
"title": "The 4 functional writing styles: how to still tell the individual style?",
"text": null,
"date": "2017-05-25"
},
{
"vote": 6,
"title": "quantitative sentiment analysis starting point?",
"text": "I'm new to sentiment analysis. Thus far everything I've seen essentially classifies text into \"positive\" or \"negative\" distinctions. Where could I start reading/looking if I was interested in training to output over a range of \"positivity vs negativity\"",
"date": "2017-05-25"
},
{
"vote": 8,
"title": "Enhance: Word Embedding in Golang: Calculate the cosine similarity between words.",
"text": "[deleted]",
"date": "2017-05-23"
},
{
"vote": 6,
"title": "Spelling and Grammatical Errors in English",
"text": "Can anyone point me to good resources for quantifying different types of errors in English text?\n\n\nI remember reading a paper a few months ago that studied, I think Wikipedia comments, and characterized the relative likelihood of different errors. But I've lost the link.\n\n\nWhat I'm trying to get to is a model of English mistakes with well-founded priors.",
"date": "2017-05-22"
},
{
"vote": 3,
"title": "Enhance: Word Embedding in Golang: Calculate the cosine similarity between words.",
"text": "[deleted]",
"date": "2017-05-22"
},
{
"vote": 10,
"title": "an example on how to perform open information extraction, using part-of-speech tags only, and applied to Portuguese",
"text": null,
"date": "2017-05-22"
},
{
"vote": 6,
"title": "[D] Explicit language model vs backtranslations",
"text": "Improving Neural Machine Translation Models with Monolingual Data (\nhttps://arxiv.org/abs/1511.06709\n) is a paper from last year that proposed augmenting a parallel training corpus with synthetic translation data to kind of implicitly learn a language model for the source language. Now it seems all the state of the art translation systems employ backtranslations.\n\n\nI am curious if anyone has experience comparing the results from this research with simple rescoring using an explicit language model.",
"date": "2017-05-19"
},
{
"vote": 5,
"title": "Fed up with inconsistent results from currently available products. Advice on building a tool to generate related keywords?",
"text": "I'm trying to build a tool that generates approximately 1k to 10k related searches given a seed keyword.\n\n\nThere are some products out there that provide this functionality but they're either inconsistent or don't offer an API which I would need.\n\n\nAny ideas how I can work around this? Ideally using NLTK but not married to it. \n\n\nUltimately I'd like to build a graph from all the related terms, and then recursively search each one while adding appropriate edges.\n\n\nThe end goal is to then use a python graph visualizing package to represent the data and show the the areas with the most dense connections.",
"date": "2017-05-18"
},
{
"vote": 11,
"title": "Hash, a simple DSL for encoding arbitrary natural language expressions in a graph",
"text": null,
"date": "2017-05-16"
},
{
"vote": 4,
"title": "How to use Word2Vec for stemming/lemmatizing?",
"text": "I'm working on building a stemming programm for Turkish which is a morphologically rich language. None of the ressources that I've found are accurate enough for my purpose and I don't think they are built on deep learning technology.\n\n\nHow could I use word embeddings to train a stemmer/lemmatizer? What are the main steps I should be considering?",
"date": "2017-05-15"
},
{
"vote": 4,
"title": "Looking for ideas to extend simple text normalization Python library",
"text": "[deleted]",
"date": "2017-05-15"
},
{
"vote": 14,
"title": "Importance of linguistics background in NLP?",
"text": "To initiate a conversation on how linguistics contributes to NLP, and how the relationship between the two will evolve... \n\n\nPossible paper of interest, by Stanford Linguistics & Computer Science professor Chris Manning \nhere\n\n\nIn addition, I have noticed that many universities' NLP groups have no connection to their Linguistics departments, while others have very strong connections (in terms of faculty with joint appointments, graduate students from CS and Linguistics in the same group, etc). Also, there is a difference between computational linguistics and NLP, though the line is often blurred. \n\n\nThis is of interest to me because my background is a mix of NLP and theoretical linguistics.",
"date": "2017-05-14"
},
{
"vote": 3,
"title": "x-post: Citation impact for contents of religious literatures",
"text": "r/MachineLearning/6b42xt\n\n\n\n\nA friend asked me a question: The book has some many stories, which are the most relevant?\n\n\nThis made me wonder if this question has been approached with data-driven methods.\n\n\n\n\nFor scientific papers, there are citation impacts. There is a source, and the number of references to it.\n\n\nCould a simular method be used to create a heatmap for the contents of religious books?\n\n\nThere is a central node, and secondary nodes. \nThese are related to the central node if there is a citation. \nIf so we have an edge.\n\n\nIn the case of academic papers, this is a straightforward problem.\nThe central node is static. \nSecondary sources are well defined. \nCitations (references) are supposed to be explicit & formalized. \nSo edges are known and determining them requires no additional calculation.\n\n\nReligious books are different.\n\n\nFor one they often exist in multiple versions, the source is actually a set of nodes. Historically, this problem has been partially handled by using canons (numerical indexes), which could also help here. There are slight variations in the indexes, but mappings could be created with limited effort. The central node could be a union of indexed versions.\n\n\nIt would be impractical to sort trough all secondary sources.\nA rough method could be to look at a large set of pages, and scan each one for an element from a group of keywords, plus an index format. Then check if there is a significant substring match in the page and the related element in central node. If so there is an edge.\n\n\nAdditional impact analysis could be performed by comparing the results from different choises of keywords, or more broadly speaking: categorising the references: sect, language, year, etc...\n\n\n\n\n\n\nDo you know about existing research in this direction, if so, could you give me a pointer?\n\n\n\n\nDoes the approach described above seem sensible? If so, any suggestions which math to use in order to perform the text matching part somewhat efficiently? I'm considering trying this for a small test, say genesis versus wikipedia dump.",
"date": "2017-05-14"
},
{
"vote": 6,
"title": "Is there any tool for authorship identification? I mean online and user-friendly one?",
"text": "Looking for some.",
"date": "2017-05-12"
},
{
"vote": 6,
"title": "Toy System For Testing Initial NMT Model?",
"text": "[deleted]",
"date": "2017-05-11"
},
{
"vote": 2,
"title": "How services like Wibbitz work? What kind of NLP algorithms their are using?",
"text": null,
"date": "2017-05-11"
},
{
"vote": 14,
"title": "Linguistics Breakthrough Heralds Machine Translation for Thousands of Rare Languages",
"text": null,
"date": "2017-05-10"
},
{
"vote": 3,
"title": "An Overview of Word Embedding Models",
"text": "Hi guys, I'm novice at NLP, and I'm making an overview and comparison of existing word embedding models as a part of a university project. Except naive bag-of-words model and classic Word2Vec/Glove models I've found papers about these ones:\n\n\n\n\nword2vec-f\n\n\nwang2vec\n\n\nfasttext\n\n\nadagram\n\n\nswivel\n\n\n\n\nAre there any other models that I should take into account? Is there something cutting edge which I missed?",
"date": "2017-05-09"
},
{
"vote": 14,
"title": "Question: SpaCy or NLTK?",
"text": "Hi guys, I'm going to start working on some NLP project, and I have some previous NLP knowledge. (I used Stanford CoreNLP for tokenization, lemmatization, POS, dependency parsing and co-reference resolution)\nI want to work in Python and it looks like the obvious candidates for my NLP tools are SpaCy (\nhttps://spacy.io/\n) and NLTK (\nwww.nltk.org\n). I've seen some discussions from 2015-2016 comparing the 2, but nothing more recent. Has anyone had experience with both? If so, which one would you recommend?\nImportant info: I'll be working with English-only datasets.",
"date": "2017-05-08"
},
{
"vote": 16,
"title": "I wrote about crawling conversation sites, which I've found to be a far more efficient way of extracting conversational LM training data than using Google queries",
"text": null,
"date": "2017-05-07"
},
{
"vote": 17,
"title": "Oxford deep nlp 2017 solutions",
"text": "My solutions here: https://github.com/mleue/oxford-deep-nlp-2017-solutions\n\n\n\nI've recently been going through the lectures of oxford's 2017 deep nlp course (\nhttps://github.com/oxford-cs-deepnlp-2017\n). The course was well presented and I've really deepened my understanding of modern NLP methods.\n\n\nNaturally I am going through the practicals as well. I've linked to the repo with my current progress but I feel a bit stuck atm.\n\n\nThe main task revolves around a multi-class classification of ~2k transcripts of TED talks. However, the dataset is heavily skewed with one class covering ~50% and some classes only around 3-5% of the data.\n\n\nPractical 2 wants you to try a basic averaging over word-vectors approach and then pumping that through a single-hidden-layer NN. I've been trying to tweak a lot with preprocessing and tokenization but I can't come beyond ~66% accuracy on the test set.\n\n\nIn Practical 3 you are then supposed to try the same task with a RNN approach. I thought this might get better but I am basically stuck at around the same test set accuracy of ~66%.\n\n\nMaybe not much more is possible, especially given the fact that there is very little data for some of the classes. Basically I am wondering if anyone else has gone through the course (or even attended the real deal at oxford) so we can get a discussion going.\n\n\nThanks in advance! //Michael",
"date": "2017-05-06"
},
{
"vote": 3,
"title": "Discourse Segmentation (by topic) Dataset",
"text": "Do you guys know of any free dataset for this task?",
"date": "2017-05-04"
},
{
"vote": 1,
"title": "word embedding and neural machine translation",
"text": "[deleted]",
"date": "2017-05-02"
},
{
"vote": 4,
"title": "Using NLP to find business models in website content",
"text": null,
"date": "2017-05-01"
},
{
"vote": 3,
"title": "Looking for suggestions for learning NLG assuming basic knowledge of NLP",
"text": "I am looking to dive deeper into NLG for a project at work building email auto responders.  I got good basic knowledge of NLP from what I got out of \"Foundations of Statistical Natural Language Processing\". Any books, articles, websites useful to get me on the right path?  I am looking to use some of the more advanced stuff such as RNN and Tensorflow",
"date": "2017-04-30"
},
{
"vote": 11,
"title": "What has been peoples' experiences in using available NLP APIs?",
"text": "[deleted]",
"date": "2017-04-30"
},
{
"vote": 3,
"title": "[Help] Where can I find free phoneme dictionary text files for syllabification experiments?",
"text": "I'm trying to run some rule-based and statistical based syllabification algorithms on IPA/Arpabet dictionary files. I was wondering if there were any good free ones, that have the word already syllabified.\n\n\nThe ones I could find were:\n\n\n\n\nThe CMU Pronouncing Dictionary\n - I didn't see any symbols to indicate syllable boundaries, but it did have numbers to indicate stress.\n\n\nMoby Project\n - I'm confused over where syllable boundaries are in the text file.\n\n\n\n\nAny input would be appreciated, thanks!",
"date": "2017-04-28"
},
{
"vote": 1,
"title": "Scanned documents text extraction",
"text": "I have a set of scanned business documents e.g. Purchase orders, invoices etc. I would like to extract text and create a document structure from these scanned images which can be fed into a system. Any ideas what will be best approach for doing it or what sub area of NLP it falls into",
"date": "2017-04-26"
},
{
"vote": 2,
"title": "Efficiently storing and accessing large amounts of text",
"text": "I have about 15M documents that working with. The html representation goes from about 5Kb (low) to 1-2Mb (high), with most documents in the 500K range. Total size of the corpus is around 200Gb. \n\n\nRight now I am just storing them unparsed on disk in a hierarchy, because that many individual documents breaks most filesystems. What is the current state of the art for storing and randomly accessing that many documents?",
"date": "2017-04-25"
},
{
"vote": 4,
"title": "Looking for NLP Expert For Help With A Project",
"text": "Hey ya'll - I'm looking for an NLP wizard for a quick project. :) \nAnyone interested?\n\n\nHere are details:\n\n\nI'm looking to do a study on a dataset of 200,000+ customer service requests.\n\n\nI want to break down all the requests into different categories, subcategories and then determine which are tier 1 and could be automated by a robot. And which are tier 2 and need human touch.\nCategory examples are below.\n\n\nThe goal is to do a feasibility study to see if we could build an AI/ chat bot to handle their Tier 1 customer support requests.\n\n\nAt the end of the study, I want to have an estimated cost savings benefit for this company if they were to implement an automated customer support desk for Tier 1 stuff.\n\n\nI'm great with sales and marketing and I'm looking to find an amazing tech partner to help build the backend.\n\n\nIf you (or anyone you know) would be interested, feel free to DM me. :)\n\n\nWith love\nandy\n\n\nExample categories / subcategories:\nBilling \n\n\n\n\nRefunds \n\n\nUpgrades\n\n\nLogins\n\n\nChange credit card\n\n\nEtc\nGeneral Questions\n\n\nFeature requests\n\n\nPositive Feedback\n\n\nNegative feedbac\n\n\nEtc",
"date": "2017-04-25"
},
{
"vote": 16,
"title": "Russian Stop Words List",
"text": null,
"date": "2017-04-23"
},
{
"vote": 2,
"title": "Natural Language Processing on multiple columns in python",
"text": null,
"date": "2017-04-22"
},
{
"vote": 3,
"title": "Question about a word:",
"text": "My google-fu is failing me but I saw this word in a paper about multilingual SDS (Weng, Bratt, Neumeyer, & Stolcke, 1997) : genonic.\n\n\nHere's the context:\n\n\n> In addition to the two sets of PTM acoustic models\n> just described, two sets of \ngenonic\n acoustic models were\n> trained [2]. Notice that in a genonic system, HMM\n> allophones of a given class share the same Gaussian\n> codebook, and the sets of HMM states that share the same\n> mixture components are determined automatically using\n> agglomerative clustering techniques.\n\n\nAnyone know what this might refer to?",
"date": "2017-04-22"
},
{
"vote": 5,
"title": "BootCaT: Aggregate, crawl, and reduce dozens of search engine results to a single, line-separated TXT digest in ~30 seconds",
"text": null,
"date": "2017-04-22"
},
{
"vote": 6,
"title": "Try this NLP library! :)",
"text": null,
"date": "2017-04-20"
},
{
"vote": 2,
"title": "Simple use of Natural Language Processing",
"text": "[deleted]",
"date": "2017-04-19"
},
{
"vote": 2,
"title": "about the paper \"named entity recognition in tweets an experimental\" by A Ritter et al",
"text": "I am a reading this paper \nnamed entity recognition in tweets an experimental study\n . I found this while searching a NER model that can help identify name & entities in tweets and facebook posts. In the paper the author provides the link to the repo that is an implementation of the models which is mentioned in the paper. I am curious to know whether the model proposed here is implemented in any of the popular NLP frameworks For Eg: NLTK or OpenNLP etc ?",
"date": "2017-04-19"
},
{
"vote": 2,
"title": "[Need A Mentor] Starting out with NLP",
"text": "Hi I am graduate student working towards my masters. I looking to work on thesis or project in the area of NLP. My university doesn't have faculty with experience in NLP. I'm looking for some advice to get started and a probably mentor to provide advice along the way.",
"date": "2017-04-18"
},
{
"vote": 7,
"title": "Yet another text classifier idea (based on word2vec vectors clusterization)",
"text": "Ok, not just idea - there some code (see  \nhttps://github.com/alex4321/w2v-cluster-distance-classifier/blob/master/classifier.ipynb\n ). Seems like it works, but maybe you'll give any advices about algorythm or code :-)\n\n\nShortly - it works next way:\n\n\n\n\nI have pretrained word2vec model\n\n\npreviously I extracted 2000 clusters from this model\n\n\n\n\nSo - I can build token vector next way:\n\n\n\n\nget word2vec vector\n\n\nget cosine similarity with cluster vectors (all 2000 before training and few clusters after training)\n\n\nset element with low similarity value to zero\n\n\n\n\nAnd when I building text vector - I just get sum of token vectors.\n\n\nSo before training I can get most \"typical\" clusters for each label (see python notebook by link), and manually (now) choose what I'll use. \n\n\nAfter my choise it:\n\n\n\n\nget text vectors with only choosen cluster distance\n\n\nnormalize it by min-max normalizer\n\n\nuse KNearestNeighbours classifier\n\n\n\n\nAnd when I trying to classify text it:\n\n\n\n\nget text vector\n\n\napply normalizer\n\n\nget nearest neighbours and their labels (e.g. 5)\n\n\nfor each neighbour - build weight (1.0 if distance is lower then threshold, else 0.0)\n\n\nuse label(i,j) * weight(i) as neighbour label prediction\n\n\nget mean prediction",
"date": "2017-04-16"
},
{
"vote": 5,
"title": "Emergence of Grounded Compositional #FriendshipCube Language in Multi-Agent Populations",
"text": null,
"date": "2017-04-13"
},
{
"vote": 7,
"title": "Any good recommendations for NLP books for a novice?",
"text": null,
"date": "2017-04-12"
},
{
"vote": 8,
"title": "Will neural machine translation become the standard in the market?",
"text": "I have no background in CS, but I'm generally interested in MT. Can somebody explain me how NMT is better than SMT (if it's better at all)? Does NMT have any disadvantages that would hinder it from totally eliminating SMT from the market? Or will there be some other different, perhaps hybrid methods?",
"date": "2017-04-11"
},
{
"vote": 10,
"title": "Ways to match similar sentences and phrases?",
"text": "[deleted]",
"date": "2017-04-09"
},
{
"vote": 3,
"title": "Coecke/Clark semantics with classic probabilities?",
"text": "Hello, \nThe semantics presented in arXiv:1003.4394 is given by a functor into the category of finite dimensional real vector spaces, and  the same ideas produce a semantic targeted to the category of also finite dimensional Hilbert spaces with a quantum flavour. Is there any work in targeting the semantic functor to the Kleisli category of the Giry Monad?",
"date": "2017-04-09"
},
{
"vote": 2,
"title": "Bret Victor - Inventing on Principle",
"text": null,
"date": "2017-04-07"
},
{
"vote": 3,
"title": "automatic parallel text generation",
"text": "What tools are available (if any) to automatically translate a text and allign it.\n\n\nAll-in-one would be great, but also a two-step process is fine, e.g.: I can translate a word doc with google translate, and now I have two .doc file; I need to merge them so that odd pages are the original text, even pages are the translated text.\n\n\nTo place sentences along side their translation it could add blank rows when needed or I could shrink/increase font.\n\n\nThanks for your help!",
"date": "2017-04-06"
},
{
"vote": 1,
"title": "How are results of dependency parsing used?",
"text": "I've read a few books on text processing and used Stanford CoreNLP on toy problems. But I still don't understand how dependency syntax trees can be used. The books and papers that I've found explain how it works but not how it will be used. The best I could find was \nit can be used in other parsers\n. \n\n\nCan you recommend books, papers or blogs that explains possible applications of dependency parsers?",
"date": "2017-04-04"
},
{
"vote": 5,
"title": "Relation Extraction with Matrix Factorization and Universal Schemas",
"text": "Hi everybody! I am asked to list strong points and technical limitations of the Riedel 2013 paper along with how these limitations can be addressed: \nhttp://www.aclweb.org/anthology/N13-1008\n\n\nI am an NLP beginner and I am yet to build on baseline knowledge about the topic. What do you think could be some of the things that can be said about this paper? \n\n\nI for sure know that the authors seem to have avoided mentioning the limitations on their own, so that's a disadvantage on its own, but beyond that I am still looking for answers.",
"date": "2017-04-03"
},
{
"vote": 2,
"title": "What do you use for entity linking",
"text": "Hi, I've currently started to work with NLP, I've played around with coreNLP and spaCy. I got really fast my first exciting results, but now I've hit my first roadblock, entity linking. At first I thought I would do just NER and look up the entity in my knowledge base, but that's not how it works. I'm currently working through a paper to see how EL works.\n\n\nI'm thinking about using DBPedia as a knowledge base, but I can also work with Wikidata, YAGO...\n\n\nWhat open source tools are you using for EL?",
"date": "2017-04-01"
},
{
"vote": 1,
"title": "Looking for a list of curse words/phrases in english",
"text": "[deleted]",
"date": "2017-03-31"
},
{
"vote": 1,
"title": "some pop-sci clickbait for you ;-): \"Baidu's artificial intelligence learned English by learning to find apples in a maze\"",
"text": null,
"date": "2017-03-31"
},
{
"vote": 0,
"title": "State of the art in chatbot.",
"text": "Hey fellow redditors,\n\n\nI have been tasked with creating a robust Conversation Chatbot for one of the new products of my company. I have read various papers about popular Neural Network models used in NLP like RNN, LSTM and Seq2Seq, and have also implemented some of these methods in Tensorflow. Although the results are good, the models often returns text which is not relevant to the conversation at hand. I am still somewhat new to NLP, although I do have experience in Machine Learning. Can somebody give me some guidance or advice about what direction I should take? Also, it would be very kind of you if you could also point me to a good dataset( tried the previous models on 5GB reddit dataset).\n\n\nRegards,\nPiyush Kathuria",
"date": "2017-03-30"
},
{
"vote": 4,
"title": "Feature selection for classification.",
"text": "[deleted]",
"date": "2017-03-28"
},
{
"vote": 4,
"title": "Datasets for Speech Synthesis",
"text": "Does there exist a dataset that contains SSML (Speech Synthesis Markup Language) tags to train on? I can't seem to find many speech synthesis datasets that are not in raw form. If need be, what is the approach to transforming raw audio to words with tags?\n\n\nAny help or direction is appreciated.",
"date": "2017-03-28"
},
{
"vote": 4,
"title": "Is combinatorics useful for natural language processing?",
"text": "Combinatorics\n.\n\n\nI have a major in linguistics and minor in computer science, I'm greatly interested in NLP and Comp Linguistics. Unfortunately, my University does not have any courses in NLP or Comp Ling.\n\n\nI am currently registered for a probably and statistics course for Comp Sci, which I am sure will be incredibly useful. However I was wondering if the following course would be useful as well.\n\n\nHere is a screenshot of the description and textbook for the course.\n\n\nhttp://imgur.com/a/tE28s",
"date": "2017-03-28"
},
{
"vote": 1,
"title": "American English - IIS Institute",
"text": null,
"date": "2017-03-28"
},
{
"vote": 0,
"title": "google translate is spooking me out",
"text": "[removed]",
"date": "2017-03-28"
},
{
"vote": 0,
"title": "Significance of Linguistic Testing and QA in Translation Projects",
"text": null,
"date": "2017-03-27"
},
{
"vote": 5,
"title": "Transliteration",
"text": "Is there any software can do the Transliteration (only sound) between different language? \n\n\nFor example, â€˜zung manâ€™  (â€˜ä¸­æ–‡â€™ in Cantonese), \ninto â€˜dschong manâ€™ (closest pronunciation in German), \nâ€˜Ø²ÙˆÙ†Ø¬ Ù…Ù†â€™ (closest pronunciation iin Arabic)",
"date": "2017-03-24"
},
{
"vote": 3,
"title": "How to Setup Training/Testing Neural Machine Translation systems?",
"text": "[deleted]",
"date": "2017-03-23"
},
{
"vote": 15,
"title": "Made word2vec from scratch in Golang",
"text": "[deleted]",
"date": "2017-03-22"
},
{
"vote": 3,
"title": "MultiLingiual Named Entity Linking?",
"text": "Hello everyone, I am working on a clustering algorithm to cluster articles from different sources, and produce a news event per cluster. Everything is working well, except for one problem.\nI am clustering Arabic articles, and the algorithm is working very good, it is showing very good results on Politics and Sports articles, but when it comes to Games and Technology, the results are not good. The problem is I am having a very low recall (fewer clusters than needed).\nAfter investigating, I found that the problem is with named entities. In Games and Tech, authors seem to be mixing between using English names, or Arabic equivalent name, and this is affecting the title terms weighing the most, which affect the final results in general.\nNow, I am looking for a way to find equal named entities even if they are in different languages. I still don't know how exactly, and I appreciate any help.",
"date": "2017-03-22"
},
{
"vote": 1,
"title": "Derrida and NLP question from someone who doesn't really know anything about either of 'em",
"text": "I'm just getting into NLP and learned a little bit about Derrida for a literature class I took in college.\n\n\nBasically I'm wondering if there is a python dictionary that tries to define a word based on a hierarchy of words that could be used in its place. So there's a sample sentence of \"I live in a two-story house with four windows.\" Is there a python dictionary that tries to define \"house\" based on the other words that could be used in its place, i.e. mansion, home, apartment, shack, hovel. The word \"house\" is maybe ranked as neutral or slightly positive, and that ranking contributes to understanding the overall meaning of the sentence.\n\n\nSo my question is 1) does this exist, 2) is it possible or even useful to make? Would it take an inordinate amount of data or processing power to make/use? Thesauruses exist, so I feel like there could be a way to make a dictionary like this.\n\n\nAgain, I'm a beginner, so explaining the flaws in my logic would be really helpful to me. And if explaining why I'm wrong would equate to you having to teach a NLP 101 course, then I understand if you move on with your life. Thanks, reddit!",
"date": "2017-03-22"
},
{
"vote": 1,
"title": "[gensim] Removing phrases from gensim.models.phrases?",
"text": "I'm using gensim.models.phrases to find common phrases within a corpus and I'm using gensim.utils.lemmatize to preprocess.  What I want is to only keep phrases that end with a noun or /NN. \n\n\nSo if I train using -\n\n\nmodel = gensim.models.phrases.Phrases(mySentences, min_count=25, threshold = 8.0)\n\n\nIs there a way I can remove phrases from model.vocab?",
"date": "2017-03-21"
},
{
"vote": 3,
"title": "\"Build It, Break It: The Language Edition\"",
"text": null,
"date": "2017-03-20"
},
{
"vote": 1,
"title": "Optimal use of conversations received in a chatbot",
"text": "We have created a bot in house without using any platforms like wit.ai or api.ai. We treat is as a text classification problem. We are using word2vec to vectorize the input documents and a linear svc to classify it into one of our pre defined classes.\n\n\nAlso we have a feedback dashboard where the admin can tag and validate incoming user queries with correct class names (similar to what existing platforms provide). This is fine until a certain level but becomes impossible later.\n\n\nSo now we have a few (5k) tagged queries and a lot (350k) untagged user queries. Is there any way we can use these to enhance our system ?",
"date": "2017-03-20"
},
{
"vote": 2,
"title": "[Request] Ways to resolve location name ambiguities?",
"text": "I'm working on geolocating small pieces of text(based on geonames), and while there are ways to resolve ambiguous location names using context, are there any standard approaches to this problem? I can work with both database level as well as \napplication level solutions. \n\n\nE.g. there are about 85 Springfields in Australia, and 70+ in the US(Not counting fictional places). What should be an approach that will reliably identify the correct place?\n\n\nAny help will be appreciated.",
"date": "2017-03-18"
},
{
"vote": 3,
"title": "This is why every paper must be accompanied by the source code.",
"text": null,
"date": "2017-03-18"
},
{
"vote": 6,
"title": "Anyone tried training an RNN to write regex patterns?",
"text": "Just an idea that popped into my head. \n\n\nI have a document, a pattern, a goal, and the regex matches for that pattern in the document. You're NLP folks. You see where this is headed. ðŸ¤– ðŸ™ƒ\n\n\nAfter kludging this problem over and over, trying a variety of strategies, iterating, testing whether the results matched the goal, suddenly I was overcome with an uncanny feeling. \nWait, something about this feels familiar... have I done this before? Hey, I have done this before! With robots....\n\n\nOk, ok, so in hindsight to be honest, it was sort of a stupid goal. I'm including a few rows of the document and the goal string below, to give you some idea what the inputs and rewards for a \"RegEx RNN Composer\" robot might look like. If you think there's a better way to preprocess these inputs for feeding, let me know. If you have reason to believe this totally won't work, also let me know. I'm not totally convinced this will work, I'd be pretty surprised if it does -- but man, if it actually works, that would be \nawesome\n.\n\n\nSo here we have the vertex edges of a digraph. They're given as a list of tuples, which is being read from a file (so it's a string). Like these: \n\n\n    (36086285,0,&#039;\\&quot;Viranchi_the_Creator\\&quot;_-_Al_Part_2_-_Shiva_Space_Japan_2004&#039;,0),\n    (1195559,0,&#039;\\&quot;Vivid_Telepathy\\&quot;&#039;,0),\n    (33205646,0,&#039;\\&quot;Vlaglied\\&quot;_Copyright_Act,_1974&#039;,0),\n    (36086285,0,&#039;\\&quot;VÃ£o_Fazer_De_Novo\\&quot;_(_with_Charlie_Brown_Jr_)_-_Ritmo,_Ritual_e_Responsa_-_EMI_Brazil_2007&#039;,0),\n    (470006,0,&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039;,0),\n    (1282996,0,&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039;,0),\n    (2295744,0,&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039;,0),\n    (2301720,0,&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039;,0),\n    (12247128,0,&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039;,0),\n\n\n\nThe ambitious idea being to construct the adjacency list of the digraph as an \nin-place\n transformation of the vertex list. There's probably a way to greedy match successive lines with a matching capturing group to produce something like this set of results:\n\n\n   {&#039;&quot;Viranchi_the_Creator\\&quot;_-_Al_Part_2_-_Shiva_Space_Japan_2004&#039; : [36086285]},\n   {&#039;\\&quot;Vivid_Telepathy\\&quot;&#039;: [1195559]},\n   {&#039;\\&quot;Vlaglied\\&quot;: [33205646]},\n   {&#039;\\&quot;VÃ£o_Fazer_De_Novo\\&quot;_(_with_Charlie_Brown_Jr_)_Ritmo,_Ritual_e_ Responsa_-EMI_Brazil_2007&#039;: [36086285]},\n   {&#039;\\&quot;W\\&quot;_Is_for_Wasted&#039; : [470006, 1282996,2295744,2301720,1224712]}\n\n\n\nBut I never did figure it out. I gave it like 3 dozen shots before I really started feeling like my loss was exploding. I gave up and just built an in-memory tree, but I'm still interested in the bigger question. \n\n\n\n\nWould an RNN maybe have some aptitude at reverse-engineering a pattern that hits a given target? \n\n\n\n\nIs the problem even tractable? (And if not tractable, is there a subset of the regex pattern library that is?)\n\n\n\n\nHas this been done already? \n\n\n\n\nOh god. I just realized the existence question (roughly: 'is there a combination of these glyphs that \n...hand waving\n and ah, fully intersects the success set?') ... I'm almost certain that problem is unsolvable. Oh jeez.  How big of an issue is this?\n\n\n\n\nAny suggestions on where to look, if I were to start building a corpus of document strings along with their corresponding regex patterns? \n\n\n\n\nOr how to preprocess those inputs. the literal regex patterns? I'm not really sure the optimal training dojo for teaching the RNN the vocabulary/char sequences that make up regex. Hypothetically it seems likely that (\ntl:dr; a truly awesome Regex bot probably knows what an pattern is going to match before she runs it\n); she might be rewarded for \"presaging\" certain features of the  matched pattern. I'm just throwing out ideas though. Suggestions welcome!)\n\n\n\n\n\n\nEdit: i think actually that is not \"in-place\" since nothing's mutating",
"date": "2017-03-16"
},
{
"vote": 8,
"title": "Is there any comparison about the common ways for new words/phrases detection?",
"text": "[deleted]",
"date": "2017-03-15"
},
{
"vote": 49,
"title": "An NLP blog for beginners that a friend and I have started. Weekly articles with code in Python.",
"text": null,
"date": "2017-03-12"
},
{
"vote": 16,
"title": "Too many courses, confusing terminology! Where to begin with NLP?!",
"text": "[deleted]",
"date": "2017-03-07"
},
{
"vote": 5,
"title": "Python library for language normalization [looking-for-ideas][self-promotion]",
"text": null,
"date": "2017-03-07"
},
{
"vote": 1,
"title": "Is there any way I can automate this work?",
"text": "Is there any software, website, algorithm that can provide me a rough description of a few hundred of in it's own words for any topic I search for in it?\n\n\nCurrently I've found Sensebot which goes through few google searches, summarizes the important lines and presents it to you in a list.\n\n\nSo is there anything better than that, which not only goes through the search results,summarizes and understand the important points, but also rewrites them for you in its own language?\n\n\nOr maybe some website that provides a few hundred words of its own on any given topic?\n\n\nI ask here, because I've only found scammy article writing softwares so far on Google search. I'm looking for something that doesn't directly cut paste others content or poorly spin content.\n\n\nThank you.",
"date": "2017-03-04"
},
{
"vote": 2,
"title": "How to check which regulations are affected?",
"text": "Hi,\nI am wondering what strategy to use for machine learning in the the following case:\n\n\nA building consultant makes a report recommending certain activities necessary to complete an upgrade or maintenance of a property. He needs to know which Governmental Building Code regulations (he has this document) are affected by the activities. Apart from searching by keyword, is there a better method using NLP, and how might that work? What would the model be? Thanks for any input.\nLawrence",
"date": "2017-02-28"
},
{
"vote": 14,
"title": "A curated list of Sentiment Analysis methods, implementations and misc",
"text": null,
"date": "2017-02-27"
},
{
"vote": 10,
"title": "Why shouldn't I simply add the probabilities in naive bayes? I'm getting a higher accuracy rate when I do that in this case",
"text": null,
"date": "2017-02-27"
},
{
"vote": 12,
"title": "What word2vec is all about",
"text": null,
"date": "2017-02-25"
},
{
"vote": 4,
"title": "Audio Data Collection Project - Multiple Languages Required.",
"text": "We are collecting anonymous speech data that we will use to improve speech recognition technology.\n\n\nCurrently, we need Irish English, French Canadian, Mexican Spanish and Korean speakers for this task but other languages and dialects will be required in the near future.\n\n\nEach participant will be compensated with an electronic gift card and we can also negotiate a recruitment reward if anyone would like to become a vendor for us.\n\n\nPlease dm me or email us for more info: \nDATACOLLECTIONRESOURCING.tre@lionbridge.com\n.\n\n\nThanks!\n\n\nLionbridge Technologies, Inc. \nwww.lionbridge.com\n\n\n\n\nWebsite: \nhttp://www.lionbridge.com/solutions/global-services-for-machine-intelligence/\n\n\n\n\nFacebook: \nhttps://www.facebook.com/LionMachineIntelligence/\n\n\n\n\nTwitter: \nhttps://twitter.com/Lionbridge_GSMI\n\n\n\n\nLinkedIn: \nhttps://www.linkedin.com/company/Lionbridge\n\n\n\n\nBlog: \nhttp://blog.lionbridge.com/\n\n\n\n\nYouTube: \nhttps://www.youtube.com/user/LionbridgeTech\n\n\n\n\nApparel: \nhttps://shop.spreadshirt.co.uk/Bestsellingproductsnow/\n\n\n\n\n\n\n[Forbes 2015 Americaâ€™s Most Trustworthy Companies] (\nhttp://www.lionbridge.com/lionbridge-named-one-of-americas-100-most-trustworthy-companies-by-forbes/\n)",
"date": "2017-02-24"
},
{
"vote": 1,
"title": "Language Translator Software | Process9",
"text": null,
"date": "2017-02-22"
},
{
"vote": 8,
"title": "How can I use Stanford CoreNLP to find similarity between/match sentences?",
"text": "Hi. I'm new to NLP. I have to develop a little software that takes a question and give the best answer based on a set pre defined answers, but I dont know how to use the output of StanfordNLP to search for the best match. If someone can point me to a direction I would truly appreciate. Thank you.",
"date": "2017-02-17"
},
{
"vote": 4,
"title": "If I hated English and Writing in school will I hate working with NLP?",
"text": "[deleted]",
"date": "2017-02-16"
},
{
"vote": 4,
"title": "Components and implementations of Natural Language Processing",
"text": null,
"date": "2017-02-16"
},
{
"vote": 2,
"title": "NER in biology/chemistry texts",
"text": "Hi\nI'm looking at a chemistry problem which involves named entity recognition. I've done this before for company names, personal names etc but that's when you have the list of all named entities in advance. It appears for biology and chemistry texts there isn't a list of all proteins or enzymes to look for, so it's harder than company names etc. I could be wrong here. So I did some research and it appears two options are CRFs or LSTM, or possibly some other methods too. I'd like to know which is currently the state of the art approach to this problem?\nThanks!",
"date": "2017-02-15"
},
{
"vote": 1,
"title": "Morgan21",
"text": null,
"date": "2017-02-15"
},
{
"vote": 0,
"title": "AFINN Simple Sentiment Analysis via API or Node.js",
"text": null,
"date": "2017-02-14"
},
{
"vote": 1,
"title": "What I mean is",
"text": null,
"date": "2017-02-12"
},
{
"vote": 4,
"title": "Tool for causal relation extraction?",
"text": "Hey, I'm looking for a standalone / API tool that can break down a paragraph into clauses, and determine the causal relation between then.\nSo \"Since I was late, the meeting didn't start\" would be broken into A->B where A is I was late, and B is the meeting didn't start.\n\n\nAnything like that exists as a consumable package/API?\n\n\nThanks",
"date": "2017-02-12"
},
{
"vote": 6,
"title": "Summarization company hiring a Jr. and Sr. NLP Engineer in NYC",
"text": null,
"date": "2017-02-11"
},
{
"vote": 1,
"title": "Translator and Engineer join to offer an open software that brings Interpreting classroom home",
"text": null,
"date": "2017-02-09"
},
{
"vote": 34,
"title": "Oxford Deep NLP 2017 course with videos and Practicals",
"text": null,
"date": "2017-02-07"
},
{
"vote": 2,
"title": "[Project] JFastText - a Java interface for FastText",
"text": null,
"date": "2017-02-06"
},
{
"vote": 2,
"title": "[X-Post] Need help with POS-Tagging with Neural Networks â€¢ /r/learnpython",
"text": null,
"date": "2017-02-02"
},
{
"vote": 3,
"title": "What are some public data set for text generation?",
"text": "[deleted]",
"date": "2017-02-02"
},
{
"vote": 2,
"title": "How does the trait entity extraction in wit.ai work?",
"text": "Wit.ai provides an entity of type \"trait\". Wit.ai defines trait entities as\n\n\nWhen the entity value is not inferred from a keyword or specific phrase in the sentence. There is no obvious association between certain words in the sentence and the value of the entity, but rather you need the sentence as a whole to determine the value. (\nhttps://wit.ai/docs/recipes#which-entity-should-i-use\n)\n\n\nI'm curious to know how does this work? For example,\n\n\n\n\nI want to know the status\n\n\n\n\nCheck the status\n\n\n\n\nHow is it going?\n\n\n\n\n\n\nI want to extract the entity entities[\"cmd\":\"status\"] from all of these statements. How do I go on about starting to implement this?",
"date": "2017-02-01"
},
{
"vote": 1,
"title": "Features of The Ideal Website Localization Service for Your Website | Process9",
"text": null,
"date": "2017-01-31"
},
{
"vote": 5,
"title": "[Project] Seeking contributors to a NLP open source project to perform database queries in natural language",
"text": "I am starting an open source project called \"JustQuery.me\". The objective is to allow users to perform natural language queries which will be translated to different database or datasources. There will be heavy ML to train a set of possible questions and allow the engine to ask further refining questions if necessary. Anyone interested please check the github project at \nhttps://github.com/justquery-me/justqueryme\n. \n\n\nThis engine will use neural networks to have multiple outputs that will tag the query into difference dimensions (data source requested, filtering, order, grouping, etc)\n\n\nIf you are interested in contributing please subscribe to the mailing list in the github page.\n\n\nHigh level architecture can be found here -> \nhttps://groups.google.com/forum/#!topic/justqueryme-development/PmJi7Pwz4SA",
"date": "2017-01-30"
},
{
"vote": 5,
"title": "[Project] Clickbait detection using Deep Learning (Github); X-post from /r/MachineLearning",
"text": null,
"date": "2017-01-28"
},
{
"vote": 3,
"title": "What do you (want to) use natural language processing for?",
"text": "I'm seeking NLP use cases for part of a tutorial series on using neural networks for NLP. My goal is to have bite-sized but nutritious projects with small real-world datasets.\n\n\nSo far the list is:\n\n\n\n\nSentiment analysis using the IMDB ratings dataset\n\n\nTranslation between two languages with the seq2seq model\n\n\nIntent parsing for an IOT system with seq2seq\n\n\n\n\nAny other ideas? Even if they aren't tutorial \"worthy\" I would like to hear what your most common uses of NLP are, and would especially like to hear what you don't know but want to know how to do.",
"date": "2017-01-25"
},
{
"vote": 0,
"title": "New to NLP here. I am looking for sample Name Entity Recognition code",
"text": "[deleted]",
"date": "2017-01-25"
},
{
"vote": 13,
"title": "Introduction to Natural Language Processing",
"text": null,
"date": "2017-01-24"
},
{
"vote": 3,
"title": "I'm starting to learn about NLP/NLU chatbots and I was pointed towards Rasa NLU from GoLastMile. Anyone know if rasa nlu uses word embeddings? Or is it using some other method to find intents and entities in the text?",
"text": "Please let me know if such a question doesn't make sense. I'm new to this field",
"date": "2017-01-23"
},
{
"vote": 2,
"title": "How Four AI Startups Help Brands Exploit Customer Reviews",
"text": null,
"date": "2017-01-19"
},
{
"vote": 9,
"title": "Python implementation of the Rapid Automatic Keyword Extraction algorithm using NLTK.",
"text": null,
"date": "2017-01-18"
},
{
"vote": 14,
"title": "Scattertext: a tool to make sexy visualizations of categorized corpora",
"text": null,
"date": "2017-01-17"
},
{
"vote": 6,
"title": "Predict tags from ML models trained on unrelated topics?",
"text": "[deleted]",
"date": "2017-01-17"
},
{
"vote": 1,
"title": "FrÐµe Ð°nd gÐ¾Ð¾d IntÐµrnÐµt sÐµx dating site with mÐ°ny differÐµnt girls",
"text": "[removed]",
"date": "2017-01-16"
},
{
"vote": 6,
"title": "Apache OpenNLP - Model version 1.6.0 not supported by this (1.5.3) version of OpenNLP",
"text": "[deleted]",
"date": "2017-01-13"
},
{
"vote": 2,
"title": "Automatic sentiment neutralizer/changer? De-emotive conjugator",
"text": "Does anyone know about any tool that tries to convert \nemotionally charged\n language into equivalent language that has the same meaning but plays differently our emotional strings? Or at least, something that provides automatic highlighting of words and phrases, from a database, that have strong emotional connotation but could easily have been said otherwise.\n\n\nI am wishing for some sort of browser plugin that we can turn on and off.",
"date": "2017-01-12"
},
{
"vote": 8,
"title": "[1701.02877] Generalisation in Named Entity Recognition: A Quantitative Analysis",
"text": null,
"date": "2017-01-12"
},
{
"vote": 6,
"title": "The written word, synthesized from input data?",
"text": "Speech synthesis:\n Audio -> Text\nNatural Language Processing:\n Text -> Data\n???:\n Data -> Text  \n\n\nI'm trying to see what research, code, discussion, or anything really, exists for the process of taking data and generating natural language.\n\n\nAny pointers to articles, papers, websites or appropriate terminology would be appreciated. The search phrases I've used so far have yielded nothing useful at all.",
"date": "2017-01-06"
},
{
"vote": 11,
"title": "I spent quite some time reading through Microsoft Speech Recognition 2016 Paper. This is my summary. Please provide some feedback and insights.",
"text": "[deleted]",
"date": "2017-01-04"
},
{
"vote": 1,
"title": "Java 8 intro",
"text": null,
"date": "2017-01-03"
},
{
"vote": 3,
"title": "Input format in Vowpal Wabbit",
"text": "I'm using Vowpal Wabbit for the first time with text input and I have a doubt for the input format.\n\n\nIf an input line is\n\n\n1 | the cat play with the other cat\n\n\nwould it be similar to\n\n\n1 | the:2 cat:2 play:1 with:1 other:1 \n\n\nor \n\n\n1 | the cat play with other\n\n\nDifferently put, what happens when you repeat a feature several times in the same feature space ? I guess it should be the first one but I haven't fount it explicitly written.",
"date": "2017-01-03"
},
{
"vote": 1,
"title": "Language Translation Services | Process9",
"text": null,
"date": "2016-12-30"
},
{
"vote": 2,
"title": "Recommended [automatic] translation service?",
"text": "Hello,\n\n\nI am looking for an automatic translation service that has better accuracy than Google Translate for Japanese -> English translation. Paid is OK. Might you have any suggestions? \n\n\nThanks in advance!",
"date": "2016-12-27"
},
{
"vote": 0,
"title": "What is hindering computer from comprehending webpages or even small passages like tweets? I thought IBM Watson can read?",
"text": null,
"date": "2016-12-25"
},
{
"vote": 6,
"title": "Good TTS platforms to read my articles to me on the go?",
"text": "I have many articles saved on Evernote (and their corresponding source on the web) that I would like to listen to on my commute instead of reading on the computer. Do you happen to know of a good text-to-speech system that would allow me to easily convert those articles to mp3 and listen to them on the go? Thanks",
"date": "2016-12-21"
},
{
"vote": 0,
"title": "Beginner Question: How to import a text file from dropbox?",
"text": "[deleted]",
"date": "2016-12-19"
},
{
"vote": 1,
"title": "Errors installing spaCy on Python 3",
"text": "[deleted]",
"date": "2016-12-19"
},
{
"vote": 10,
"title": "Insight into a commercial-quality Named Entity Recognizer",
"text": null,
"date": "2016-12-16"
},
{
"vote": 7,
"title": "NLP positive data augmentation",
"text": "When it comes to image classification, there are data augmentation techniques in order to boost the number of samples such as adding blur, adding shadows, rotating an image,etc, all of which will boost the number of samples you have. Im wondering if there exists similar techniques in the NLP field for increasing the number of positive examples that one has, or boosting the amount of training data points one has without having to annotate additional data or find larger data sources.",
"date": "2016-12-15"
},
{
"vote": 1,
"title": "Star Wars chatbot - become the hero of an interactive space adventure!",
"text": null,
"date": "2016-12-15"
},
{
"vote": 0,
"title": "Bot Connector by Recast.AI - Your one API to connect bots to any channel",
"text": null,
"date": "2016-12-13"
},
{
"vote": 4,
"title": "What is a good test for text-based conversational UI?",
"text": "I am looking for how an objective evaluation process for a competition in text-based conversational UI.\n\n\nIdeally the scoring would be\n1 meaningful: somehow correlate to real-world usefulness\n2 achievable: continuous not binary\n\n\nIt need not be deterministic nor fully-automated, for example: how many responses before a human correctly guesses that it is a bot.\n\n\nThoughts and ideas?",
"date": "2016-12-07"
},
{
"vote": 12,
"title": "Introducing Factmata â€” Artificial intelligence for political fact-checking â€“ The Factmata Project",
"text": null,
"date": "2016-12-06"
},
{
"vote": 2,
"title": "Back with another question about POS tagging in Spanish!",
"text": "I am still having an issue with clitics and spanish verbs.  I have implemented my own version of the CoreNLP script with a modified verb database to handle verbs with clitics, but the POS tagger makes a lot of mistakes with these and marks them as nouns.\n\n\nI tried using the Stanford POS tagger as well, but also had issues.  I think I've tried ~4 taggers now, and I'm really not getting the kind of results I would like.\n\n\nInitially, I had just said 'ok, if it matches the format of a strippable verb, try to strip it, conjugate it, and see if it is in a list of known infinitives.'  This worked for a whole until I realized \"hermanos\" => \"herman/os\" => \"hermanar\"...which is apparently a verb, and obviously this happens somewhat frequently (maybe ~5-10% of the \"verbs\" are actually nouns that end up being miscategorized as verbs).\n\n\nWhat would be my next steps to implementing a better tagger?  One idea I had come up with would be to build a list of all POS tagged nouns/verbs for my corpus, stem them, and see if the suffixes are frequently the same.  I'm hoping this would be indicative of something that is always used the same way, and then would statistically be more likely to be a noun.  I could also maybe look at how often those words are categorized as a noun/verb, and then in the future try to make an assumption off that?",
"date": "2016-12-05"
},
{
"vote": 5,
"title": "Would you like to build a meeting extractor together?",
"text": "I have recently seen x.ai and I found meeting extraction an extremely interesting technical project for NLP. I would like to build a Python tool to extract meetings from texts and return the place and time of a meeting.\n\n\nI need help building a meeting classifier and the initial labeled data set to decide whether documents are meeting invitations or not. To parse the meetings seems a simpler problem but I think there's opportunities for some interesting models in this area as well.\n\n\nTo this end initially to build a meeting classification model I need some help extracting meeting proposals, I have decided to use the ENRON email data set and the leaked Hillary emails as an initial corpus to build my meeting classifier.\n\n\nAfter having the meeting invitations I would like to extract and validate the times and places involved.\n\n\nGet in touch if you are interested! I think building the meeting corpus is the most time consuming part of the project my current strategy is to think about a list of relevant regex and get a subset of the emails I have based on these and hand mark these to build a labeled data set.",
"date": "2016-12-02"
},
{
"vote": 6,
"title": "Question about preprocessing Spanish text for use with Python NLP libraries",
"text": "I am wondering if there is a guide anywhere that states the kind of common preprocessing operations that are required for Spanish.  Specifically one of the issues I encountered is the tacking on of direct/indirect objects to verbs, and I am wondering if reflexive verbs might become an issue as well.  I don't want to reinvent the wheel if I don't have to, and I'm starting to think it might be worthwhile to look over someone else's standard approach.\n\n\nSorry if this is the wrong place to post this, I couldn't find a better place :P",
"date": "2016-11-29"
},
{
"vote": 10,
"title": "Extracting meaning from dependency graph",
"text": "What post processing is typically used to extract meaning from a sentence's dependency graph?",
"date": "2016-11-27"
},
{
"vote": 1,
"title": "Need some guidance on calculating the cost for natural language predictions",
"text": "[deleted]",
"date": "2016-11-26"
},
{
"vote": 5,
"title": "What are some most state-of-the-art aspect-based sentiment analysis model?",
"text": "[deleted]",
"date": "2016-11-26"
},
{
"vote": 8,
"title": "What should I do when the corpus quality is not so high and the word2vec doesn't work as expected?",
"text": "[deleted]",
"date": "2016-11-25"
},
{
"vote": 8,
"title": "Efficient text classification with large number (tens of thousands or more) of classes.",
"text": "I'm working on a text classification problem in which the number of classes/labels is large (tens of thousands or more). What approaches/methods would you use? Can you point me towards any relevant papers?",
"date": "2016-11-23"
},
{
"vote": 1,
"title": "Internet Security| REVE Antivirus",
"text": null,
"date": "2016-11-22"
},
{
"vote": 4,
"title": "NLP entry-level or freelance roles?",
"text": "[deleted]",
"date": "2016-11-22"
},
{
"vote": 6,
"title": "Countdowns to top CV/NLP/ML/Robotics/AI conference deadlines.",
"text": null,
"date": "2016-11-19"
},
{
"vote": 7,
"title": "How can I verify the correctness of a sentence, preferably with a custom corpus?",
"text": "The Goal\n\n\nMy goal is to create an API that can verify how grammatically correct a sentence is.  I am using a Markov Chain to generate a bunch of lines and I want to rank them by how much sense they make.\n\n\nI want to be able to have some input like:\n\n\n[ \n  &quot;This sentence is totally great!&quot;,\n  &quot;Not great so sentence this one.&quot;,\n  &quot;From on in where is are for pig.&quot;\n]\n\n\n\nand then get some output like:\n\n\n[\n    0.71,\n    0.30,\n    -0.43,\n]\n\n\n\nWhere I'm currently at\n\n\nI've looked at using the Stanford Parser but I don't think there's a way to use your own corpus.\n\n\nCurrently, I am using a Microsoft joint probability cognitive service, which also doesn't allow a custom corpus and seems pretty rudimentary.\n\n\nDirect questions\n\n\nIs this a solved problem?\n\n\nWhat is this kind of problem/research called? (So I know how to google around for it)\n\n\nWhat methods are there for accomplishing something like this?",
"date": "2016-11-19"
},
{
"vote": 3,
"title": "Skip-Thought Vectors",
"text": null,
"date": "2016-11-17"
},
{
"vote": 5,
"title": "What is your preferred library for diarization?",
"text": "We've had some success with pyAudioAnalysis and LIUM, but are trying to find better solutions. Anybody have any hidden gems you can recommend?",
"date": "2016-11-15"
},
{
"vote": 20,
"title": "[NLP Research] What papers would you recommend I read to get a broad idea about the current state of NLP research?",
"text": "[deleted]",
"date": "2016-11-15"
},
{
"vote": 4,
"title": "NLP: Who knows a software that can create readable text from keywords?",
"text": "Dear Language Tech community, we are building a robot that create text from bits and pieces from the web (WikiMedia, dictionaries, etc.) for a Research project at the local Uni.\n\n\nDoes anyone know or work on a similar project, we'd like to exchange on our pains. Thanks.",
"date": "2016-11-15"
},
{
"vote": 10,
"title": "How to deal with probabilistic selection of topics in LDA (robustness)?",
"text": "I'm using gensim to model  the hidden topics of a corpus. Most of the time the topics make good sense, but a few times they make obviously no sense (for example getting a topic with the word \"internet\" associated strongly to a document from mid XXth century). It seems to me this is bound to happen in a few cases due to the probabilistic nature of the LDA model. This is an issue because any inference made with a single pass of LDA is not very robust, as the \"important\" topics will likely change a bit next iteration. I'm curious to hear some strategies you might use to overcome this. Thanks.",
"date": "2016-11-13"
},
{
"vote": 1,
"title": "Magento Translate Extension | Process9",
"text": null,
"date": "2016-11-09"
},
{
"vote": 3,
"title": "Can I use Google's SyntaxNet to parse datetimes?",
"text": "I know there are many NLP Date parsers out there, but I am interested in learning how to use Google Syntaxnet so if possible I would like to take the opportunity.\n\n\nFor starters, I want to be able to give in inputs with explicit time windows such as (warning, will be butchering time formats for readability): \n\n\n\n\n\"Come over to my place on Friday, for my birthday\" -> \"November 11, 2016\"\n\n\n\"Come over to my places this Friday evening for dinner\" -> \"November 11, 2016 - 18:00:00\"\n\n\n\n\nIf this is possible, I would even want to extend its capabilities to recognizing holidays such as:\n\n\n\n\n\"Come over to my place for Christmas dinner\" -> \"December 25, 2016\"\n\n\n\"Let's go to NYC for New Years Eve\" -> \"December 31, 2016\"\n\n\n\n\nNote: I'll only ever care about the future so I wouldn't care for past events at all.\n\n\nGiven the task at hand, is Google Syntaxnet capable of used as a solution to this problem and if so where do I start? I wasn't able to find any articles or tutorials on the subject.\n\n\nThanks.",
"date": "2016-11-06"
},
{
"vote": 2,
"title": "GATE for specific topic extraction",
"text": "I am new to both GATE and NLP. I have worked with some of the tutorials available on the GATE site and I have gotten a little (too little) insight into some of the concepts and methods. \n\n\nI want to use GATE to identify and extract specific topics in documents. For example suppose there was a corpora of home construction related documents. How would I go about identifying sections that discussed various risks/hazards in construction and extracting the topics in those sections? \n\n\nI can come up with some conceptual ideas combining JAPE and machine learning, but at this point I don't know if that is the best way to approach this. \n\n\nThank you for any suggestions/insight.",
"date": "2016-11-06"
},
{
"vote": 4,
"title": "Request For Data: original:corrected parallel corpora",
"text": null,
"date": "2016-11-03"
},
{
"vote": 4,
"title": "Resources for learning the math behind computational linguistics?",
"text": "[deleted]",
"date": "2016-10-27"
},
{
"vote": 3,
"title": "Wordsmith + IBM Watson to automate e-Commerce product descriptions",
"text": null,
"date": "2016-10-26"
},
{
"vote": 3,
"title": "What would be a good NLP project?",
"text": "I am a university student attending an NLP course this semester, and we will later this semester be graded on an NLP project. From his explanation, it seems that an application level project would not be good enough. The prof wants us to try to work closer with the theory.\n\n\nI have a few ideas, and I wanted to hear /r/LanguageTechnology's thoughts on them\n\n\nSome of them are very broad ideas: Recognize utterances, slang or speech/text involving noise.\n\n\n\n\nHave the model generate a question to demand clarification on a part of the provided sentence.\n\n\nA model that would label parts of a discourse with the topic being discussed at different moments\n\n\nRecognize lyrics in songs\n\n\nTry to find if two words are synonyms. I have been told by a classmate that this might not be as straightforward as it seems, because two words can appear in the same context but not be synonyms\n\n\nSummarize many noisy utterances into one sentence. Think of how on messenger, people might speak as such: \"Want to come over?; to the stadium; ?\"\n\n\n\n\nI think all of them are interesting, and it seems to me that good ones would be the lyrics, the summary of utterances, and the synonyms",
"date": "2016-10-26"
},
{
"vote": 1,
"title": "Which is a good/reliable free language translation API available?",
"text": null,
"date": "2016-10-26"
},
{
"vote": 3,
"title": "Does anyone know an NER tagger for organisations with gazetteer?",
"text": "Hi,\n\n\nDoes anyone know an NER tagger for organisations that features a gazetteer?\n\n\nThe problem I have is the following:\n\n\n\n\nI would like to tag organisations in sentences (ORG)\n\n\nThe NER tagger I have tried (i.a. spacy.io, Stanford, ...) are supersensitive to capitalisation and don't really understand if a noun really is an organisation or not (Toyota = ORG; toyota = not ORG).\n\n\nAlso the NER taggers I have seen are completely ignorant of clues in the sentence (e.g. verbs like \"manufactures\", \"issues shares\" etc. that are more suggestive of a company)\n\n\n\n\nWhich solution would be the best ORG NER tagger?\nIs there any NER tagger out there that has been preloaded with a list / ontology of the most well-known companies at least?\n\n\nMany thanks for any hints.",
"date": "2016-10-22"
},
{
"vote": 3,
"title": "Using doc2vec to detect offensive tweets",
"text": null,
"date": "2016-10-20"
},
{
"vote": 1,
"title": "Free Course -What You Need for Website Translation",
"text": null,
"date": "2016-10-20"
},
{
"vote": 1,
"title": "Multilingual Website | Process9 - Classified Ad",
"text": null,
"date": "2016-10-20"
},
{
"vote": 1,
"title": "What would be the best way to translate a word programmatically?",
"text": "Given a word and its POS in original language (say English), I need to obtain its most frequent translation into another language (say Italian). What is the most reliable way to do that programmatically?\n\n\nThanks!",
"date": "2016-10-19"
},
{
"vote": 1,
"title": "Is training a named entity tagger the right approach for this problem?",
"text": "So I have a large dataset of grade information tied to classes and professors and I'm trying to build an application to convert raw user queries into structured SQL queries.\n\n\nAn example of a couple queries that users can type are below:\n\n\n\n\n\"Compare CS 100 and CS 99\"\n\n\n\"Compare CS 100 from Tom Johnson vs Fred Allen\"\n\n\n\"BIO 74\"\n\n\n\"Autumn 2009 CS 210\" \n\n\n\"Autumn CS 200 vs Winter CS 200\"\n\n\n\n\nThe word_tokenizer from the python's nltk package doesn't seem to cut it since it will effectively split the strings on spaces, but I want the tokenizer to also group entities like \"CS 200\" and \"Tom Johnson\" together so it'd be relatively easy to query for. I was looking into training something like Stanford's Word Segmenter, but the API didn't look that well documented and I would prefer to keep my stack in a single language.\n\n\nAny other approaches that would work well? Would using something like an ANN or Naive Bayes Classifier be appropriate for classifying different parts of these phrases? I fear that I wouldn't have a ton of features to work with considering the phrases are fairly short.",
"date": "2016-10-19"
},
{
"vote": 17,
"title": "Topic Modeling for Humans, and the Advance of NLP (interview with gensim's Lev Konstantinovskiy)",
"text": null,
"date": "2016-10-19"
},
{
"vote": 3,
"title": "Train rnn using ea",
"text": "Training vanilla rnn's is hard because of exploding and vanishing gradient problem. What if we don't use gradient at all. Is there any hope of training rnn's with ea like pso, aco, etc. Has anyone tried it? What were the results like?",
"date": "2016-10-17"
},
{
"vote": 3,
"title": "Incorporating word embeddings to train LSTM.",
"text": "Hi, \nI'm an undergraduate student working on a text generation task. I am unable to train a network using my pretrained word embeddings as weights for input layer to LSTM. My word2vec embedding is trained on a larger corpus and the training corpus is a subset of it. I'm mapping word vectors to embedding weights using word2vec model. My vocabulary for the task consists of some word2vec_vocab+additional words in corpus. The model is as follows:\n\n\nTrain data: vector with indices mapped using a dictionary consisting of word2vec model indices + additional word indices = full dict\n\n\nTest data: one hot vector with 1 at position w.r.t corpus(I am not using full dict for mapping, thus position indices differ.)\n\n\nModel\n:\n\n\n w2v_dim= 200\n\n\nseq_length= 7\n\n\nvocab_size= # of unique words in corpus\n\n\nmodel.add(Embedding(corpus_size, w2v_dim, mask_zero=False, weights=[embedding], input_length=seq_length)) \n\n\nmodel.add(LSTM(memory_units, return_sequences=True, init= \"orthogonal\"))\n\n\nmodel.add(Dropout(0.5))\n\n\nmodel.add(TimeDistributed(Dense(vocab_size, activation='softmax', init= \"orthogonal\")))\n\n\nProblem\n: Model not training. Overfitting on training data with increasing loss on validation set. \n\n\nWhat am I missing? Any ideas. Thanks.",
"date": "2016-10-15"
},
{
"vote": 8,
"title": "What NLP algorithms should I use for matching resumes and job posts?",
"text": "Hi, I'm new to NLP.\n\n\nI've scraped 200+ data analyst positions. What kind of strategies can I use to find the best-fitting and matchable jobs given a resume? The use case is that the user submits a resume and the algorithm will suggest the top 5 or 10 most suitable positions.\n\n\nThank you.",
"date": "2016-10-13"
},
{
"vote": 1,
"title": "generate a list of nicknames for a forum",
"text": "What's a decent algorithm to generate a list of nicknames for a forum? Or is there any database?",
"date": "2016-10-12"
},
{
"vote": 11,
"title": "Any ideas on how to go about programmatically checking if a sentence makes sense ?",
"text": "The goal is to be able to detect computer generated spun content. Here are some examples of spun text if you're not familiar:\n\n\n\"As a explicit art fashionable for an advertising organization, you will job to assist put up for auction customers' crop and/or armed forces to their aim marketplace by your original skill and technological ability.\"\n\n\n\"The actual apple iphone application shop is definitely an abundant cherish residence of useful apps.\"\n\n\nBasically, the computer has replaced words with various synonyms in an attempt to make content unique to bypass plagiarism detection. My goal is to make a system that can detect this gibberish text to a certain degree. If anyone has any ideas on where to start it would be much appreciated. Someone on another forum proposed POS tagging to begin with, but if the words are spun as synonyms I don't think that would work very well?",
"date": "2016-10-11"
},
{
"vote": 21,
"title": "A Natural Language Query Engine without Machine Learning",
"text": null,
"date": "2016-10-08"
},
{
"vote": 1,
"title": "Developers Needed For IPTC's EXTRA Rules-based Classification Engine",
"text": "[deleted]",
"date": "2016-10-07"
},
{
"vote": 1,
"title": "How to learn japanese",
"text": null,
"date": "2016-10-07"
},
{
"vote": 9,
"title": "Semantics â€” What does data science reveal about Clinton and Trump ?",
"text": null,
"date": "2016-10-06"
},
{
"vote": 4,
"title": "DC NLP meetup, partnering with Hack &amp;&amp; Tell -- Oct 11th",
"text": null,
"date": "2016-10-04"
},
{
"vote": 1,
"title": "What Are the Benefits of Giving Chatbots a Personality? a Q&amp;A with Jordi Torras, Founder of Inbenta",
"text": null,
"date": "2016-10-03"
},
{
"vote": 1,
"title": "Language Translation Services | Process 9",
"text": null,
"date": "2016-09-30"
},
{
"vote": 6,
"title": "Is there a way to extract the main word from a list of synonyms that represents the list?",
"text": "The list contains mostly adjective words for facial expressions, i.e. \"happy\", \"joyful\", \"exuberant,\" etc. but may also contain dissimilar words like \"peaceful\" or \"content\". I've tried Similarity for WordNet, but it doesn't work well across parts of speech and the normalization feature is buggy.",
"date": "2016-09-29"
},
{
"vote": 3,
"title": "how do you train your kneser-ney smoothing model for next word prediction?",
"text": "I am working on a project to predict the next word in a text. I have used the quanteda package in R to generate tri-grams and bi-grams. I am aware that we need to maximize the probability of the sentences that are in the held out test set. However I am not sure how to go about it. Any help would be great :)",
"date": "2016-09-28"
},
{
"vote": 7,
"title": "Word2Vec and multiple sentence skips",
"text": "Lets say I Have the following two setences \n&quot;I went to the beach on Saturday. Its unlikely it will rain&quot;\n. If the current center word is \nSaturday\n and the number of skips is 2, would my batch for this window look like \n[&quot;beach&quot;, &quot;on&quot;, &quot;Its&quot;, &quot;unlikely&quot;]\n ? Im trying to understand how do these models deal with periods, and separate sentences, especially if you have paragraphs.",
"date": "2016-09-27"
},
{
"vote": 1,
"title": "Language Translation Software | Process9",
"text": null,
"date": "2016-09-27"
},
{
"vote": 13,
"title": "Tweets analysis about candidates of presidential elections 2016",
"text": "Hello all! \n\n\nMy name is Alex and I made a website to understand what people feel about candidates of the US Presidential elections 2016. \nBriefly - the website reads all tweets in realtime with mentions of the candidates, calculates polarity and subjectivity using \nhttp://nltk.org/\n and simply counts the amount of positive and negative tweets.\nThe outputs are simple \"bars\" that show how many tweets are good/bad.\n\n\nPosting here to find professional opinion on how to make results more interesting and accurate.\n\n\nThe current problems I am facing are:\n\n\n\n\nI cannot differ \"sarcasm\" from real positive tweets - yes, there is a lot of sarcasm about candidates and I realise that this is hard to achieve, maybe this is not an option for such short-term-made website (2 weeks)\n\n\nI collected about 5.5M tweets since beginning of September and honestly I have no idea how to visualise something important from this bulk, maybe you can help me with an idea (like how often people use bad language referring to the candidates)?\n\n\n\n\nThanks a lot for your help!\n\n\nLink to my website: \nwww.whoispresident.com",
"date": "2016-09-26"
},
{
"vote": 13,
"title": "Gentle Introduction Into WordNet with Python 3",
"text": null,
"date": "2016-09-25"
},
{
"vote": 2,
"title": "How to fill out this info?",
"text": null,
"date": "2016-09-23"
},
{
"vote": 3,
"title": "NK government probably needs accurate sarcasm classification!",
"text": null,
"date": "2016-09-22"
},
{
"vote": 5,
"title": "List of English sentences",
"text": "Where can I find a plain text file of sample English sentences? Preferably thousands of examples. Also preferably not books because of the large amount of \"quotations\".",
"date": "2016-09-19"
},
{
"vote": 9,
"title": "What is the most efficient way to preprocess annotated text in markup formats for deep learning?",
"text": "[deleted]",
"date": "2016-09-15"
},
{
"vote": 10,
"title": "Question: When, and at what word error rate, will speech recognition make human transcribers mostly redundant",
"text": "[deleted]",
"date": "2016-09-14"
},
{
"vote": 6,
"title": "â€‹Microsoft's newest milestone? World's lowest error rate in speech recognition | ZDNet",
"text": null,
"date": "2016-09-14"
},
{
"vote": 11,
"title": "What publications are helpful for developing content ingestion engine?",
"text": null,
"date": "2016-09-13"
},
{
"vote": 4,
"title": "How do I convert audio to natural language text processing?",
"text": null,
"date": "2016-09-11"
},
{
"vote": 1,
"title": "News Aggregation for Trading Solutions",
"text": null,
"date": "2016-09-08"
},
{
"vote": 5,
"title": "Is there tutorial about depiction module?",
"text": null,
"date": "2016-09-05"
},
{
"vote": 6,
"title": "Extraction of a characters, that are used to segmentation of a text, without language model (in Java).",
"text": "Quick problem explanation: let's say there is a text that has a lot of emoji that are used in the text segmentation.\n\n\nHere is an example of such: \nhttps://gist.github.com/b0noI/4b4ac7702f5ab36b5add00196cc2fdc0\n . Text where the emoji is used as a sentence splitter (however not all emoji from the text are the sentence splitters). This could be a common case in the chat between a people who actively using emoji. \n\n\nSuch example cannot be correctly split on a sentences by majority of the algorithms(and language models). Here, check yourself, try to split the text on the sentences with NLTK: \nhttp://textanalysisonline.com/nltk-sentence-segmentation\n (just paste the text there) and you will see how far is the result from the correct answer.\n\n\nIn order to solve the problems I have created an algorithm that allows to extract characters that are used for the sentence segmentation, directly from the text without any language models. The algorithm has been explained (with the Python examples in one of my previous videos: \nhttps://youtu.be/L5sxl3BSXz8\n). Today I have created new video that show how the algorithm was implemented in Java in the AIF library as well as how AIF could be used in any Java/Maven projects.\n\n\nLink to the new video: \nhttps://youtu.be/7LwPVDOm13s\n\n\nAny feedback and help are very welcome. Also if someone interested in helping - i'm building a cloud service that can provide all the AIF functionality through the REST API. \n\n\nPS: also eventually I will cover all algorithms from the AIF on my course. At the moment AIF can: split sentences, naive stemming, NER, build relation graph between NE, find most informative words/sentences of the text, build short summary of the text, compare similarity of texts (essentially grouping of the texts by topics); // this part is not production ready.",
"date": "2016-09-05"
},
{
"vote": 4,
"title": "â–¼ Is there any probabilistic parser online?",
"text": null,
"date": "2016-09-04"
},
{
"vote": 4,
"title": "Dear NLP Overlords, Please help me with Speech Analytics!",
"text": "I work in an organization which lends to SMEs and individuals. Before the loan is given, an interviewed is conducted of the potential recipient. I want to do speech analytics of this interviewed audio file and figure out people who might possibly default\nPlease suggest me how to do speech analytics of this audio file. \n\n\nI have few initial thoughts but not able to make any headway\n\n\n\n\nMaking features for every 10 seconds and note down amplitude (or some other parameters) and make a predictive model. \n\n\n\n\nIndependent Component Analysis of audio files. To be honest, I donâ€™t know what to with this technique and what to look for\n\n\n\n\nAudio to text conversion. I can make bag of words and use the most recurring words as variables in the logistic regression if the person will default or not. However, I donâ€™t know how to convert audio files into text in R or Python or any other software.\n\n\n\n\n\n\nI would really appreciate any help in this regard.\n\n\nThanks",
"date": "2016-08-31"
},
{
"vote": 13,
"title": "Video about the algorithm that extracts a special characters(.,:!?) from a tokens without language model. Even if special characters are emoji.",
"text": "In scope of my course \"NLP in a nutshell\"(\nhttps://www.youtube.com/channel/UC9EwPNhdtRob_IhKQ0ViVnQ\n) I have recorded a new video (\nhttps://youtu.be/L5sxl3BSXz8\n) about the algorithm that extracts special characters from the text. By special characters I mean characters that are divides tokens on segments (like sentences, or parts of the sentences).\n\n\nThe algorithm works without any language model. It \"learns\" language from the text, so it correctly works even if, for example, emoji are used as a text splitters.  In the video I'm showing the example when NLTK with \"default\" english model have failed to segment the text onto sentences.\n\n\nAlgorithm that was described in the video had been implemented in Java, as a part of the AIF: \nhttp://aif.io\n\n\nAny feedback are welcome, as well as any contributors to the course or to the AIF library.",
"date": "2016-08-28"
},
{
"vote": 8,
"title": "Bukowski's Poems Sentiment Analysis",
"text": null,
"date": "2016-08-26"
},
{
"vote": 1,
"title": "Install WordPress Multilingual Plugin and Grow Your Business.",
"text": null,
"date": "2016-08-25"
},
{
"vote": 1,
"title": "Process9 - Online Language Translation",
"text": null,
"date": "2016-08-24"
},
{
"vote": 5,
"title": "identify and extract structured data from unstructured text",
"text": null,
"date": "2016-08-23"
},
{
"vote": 1,
"title": "Get Language Translation API for Your Business from Process9",
"text": null,
"date": "2016-08-23"
},
{
"vote": 1,
"title": "Language Translation Services - Process9",
"text": "[deleted]",
"date": "2016-08-20"
},
{
"vote": 27,
"title": "Facebookâ€™s Artificial Intelligence Research lab releases open source fastText on GitHub",
"text": null,
"date": "2016-08-18"
},
{
"vote": 16,
"title": "Natural Language Interface for Databases",
"text": "[deleted]",
"date": "2016-08-16"
},
{
"vote": 6,
"title": "Looking for research internships in NLP.",
"text": "[deleted]",
"date": "2016-08-16"
},
{
"vote": 8,
"title": "From Stanford - Kaggle for word embedding evaluation",
"text": null,
"date": "2016-08-12"
},
{
"vote": 1,
"title": "Visualizing &amp; Understanding Recurrent Neural Networks with Andrej Karpathy, OpenAI",
"text": null,
"date": "2016-08-10"
},
{
"vote": 6,
"title": "A hosted API for conversational analysis and telephony. It's still early, but I'd love to hear your feedback and suggestions.",
"text": null,
"date": "2016-08-09"
},
{
"vote": 10,
"title": "Hey folks! I'm looking for a .txt of all of the words from Webster's dictionary. Any links?",
"text": "I've found at least one dictionary.txt on GitHub, but it contains a lot of interesting, foreign sounding words that aren't defined in Webster's.  I intend on using this list for an oncology NLP project and thought you folks would be able to help out.  Thanks in advance!",
"date": "2016-08-08"
},
{
"vote": 2,
"title": "Need help reproducing a paper (Linguistic Regularities in Sparse and Explicit Word Representations)",
"text": "Hello everyone,\n\n\nI am a student in computational linguistics from germany and one of my tasks is reproducing the results of this paper: \nhttp://www.aclweb.org/anthology/W14-1618\n\n\nFor that, I downloaded a wikipedia corpus (enwiki-20151201-pages-articles-multistream.xml, couldn't find a version of the years 2012) and cleaned it with the help of \nhttp://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n . Additionally I performed the steps written in the paper:\n\n\n We extracted\nall sentences from article bodies (excluding titles,\ninfoboxes, captions, etc) and filtered nonalphanumeric\ntokens, allowing mid-token symbols\nas apostrophes, hyphens, commas, and periods.\nAll the text was lowercased. Duplicates and sentences\nwith less than 5 tokens were then removed.\n\n\nThen I used the hyperword corpus to train the explicit embeddings and the word2vec embeddings (using the word2vecf library).\n\n\nHere are the parameters used:\n\n\nA) Window size 2 with \"clean\" subsampling\n\n\nmkdir w2.sub\npython hyperwords/corpus2pairs.py --win 2 --sub 1e-5 ${CORPUS}.clean > w2.sub/pairs\n\n\nscripts/pairs2counts.sh w2.sub/pairs > w2.sub/counts\n\n\npython hyperwords/counts2vocab.py w2.sub/counts\n\n\nCalculate PMI matrices for each collection of pairs\n\n\npython hyperwords/counts2pmi.py --cds 0.75 w2.sub/counts w2.sub/pmi\n\n\nCreate embeddings with SVD\n\n\npython hyperwords/pmi2svd.py --dim 500 --neg 5 w2.sub/pmi w2.sub/svd\n\n\ncp w2.sub/pmi.words.vocab w2.sub/svd.words.vocab\n\n\ncp w2.sub/pmi.contexts.vocab w2.sub/svd.contexts.vocab\n\n\nCreate embeddings with SGNS (A). Commands 2-5 are necessary for loading the vectors with embeddings.py\n\n\nword2vecf/word2vecf -train w2.sub/pairs -pow 0.75 -cvocab w2.sub/counts.contexts.vocab -wvocab w2.sub/counts.words.vocab -dumpcv w2.sub/sgns.contexts -output w2.sub/sgns.words -threads 10 -negative 15 -size 600;\n\n\npython hyperwords/text2numpy.py w2.sub/sgns.words\n\n\npython hyperwords/text2numpy.py w2.sub/sgns.contexts\n\n\npython hyperwords/analogy_eval.py PPMI w2.sub/pmi testsets/analogy/google.txt\n\n\npython hyperwords/analogy_eval.py --eig 0 SVD w2.sub/svd testsets/analogy/google.txt\n\n\npython hyperwords/analogy_eval.py SGNS w2.sub/sgns testsets/analogy/google.txt\n\n\nNow, my Problem:\n\n\nGoogle Analogy Results   cosadd cosmul\n\n\n-â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â -â \n\n\nPPMI w2.sub/â pmi         0.495   0.637\n\n\nSVD w2.sub/â svd          0.371   0.413\n\n\nSGNS w2.sub/â sgns        0.493   0.531\n\n\nThe PPMI Embeddings look great with 3CosAdd and 3CosMul, nearly the same how the paper reports them:\n\n\nme      | paper\n\n\n0.495 |  0.45\n\n\n0.637 |  0.68\n\n\nSo, my results differ only slighty. But with the word embeddings, the picture is absolutely different:\n\n\nme | paper \n\n\n0.493 | 0.62\n\n\n0.531 | 0.66\n\n\nAs you can see, I nearly have NO difference between CosAdd and CosMul.\n\n\nAnyone can see what I am doing wrong?\n\n\nDid further analysis, and my explicit embeddings are able to reproduce the paper results very good. Only the Neural Embeddings are worse, so only the word2vecf calculation could be wrong:\n\n\nCreate embeddings with SGNS (A).\n\n\nThe rest, which is created in the following paragraph, is also used for the explicit embeddings and gives very good results there:\n\n\nA) Window size 2 with \"clean\" subsampling\n\n\nmkdir w2.sub\npython hyperwords/corpus2pairs.py --win 2 --sub 1e-5 ${CORPUS}.clean > w2.sub/pairs\n\n\nscripts/pairs2counts.sh w2.sub/pairs > w2.sub/counts\n\n\npython hyperwords/counts2vocab.py w2.sub/counts\n\n\nThank you in advance.\n\n\nBest, Michael.",
"date": "2016-08-08"
},
{
"vote": 3,
"title": "List of 500+ intransitive verbs?",
"text": "I'm looking for an extensive list of intransitive verbs, to keep my program's language generation from asking \"What do you sleep?\" and the like. Alternatively an extensive list of transitive verbs would allow me to restrict these questions to transitive verbs only. All I've been able to find were very short lists.",
"date": "2016-08-08"
},
{
"vote": 1,
"title": "Clean Simple Wikipedia dataset? (x-post /r/MachineLearning)",
"text": null,
"date": "2016-08-08"
},
{
"vote": 3,
"title": "Want to make twitterbot sound disappointed â€“ best way to go about?",
"text": "Should I try to premake 100-200 different tweets that sound disappointed and use that to train my model? Or should I just premake the tweets and have the bot randomly spit them out? I feel like 200 tweets wouldnt be enough.",
"date": "2016-08-05"
},
{
"vote": 11,
"title": "Learning to generate names with restricted Boltzmann machines (x-post /r/machinelearning)",
"text": "I trained some RBMs on domains like place names, people names, and GitHub repos. The results were pretty amusing! Writeup \nhere\n\n\nIf you just want to see some of the generated samples, I made some little web apps for that:\n\n\n\n\nRepo generator\n\n\nPlace name generator\n\n\nActor name generator",
"date": "2016-08-02"
},
{
"vote": 3,
"title": "ACL 2016 poster size",
"text": "Is there anyone making a poster for ACL 2016 in Berlin that can tell me the size of the posters to make.\n\n\nI can't find anything on the ACL page, though for NAACL 2016, I see the guideline was 8' wide (92 inches/233 cm) and 4' high (46 inches/116 cm).\n\n\nhttp://naacl.org/naacl-hlt-2016/presenter_info.html\n\n\nThanks",
"date": "2016-08-01"
},
{
"vote": 9,
"title": "How to write a spelling corrector in python",
"text": "[deleted]",
"date": "2016-07-29"
},
{
"vote": 5,
"title": "Using IBM Watson Conversation Service to Answer Questions Related to Home Insurance",
"text": null,
"date": "2016-07-28"
},
{
"vote": 0,
"title": "Easy to learn R Language. Basic syntax and comments",
"text": null,
"date": "2016-07-28"
},
{
"vote": 4,
"title": "Reverse spellcheck module for JavaScript in 100 sloc. Not technically NLP, but I thought I'd share to get some feedback.",
"text": "[deleted]",
"date": "2016-07-27"
},
{
"vote": 6,
"title": "Why Google Scholar won't tell you the best comp.ling. journals and conferences",
"text": null,
"date": "2016-07-26"
},
{
"vote": 7,
"title": "Exploring social media diversity using language processing algorithms",
"text": null,
"date": "2016-07-25"
},
{
"vote": 1,
"title": "shadow beast",
"text": null,
"date": "2016-07-23"
},
{
"vote": 2,
"title": "Applying NLP methods to Native American Language Maintenance/Revitalization?",
"text": "Hi folks. New kid on the block here. I work for a Native Californian tribe coordinating a language revitalization program.\nI'm super interested in figuring out various applications of NLP to Language Revitalization efforts. Anyone have any experience/ideas with this?\nMaybe I'm living a dream, but wouldn't it be cool if we had software that could read APA or IPA and spit out (even a rough) version of the input language?\nImagine something like Scannable that reads the handwriting on fieldnotes out loud?\nWhat's out there",
"date": "2016-07-20"
},
{
"vote": 21,
"title": "Google cloud natural langurage API",
"text": null,
"date": "2016-07-20"
},
{
"vote": 7,
"title": "Computer Scientist looking for a NLP career opportunity abroad",
"text": "[deleted]",
"date": "2016-07-14"
},
{
"vote": 12,
"title": "Companies Working in NLP That Would Hire Entry Level Positions",
"text": "Hi, I'm a recent college graduate who majored in computer science with a focus in NLP and digital humanities. I have about a year and a half's worth of experience assisting graduate research in NLP, including a fairly extensive experience with LDA modeling and word2vec.  However, most of the jobs I've looked at that work in NLP require 3+ years of experiance in the field, or a grad degree. I'm wondering if it would be possible for me to find a job in NLP (or any similar field of big data analytics) considering my background in undergrad research. Does anyone know of any companies that are known or might be willing to hire recent graduates? I'm really passionate about the field, and would love to continue in a similar field as my research without taking the leap to grad school quite yet.",
"date": "2016-07-12"
},
{
"vote": 10,
"title": "Natural Language Understanding by Matching Parse Trees",
"text": null,
"date": "2016-07-09"
},
{
"vote": 2,
"title": "French POS Tagger query",
"text": "Does anyone know of a POS tagger for french that tags numbers explicitly and that is free for commercial use? \nThanks, much appreciated!",
"date": "2016-07-07"
},
{
"vote": 7,
"title": "By data mining a vast collection of novels, researchers have identified the six basic plots that all stories follow (X-post from /r/books)",
"text": null,
"date": "2016-07-07"
},
{
"vote": 7,
"title": "Analysing NLP publication patterns",
"text": null,
"date": "2016-07-07"
},
{
"vote": 4,
"title": "How to teach a computer common sense",
"text": null,
"date": "2016-07-05"
},
{
"vote": 7,
"title": "Where does this quote of Michael Collins come from?",
"text": "[deleted]",
"date": "2016-07-04"
},
{
"vote": 31,
"title": "Heavy Metal and Natural Language Processing - together at last",
"text": null,
"date": "2016-06-30"
},
{
"vote": 1,
"title": "Question: How change a text to fit a different language model",
"text": "Let's say I have a bunch of tweets and some newspaper articles and for some reason I want the newspaper to to look more like tweets. \n\n\nI'm thinking of applying a generative language model, either as an RNN that is trained on twitter and then generates text with a boost to its confidence on the current correct word, or something along the lines of p(twitter_word | newspaper_word_up_til_current_point)\n\n\n\n\nDo anybody have experience with doing this?\n\n\nAny papers that apply such methods, perhaps for domain transfer?",
"date": "2016-06-30"
},
{
"vote": 1,
"title": "Question About LIWC",
"text": "Hello to all readers,\n\n\nI have a question that i hope to get some help with, i'm looking for a way to detect personality traits in text and i've stumble onto LIWC, but since the project is proprietary is there a similar lexicon dictionary but in an open source mode ? or are we limited to other style features like the penn-treebank ?\n\n\nThank you all in advance",
"date": "2016-06-25"
},
{
"vote": 7,
"title": "I have a specific usecase in NLP",
"text": "The thing is, I am trying to build an educational knowledge map which will essentially do the following:\n\n\n\n\nRead through Wikipedia and other resources and gather tonnes of documents that fall under the educational category.\n\n\n\n\nNext, I want to leverage these documents to identify multi-word tokens that, essentially are \"domains\" and multi-level \"sub-domains\" for these domains.\n\n\n\n\n\n\nI can collect the documents now because I have a fairly decent experience in creating data mining models but I have no experience in extracting syntax or establishing relations or even creating smart and contextual multi-words keywords in the first place. I need help in identifying resources where I can get started on working on this. Additionally, if there is a tool that already does that, that will be even more helpful.\n\n\nThanks in advance.",
"date": "2016-06-20"
},
{
"vote": 1,
"title": "Sentiment Analysis and Topic Detection in R using Microsoft Cognitive Services",
"text": null,
"date": "2016-06-20"
},
{
"vote": 1,
"title": "StrepHit: an intelligent reading agent that understands human language and extracts facts from text. 1.0 beta release",
"text": null,
"date": "2016-06-15"
},
{
"vote": 14,
"title": "Build an AI Reader w/ Parsey Mcparseface",
"text": null,
"date": "2016-06-12"
},
{
"vote": 5,
"title": ".sgm formatting in the newstest2015 from wmt'15",
"text": "The test (and development) set used in the popular \nwmt'15 dataset\n is in the format \n.sgm\n - Standard Generalized Markup Language.\n\n\nThe file format does not seem very common, so I assume there is a specific script (somewhere) to turn the test data - i.e. \nnewstest2015_ende.de.sgm\n into a one-segment-per-line format.\n\n\nAt least wmt supplies a \nscript\n to turn one-segment-per-line format into a \n.sgm\n format.\n\n\nHow do I go from \n.sgm\n to one-segment-per-line?",
"date": "2016-06-12"
},
{
"vote": 0,
"title": "Helical Insight : First open source BI tool with inbuit workflow, API driven architecture, inbuilt machine learning.",
"text": null,
"date": "2016-06-11"
},
{
"vote": 9,
"title": "Deep Learning for NLP (without Magic) - Part 1",
"text": null,
"date": "2016-06-10"
},
{
"vote": 8,
"title": "Build a Chatbot in 5 Minutes",
"text": null,
"date": "2016-06-08"
},
{
"vote": 6,
"title": "Are there any resources for learning Natural Language Generation which explains using code? Any open source projects? Preferably in Python.",
"text": null,
"date": "2016-06-08"
},
{
"vote": 6,
"title": "How we use NLP to qualify sales leads",
"text": null,
"date": "2016-06-07"
},
{
"vote": 1,
"title": "Best way to automatically validate/verify information?",
"text": "I'm doing research where I detect rumors on Twitter using a classifier with 40 features (already completed) and then verify the rumor (starting this now) to gauge the spread of misinformation around a particular topic.\n\n\nWhat would be the best technique to go about validating this information?\n\n\nSpecifically, I am looking at Zika and rumors around it. I can use CDC data to verify things like cases and places with Zika but other things I am unable to verify this way.\n\n\nAny ideas on what I should look into to automatically verify the classified rumors? Thanks!",
"date": "2016-05-31"
},
{
"vote": 2,
"title": "Natural Language Interfaces and Chatbots",
"text": null,
"date": "2016-05-31"
},
{
"vote": 10,
"title": "Please recommend a good, readily available solution for Semantic Similarity of text!",
"text": "I've been looking at \nhttp://lcl.uniroma1.it/adw/\n after reading a paper on it, but it doesn't handle negations at all (according to the system \"I love you\" is 100% similar to \"I don't love you\") and also has some other weird results every once in a while.\n\n\nIt's okay if it's not entirely state of the art, as long as it can handle negations.",
"date": "2016-05-30"
},
{
"vote": 4,
"title": "Using the Microsoft Cognitive Services Web Language Model REST API from R",
"text": null,
"date": "2016-05-24"
},
{
"vote": 2,
"title": "Generating TF-IDF scores on new samples",
"text": "[deleted]",
"date": "2016-05-19"
},
{
"vote": 8,
"title": "What is the most advanced open source natural language generation platform as of now?",
"text": "Hello guys,\n\n\nI am researching natural generation platforms. \n\n\nI have come accross \nQuill\n from Narrative Science. However, it is a closed platform.\n\n\nFurther, I found the following resources about open source implementations about \nNLG Systems\n. However, it seems pretty outdated.\n\n\nAny suggestions, what are the most advanced \nopen source\n natural language generation platform currently?\n\n\nThx in advance for your replies!",
"date": "2016-05-18"
},
{
"vote": 0,
"title": "Gadget claims to fit in your ear and translate foreign languages in real-time",
"text": null,
"date": "2016-05-17"
},
{
"vote": 2,
"title": "A question on text understanding",
"text": "[deleted]",
"date": "2016-05-13"
},
{
"vote": 3,
"title": "[Results] Poem Generator Evaluation",
"text": "I made a post a few days ago: \nhttps://www.reddit.com/r/LanguageTechnology/comments/4hxb11/turing_test_volunteers_for_my_poem_generator/\n\n\nLinks to the Tests:\n\n\nTest 1: \nhttps://docs.google.com/forms/d/1cVzoCUykBWqVIXMW1T3r-v_w9PlBphRoTBOkQr_7y9M/edit?usp=forms_home&ths=true\n\n\nTest 2: \nhttps://docs.google.com/forms/d/1SEk_8aFKS4FB2cbQwepUcKKx_NU04kLYyBt2vYHiHCY/edit?usp=forms_home&ths=true\n\n\nI thought I'd post the results and solutions for those interested.\n\n\nResult for test 1: \nhttp://imgur.com/qHE6Ufq\n\n\nResult for test 2: \nhttp://imgur.com/Wnnvtpe\n\n\nAlso solutions to the tests (these were the generated poems):\n\n\nTest 1: 3, 4, 7, 10\n\n\nTest 2: 2, 3, 6, 8",
"date": "2016-05-12"
},
{
"vote": 3,
"title": "Best technique for fact checking a tweet?",
"text": "I am researching how to id rumor tweets related to diseases and then verifying if the rumor is true or not.\n\n\nI currently have a classifier that is working pretty well at classifying the rumors but now I'm not sure of the best way to tackle the verification part.\n\n\nMy initial plan was to cluster the rumors into certain categories like location-based or symptom-based and then fact check against CDC data but I was wondering if anyone is aware of a NLP technique that I should look into that would accomplish this with decent accuracy. \n\n\nThanks!",
"date": "2016-05-11"
},
{
"vote": 7,
"title": "Is this the right way to identify idiomatic expressions?",
"text": "Hi, I'm making a study helper that analyzes the difficult vocabulary of a book or an article so that I can study/memorize them before reading it. I believe that is the best way to study because it does not interrupt the flow of reading and also reinforces the memorized vocabularies quickly. \n\n\nI have succeeded in extracting vocabularies but now I would like to extract idioms also. Since I'm new to natural language processing I did not find any useful tools for doing so and concepts such as collocations doesn't quite fit my need. So I have devised an algorithm that identifies the idiomatic expressions and I would like to know if there's any problems with this approach, because it will be a long and hard journey constructing regex for every idioms...\n\n\nStep 1: Convert the given sentence to their basic form via nltk.morphy\n\n\n'I think I bit off more than I could chew by taking the jobs'\n->\n'I think I bite off more than I can chew by take the job'\n\n\nStep 2. Make a regex out of each idioms\n\n\n'Bite off more than \\w+ can chew'\n\n\nStep 3. Match every regex made to the result of step 1. \n\n\nThanks for reading and here is the current version of my python program if it interests you. \n\n\nhttps://github.com/qria/Qria/blob/master/vocabulary.py\n\n\nanalyze_hard_vocabularies()\n analyzes the text and returns difficult vocabs and their definitions.",
"date": "2016-05-10"
},
{
"vote": 7,
"title": "[Hiring] NLP post-doctoral fellow position at Boston Children's Hospital and Harvard Medical School",
"text": "The Health Natural Language Processing Lab at Boston Childrenâ€™s Hospital is seeking a post-doctoral fellow to contribute to cutting edge research in the field of health natural language processing. Please see the linked job posting for more information and encourage your bright and energetic graduating students to apply.\n\n\nPDF with the job details and contact information:\n\nhttps://www.dropbox.com/s/wieilulmr49xz01/post_doc_description_2016_v3.pdf?dl=0",
"date": "2016-05-09"
},
{
"vote": 6,
"title": "Creating a DSL (Domain Specific Language) using ANTLR ( Part-II) : Writing the Grammar file.",
"text": null,
"date": "2016-05-06"
},
{
"vote": 1,
"title": "TOEIC Switzerland",
"text": null,
"date": "2016-05-04"
},
{
"vote": 4,
"title": "[Question] Coming up with a scoring scheme for a collection of documents",
"text": "I have a set of words (concepts), and I want to look for them in a collection of documents (forum posts, basically). I suppose this can be represented as\na Vector Space Model. The scores should represent how much a concept has been discussed. Individually (per post vector score) and globally (within the group of posts).\n\n\nThe scoring methods I know are based on inverse document frequency, which doesn't really work on this scenario (if a concept appears accross all posts should get a higher group score, not the opposite). I guess that length and some measure based of \"semantic richness\" (how many different concepts appear in a post, how many times) could be helpful. Any ideas? Maybe some papers that could point me in the right direction?",
"date": "2016-05-01"
},
{
"vote": 5,
"title": "Question generation from conversational text",
"text": "Hello all! \n\n\nI want to basically generate questions from conversational text such as video lectures. But this seems to be an impossible task due to the informality of the text. I have managed to extract the keywords using TextRank but am not able to generate any meaningful questions. Heilman's code is also not able to generate any meaningful questions at all.\n\n\nAny tips/suggestions on how to do this? Is it possible at all?\n\n\nThanks!",
"date": "2016-04-28"
},
{
"vote": 3,
"title": "State of the art models for anaphora/ coreference resolution?",
"text": "Was wondering if any one could provide me with papers describing the state of the art model for anaphora/corefrence resolution. Both neural and non-neural approaches are appreciated. Thanks.",
"date": "2016-04-27"
},
{
"vote": 9,
"title": "Can Natural Language Generation be used to write a patent? (simplest way?)",
"text": "Hi, \n\n\nI'm working on a pet project and could use some help of the \"point me in the right direction\" and/or \" don't bother with that\" variety.\n\n\nThe goal is to receive input from someone via a web form, which would be the seed of the document to be generated, and to output  a patent-formatted document fitting some reasonable parameters (subject matter restrictions primarily).  I need the document to be generated without human intervention/supervision.  It would be like a vending machine - input a brief description of your idea and receive a pretty decent looking document.  The more you write, the better the result.\n\n\nThe goal (initially) is not to produce an extraordinary document that would compete with one that was hand-crafted by an expert.  It would also not be tuned to reducing detailed academic papers into a different format. Instead, the goal would be taking a few paragraphs and fluffing them up with content obtained from the patent database and wikipedia.  \n\n\nI also need a practical solution.  I was an electrical engineering major in college, so while I'm not scared of programming, I am a crappy programmer at best, and too out-of-date, slow, and incompetent of a programmer at worst.  That means that I could modify some code if it is already very close to what I am trying to achieve (e.g., changing the corpuses or various tuning parameters), but not from scratch.\n\n\nI've been reading up on the subject, and there is so much information, it has been hard to determine what is possible within the state of the art, what is possible for me with my skills, and which particular tools / methods I should be focusing on.\n\n\nMy hope is that because the desired output document will be very structured and has limited scope, it may be easier to achieve.  I hope to leverage the stiltedness and technical vocabulary of patents to smooth over the limitations of the NLG system.  \n\n\nMethod 1: Smarten-up something simple like a markov chain.  I was reading about Pointwise Mutual Information, collocations, and the NLTK.  I think that this is mostly used for writing joke gibberish academic papers and the like.  I would need, however, for the document to not be gibberish.  It would need to be on topic - like a copy and paste job from a plagiarist who waited until the last minute to write a paper.  \n\n\nMethod 2:  A simplified version of deep learning, AI, NLG.  For training, I was looking at Digits, Tensor Flow, Torch etc.  One concern was properly identifying the most important topics from the seed data to use to search the corpuses, which would be used for fluffing.  Another concern was in how best to treat different structural features of the document (e.g., background versus detailed description).\n\n\nAn example: I want a patent document about my idea - a new eighth layer in the OSI model between the fourth and fifth layers.  I write up a blurb about my new layer, what it does, and its advantages.  The NLG system would pull information from wikipedia and the patent database relating to keywords I used (OSI, layers, etc.)  It would take my problem statement and fluff it up to generate a background section.  The beginning of the body of the document might include a discussion of the other seven layers to provide context for the new layer.  And so on.  It wouldn't get too off topic / down the rabbit hole and start producing documents filled with 100 page discussions of the history of tcpi/ip or anything.  And if there are 5 good discussions of a particular term, it may mix/remix them together so that even if two people submit very similar text seed inputs, the output will be noticeably different.",
"date": "2016-04-21"
},
{
"vote": 8,
"title": "News filters?",
"text": "Anybody using any existing services to aggregate web content (RSS, web scraping), filter it (boolean expressions), and spit out the resulting stream(s)?  Zapier / IFTTT not quite doing the trick for me...",
"date": "2016-04-20"
},
{
"vote": 5,
"title": "Java port of NLTK's VADER sentiment analyzer",
"text": "I was looking for a Java variant of \nNLTK's VADER\n sentiment analyzer but could not find one. So, I decided to make one myself. The full project is \nhere\n.\n\n\nI compared its results with that of NLTK's to make sure that the port if correctly programmed.",
"date": "2016-04-18"
},
{
"vote": 8,
"title": "LIWC or NLTK and n-grams?",
"text": "Hey there, i have to say first that im a total newb to Natural Language Processing, so forgive me my some naive questions ;)\n\n\nSo for my master thesis id like to analyse some text and i found LIWC and NLTK as options. Does anybody has experiences with it and if so, what is your experience? \n\n\nCan i calculate n-grams with these programs? If not, do you know any programs/ codes on github (preferably python code) which can do that? \n\n\nI want to form a weighted word matrix with the text. Does anybody has expereinces witth that? \n\n\nAny advice or sharing of experiences is highly appreciated :)",
"date": "2016-04-14"
},
{
"vote": 7,
"title": "[Question] Do I need to manually annotate more of my data so that I can train a system to automatically annotate it?",
"text": "Hey /r/LanguageTechnology  sorry if this seems like a dumb question but I've run into a bit of trouble with a task I'm working on and advice would be appreciated. (I also will post something over on /r/MachineLearning  since my question is NLP and ML related if nobody here could really help me). \n\n\nFor all of the machine learning tasks I've had in the past my data has been fully classified (usually with a numeric value or True/False associated with a string of text). I'm a total noob to machine learning so all I've done is really cleaned up data, put it into arff files, and run it through Weka (I work primarily in Java and Python).\n\n\nThe project I'm working on now is a corpus 50,000 words of text divided into strings of sentences. The basis of my corpus is 4 .txt files of speech from commentators of the video game CS:GO. I used the UAM Corpus to manually annotate events (single words that are \"game-changing\" events in the game) for one of the 4 .txt files, which then turns the .txt file into an XML file. I've never done anything with XML before, nor have I have done any machine learning work where only a portion of the data is annotated. I want to do feature extraction and POS tagging (also available with the UAM Corpus tool as a level of annotation) to see if a system could (I guess) extract events from a text.\n\n\nMy question is what would be the best way to go about training a system with a limited amount of annotated data either using NLTK or Weka, and how would I format the data to do so. I do plan on manually annotating everything at some point but as of now I can't since time is an issue. Sorry if I didn't really explain this that well and thanks in advance!",
"date": "2016-04-11"
},
{
"vote": 3,
"title": "Question Classification",
"text": "Hi i want to classify the questions at a very coarse level of what wher e who and Affirmation.I read a couple of papers i got you could use unigrams,head nouns (1st noun from the left and right ) and hypernym . But my question was if i was using unigram then do i need head nouns because it would include head noun.Are there any better features.And for vecctoring the features i am using Hashing Vectoriser from sklearn is there any other for vectoring the features.",
"date": "2016-04-07"
},
{
"vote": 2,
"title": "[Question] Matching dictionary phrases to corpus to form a \"word cloud\" with different weights",
"text": "NLP newbie here. This may be a loaded question. I'm currently using a Clojure/Java-based stack if that info helps.\n\n\nI have a large dictionary of keywords/phrases, many with only one word (e.g. \"cancer\") but some with multiple (e.g. \"public health\"). Some phrases are plural, many are not.\n\n\nWhat's they best way to get a list of all the keywords from the dictionary that are found in a given corpus, along with keyword frequencies, accounting for arbitrary pluralization?\n\n\nI'm open to any library/platform that will help me achieve this, although Clojure/Java would be preferred. Can OpenNLP help me here?",
"date": "2016-04-07"
},
{
"vote": 13,
"title": "Semantic Search with Latent Semantic Analysis: The Ugly Truth",
"text": null,
"date": "2016-03-29"
},
{
"vote": 5,
"title": "Use of log-likelihood keyness to compare successful and unsuccessful whitehouse.org petitions [OC]",
"text": null,
"date": "2016-03-29"
},
{
"vote": 7,
"title": "Trying to make a plugin that highlights well-written comments",
"text": "Hi everyone, I came here wondering if someone could give me some suggestions for creating a program in Python that basically shows the user the most intelligent sounding comments on a basis of vocabulary and sentence structure. Now I understand that this is a naive approach, but this is actually my first real project and I just want to see where it goes. Thanks.",
"date": "2016-03-28"
},
{
"vote": 7,
"title": "enter words, get a sentence or question back",
"text": null,
"date": "2016-03-24"
},
{
"vote": 4,
"title": "A Fast Unified Model for Parsing and Sentence Understanding",
"text": null,
"date": "2016-03-22"
},
{
"vote": 4,
"title": "Generating lists of words with matching Soundex codes",
"text": "I'm looking for suggestions for a starting point regarding my goal of generating lists of words with matching Soundex and/or Metaphone codes, perhaps using NLTK and WordNet.  Litscape.com has a tool that does just that:  \nhttp://www.litscape.com/word_tools/soundex_match.php\n\n\nbut I'd like to be able to do this programmatically with Python, so that I can give a list of words, generate Soundex codes for those words, then generate lists of matching words for each code.",
"date": "2016-03-16"
},
{
"vote": 5,
"title": "Part-Of-Speech-Tagging tool as learning aid",
"text": "Hey /r/LanguageTechnology!\n\n\nI am a compsci guy with some spare time and a passion for languages. I want to build a tool where you input some text and it outputs the text with the parts-of-speech and their marked in a visually appealing way. \n\n\nI know this would be helpfull atleast in the initial stages of learning the grammar of a language (at least for me!^^). Now my question to you: \nDo you know if something like this already exists?\n I did some research, but did not come up with anything too similar.\n\n\nAlso I want to offer many different languages in the later run, so I will build the tagger myself:) I don't want to base the tagging on a markov-chain or another probalistic approach. Instead I want to try do use merge and move from the unified grammar theory to build a really general framework (which maximizes code reuse) and then code in specific grammar rules by hand. \nI would be happy if you could also comment on this approach and/or its feasability\n.\n\n\nThank you for your time!\n\n\nEdit: As for the parser: A parser based on a principle & paramaters approach exists already! \nPAPPI\n This is basically what I wanted to build. It has a core grammar and adds variations to languages - which saves much work! Always a bit frustrating too see that someone built this idea I have like 20 years ago, but it also a sign that I am on the right track:)\n\n\nSo for now I will focus on build the learning tool.",
"date": "2016-03-13"
},
{
"vote": 1,
"title": "[Question] Question Answering Models",
"text": "Is there any paper or tutorial that explain how to develop  Question Answering Models ?",
"date": "2016-03-12"
},
{
"vote": 3,
"title": "What are some good multi-language sentence correction (spell check and grammar check) apis?",
"text": "I am working on a 140 char QandA app and the one of the main problems is it that there are a lot of spelling and grammatical errors. Android has an inbuilt spell checker! Is there a similar facility on iOS? Looking for something that can handle the grammar as well and a bunch of other features such as categorization, entity recognition, etc.",
"date": "2016-03-11"
},
{
"vote": 3,
"title": "Using word clusters as gazetteers for NER?",
"text": "From looking at relevant research it seems like that word clusters can somehow be used as \"gazetteers\" to help in NER tasks (see \nexample here\n). \n\n\nHowever, it's still not clear to me how one can use existing clusterings of words to help with NER benchmarks. For example coNLL-2003 requires a classification for each word in a sentence, not just labeling places or people etc.\n\n\nDoes anyone have any experience with such a task? What is a good/general way to use word clusters to help with NER?",
"date": "2016-03-08"
},
{
"vote": 8,
"title": "Evaluating Stephen Bax's proposed words for the Voynich manuscript using Word2Vec (x-post /r/linguistics)",
"text": "For the past couple weeks, I've been working on training a Word2Vec model on the \nVoynich Manuscript\n (mostly I've been trying to preprocess the transcription properly). I decided to try to use this Word2Vec model to evaluate \nStephen Bax's proposed translations\n of a few words in the manuscript. What I found was kinda interesting: although most of the plant names he translated seem to be close together, the word he translates as \"coriander\" doesn't fit in at all with the other words. You can read all about what I did (and check out some source code!) at \nmy blog\n.\n\n\nAlso, I'm fairly new to this sort of stuff, so if anyone notices something I'm doing wrong I'd love to know.",
"date": "2016-03-06"
},
{
"vote": 2,
"title": "I have a bunch of sentences of text messages I sent that I tagged. How do I create a concept cloud, where clicking on a word will link it to other words commonly associated with it?",
"text": null,
"date": "2016-03-05"
},
{
"vote": 1,
"title": "Want some use cases for Using NLP for development (or promotion or anything similar) for Language and Literature",
"text": "Hi! Please help me with be examples, or ideas, or anything related on how NLP can be used for Language and Literature. We have started a NLP innovation community in Nepal. We want to start working by researching NLP for Language and Literature.",
"date": "2016-03-05"
},
{
"vote": 1,
"title": "Divine Jessica shows on webcam appetizing forms in underwear QjUjOHSW7lP",
"text": null,
"date": "2016-03-03"
},
{
"vote": 1,
"title": "Divine Jessica shows on webcam appetizing forms in underwear dRwSq5",
"text": null,
"date": "2016-03-02"
},
{
"vote": 3,
"title": "Understanding conlleval (precision, recall, F1)",
"text": "I don't think I understand the output given by the famous \nconlleval\n script used to asses sequence classification tasks. \n\n\nIf you follow the link you'll see that the second line output is presumably the global (precision, recall, F1). \n\naccuracy:  84.08%; precision:  68.83%; recall:  80.83%; FB1:  74.35\n \n\n\nI don't understand how conlleval comes up with these numbers? I always thought they were meant to be averages of the columns beneath them, but they're not. :( \n\n\nAny ideas?",
"date": "2016-03-01"
},
{
"vote": 1,
"title": "Let ma be your slave. I really like it, baby! Sign up and find me AmazonGirl444 0uEXhh9swfn",
"text": null,
"date": "2016-02-28"
},
{
"vote": 4,
"title": "Word Embedding as a Learning To Rank Problem",
"text": null,
"date": "2016-02-28"
},
{
"vote": 1,
"title": "Get free webcam show! Just for registered users. QVhuzmgFhU",
"text": null,
"date": "2016-02-28"
},
{
"vote": 1,
"title": "Come to my page, a private show for free just for registred and eZhBWoNZ",
"text": null,
"date": "2016-02-28"
},
{
"vote": 1,
"title": "Beautiful girl shows pussy V4JKZICRbui",
"text": null,
"date": "2016-02-27"
},
{
"vote": 1,
"title": "Thank you for your service! I am very happy! wlZgPd",
"text": null,
"date": "2016-02-27"
},
{
"vote": 5,
"title": "Multiclass paragraph classification. Moving beyond tf-idf?",
"text": "Hi all,\n\n\nI've trained a basic linear SVM classifier that uses stochastic gradient descent to classify document by topic (15 potential topics).  It works pretty well, but I'm interested in seeing what would be possible using neural networks.  The problem, as I understand it, is that neural networks are not well suited to dealing with sparse data such as an tf-idf matrix.  I'd love to hear any suggestions on how to approach this problem.  Should I be using something like PCA to reduce the dimensionality beforehand?  If so, how do I empirically determine the number of features to retain?\n\n\nThanks!",
"date": "2016-02-27"
},
{
"vote": 1,
"title": "I have really enjoyed your site and I have met a new lady and things are going well dMERLOf",
"text": null,
"date": "2016-02-27"
},
{
"vote": 1,
"title": "I have met the most amazing man and cannot quite believe what is happening between us 4Yx2Jiea3V8",
"text": null,
"date": "2016-02-27"
},
{
"vote": 1,
"title": "I've found the perfect girl - the love of my life VYEPBz03v",
"text": null,
"date": "2016-02-27"
},
{
"vote": 1,
"title": "Watch lesbians touch a penis for the first time Cod27LW9",
"text": null,
"date": "2016-02-27"
},
{
"vote": 1,
"title": "acquaintances and virtual striptease MDeWv2QVd",
"text": null,
"date": "2016-02-26"
},
{
"vote": 6,
"title": "What is the state-of-the-art in machine translation on Chinese/CJK?",
"text": "Sorry for my unfamiliarity with the field. Hopefully this question is on-topic here.\n\n\nCurrently the machine translation from western European languages to English is arguably quite robust, with Google Translate able to quite accurately translate articles on Wikipedia or news site. However, it seems to me that the translation between Chinese/Japanese and English is still quite off: Google Translate is frequently unable to produce a reasonably coherent/meaningful translation for even just one Chinese/Japanese sentence.\n\n\nIt gets me wondering, is the state-of-the-art in CJK machine translation still much off the pace compared with European languages? Or is it just that some research advances haven't been applied to the industry yet? What is the current status of the cutting-edge researches on this field, e.g. novel translation models/improvements on precision rate. Where should I go to look for more information, e.g. relevant literature review/paper/conference?",
"date": "2016-02-26"
},
{
"vote": 1,
"title": "Shows itself here q1k1PBD4",
"text": null,
"date": "2016-02-25"
},
{
"vote": 1,
"title": "Does anybody know where I can find the CONLL-2007 English data?",
"text": "[removed]",
"date": "2016-02-25"
},
{
"vote": 1,
"title": "I am 18 years old and I love sex with unfamiliar guys. Register - arrange a meeting, it's free. 8dfhx6WL",
"text": null,
"date": "2016-02-24"
},
{
"vote": 6,
"title": "I extracted parts of speech from a set of text. How can I generate random sentences from that?",
"text": "[deleted]",
"date": "2016-02-24"
},
{
"vote": 3,
"title": "Counting counts -- philosophical arguments for using statistics to process language",
"text": null,
"date": "2016-02-23"
},
{
"vote": 4,
"title": "I want to build a spam classifier. But where do I start?",
"text": "[deleted]",
"date": "2016-02-22"
},
{
"vote": 3,
"title": "Sentiment of pronouns in different languages?",
"text": "Does anyone have links to a study or a paper on the sentiment of different pronouns in different languages?\n\n\nFor example, does the pronoun \"I\" in English have a different emotional polarity from the pronoun \"yo\" in Spanish? Would referring to \"you\" directly be considered more aggressive in one language than in another?\n\n\nThis seems like an interesting area of research to me. I'm new to NLP so I apologize if this is an trivial or unproductive question.",
"date": "2016-02-19"
},
{
"vote": 1,
"title": "HEY! THOUSANDS OF MEMBERS ARE LOOKING FOR CASUAL SEX IN YOUR NEIGHBOURHOOD! 87y2Abh59Cm9",
"text": null,
"date": "2016-02-19"
},
{
"vote": 1,
"title": "THOUSANDS OF MEMBERS ARE LOOKING FOR CASUAL SEX IN YOUR NEIGHBOURHOOD! RQ8r",
"text": null,
"date": "2016-02-18"
},
{
"vote": 1,
"title": "Videos from the REâ€¢WORK Deep Learning Summit, San Francisco, January 2016 - feat Andrej Karpathy, OpenAI; Hassan Sawag, eBay; Clement Farabet, Twitter; Andrew Ng, Baidu &amp; more",
"text": null,
"date": "2016-02-17"
},
{
"vote": 7,
"title": "Evaluation of word embeddings",
"text": "Hi!\n\n\nI am working on my Bachelor Thesis in Computational Linguistics, for which I produce word embeddings with word2vec, using different sets of parameters.\n\n\nNow I want to evaluate them to find which set of parameters results in the most expressive word embeddings in my case. I know that people used various techniques for this step in the past: \n\n\n\n\nFinding nearest neighbors (similar words), Question-Answer-Tasks (Berlin to Germany is like Paris to ...?)\n\n\nUsing standard benchmarks test like sentiment classification to feed the word embeddings into and see which produce the best results.\n\n\n\n\nHowever, I want to avoid those types of evaluation for the following reasons:\nAbout 1.: I don't work with English word embeddings, so evaluation data sets are much more sparse. Also, I feel like a human bias might be (unconsciously) introduced when picking the semantic relations used for Q&A-tasks: Relations like \"x is capital of y\" proved to work very well word embeddings, still they are very prominent candidates in these data sets.\nAbout 2.: Those standard benchmarks are extrinsic ways of evaluation. And while they might show a tendency about the quality of word embeddings, the fact that my word embeddings work well on benchmark task A does not mean that they work well on my own task B.\n\n\nSo... am I missing something? And are there completely different ways (maybe in a mathematical way) to evaluate those vector spaces?\nI would look forward to feedback on my problem :-)",
"date": "2016-02-17"
},
{
"vote": 1,
"title": "I Made $12 000 in 26 Days without spending a Dime. How? 4g_RS3e_2",
"text": null,
"date": "2016-02-17"
},
{
"vote": 11,
"title": "Sense2vec with spaCy and Gensim",
"text": "[deleted]",
"date": "2016-02-16"
},
{
"vote": 1,
"title": "Sex with hot Girls? Yes?! Here Mn9__Cb28_Ak",
"text": null,
"date": "2016-02-14"
},
{
"vote": 1,
"title": "WAnt SEx T0day? Here SUPER girls 8Tr_Mn_4_6pPY_",
"text": null,
"date": "2016-02-14"
},
{
"vote": 3,
"title": "Hi - do you know of POS library for Android?",
"text": "I have an app for iPhone that uses the NSLinguisticTagger to do part-of-speech tagging of sentences. I'm now trying to port it over to Android.\n\n\nHas anyone here worked with an Android POS tagger (and I don't mean querying a server over HTTP but rather running it locally on an Android device).\n\n\nThank you!",
"date": "2016-02-14"
},
{
"vote": 2,
"title": "HR Mishap?",
"text": "Don't know much about the company/listing, but I think some HR person messed up and assumed that NLP meant \"Neuro-Linguistic Programming,\" not \"Natural Language Processing.\"\n\n\nUs computer scientists need to cool it with the achronyms.\n\n\nhttp://www.nextstepsystems.com/jobs/mw-chicago-jobs-sr-nlp-machine-learning-developer.html",
"date": "2016-02-13"
},
{
"vote": 5,
"title": "Is there a large and open English word corpus?",
"text": "Hey everyone in this subreddit, Does anyone know where can i find a large (say about 500Mbytes or bigger, I.E similar to BNC in size) English word corpus? That is either fairly cheap or free too download?",
"date": "2016-02-08"
},
{
"vote": 9,
"title": "Spanish Billion Words Corpus and Embeddings",
"text": null,
"date": "2016-02-06"
},
{
"vote": 1,
"title": "Learn Spanish With Wlingua - Learn Spanish App. Wlingua is the easiest app I've tried to learn Spanish.",
"text": null,
"date": "2016-02-05"
},
{
"vote": 1,
"title": "What Will Machine Translation Be Like in 100 Years?",
"text": null,
"date": "2016-02-04"
},
{
"vote": 10,
"title": "Why is add one smoothing used for text classification? Why not just eliminate unknown words from the bag of words?",
"text": "I'm watching the Jurafsky-Manning videos on NLP, currently on the text classification section. They talk about using add one smoothing to deal with words that do not appear in the training sets and gives the following logic:\n\n\n>A word that does not appear in training has a prior probability of zero for that class, and since MLE is calculated as a product of probabilities, any zero among the multiplied terms results in a zero estimate of likelihood for the class.\n\n\nBut it seems to me that this problem would simply go away if just didn't include unknown words in the MLE calculation, and given how much MLE skews the probabilities of known words that this would give a more accurate estimate.\n\n\nWhat am I missing here?",
"date": "2016-01-29"
},
{
"vote": 3,
"title": "NGram index library",
"text": "I'm in need of a c/c++ library that can efficiently assign a unique index to any n-gram. I would like to be able to do this for any rank ranging from 1..N, without storing the suffix multiple times.\n\n\ni.e. I want to retrieve a unique ID for 'is', 'house is' and 'the house is', while being relatively memory efficient and fast. I know LM tools do a similar job, but they also need to store probabilities and backoffs.\n\n\nDoes anyone know of a library that does this?",
"date": "2016-01-25"
},
{
"vote": 4,
"title": "Looking for a list of categorized nouns",
"text": "Hi - \n\n\ndoes anyone  know of good lists of \"categorized\" nouns? I am looking for something like this - \nhttp://www.enchantedlearning.com/wordlist/compoundwordscategorized.shtml\n - but preferably a corpus that's been widely used in academic research.",
"date": "2016-01-24"
},
{
"vote": 1,
"title": "STT engine as a blackbox",
"text": "I have been looking at various speech to text engines and was wondering if anyone has experience with one that will take an audio file as input or streaming audio and output the raw text. I'd prefer a command line tool that would display text as it is spoken. Anyone have experience with these?",
"date": "2016-01-20"
},
{
"vote": 8,
"title": "Standardized Reading Test Corpus",
"text": "I'm trying to find a corpus with many different passages labelled with the target grade level. I thought there should probably be a dataset like this from standardized tests, but I can't seem to find one. Anyone know of such a dataset?",
"date": "2016-01-20"
},
{
"vote": 0,
"title": "Intellexer releases natural language processing API with 10 components",
"text": "[deleted]",
"date": "2016-01-19"
},
{
"vote": 15,
"title": "Voynich Manuscript: word vectors and t-SNE visualization of some patterns",
"text": null,
"date": "2016-01-19"
},
{
"vote": 1,
"title": "Review funny pop american language",
"text": "[removed]",
"date": "2016-01-15"
},
{
"vote": 1,
"title": "Interview with Adam Coates, Director of Baidu's Silicon Valley AI Lab - talking Warp-CTC, Deep Speech 2 &amp; more",
"text": null,
"date": "2016-01-14"
},
{
"vote": 3,
"title": "Text Mining with MongoDB and Document Databases",
"text": null,
"date": "2016-01-11"
},
{
"vote": 5,
"title": "Open Natural language processing data sets",
"text": null,
"date": "2016-01-08"
},
{
"vote": 4,
"title": "[Request] Open Source library for extracting day/date information.",
"text": "Is there a library that can extract arbitrary day and date information, like 'today', 'last tuesday', 'seventeenth december' from large pieces of text? Python 3.x preferably. I tried Timex but it didn't quite fit. Any leads would be highly appreciated.",
"date": "2016-01-07"
},
{
"vote": 11,
"title": "Is there an NLP dictionary with approx. positive/negative sentiment of words and phrases?",
"text": "This is kind of a shot in the dark but I have a list of x thousands of words & short phrases and I'd like to quickly identify which of them generally express a negative sentiment (and, if possible, the level of negativity). Is there a dictionary that exists to help with this task or, alternately, does anyone have a good idea of how to attack the problem?\n\n\nEDIT: Forgot to mention that these are all English.",
"date": "2016-01-06"
},
{
"vote": 8,
"title": "Deep Grammar - a neural net based grammar checker",
"text": null,
"date": "2016-01-06"
},
{
"vote": 6,
"title": "Identifying medical terminology from colloquial language",
"text": "I am developing a project that maps Twitter language to medical terminology for disease surveillance. \n\n\nHere is my project breakdown\n:\n\n\n\n\nCollect random tweets\n\n\nIdentify twitter language that is equivalent to symptom data from the collected tweets\n\n\nLink these terms with actual symptom data for certain diseases.\n\n\nCreate an ontology containing disease names, their symptoms, and their twitter language synonyms for both name & symptoms.\n\n\nCollect more data that contains data from the ontology\n\n\nSee if the tweets have any correlation with actual disease occurrence\n\n\n\n\nI've currently gathered ~5GB worth of random tweets, and I'm on step #2 of my project. \n\n\nI got the idea for my project from a paper that tried to use Tweets to gauge flu infection levels. Their process of identifying tweets didn't seem effective so I'm trying to brainstorm new techniques for identifying medical terminology from the colloquial language used in Tweets.\n\n\nFor example, someone might say \"throwing up all day\" and this could translate to the symptom \"emesis\" which could be useful data.\n\n\n\n\nTL;DR: I am looking for any suggestions on how to accomplish #2 in my project breakdown since I'm new to natural language processing.",
"date": "2016-01-02"
},
{
"vote": 0,
"title": "how to apply LDA(latent Dirichlet allocation) model?",
"text": "I just went through David's paper of LDA. And I have some questions about its application. Do we depend on the param theta of each document to tell its topic? Besides telling topic, what is the other scene of applying LDA?\n\n\nThanks",
"date": "2015-12-18"
},
{
"vote": 5,
"title": "Computational Linguistics and Deep Learning - C. D. Manning, University of Stanford",
"text": null,
"date": "2015-12-18"
},
{
"vote": 5,
"title": "Web app or software for reading level with eye on International Readers?",
"text": "I am writing some documentation that, because we don't have the money for translations, but, because we have a lot of international customers, needs to be in very simple (but not condescending) English.\n\n\nI feel I can do this on my own pretty well, but a reality check from an online thing or software that uses an algorithm would be great. Perhaps all I need is the simple FK reading scale?\n\n\nI took a comparative linguistics course in college, so I'm aware of the grammar that is unique (and hard to understand) in English, but extra help from software would be great.\n\n\nTHANK YOU.",
"date": "2015-12-17"
},
{
"vote": 5,
"title": "Anyone else working on the Winograd Schema Challenge?",
"text": "The Winograd Schema Challenge\n is due in one month, but the details are still incomplete, and I don't hear a lot of activity (but then I don't dwell in academic circles). I did read that \nthe human benchmark\n is around 92%, so that's going to be hard to match. I'm wondering whether there is a lack of interest and if it's heading to be postponed a second time. Has there been any recent activity?\n\n\nEDIT: The contest has been postponed to June.",
"date": "2015-12-12"
},
{
"vote": 3,
"title": "Multilingual Language Processing From Bytes (Google)",
"text": "[deleted]",
"date": "2015-12-04"
},
{
"vote": 1,
"title": "Counseling and machine learning",
"text": "[deleted]",
"date": "2015-12-03"
},
{
"vote": 33,
"title": "A curated list of resources dedicated to Natural Language Processing",
"text": "I am starting a new awesomeness list about NLP (there is already a list about speech recognition and language processing, but I want to focus more on Natural Language Processing itself), feel free to join!\n\nhttps://github.com/keonkim/awesome-nlp",
"date": "2015-12-01"
},
{
"vote": 3,
"title": "Seeking prior research on mapping spatial, temporal, and compositional relationships between terms",
"text": "I'm rather new to NLP and am trying to figure out how one might approach the problem of extracting relational information between specific types of words in text, such as \"item A is below item B\" or \"item C is composed of items X, Y, and Z.\"  I'd like to extract the information from a handful of documents and plot the relationships as perhaps a network or hierarchical clusters. \n\n\nCan anyone suggest some examples from NLP literature on approaches for capturing spatial relationships, compositional relationships, or temporal relationships (before/after/synchronous) from text?  What general area of NLP would this kind of problem fall under?",
"date": "2015-11-30"
},
{
"vote": 5,
"title": "Using NLP-based semantic representational models on a collection of 77K tweets",
"text": null,
"date": "2015-11-27"
},
{
"vote": 8,
"title": "Looking for direction on how to read a sentence and formulate response based off of sentence.",
"text": "So please bare with me as I have next to zero knowledge other than my youtube and wikipedia research. I'm just looking to learn more about this stuff. \n\n\nSo Slack is a chat service you can use that allows for you to write your own bots that can integrate into Slack. Then a user can type a command to your service and a sentence. I put together a little bot then helps you find restaurants around your location. It recognizes a couple of specific commands and responds accordingly. The way I went about doing this was pretty basic. I basically just split the sentence up and look for certain words and if they said one of those then I respond. Its literally a series of if statements :). \n\n\nThis got me to thinking. What is the right way to actually go about doing this? Then I started looking at NLP and IBM's Watson API that allows you to create a classifier. This seemed cool because I could give it a bunch of variations of sentences and it would then respond with a class and then from there I can react based off of the class. However, I still would need to know specifics from the sentence. The example they use is if the user asks something about the weather. For example, \"What is the weather like on Sunday?\". So I could give this sentence a class of \"question\". Then I could respond. However, its asking for specific day. I suppose I could add more classes which is what I think it suggests but I feel like this doesn't lend itself to dynamic data very well. You would constantly have to be updating the data probably daily.\n\n\nAnyways. Does anyone know the proper way to go about doing this sort of thing? Remember just looking for direction here.\n\n\nThanks!",
"date": "2015-11-24"
},
{
"vote": 4,
"title": "Online resources in Odia (Oriya) language: Input tools for Android/iOS/Desktop, Fonts and Font encoding converter for Akruti, OR-TT Sarala, Shreelipi and Sambad fonts.",
"text": null,
"date": "2015-11-19"
},
{
"vote": 1,
"title": "Query Classification for Bitcoin",
"text": "[deleted]",
"date": "2015-11-18"
},
{
"vote": 4,
"title": "[META] Merge r/languagetechnology &amp; r/textdatamining --&gt; r/nlproc [x-posted to r/textdatamining]",
"text": "These subreddits have an enormous amount of overlap, and are basically both about NLP. I created r/nlproc, to match the Twitter hashtag for Natural Language Processing.",
"date": "2015-11-09"
},
{
"vote": 9,
"title": "WordsEye: Type a Picture",
"text": null,
"date": "2015-11-09"
},
{
"vote": 1,
"title": "Translation Project Management",
"text": "[removed]",
"date": "2015-11-09"
},
{
"vote": 0,
"title": "Any idea for NLP undergraduate project using Stack Exchange Data Dump?",
"text": null,
"date": "2015-11-07"
},
{
"vote": 4,
"title": "Tiny JavaScript implementation of Earley parser",
"text": null,
"date": "2015-11-05"
},
{
"vote": 1,
"title": "NLP API - ThatNeedle.com",
"text": "[deleted]",
"date": "2015-10-29"
},
{
"vote": 6,
"title": "Test your knowledge and help with my research (please)! (a word annotation game)",
"text": null,
"date": "2015-10-28"
},
{
"vote": 1,
"title": "Interpolate between clauses or words",
"text": "Hi there,\n\n\nmostly new to NLP -- are there some techniques to \"blend\" between sentence structures and/or individual words? In the latter case, I'm imagining that I have for example two nouns A and B, and I want to consult some dictionary or thesaurus structure to give me some hits that are close to A and B but for example closer to A. Does something like that exists? The application is artistic and can involve manual post-processing, so total accuracy is not of the essence. I would use the English language. Thanks!",
"date": "2015-10-27"
},
{
"vote": 3,
"title": "Using Word2Vec to determine which famous author you write like",
"text": null,
"date": "2015-10-27"
},
{
"vote": 1,
"title": "Here are some free datasets of video game reviews from the Steam website",
"text": "[deleted]",
"date": "2015-10-26"
},
{
"vote": 3,
"title": "Phone segmentation algorithm?",
"text": "Is there a standard algorithm for segmenting speech waveforms into (roughly) phonetic segments? If not, how complex would the construction of such an algorithm likely be?",
"date": "2015-10-24"
},
{
"vote": 7,
"title": "Any NLP based extensions for Chrome?",
"text": "I am looking for Chrome extensions that would make use of NLTK or something else to mark POS, word frequency, etc, on a page. Do you know of any? If not, do you know any great extensions that language learners and computational linguistics could be making use of to accelerate their language learning? Thanks!",
"date": "2015-10-21"
},
{
"vote": 1,
"title": "t-CONSPECTUS now understands German and Russian",
"text": "Hello!\n\n\nA while ago I wrote a post about an \nonline summarizer\n which worked with articles in English. Now the \nservice\n can handle news in German or in Russian.\n\n\nIt would be awesome if someone could test it and provide constructive critic.\n\n\nWhile researching I failed to found any free/online german summarizers, except maybe \nSweSum\n... Do you know any?",
"date": "2015-10-20"
},
{
"vote": 4,
"title": "What database do you use to store NLP data?",
"text": "Looking to work with databases to store NLP data. I am thinking about using \nCassandra\n with Apache spark for its in Memory cluster computing tool. Any suggestions?",
"date": "2015-10-20"
},
{
"vote": 9,
"title": "Word2phrase in Python: extract multiword phrases from documents",
"text": null,
"date": "2015-10-18"
},
{
"vote": 3,
"title": "Google's translation methodology",
"text": "Why does google prefer statistical machine translation over rule-based translations? In general, what are the pros and cons of both?",
"date": "2015-10-16"
},
{
"vote": 22,
"title": "Auto-Generating Clickbait With Recurrent Neural Networks",
"text": null,
"date": "2015-10-13"
},
{
"vote": 6,
"title": "QANTA (A quizbowl playing deep learning system) vs. Ken Jennings at UW",
"text": null,
"date": "2015-10-10"
},
{
"vote": 2,
"title": "Paraphrasing with NLP",
"text": "Hello all!\n\n\nI'm just wondering how sentence generation works in relation to NLP, i.e. The science behind how a sentence can be generated from another sentence, not a completely different sentence, a paraphrase. How would the machine interpret a sentence searched, and how would it generate a new one from the initial sentence.\n\n\nThanks",
"date": "2015-10-09"
},
{
"vote": 3,
"title": "Machine Translation",
"text": "A friend wants to work on an English-Japanese translator app, but isn't sure where to start. What would be the best resources for her to gain some insight into building an app for machine translation, and what are some major issues she could run into?",
"date": "2015-10-04"
},
{
"vote": 3,
"title": "Internship offer for data scientist at Infineon in Munich",
"text": null,
"date": "2015-10-02"
},
{
"vote": 3,
"title": "Word Frequency by Education Leveled",
"text": "Is there a set of words and their frequencies available based on someone's education level? Maybe only using text books as the corpus? Like, by 7th grade, 12th grade and college graduation, the frequency of the word \"pesticide\", \"down-trodden\" and \"quarry\" (just random words I thought of).",
"date": "2015-09-29"
},
{
"vote": 4,
"title": "Translate abbreviation-like text into plain words",
"text": "Hi, I'm new to NLP, but there's been a question that's been bothering me for some time, so I thought I'd ask.\n\n\nFor those of you who don't know Chinese, there's a way of typing Chinese characters on an English keyboard called Pinyin. Essentially, you write the phonetic equivalent and suggestions will pop up.\n\n\nFor example, if I type\n\"wobuzhidao\" it will suggest æˆ‘ä¸çŸ¥é“ (I don't know)\n\n\nChinese has tons of homophones, but the keyboard software is very good at predicting what you want to say. But it gets better. Due to the predictable nature of Chinese phonetics, you can easily shortform things.\n\n\nFor example: \"wbzhd\" will also produce the suggestion æˆ‘ä¸çŸ¥é“\n\n\nThis makes typing in Chinese extremely fast. That's when I got to wondering if something can be done for English.\n\n\nSome basic examples:\nidk --> I don't know;\n\n\nhrutoday? --> how are you today?\n\n\ng2g --> gotta go\n\n\nI realize it's probably a hard problem to solve, but any suggestions would be great.\n\n\nThanks!",
"date": "2015-09-24"
},
{
"vote": 1,
"title": "StrepHit: automatic Wikidata references via NLP",
"text": null,
"date": "2015-09-21"
},
{
"vote": 10,
"title": "Software tool and library for efficient n-gram, skipgram extraction and corpus analysis",
"text": "I would like to announce release v1.0 of Colibri Core, free open-source software for working with basic linguistic constructions such as n-grams and skipgrams, in a quick memory-efficient yet lossless way suitable for big data:\n\n\nMore information and download on the \nColibri Core website\n .\n\n\nColibri Core enables you to:\n\n\n\n\nextract patterns and their frequency from corpora\n\n\npreserve the exact indices where patterns occur in the corpus, allowing reverse-lookup as well\n\n\nmodel various relationships between patterns (subsumption, succesion, abstraction, co-occurrence)\n\n\ncompare patterns between different corpora (using coverage metrics and/or log-likelihood)\n\n\n\n\nThe software consists of command-line tools and a\nprogramming library for both \nC++\n and bindings for \nPython\n. The software aims to lay a foundation for more specialised or end-user-oriented software to be built upon. I'm hoping the NLP community finds it useful.",
"date": "2015-09-21"
},
{
"vote": 4,
"title": "Gathering Game Reviews for Sentiment Analysis",
"text": "Hello, I'm planning to do a project that requires a large corpus of game reviews. Gamespot has plenty but going through each individual review is extremely impractical. Does anyone here have advice on how I could extract all that material easily?\n\n\nThank you very muchhhhhhhhhhhhhhhhhhhhhh.",
"date": "2015-09-16"
},
{
"vote": 1,
"title": "Detailed Video Tutorial on Computational Lignuistics and MLP by Mark Johnson (Tagged and Searchable)",
"text": "[deleted]",
"date": "2015-09-14"
},
{
"vote": 3,
"title": "POS tagging in R langauge",
"text": "I am working with roman urdu. I have a .txt file that contains all urdu words with their POS tags(say file A). I have another text file (say file B). I want to assign POS tags to words of file B using using file A. Please help.",
"date": "2015-09-11"
},
{
"vote": 2,
"title": "Is there such thing as an \"un-stemmer\"?",
"text": "I've lately been into using natural language generation, mostly to make jokes. (Markov chains and _ebooks Twitter accounts are SO 2014.) \n\n\nI don't really have a good sense of what kinds of techniques already exist in the NLG space, so it's hard to find tools to do what I want. Here's what I'm looking for.\n\n\nI want to be able to take an English word, stem it, do something fun with it, then -- and this is the step where I want your help -- return the root word to the same part of speech as the original word. For instance, I might take the word \"kicking\", stem it to \"kick\", do some magic to get back another word like \"hug\" and then \nunstem\n that word to get \"hugging\".\n\n\nWhat should I be looking for here? \"unstemmer\" doesn't return a lot of useful results, and as far as I know there's no stemmer that's also a tagger that will give the grammatical information about my input form \"kicking\" (e.g. \nthat's a gerund\n) that I can then use to add back the morphology to \"hug\".",
"date": "2015-09-06"
},
{
"vote": 1,
"title": "/r/Elastic - for Elasticsearch fanatics!",
"text": null,
"date": "2015-09-03"
},
{
"vote": 5,
"title": "Starting my studies on NLP",
"text": "Hello everyone,\n\n\nI don't know if this kind of post is appropriate to this subreddit and I don't use Reddit on a regular basis, so forgive me if I'm doing something wrong.\n\n\nAnyway, to the reason I decided to make this post:\n\n\nAs the title suggests, I'm starting to study NLP, as I think it is a very interesting topic, but since I'm studying it on my own, I'm kind of lost as to where to start from.. \nKnowing NLP requires a good statistical background, I thought maybe I could review some of the \"Probability & Statistics\" basics using Khanacademy and then start tackling at some chapters from the book I got from my office (\"Foundations of Statistical Natural Language Processing - Christopher D. Manning and Hinrich Schutze\").\n\n\nSo, basically what I wanted to ask here is if this is a good approach to start unravelling this field of study or if anyone has any suggestions that could help/improve my learning experience. \n\n\nAny tips on good starting points would be greatly appreciated !\n\n\nOBS: I'm sorry for any possible grammar errors.. English is not my native language :)\n\n\nThank you for your attention.",
"date": "2015-09-02"
},
{
"vote": 2,
"title": "Help compiling Dada Engine",
"text": "Does anyone have a good guide for installing and compiling Dada Engine on mac? I have run the configure bash script and run \"sudo make\" but when I run \"make install\" as per the README I get the following line: \"make: *** No rule to make target `install'.  Stop.\"\nAlso when I run \"dada manifesto.pb\" it tells me that the command dada cannot be found. Any help to get this program up and running would be greatly appreciated.",
"date": "2015-09-01"
},
{
"vote": 2,
"title": "Where can I download a French-English dictionary database that includes parts of speech?",
"text": "Hi,\n\n\nI've been looking all over for a French English dictionary for some basic NLP but I'm having no luck finding anything.    Anyone know where I can get a word database with parts of speech (noun, verb, etc)?  I really appreciate any help\n\n\nFor example:  \n\n\ncat, noun, chat",
"date": "2015-08-26"
},
{
"vote": 0,
"title": "Anyone know of a dataset with a list of common nouns?",
"text": "I'm looking for something with a list of words like \"dogs\", \"cats\", \"bus\" etc.",
"date": "2015-08-25"
},
{
"vote": 2,
"title": "How to apply Witten-Bell smoothing on Unigrams?",
"text": "I am having trouble understanding how Witten-Bell smoothing can be applied to Unigrams. Is there any good tutorial available which provides a good intuition?",
"date": "2015-08-24"
},
{
"vote": 0,
"title": "Relevant Term Finder (Automatic Hashtag Suggestion)",
"text": "Hi guys,\nSo I need a tool to find relevant terms in a text (not very big, 1-3 paragraphs). I've searched the web and the only thing I found is \naylien\n.\nIt doesn't matter if it's free or not, the only requirement is that it should be able to annotate text if French. Do you know any ?\n\n\nThanks in advance for your input !",
"date": "2015-08-24"
},
{
"vote": 1,
"title": "NLTK Corpora NE Tags",
"text": "Does NLTK have a corpora that is tagged with NE Tags, and if so which one?\n\n\nEdit:\n\n\nIs there such a corpus in english?",
"date": "2015-08-23"
},
{
"vote": 2,
"title": "A question-answering system for the Mneumonese language--what it does and how it works",
"text": null,
"date": "2015-08-19"
},
{
"vote": 5,
"title": "Training a Neural Network to do Machine Translation",
"text": "Hi,\n\n\nI am trying to train an NN to do translation from en to fr based on Europarl corpus. Since the whole thing takes ages to train, I would like to make sure that I have no bugs and I have a few questions:\n(context: my LSTM network has 4 stacks of 512 units, input size: 512, in/out vocab is 20000, batch size 64, I can't do more because GPU).\n\n\n\n\nWhat should the cross entropy per word plots look like? (my currently hover around 5,6 for the first two epochs...)?\n\n\nWhat should the reconstructed translations look like after 1st, 2nd etc. epochs? Will they look hopeless until the network reached local minima, or should I see sensible translations after about an epoch of training?\n\n\n\n\nThanks!",
"date": "2015-08-15"
},
{
"vote": 2,
"title": "What is The Size of the Translation Industry?",
"text": null,
"date": "2015-08-14"
},
{
"vote": 15,
"title": "What's a good English grammar book for NLP?",
"text": "I'm pretty new to natural language processing, and I'm finding that my knowledge of English grammar and syntax could use some improvement.  \n\n\nDo you guys have any recommendations for good books on English grammar and syntax, especially ones that approach it from the perspective of natural language processing?\n\n\nI would also appreciate recommendations for introductory books on NLP, especially NLP applications.",
"date": "2015-08-12"
},
{
"vote": 6,
"title": "Classify US Court opinions; find Panel Effect.",
"text": "*assume these opinions are from unanimous decisions. \n\n\n\n\nopinions - are the text documenting the decision of the panel written by a single judge on the panel ( the 'authoring judge' ), which reflects the unanimous views of the panel\n\n\n\n\n'panel effect' - There are usually 3 judges on a 'panel' at a US Appeals court. Political observes have claimed to notice a panel effect.  In particular, they have noticed that on a panel that has one 'Liberal' judges (judging from party affiliations) and two conservative judges, the ideology of the panel is much more associated with a Liberal point of view that that of an 'conservative' point of view, where conservative might be measured by the opinions written by and all conservative panel.\n\n\n\n\n\n\nThis is a problem that I'm curious about and using a just a Bag words and simple logic regression approach can report a CV accuracy score of ~ .67. \n\n\nWhat would you try?",
"date": "2015-08-08"
},
{
"vote": 2,
"title": "My new browser extension that makes reading web pages faster than fast.",
"text": "[removed]",
"date": "2015-08-07"
},
{
"vote": 1,
"title": "Code/Library for extracting all dependency paths from a syntactic n-gram",
"text": "Hey guys,\n\n\nI'm using the Google Books Syntactic N-grams data set, and I need to extract all the possible dependency paths from a single syntactic n-gram... In trying to not \"reinvent the wheel\" (and if I do, not miss any cases) I thought if maybe someone could point me to some how I can go about doing this?\n\n\nThanks in advance for any help!",
"date": "2015-08-03"
},
{
"vote": 2,
"title": "License ( Bachelor ) degree paper in NLP",
"text": "Hello,\n\n\nMy License/Bachelor degree paper is about NLP. I can choose pretty much anything NLP related, but I don't really know where to start. Can you give me some concrete ideas of NLP subjects easily approachable by a undergrad student?\n\n\nI have to write ~50 pages about my subject and have a computer program to demonstrate my paper.\n\n\nI know a thing or two about the subject, but I'm not exactly an expert. My first attempt was machine translation, but I found it too hard.\n\n\nWhat do you recommend?",
"date": "2015-07-31"
},
{
"vote": 1,
"title": "Short guide on why &amp; how to tune Brown clustering",
"text": null,
"date": "2015-07-30"
},
{
"vote": 1,
"title": "Call for guest writers with interest/expertise in rapidly emerging tech such as Machine Learning, Nanotech, AI, HCI, Robotics, AR, Affective Computing etc",
"text": "[removed]",
"date": "2015-07-28"
},
{
"vote": 1,
"title": "The Power of Voice (solid Electronics360 piece on voice interfaces)",
"text": null,
"date": "2015-07-21"
},
{
"vote": 8,
"title": "Multi-word term identification software? Please help!",
"text": "Hi everybody! I'm a Ph.D. candidate in a quantitative social science but need help in finding software to pre-process some data. In particular, I'm looking for software that will identify multi-word terms and concatenate them in the correct contexts. For example:\n\n\n\n\nDolphins like to swim. -> {\"Dolphins\"}\n\n\nMiami is a city. -> {\"Miami\"}\n\n\nThe Miami Dolphins are a football team. -> {\"Miami_Dolphins\"}\n\n\nAt the aquarium in Miami, dolphins are an attraction.\" -> {\"Miami\", \"Dolphins\"}\n\n\n\n\nI know this is where \nC-value/NC-value\n comes in. However, I can't find an implementation anywhere and am not interested in re-inventing the wheel. Also, I do not know if there is a more state-of-the-art approach.\n\n\nAny help or suggestions would be greatly appreciated! Please point me to the current literature on this or to software that accomplishes this.",
"date": "2015-07-18"
},
{
"vote": 0,
"title": "Is there a Word2Vec tutorial in Java?",
"text": "I'd like to use word2vec for a NLP project I'm working on, however, I can't seem to find any good tutorials on how to use it in Java. I've got it installed on Linux, but besides looking up coside distances within the Terminal, I'm not sure what to do now.\n\n\nAlso, side question, are the vectors all normalized sizes?",
"date": "2015-07-15"
},
{
"vote": 1,
"title": "This Dictionary Will Get You Ready For â€œTalk Like Silicon Valleyâ€ Day",
"text": null,
"date": "2015-07-10"
},
{
"vote": 0,
"title": "ELI5: Oauth2",
"text": "I'm interested in using Python to get data from Reddit and Twitter using praw and Tweepy.  I keep hearing that I need to get Oauth2 permission, but I cannot find any step-by-step explanation as to how this works.  Every tutorial or documentation I come across seems to skip over this part.  If someone could please explain how this works to someone who is a beginner programmer, I would be extremely grateful.",
"date": "2015-07-09"
},
{
"vote": 4,
"title": "How does wit.ai work ?",
"text": "Was just wondering how wit.ai work so well with just so few examples in it ? There seem to be quite a few other such services that have popped up recently and was wondering how they all do it. Even the new Amazon Echo is doing similiar stuff. \n\n\nIf someone can point me to some paper or article explaining the theory behind it, that would be great.",
"date": "2015-07-07"
},
{
"vote": 4,
"title": "Dealing with compound terms and collocations",
"text": "I am working on a document classifier. I want to tokenize some compound terms like \"San Francisco\", \"hot dog\", \"balance sheet\", \"serial number\" to improve the accuracy of my classifier.\nWhat is the best way to do this? \nI thought of 3 ways \n\n\n\n\nuse a dictionary and brute force match and replace with an under score. This method might be very memory intensive and depends on the accuracy of the dictionary\n\n\n\n\nUse bigrams and manually choose the top 100 or so results. This method depends on the accuracy of the training set so if I have data outside of the set it won't be caught.\n\n\n\n\nUse a POS tagger/NER and islolate out Noun phrase and tag them. Not sure how accurate this is.\n\n\n\n\n\n\nAny Suggestions?",
"date": "2015-07-05"
},
{
"vote": 1,
"title": "10 Regions to Target Instantly Via Hindi Translation",
"text": "[removed]",
"date": "2015-07-02"
},
{
"vote": 0,
"title": "Usability study: seeking Big Data Analytics users, NLP developers, Hadoop administrators",
"text": "The IBM Design team is seeking users who are proficient in data science, big data analysis, Natural Language Processing (NLP), or Hadoop administration. We'd like to learn more about how you work.\n\n\nWHO\n: Users who work with and analyze big data, users who develop applications that use Natural Language Processing (NLP), or users who install or administer Hadoop.\n\n\nWHEN\n: Sessions will be held \nJuly 2-15, 2015\n. We'll confirm a date and time based on a mutual schedule. Each session will last 1 hour.\n\n\nWHERE\n: Sessions will be held remotely over the phone.\n\n\nPlease complete this brief survey if you're interested in talking with us: \nhttps://www.surveygizmo.com/s3/2190258/BigDataStudyScreener-r\n.\n\n\nAs a token of appreciation, you will receive a \n$25 prepaid Visa\n virtual reward card (e-giftcard) if you qualify and participate in the session.\n\n\nThank you!",
"date": "2015-07-01"
},
{
"vote": 7,
"title": "A Neural Conversational Model",
"text": null,
"date": "2015-06-23"
},
{
"vote": 2,
"title": "GermEval 2014 Named Entity Recognition Shared Task: How do I work with the data?",
"text": "It's part of a homework for university and I've done sentiment analysis of tweets and other things, so I know how to do it in general. However, I don't know how to work with the data that is provided.\n\n\nSpecifically: How do I predict the labels here if I have two labels, so to say?\n\n\nIn sentiment analysis I had 3 possible labels: positive, negative and neutral. So, for 1 sentence I had one feature that was predicted using the features (I replaced words in the tweet with entries from a lexicon) and bag of words from the tweet.\n\n\nHere is the link: \nhttps://sites.google.com/site/germeval2014ner/data\n\n\nExample entry:\n\n\n# http://de.wikipedia.org/wiki/Manfred_Korfmann [2009-10-17]\n1 Aufgrund O O\n2 seiner O O\n3 Initiative O O\n4 fand O O\n5 2001/2002 O O\n6 in O O\n7 Stuttgart B-LOC O\n8 , O O\n9 Braunschweig B-LOC O\n10 und O O\n11 Bonn B-LOC O\n12 eine O O\n13 groÃŸe O O\n14 und O O\n15 publizistisch O O\n16 vielbeachtete O O\n17 Troia-Ausstellung B-LOCpart O\n18 statt O O\n19 , O O\n20 â€ž O O\n21 Troia B-OTH B-LOC\n22 - I-OTH O\n23 Traum I-OTH O\n24 und I-OTH O\n25 Wirklichkeit I-OTH O\n26 â€œ O O\n27 . O O    \n\n\n\nHow exactly do I get from this data to data that I can pass to a SVM, for example?",
"date": "2015-06-22"
},
{
"vote": 4,
"title": "pre-newb question: What tools might be well-suited for inferring semantic maps or frames from sentences or texts?",
"text": "https://framenet.icsi.berkeley.edu/fndrupal/\n\n\nhttps://www.google.com/webhp?q=inferring%20semantic%20maps\n\n\nhttps://www.google.com/webhp?q=inferring%20semantic%frames",
"date": "2015-06-15"
},
{
"vote": 7,
"title": "TED-RNN: Machine Generated TED-Talks.",
"text": null,
"date": "2015-06-13"
},
{
"vote": 3,
"title": "[0907.1558] Towards the quantification of the semantic information encoded in written language",
"text": null,
"date": "2015-06-12"
},
{
"vote": 2,
"title": "Identify a user's job and skillset based on testimonies he received",
"text": "Hi. I want to add a feature using NLP to a web app. I'm clueless on where to start. I took a bit of Compiler Theory and Theory of Programming Languages in college but never really stuck to it or NLP. The NLP library I plan to use is \ntreat\n using Ruby since the app's backend and API is in Rails. \n\n\nFor the gist of what I want to do, say a user receives a testimony, \"Hey man! Great design on the website you just launched! My friends liked it.\" The app could suggest or automatically tag that user as web designer or web developer.\n\n\nI've come so far to understanding tokenization and parsing using treat but I don't know how I should approach what I want to do.\n\n\nThanks. Any help would mean alot and appreciated!",
"date": "2015-06-09"
},
{
"vote": 1,
"title": "Survey about attitudes to postgraduate study - chance to win a free SanakoStudy Solo License",
"text": "[removed]",
"date": "2015-06-08"
},
{
"vote": 11,
"title": "SoundHound's New \"Siri-like\" Interface. Demo Looks Very Impressive. I'm Curious to See What It Can Really Do.",
"text": null,
"date": "2015-06-03"
},
{
"vote": 1,
"title": "Patterns Beyond Pixels: Building a Video Recommendation System With NLP [x-post from r/datascience]",
"text": null,
"date": "2015-05-30"
},
{
"vote": 1,
"title": "How MindMeld Is Letting Companies Add Voice Recognition to Any App",
"text": null,
"date": "2015-05-26"
},
{
"vote": 1,
"title": "Unsmoothed Maximum Likelihood Character Level Language Model",
"text": "[deleted]",
"date": "2015-05-26"
},
{
"vote": 7,
"title": "How do phonetic transcription programs work?",
"text": "I'm not sure if this is the right place to ask, but I figured it's worth a shot.\n\n\nI just started learning NLTK in Python and I got a section dealing with the CMU pronunciation corpus. It sparked an idea for a program that would transcribe those words according to the IPA.\n\n\nI cobbled together a nice little program that does such, but it's limited to the 100k or so words in that corpus.\n\n\nHow do sites such as \nhttp://lingorado.com/ipa/\n do it? Do they just have a larger corpus?",
"date": "2015-05-20"
},
{
"vote": 5,
"title": "Beginner : Need to read technical documents and extract information. Where do I start?",
"text": "Hello!\n\n\nI'm developing a system where I would have to read technical documents and make the system understand it.\n\n\nCan anyone recommend any books/resources for me to start learning NLP? My confusion is if I have to learn from the basics or if there are any hacks where I can start learning at a certain level and use tools that already do most of the low level stuff like word and grammer extraction?\n\n\nThanks in advance!",
"date": "2015-05-19"
},
{
"vote": 1,
"title": "Cherokee Language Now Available on Google Android",
"text": null,
"date": "2015-05-14"
},
{
"vote": 13,
"title": "Polyglot: NLP Toolkit for 100+ languages with word embeddings support",
"text": null,
"date": "2015-05-08"
},
{
"vote": 1,
"title": "Approaches To Machine Translation",
"text": null,
"date": "2015-05-06"
},
{
"vote": 5,
"title": "Can you help me? - Developing a system that takes the textual content of an article and returns an appropriate music recommendation.",
"text": "I'm doing a study on the effectiveness of the system for my dissertation project. Problem is, I'm struggling to get the sample size large enough\n\n\nThe study involves listening to music and reading several articles.\nIt shouldn't take more than 10 mins to fill out if you move on after getting the general idea for each question. \n\n\nLink in comments",
"date": "2015-05-02"
},
{
"vote": 4,
"title": "Speakable Programming for Your Language, add vocab and grammar of any language, translates automatically to world languages. : conlangs",
"text": null,
"date": "2015-05-01"
},
{
"vote": 2,
"title": "Entity Linking APIs - Korean, Japanese - Any startups?",
"text": "Are there any Startups doing Entity linking for those languages? \n\n\nAre they reachable for english clients? I can't seem to see any Japanese company/Korean company offering API services.\n\n\nI've come accross Semantria(Korean, Jap, Chinese), Babelfy(Korean, Jap, Chinese).\nI know various startups in South America working on Spanish APIs so it seems weird I can't find any japanese/korean startup doing it.",
"date": "2015-04-07"
},
{
"vote": 9,
"title": "Topic Modelling Explained",
"text": null,
"date": "2015-04-03"
},
{
"vote": 5,
"title": "Standard practice for converting tokens into phrases?",
"text": "I'm doing a task where I want to use essentially a bag-of-words representation of my corpus. However, I want to be a little smart in how I do this, and convert ngrams like \"new york\" to single tokens like \"new_york\".  So far, I've just been using word2phrase in Google's word2vec package. Is this a good method, or is there a better/more standard way of doing this?",
"date": "2015-04-02"
},
{
"vote": 1,
"title": "Translate texts online with an average of ~8$/hour",
"text": "[removed]",
"date": "2015-03-30"
},
{
"vote": 1,
"title": "Computers That Can Learn: How Will Deep Learning Affect the Lives of Millions of People?",
"text": null,
"date": "2015-03-30"
},
{
"vote": 9,
"title": "A topic model of conspiracy theories with the R topicmodels package",
"text": null,
"date": "2015-03-22"
},
{
"vote": 4,
"title": "Tf-Idf: Document v.s. Overall Frequancy",
"text": "When using \nTf-Idf\n, why is it that the Inverse Document Frequency is the occurrences in the number of documents and not just overall frequency within the language? Is there any research that discusses why one is used over the other, or why we don't consider both of these things?\n\n\nFor example, I have 10 documents and 4 of them contain the word 'flower' once, but only once. Then I also have the word 'next' which occurs in 2 of the 10 documents, but occurs 20 times. For the sake of this example we'll say that each document has on average 500 words. Using occurrences within documents I would get 'flower' at 0.4 and 'next' at 0.2. If I were instead going to use overall frequency I would have 'flower' at 0.0008 and 'next' at 0.004.\n\n\nAs we can see we could get very different results depending on which of these approaches we take. Are there any similar algorithms that use both of these measures, or use overall frequency over document frequency?",
"date": "2015-03-19"
},
{
"vote": 1,
"title": "Change Your Words Change Your Life",
"text": null,
"date": "2015-03-17"
},
{
"vote": 4,
"title": "Writing a paper on NLP",
"text": "For the capstone course in my CIS Associate Degree program, I have to write an essay on an \"emerging\" topic, and I chose NLP. I'm wondering if you guys had some ideas on how to approach the paper.\n\n\nThe only criteria is it must have an executive summary at the beginning, show examples to better explain concepts, and have a conclusive summary at the end. The length should be a little over five pages.\n\n\nI was thinking of including some mentions of Chomsky's work, a history of some AI breakthroughs, identifying problems currently faced, and then giving examples through Python/NLTK. I do enjoy writing persuasive papers, but I don't know enough about the field to hold any strong opinions yet. So, any books, films, coding examples, or perhaps other Reddit posts that would get me started?",
"date": "2015-03-15"
},
{
"vote": 11,
"title": "Basic Information Extraction?",
"text": "I'm working with EMTs and Paramedics who write standard reports like:\n\n\n\"Ambulance 2 was dispatched to 123 Fake Street for a 77 year old female with dizziness and shortness of breath.  Upon arrival we found her seated at a kitchen chair in moderate distress.  Initial vital signs were BP 178 over 102, pulse 105, respiration 20 with audible wheezes.....\"\n\n\nI've had pretty good luck with simple structured tokenizing to extract (who) was dispatched (to where) for (age/sex) having (complaint).   I have a bunch of -if this then that- kind of rules that are obviously fussy and fragile.   What tools should I be looking into to try and extract my medical domain specific features?  \n\n\nI \nmight\n have a corpus of a few thousand narratives that I could throw at this.",
"date": "2015-03-14"
},
{
"vote": 11,
"title": "A Word is Worth a Thousand Vectors",
"text": null,
"date": "2015-03-12"
},
{
"vote": 2,
"title": "Beginner's question: what are the common ways to treat unknown tokens in grammar based parsing?",
"text": "I am going through NLTK tutorial on rule based grammar parsing and one thing came up: if there is a token in input text that is not a in the set of terminal symbols, then the parser chokes on that and returns error. In a practical application one would want to guess the unknown word and maybe automatically enrich the grammar if it's very sure of the new token. I don't think there is anything in NLTK that does such thing. What are the common ways to treat unknown tokens in grammar based parsing?",
"date": "2015-03-12"
},
{
"vote": 24,
"title": "CS224d: Deep Learning for Natural Language Processing (/r/programming)",
"text": null,
"date": "2015-03-08"
},
{
"vote": 8,
"title": "NLUI Server: your own Siri or Cortana in 20+ languages",
"text": null,
"date": "2015-03-02"
},
{
"vote": 5,
"title": "Platypus, a nice open source query engine",
"text": null,
"date": "2015-02-23"
},
{
"vote": 6,
"title": "Applying NLP to screenplays. Tutorials, tools, and tips welcome.",
"text": "I'm interested in applying NLP to a collection of screenplays in the hopes of identifying some trends and outliers along several fronts (structure, semantic tone, character relationships, theme, etc...). However, I have ZERO training in comp science or linguistics and will be learning from scratch. \n\n\nCan anyone recommend some step-by-step, noob friendly tutorials similar to\n this one\n? \n\n\nI'd like to apply the analysis to several screenplays at once, and not just to individual scripts. \n\n\nThanks!",
"date": "2015-02-22"
},
{
"vote": 14,
"title": "Should NLP people learn traditional linguistics?",
"text": "In my own experience, I feel that analyzing linguistic phenomena sometimes requires methods from \"traditional\" linguistics.\n\n\nIs it the case that to do well in NLP we need to be knowledgable in linguistics or work with people who do?\n\n\nAnd what are some books you would recommend for NLP people to start with linguistics?",
"date": "2015-02-22"
},
{
"vote": 9,
"title": "Document/text summarization algorithm demo",
"text": null,
"date": "2015-02-13"
},
{
"vote": 6,
"title": "GitHub list of open source code for minority languages",
"text": null,
"date": "2015-02-03"
},
{
"vote": 8,
"title": "Are there any new text summarization competitions being held?",
"text": null,
"date": "2015-01-28"
},
{
"vote": 27,
"title": "spaCy: Industrial-strength NLP",
"text": null,
"date": "2015-01-26"
},
{
"vote": 13,
"title": "Gentle introduction on NLP",
"text": "Hey there. I'm writing a book, along with my phd supervisor, on digital humanities. The book is aimed at third-year college students, but students with a humanities major. So no CS majors or anything. It won't be in English, either.\n\n\nI am in charge of the NLP part of the book, since that's what I'm researching (I've done some MT in the past and am now more focused on information extraction). Problem is: even though I have a linguistics background, I've always been a major geek and never had any real problem with understanding (basic) algorithms or using software without GUIs. So I kinda learned a lot by myself and take a lot of background information and skill for granted. Writing a vulgarization book has that problem: you need to explain everything in clear words.\n\n\nI've done some research on introductory books, namely:\n\n\n\n\nNLTK Book with python, O'Reilly: \nhttp://www.nltk.org/book_1ed/\n\n\nNatural Language Processing (Almost) from Scratch : Collobert et al., \nhttp://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf\n\n\nSpeech & Language Processing de Jurafsky & Martin (obviously, even though it's not very \"didactic\")\n\n\n\"The Information Retrieval Series\" edited par W. Bruce Croft, Springer, for the \"information extraction\" part.\n\n\n\n\nDo you have any other idea? The aim is to have a general introduction (J&M are good at that) and then more of a \"case study\"-based approach with very easy to understand examples for most of the NLP disciplines (IR, Tokenizing, POS-tagging, MT, NER, Topic Modeling, etc.)\n\n\nPapers which mainly consist of equations are of course out of the question...\n\n\nThanks!",
"date": "2015-01-19"
},
{
"vote": 8,
"title": "Detecting actionable clauses in text",
"text": "Just a disclaimer: I know virtually nothing about NLP and not much about grammar, so I may not be using the right terminology.\n\n\nI'm looking for a library that I can send a body of text to, which outputs any actionable clauses it finds. I'm pretty sure that most actionable clauses are imperative sentences, so something that could even just give me all the imperative sentences would be useful. Does such a thing exist? I've looked at things like the Stanford parser, but from the website it appears that it doesn't have this feature. If no library exists, is there any research published that might describe potential algorithms that accomplish this task?\n\n\nAn example of things I would want to detect:\n\"Pick up some milk at the store\"\n\"Create a Powerpoint presentation by 3pm tomorrow\"\n\n\nAny info around this topic would be very helpful, thanks!",
"date": "2015-01-14"
},
{
"vote": 3,
"title": "Kneser-Ney, modified Kneser-Ney examples?",
"text": "[deleted]",
"date": "2015-01-13"
},
{
"vote": 1,
"title": "Deutsche Dialoge - Kleidung, SchmÃ¼cke und Farben Â« L E A R N G E R M A N",
"text": null,
"date": "2015-01-12"
},
{
"vote": 1,
"title": "20,000 Decision Reviews, Graphed [OC] X-Post /r/dataisbeautiful",
"text": "[deleted]",
"date": "2015-01-07"
},
{
"vote": 2,
"title": "Some more general information about Mneumonese : Mneumonese",
"text": null,
"date": "2015-01-07"
},
{
"vote": 7,
"title": "An attempt to create a language that's easy to do NLP on, among other things : conlangs",
"text": null,
"date": "2015-01-02"
},
{
"vote": 3,
"title": "Is there any terminology in NLP to specify different types of abbreviations?",
"text": "I am working on some documentation for a rules-based sentence segmentation algorithm. This algorithm at times differentiates between abbreviations based on whether that abbreviation comes before or after its referent.\n\n\nFor example:\n\n\n\"Mt. Fuji\" - 'Mt.' always comes before its referent \n\n\n\"JFK Jr.\" - 'Jr.' typically comes after its referent\n\n\n\"St. Michael's\" - when upper case 'St.' comes before its referent\n\n\n\"5th st.\" - when lower case 'st.' comes after its referent\n\n\nIs there any specific terminology to differentiate abbreviations with different placements in relation to their referents?",
"date": "2014-12-24"
},
{
"vote": 1,
"title": "Is it possible to use cortical.io to get the tone of some text?",
"text": "I've been messing with cortical.io and the idea of an SDR is really cool. I got a developer key and started pinging it with words but really all I got out of it was a thesaurus. I know it's more than that but I don't know how to harness it.\nSo, for example, as a test I want to scrape some subreddits and do analysis on the comments. Like, is the tone of the comments optimistic, rude, sad, oppressed, etc. Is anything like this possible? If so can you point me in the right direction.",
"date": "2014-12-20"
},
{
"vote": 2,
"title": "\"I created Rant, a language to make procedural text generation easier.\" - Rantjs - procedurally generated text with JavaScript",
"text": null,
"date": "2014-12-19"
},
{
"vote": 3,
"title": "Corpus request: Children's stories/Grade-school science textbooks",
"text": "I'm looking to train and test algorithms having to do with factual inference and story understanding, and I can't find a good corpus to work with. Trying to get facts from something like Wikipedia, even SimpleWikipedia, is a little out of my scope. I was hoping someone could point me to a corpus of children's stories (i.e. a series of simple events happening to a couple characters) or grade-school-level science textbooks (i.e. a series of simple facts and relationships about basic features of the world).\n\n\nAny ideas? Thanks!",
"date": "2014-12-18"
},
{
"vote": 6,
"title": "What expectations should I have as someone just starting out with Natural Language Processing?",
"text": "I'm new to Natural Language Processing and programming in general.  I think it's super interesting, especially automatic text summarization via abstraction, but am a little lost on how to go about teaching myself.\n\n\nSo far, I've looked at a few resources, and from what I can tell, they all look really useful.  These are the ones I'm considering devoting time to (although I'm open to suggestions).\n\n\n\n\nSpeech and Language Processing\n\n\nAn Introduction to Language Processing with Perl and Prolog\n\n\nNatural Language Processing With Python (the NLTK book)\n\n\n\n\nHere are my questions:\n\n\n\n\nWhat kind of (very) basic applications should I expect to be able to make after working through one of the above introductory texts?\n\n\nIf you were to recommend one of the resources above (or another altogether!), which would you and why?\n\n\nIs there anything you wish you would've known before entering this field that you do now?\n\n\n\n\nAnyway, I hope to hear back from some of you guys when you get a chance. :)  Live well, and dream deeply.",
"date": "2014-12-16"
},
{
"vote": 6,
"title": "Where to find a machine translation data set?",
"text": "I've been wanting to write a toy machine translation software for a while now. The problem I'm running into is that I'm unable to find a data set. Does anybody know where I can find one? I've tried using google and couldn't find much of anything.",
"date": "2014-12-10"
},
{
"vote": 4,
"title": "What are the best LDA variants for Opinion Mining?",
"text": "In paper [On the design of IDA models for aspect-based opinion mining] (CIKM'12), the authors introduced some LDA variants and discussed their performance. But those models are a bit simple. Also, some nice models like mgLDA were not mentioned.\n\n\nI am interested but new to Opinion Mining, so can you list some of the best LDA variants for aspect based opinion mining so far?\n\n\nThanks.",
"date": "2014-12-09"
},
{
"vote": 6,
"title": "I'm writing a new NLP library. What are your most commonly used/most needed NLP tasks?",
"text": "Title says it all.\n\n\nSome example tasks:\n\n\n\n\ntokenizing\n\n\nsentiment analysis\n\n\nPOS tagging\n\n\netc",
"date": "2014-12-03"
},
{
"vote": 1,
"title": "Il n'y a pas plus franÃ§ais que l'anglais",
"text": null,
"date": "2014-12-01"
},
{
"vote": 0,
"title": "Psych PhD student looking to transition into NLP/data science career : datascience",
"text": null,
"date": "2014-12-01"
},
{
"vote": 3,
"title": "Mining for more consistent rules than \"I before E except after C\"",
"text": null,
"date": "2014-11-29"
},
{
"vote": 3,
"title": "Undergraduate interested in doing speech recognition project.",
"text": "Hi, I am a 2nd year undergraduate CS major, and I am really interested in speech recognition stuff. I am planning to do a self project on speech recognition for a vocabulary detection system using arduino. I know CMUsphinx or bitvoicer is probably the way to go. But I would like to code the project from the ground up as the purpose of the project is learning. \n\n\nI have been reading different papers like \n\n\nhttp://revistaie.ase.ro/content/46/s%20-%20furtuna.pdf\n (dynamic time warping)\n\n\nhttp://citeseer.ist.psu.edu/viewdoc/download;jsessionid=DEC7F40E5AE89F1508A3B6D0625CC1DA?doi=10.1.1.131.2084&rep=rep1&type=pdf\n\n(HMM)\n\n\nBut I have encountered the following difficulties\n\n\n\n\nI have a hard time reconciling the entire process with higher level algorithm. From what I have read so far the entire process is the following\n\n\n i. input signal\n\n\n ii. filter all noise, getting all signals that is high energy volume and low frequency. (How do I know what to compare against? What is consider high energy volume, and low freqency?)\n\n\n iii. divide the signal into phones using FFT (where do I get dataset for phones, what constitutes a boundary between two phones, and how similar does the detected phones have to match the dataset?)\n\n\n iv. matching the list of phones to each word using HMM or DTW\n\n\n\n\n\n\nFor beginners like me I feel that steps 2-3 is the hardest part due to the difficulties I listed.\n\n\nCan someone point me a direction?\n\n\nThanks",
"date": "2014-11-28"
},
{
"vote": 1,
"title": "[Question] Sentiment analysis",
"text": "[deleted]",
"date": "2014-11-22"
},
{
"vote": 2,
"title": "K-best dependency parsing?",
"text": "I'm trying to get k-best output from a dependency parser. Malt and Redshift don't seem to do this, and I don't think that MST does (although Hall et al published a paper about k-best MST in 2007). Does anyone know of a dependency parser that can be trained, and that will give k-best output? Maybe I just missed something in the documentation of those 3...",
"date": "2014-11-20"
},
{
"vote": 5,
"title": "An apples-to-apples comparison of GloVe and word2vec",
"text": null,
"date": "2014-11-20"
},
{
"vote": 2,
"title": "Simplifying a Single Sentence",
"text": "I'm currently working a project which among other things uses NLP POS tagging to simplify a sentence into a text phrase which gives the gist of the idea.\n\n\nDoes there exist an algorithm using POS tagging or another mechanism to do this?\nWe're BTW using Python NLTK for much of the work.\n\n\nAny help would be much appreciated.\nTIA.",
"date": "2014-11-20"
},
{
"vote": 12,
"title": "â€œMachine-learning system that can automatically produce captions to describe imageâ€ â€“ â€œBy constructing a model that combines a vision Convolutional Neural Network (CNN) with a language-generating Recurrent Neural Network (RNN), system can take image and generate a fitting natural-language captionâ€",
"text": null,
"date": "2014-11-18"
},
{
"vote": 3,
"title": "Need pointers on using review text to make recommendations",
"text": "I'm new to NLP and am trying to build a recommendation system (for educational purposes) that uses numerical and text features from reviews. I'm thinking of making an item based model, and possibly clustering similar items together using the text from reviews (maybe using cosine or jaccard distance of the top TF-IDF words), and then learning the users preferences for these items based on the 1-5 score they give an item and that items features (TF-IDF words, maybe some numeric stuff?). Is there a better use of the text? Does sentiment of these popular words matter (like, a food being \"too spicy\" may be negative, but \"very spicy\" may not be) or should I just have it be a binary feature (the word is included or it isn't). \n\n\nI'm having trouble visualizing exactly how to do the recommendations though. I guess use the 1-5 (normalized) scores as a multiplier and then use this to weight the word scores for each vector? And then find clusters of items or items similar to that user? I also don't know whether this is at all a reasonable approach, so any feedback would be great.\n\n\nI'm wondering if anyone had any ideas, pointers, references, papers, books etc on intelligent ways to handle using text in recommendations, or anything else related. Thanks!\n\n\nEdit: Clarifying my question",
"date": "2014-11-17"
},
{
"vote": 3,
"title": "Detecting linguistic change across time using change point detection",
"text": null,
"date": "2014-11-17"
},
{
"vote": 2,
"title": "NPR: Wit AI â€“ free speech recognition platform",
"text": null,
"date": "2014-11-14"
},
{
"vote": 0,
"title": "[Help] Statistics Knowledge Background",
"text": "[deleted]",
"date": "2014-11-07"
},
{
"vote": 0,
"title": "Multilingual Desktop Publishing Services",
"text": null,
"date": "2014-10-28"
},
{
"vote": 0,
"title": "TextBlob for NLP - A Gentle Introduction",
"text": null,
"date": "2014-10-26"
},
{
"vote": 17,
"title": "I made a web app for building corpora from Twitter - TweetSet",
"text": null,
"date": "2014-10-26"
},
{
"vote": 1,
"title": "Our Experience Testing Our NLP While Being Featured On The Front Page of Hacker News",
"text": "[deleted]",
"date": "2014-10-26"
},
{
"vote": 3,
"title": "NLP newbie needs help classifying website text data for research",
"text": "Hello, I'm an MIT researcher, and machine learning methods are new to the kind of work that I do.  Here's what I'm looking to do -- I have scraped text data from a large sample of websites, essentially domain names like abc.com, pqr.org, rsv.net etc. I expect to have text data from many million such websites, and in the future I hope to expand this dataset to include data from the Common Crawl Project.\n\n\nMy task is to identify \"high growth\" potential websites from this data. I.e. i want to be able to identify potential startups in this sample and throw out personal websites, websites for SMEs, news websites, blogs etc. \n\n\nI have a few questions that I was hoping I could get some help on:\n\n\n\n\nAm i right in thinking that this is a standard classification ML problem? what algorithms should I be looking at? Will I be alright using something like python's scikit-learn?\n\n\n\n\nHow would this task change if I wanted to classify websites into more types, say by industry (travel, holiday, finance etc).\n\n\n\n\nDo you know existing papers, researchers who try to classify website text data? any references would be very helpful!",
"date": "2014-10-22"
},
{
"vote": 15,
"title": "Tweet NLP - a tokenizer, POS tagger, word clusterer, and dependency parser for tweets",
"text": null,
"date": "2014-10-22"
},
{
"vote": 6,
"title": "IBM released some public Watson APIs",
"text": null,
"date": "2014-10-18"
},
{
"vote": 7,
"title": "Please critique a joke-telling chat bot that I created for my school's hackathon",
"text": null,
"date": "2014-10-13"
},
{
"vote": 0,
"title": "Translator Services in India",
"text": null,
"date": "2014-10-11"
},
{
"vote": 9,
"title": "GloVe: Global Vectors for Word Representation, a new project from the Stanford NLP Group",
"text": null,
"date": "2014-10-10"
},
{
"vote": 0,
"title": "language translation service in india",
"text": null,
"date": "2014-10-10"
},
{
"vote": 0,
"title": "Russian Translation Services in India",
"text": null,
"date": "2014-10-08"
},
{
"vote": 0,
"title": "French translation services in india",
"text": null,
"date": "2014-10-06"
},
{
"vote": 0,
"title": "fm-interpretation-equipments for rent",
"text": null,
"date": "2014-10-04"
},
{
"vote": 10,
"title": "A review of (free) sparse sequence taggers (for NLP PoS tagging)",
"text": null,
"date": "2014-10-03"
},
{
"vote": 0,
"title": "infrared-interpretation-equipment for rent",
"text": null,
"date": "2014-10-02"
},
{
"vote": 3,
"title": "Cortical Engine for Processing Text - Introductory video explaining cortical.io's approach to NLP",
"text": null,
"date": "2014-09-30"
},
{
"vote": 4,
"title": "Reactive LDA Library",
"text": null,
"date": "2014-09-25"
},
{
"vote": 5,
"title": "Does anyone know of any good neurobiological models that are useful for NLP applications?",
"text": null,
"date": "2014-08-27"
},
{
"vote": 7,
"title": "Does a keyword search tool like this exist?",
"text": "I'm looking for a tool where I can enter list of keywords/phrases and see a list of the ngrams that are most likely to co-occur with those keywords in some corpus (ideally Wikipedia). I would like to use it to look up collections of words I'm interested in inferring something from like \"fatigue\", \"fever\", \"joint pain\", \"bleeding\", then see a list of co-occurring words with probabilities like:\n\n\n\n\nebola 2%\n\n\nheadache 1.6%\n\n\n...\n\n\n\n\nIdeally, it would also be possible to see only co-occurring words in certain categories, such as healthcare or diseases.\n\n\nI would want this tool to use some kind of probability model that allows it to extrapolate probabilities of co-occurring words when the set of keywords being queried is not entirely contained in any documents in the corpus.",
"date": "2014-08-27"
},
{
"vote": 16,
"title": "Teaching machines to read between the lines (and a new corpus with entity salience annotations) - \"Now, weâ€™re releasing a new dataset, based on another great resource: the New York Times Annotated Corpus, a set of 1.8 million articles spanning 20 years.\"",
"text": null,
"date": "2014-08-25"
},
{
"vote": 0,
"title": "The best foreign language blogs you should be reading:",
"text": null,
"date": "2014-08-22"
},
{
"vote": 0,
"title": "Setting New Bench Marks in the Field of Education with Translation Servicesâ€¦..!!!",
"text": null,
"date": "2014-08-19"
},
{
"vote": 7,
"title": "Enter Reddit username, get sentiment analyzed comments",
"text": null,
"date": "2014-08-17"
},
{
"vote": 8,
"title": "The OpeNER project: Natural Language Technology.",
"text": "OpeNER is a project funded by the European Commission under the FP7 (7th Framework Program). Its acronym stands for Open Polarity Enhanced Name Entity Recognition. It is a two year duration project which officially started at July 2012, and finishes at July 2014. In OpeNER are collaborating partners from Italy, Holland and Spain. \n\n\nOpeNERâ€™s main goal is to provide a set of ready to use tools to perform some natural language processing tasks, free and easy to adapt for SMEs to integrate them in their workflow. More precisely, OpeNER aims to be able to detect and disambiguate entity mentions and perform sentiment analysis and opinion detection on the texts, to be able for example, to extract the sentiment and the opinion of customers about certain resource (e.g. hotels and accommodations) in Web reviews.",
"date": "2014-08-10"
},
{
"vote": 3,
"title": "Need an idea for an AI and/or NLP project! (x-post from /r/learnprogramming)",
"text": "[deleted]",
"date": "2014-08-05"
},
{
"vote": 12,
"title": "Does anyone know of good repositories of trained models for word similarity?",
"text": "As the title says, I am looking for pre-trained models to play with. Anything is welcome: LSA models, RNN models, word2vec output, you name it! Here is a short list I compiled so far.\n\n\nLSA models:\n\n\n\n\nEnglish, Dutch, German\n LSA spaces for use with R. (I also wrote some scripts to use these LSA spaces in Python, using Rpy. I can post these as well if you're interested.)\n\n\n\n\nWord2Vec models:\n\n\n\n\nBiomedical\n\n\n\n\nNews\n\n\n\n\n\n\nRNNLM models:\n\n\n\n\non the RNNLM Toolkit site\n\n\n\n\nBullinaria & Levy (2012):\n\n\n\n\nVectors in MATLAB formatted binary files",
"date": "2014-08-04"
},
{
"vote": 12,
"title": "The OpeNER Project: free tools for NER, sentiment analysis and opinion mining",
"text": null,
"date": "2014-08-02"
},
{
"vote": 7,
"title": "Markov-Chain-Sentence-Generator - Uses Markov chains to generate real-sounding sentences based on the sentence patterns of a sample text",
"text": null,
"date": "2014-07-31"
},
{
"vote": 4,
"title": "Python or C++?",
"text": "Python has NLTK and other tools, and C++ is C++. As far as I know serious scientific institutions prefer C++ but science in my country sucks anyway.\n\n\nI feel more prone to working with Python now, but I am curious about your opinion. What are advantages or disadvantages of these languages?",
"date": "2014-07-25"
},
{
"vote": 3,
"title": "CHILDES for NLTK?",
"text": "I recently downloaded NLTK and the NLTK data package, but the CHILDES corpus is apparently no longer a part of the NLTK data download.  Does anyone know where I can get this?",
"date": "2014-07-24"
},
{
"vote": 7,
"title": "MITIE v0.2 Released: Now includes Python and C++ APIs for named entity recognition and binary relation extraction",
"text": null,
"date": "2014-07-10"
},
{
"vote": 2,
"title": "Cortical Learning Algorithm for Language Processing: cortical.io",
"text": null,
"date": "2014-07-10"
},
{
"vote": 7,
"title": "Machine Learning Tutorial: High Performance Text Processing",
"text": null,
"date": "2014-06-27"
},
{
"vote": 4,
"title": "How does the human brain process semantics? Are there still rules to semantics, or is it a free-for-all?",
"text": "The idea that a machine could imitate a human regarding natural language was first attempted in AI research in 1983 with the project called CYC.  The foundational epistemic commitment of CYC is that the semantics of an english sentence are found somewhere in the deductive consequences that flow from the first-order predicate of the sentence.  That is, all english sentences are converted into an (allegedly) equivalent formula in a predicate calculus.  This never particularly specifies where semantics is located, because it only says that meaning is found \n\"somewhere, I dunno where\"\n in the endless tangle of deductions that flow from a premise.  This method also presumes that an actual word appearing in a sentence names a member of a category.  That this might be false is never questioned by CYC's methodology nor by the methodology of all CYC-derivatives in the 3 decades after CYC's failure.   \n\n\nI posed a question to reddit asking if there are any other alternatives to semantics other than this methodology.  The answers were spotty, and only pointed me towards larger and larger databases, or databases that are automatically generated through deep learning, and so on.  (the 'databases'  were basically tagged words, tagged phrases, tagged n-grams, and perhaps some relations among them).  But there was nothing regarding challenges to the foundational structure of CYC's semantics.  To first approximation, the answer is no.  Ai research has no other alternatives to semantics other than converting english sentences into predicate logic. \n\n\nA burning question still remains. \n\n\nHow does the human brain realize semantics?    \n\n\nThat question is not anathema to Language Technology or NLP in Ai.  Here are the reasons:\n\n\n\n\n31 years later, CYC still cannot perform \nanaphora resolution\n natively. Researchers are scrambling to try to add this into the system from the outside. \n\n\n\n\n31 years later, and no progress has been made on \nCloze deletion tests.\n The only progress on this is machine learning that tries to compile n-gram statistics from large corpora.  This is obviously not how the human brain performs this. \n\n\n\n\n31 years later and CYC still cannot generate \nmetonomy\n natively.  Human beings show the capacity to create metonomic references on-the-fly in a context , and understand them on-the-fly in a context.  CYC methodology only approaches metonomy by hand-coding specific rules for all the instances in which metonomy is used in english conversation. \n\n\n\n\nPredicate logic and deductive calculus are known not to be able to perform meta-lingual references.  (e.g. reasoning about the behavior of recursive functions).  This happens to be precisely where CYC fails to understand pronouns. Pronouns which appear at the very beginning of a sentence, such as \"THAT\" and \"THIS\",  often reference the exact utterance that appears previously in a text.  The \nutterance itself\n is the resolved referent. This  problem should not have surprised any of the researchers, who feverishly began to try to \"code around it\" because it did not fit their methodology quite right.\n\n\n\n\n\n\nFrom several interactions, it is obvious that there are a handful of hold-outs in Ai research who still believe in their hearts that CYC's failure is not endemic to its method. Rather these \"true believers\"  think CYC failed because the sheer number of language rules in its database was too small.   They still think that when the rule set becomes large enough, CYC will suddenly succeed in doing all the things it has failed to do in 3 decades. \n\n\nI think we do have room, and should have room to challenge this old paradigm. \n\n\n\n\nWe challenge it by asking how the human brain itself performs semantics,  along with its poverty-of-stimulus.  Grade school children can perform Cloze Deletion tests pretty well, without having \"deep learned\" 522 english novels. \n\n\n\n\nWe challenge it by asking:  Does a word in a sentence really name a member of a category, or does the presence of a word in fact name a category?   \n\n\n\n\nAre their rules in the brain for semantics related to rich categories,  or to the richness of prototype categories?\n\n\n\n\n\n\nIn my opinion, these are all valid questions to be asked by neuroscientists, cognitive scientists, and by Ai researchers as well.  I am open to listen to defenses of converting all english sentences into predicate calculus formulas, if such a defense exists.  I know of no defense other than \"computers can read it in that format\". That answer is purely pragmatic, as far as I am concerned. It has no 'theoretical force', as it were.",
"date": "2014-06-14"
},
{
"vote": 0,
"title": "Are there any other wild alternatives to semantics, other than conversion to FOPL?",
"text": "Are there any other wild alternatives to semantics, other than conversion to First-order Predicate Logic?    To understand what I am asking, read the description below.   \n\n\nSOME BACKGROUND\n\n\n\n\nBeginning around 1983, Ai research began to handle the problem of semantics in natural language. This project eventually came to be known as CYC.  This research kicked off an established paradigm of representing the meaning of english sentences as a corresponding formula written in first-order Predicate logic  ('FOPL'). \n\n\n\n\nEven today, with modern PLNs, and weighted Markov networks, the foundational paradigm is still converting all english sentences into a corresponding formula in FOPL. \n\n\n\n\nConsider chapter 12 within \nArtificial Intelligence: A Modern Approach 2nd ed. (2010)\n  Here the author (Norvig) simply presumes that meaning of any english sentence should be appropriately captured by converting it into FOPL.  \n\n\n\n\nNorvig repeats this paradigm again in chapter 22 and 23 of the same text. \n\n\n\n\nIn section 12.5.1  Norvig makes the audacious claim that semantic networks are themselves a form of logical network,  further reinforcing the  semantics-as-FOPL paradigm.\n\n\n\n\n\n\nTHE ALTERNATIVES\n \n\n\n\n\nConsider the wild idea that the rules governing semantics in natural language is not the same as the rules that govern predicate logic. \n\n\n\n\nSemantics-as-FOPL  makes a presumption at its core. Namely, it presumes that the words appearing in a sentence are instantiations of a particular member of a category.  This may be completely false in the way humans understand language.  Yes, I am claiming that particular words in a sentence are not naming the member of a category.\n\n\n\n\nOn Imagining alternatives.  We want to imagine that english utterances create categories spontaneously, that did not previously exist prior to the speaking, and reception of the sentence.  The meaning of  a sentence in english becomes the place where prototype categories overlap, likened unto venn diagrams.  The number of dimensions in the category diagram is equal to the number of parts-of-speech which appear in the sentence. \n\n\n\n\nAdult language is very sophisticated, and it contains subordinate phrases.  The function of subordinate phrases is the creation of categories in mid-sentence. \n\n\n\n\nThe semantics of the sentence \nThe boy kicked the ball.\n  is actually  a singular object with its own categorical properties. Namely the semantics is \na boy kicking a ball.\n.  The verb is tensed to the past only because that is the method of locating the sentence in a temporal narrative. That is,  natural language has used the social method of tensing verbs to locate it in a narrative, but the categorical aspects are orthogonal to this 'verbal' tool.\n\n\n\n\nImagine that semantics is not the denotation of truth value, but is instead the process of spontaneous category creation in realtime.  That sounds spooky and crackpotterish at first. But to see why this is probably true, perform this exercise by yourself with a magazine.  Find a noun from one article in the magazine.  Find a verb from another article, and find a second noun in a third article.  Combine these three words, so that you get a sentence of the form \nX verbs Y\n.  You will find, as I did, suprisingly:  that in 95% or more of the cases, the sentence that you form will make perfect semantic sense.  That is, it is extremely difficult to formulate a random sentence that has perfect grammar and absolutely no meaning.  (the infamous \"Green ideas sleep furiously.\")  \n\n\n\n\nFrom  a magazine on finance and economics, I formed the sentence \nThe thunder called upon the ninja.\n  This was totally unexpected from a magazine of this type, but the strangest thing is that the sentence makes perfect semantic sense. \n\n\n\n\nThe existence of (rare)  sentences which are grammatically perfect, but whom do not make any sense is a strong indication that the way human brains understand semantics is not by the rules of FOPL.  There is some other process going on , of which the academic Ai community is not yet aware.\n\n\n\n\n\n\nSo we come full circle and back to my original question:\n\n\nAre there any other wild alternatives to semantics, other than conversion to First-order Predicate Logic?",
"date": "2014-06-13"
},
{
"vote": 2,
"title": "How to make a spell checker for Firefox?",
"text": "I want to make a spell checker, I knew all steps but only one thing I didn't understand.. what I should put in .aff file? what's the suffix and prefix and how I use them? does it make the work faster? \n\n\nI downloaded a book about Huspell but I understood nothing, so please explain that to me.. \n\n\nfor example what's that\n>SFX  N  Ã¥  ogs  slÃ¥\n\n\nwhy they put \nslÃ¥\n in the end of the line? \nplease if you know another place where I can get the answer of my question tell me.",
"date": "2014-06-13"
},
{
"vote": 6,
"title": "Measuring the Complexity of the Law",
"text": null,
"date": "2014-06-03"
},
{
"vote": 5,
"title": "ClearTK 2.0 released",
"text": null,
"date": "2014-05-29"
},
{
"vote": 5,
"title": "Is legal langauge machine readable yet?",
"text": "Is legal langauge machine readable yet?",
"date": "2014-05-26"
},
{
"vote": 1,
"title": "Using NLTK, I made a Python program that writes (mostly) rhyming sonnets in (mostly) iambic pentameter from any text corpus",
"text": "[deleted]",
"date": "2014-05-10"
},
{
"vote": 6,
"title": "Segmenting a Text Document using the Idea of a Cellular Automata",
"text": null,
"date": "2014-05-07"
},
{
"vote": 3,
"title": "Online course - intro to CAT tools",
"text": null,
"date": "2014-04-29"
},
{
"vote": 7,
"title": "Parsing English with 500 lines of Python",
"text": null,
"date": "2014-04-28"
},
{
"vote": 3,
"title": "Launching MonkeyLearn private alpha at PyCon 2014",
"text": "Hi Reddit! We are launching \nMonkeyLearn\n private alpha at \nPyCon 2014\n in MontrÃ©al. \n\n\n@RaÃºl\n is there presenting MonkeyLearn and giving out some invites, so if you are in MontrÃ©al don't miss the chance and ask him for an invite.\n\n\nOur main goal with MonkeyLearn is to \nmake text mining simple\n. With MonkeyLearn you can easily create your own text mining modules or grab already created ones. You'll have access to a web panel to upload your text data to train, test and improve a Machine Learning model. An API will be instantly published to integrate with your project within minutes.\n\n\nYou can also request your invite at \nhttp://www.monkeylearn.com\n\n\nProtip: don't wait too much to request your MonkeyLearn invite because we only have a limited amount of public invites.",
"date": "2014-04-14"
},
{
"vote": 7,
"title": "Can't Read, Won't Buy: How Translation Affects Global E-Commerce",
"text": null,
"date": "2014-03-28"
},
{
"vote": 5,
"title": "Google Translate Sings: \"Part of Your World\" from The Little Mermaid",
"text": null,
"date": "2014-03-19"
},
{
"vote": 7,
"title": "Real-time NLP with Twitter and Yhat",
"text": null,
"date": "2014-03-14"
},
{
"vote": 9,
"title": "Stanford scientists put free text-analysis tool on the web",
"text": null,
"date": "2014-02-07"
},
{
"vote": 1,
"title": "Spanish Translation services Company",
"text": null,
"date": "2014-01-24"
},
{
"vote": 14,
"title": "Deep Learning resources for NLP?",
"text": "I'm interested in using Deep Learning for NLP, but I can't seem to find any useful resources. I found some tutorials by the 'Deep Learning musketeers', but they talk only about the concepts involved and have no mention of how to actually go about implementing them. Does someone have resources which will actually help in implementing these concepts?",
"date": "2014-01-15"
},
{
"vote": 6,
"title": "Fuzzy string matching; any good alternatives to/variations on Levenshtein distance?",
"text": "I like Levenshtein distance, I really do, but I'm frequently encountering situations that make it less useful.  For example, if I want to find the string that most closely matches \"Polytechnic Institute\" from a translated text, \"Political Institute\" has a distance of 7 whereas \"Institute Polytechnic\" has a much higher one (and Institute Polytechnic would be a much better match for my purposes).",
"date": "2014-01-13"
},
{
"vote": 10,
"title": "Recommended free introductory resources to NLP?",
"text": "[deleted]",
"date": "2013-12-15"
},
{
"vote": 5,
"title": "Newbie question - Pointers on something similar to sentiment anlysis",
"text": "I've just started reading up about NLP and am trying out a few examples via NLTK. \n\n\nMy aim is to be able to state a few conditions for a piece of text (not longer than 30 words) and return a number (between 0 and 1) as to whether these conditions are met; similar to sentiment analysis but a bit more specific.\n\n\nFor example if I was looking for text containing anything pertaining to the \"inflation risks to the upside in south africa\", what would be the best way to approach this using tags / tokens etc.\n\n\nAny help at pointing me in the right direction would be greatly appreciated",
"date": "2013-11-26"
},
{
"vote": 10,
"title": "Newb Q: Where (online) can I learn about statistical language models?",
"text": "I want to learn how to generate and train language models (for text analysis), but I have no idea where to begin. So if anyone can point me to a good general purpose theoretical overview with some practical examples, I would be very appreciative (especially if it is written in Python).\n\n\nAlso, if I can ask a few more specific questions:\n\n\n\n\nIs it possible to have a model which will look at a single unit (e.g. word, phrase) in the context and analyze if that unit fits the model?\n\n\n\n\nIs it possible to have a model which not only looks at word frequency but takes into account say, part of speech, or semantic tags?\n\n\n\n\n\n\nThank you very much for your help.",
"date": "2013-11-24"
},
{
"vote": 4,
"title": "What techniques/services can I employ to summarize a body of text up to N characters?",
"text": "I have a body of text, and I'm looking to summarize a body of text up to a maximum of N characters.\n\n\nIn other words, a function that does summarize(body_text, n) where n is the upper limit of the summary text.\n\n\nThanks in advance!",
"date": "2013-11-06"
},
{
"vote": 6,
"title": "Newb Question: IR(lucene) On stemming on index and query time.",
"text": "Hi guys/gals.\n\n\nI was curious how I should go about stemming? Should I stem during indexing and query? Which would make sense. \n\n\nBut if I stem during indexing (in lucene) and then during query can I just rely on fuzzy, since I'm already planning to use it?\n\n\nOr is this more of a you gotta experiment with your data?\n\n\nThanks guys/gals!",
"date": "2013-11-01"
},
{
"vote": 1,
"title": "The Paradigm Shift Happening in Natural Language Understanding",
"text": null,
"date": "2013-10-08"
},
{
"vote": 1,
"title": "Do speech recognition programs understand you? If so, you might be a sheep",
"text": null,
"date": "2013-10-02"
},
{
"vote": 0,
"title": "107 Regional Slang Words - mental_floss on YouTube (Ep. 25)",
"text": null,
"date": "2013-09-06"
},
{
"vote": 1,
"title": "FOSS lexicon proofing tools for a small group of non-technical people?",
"text": "[removed]",
"date": "2013-08-12"
},
{
"vote": 2,
"title": "Can Dogs Speak?",
"text": null,
"date": "2013-07-17"
},
{
"vote": 2,
"title": "tool for predicting the etymology of an English word.",
"text": "Hello, \n\n\nI admit I haven't worked as hard as I should have to find this, but I was wondering if anyone knew of a tool or algorithm that predicts whether an word is Latin or Greek or Germanic or whatever in origin? I think this might be a useful feature in a pet project I'm writing but I don't know how useful it would be.",
"date": "2013-07-15"
},
{
"vote": 6,
"title": "Writing programs using ordinary language",
"text": null,
"date": "2013-07-11"
},
{
"vote": 3,
"title": "[Question] Where should I post questions about natural language processing -- specifically questions about improving my text classification processes?",
"text": "[deleted]",
"date": "2013-07-10"
},
{
"vote": 7,
"title": "The weirdest languages [x-post from /r/truerreddit]",
"text": null,
"date": "2013-07-02"
},
{
"vote": 1,
"title": "Is it possible to set something up like an itunes for pdf without getting your hands dirty?",
"text": "Is there anything which uses nlp to brows pdf's? i really would like to be able to analyze let's say 1000 research papers and apply different nlp-techniques on them. Indexing, Calculating the distance between paper, calculating references etc. People have done this already but is there anything i can use to \"browse\" my pdf's?  etc.?",
"date": "2013-06-01"
},
{
"vote": 3,
"title": "Brainstorm: Algo to make your writing sound like someone else's? Fool authorship-detectors, retain privacy.",
"text": "Learned a ton last night talking to a language hacker for many hours -- he's researching how to establish authorship on politically-significant anonymous writing, using n-gram maximum entropy models. It is now my goal to write an algo which can fool his (and a human) -- aka a way to make my writing sound like yours.\n\n\nThe ethics of such a hack is double-edged: 1) it re-establishes anonymity, which is good for privacy 2) makes plagiarism way easier and stealthier. Copy your essay from wikipedia and have it sound like you wrote it. O_O! 3) the creative potential is through the roof. What happens when you mashup your professor's lectures with Dr. Seuss? What does your blog sound like in the voice of Chuck Palahniuk? Imagine college freshmen reading history text books through the lens of Langston Hughes or Saul Williams. \n\n\nThere are three problems here. \n\n\n\n\nWhat salient features of language make style unique? \n\n\n\n\nHow do we detect them? \n\n\n\n\nHow do we remix/generate language to have these features?\n\n\n\n\n\n\n1 & 2: n-gram models (the probability that particular words and groups of words are used) have worked well enough in authorship detectors, but they don't highlight the nuances of emphasis and style needed for linguistic remixology.  With enough human-tagged examples of features, machine learning can find patterns where humans struggle. \"If we could think in higher dimensions, we wouldn't need machine learning.\"\n\n\n3: Either we develop a deep understanding of the epigenetic activity in the bone tissue of the skeletons underneath the skin of language, or we go the way of brute force: we make an algo which can generate every possible way say pretty much the same thing, all possible ways to say the same things, all the ways a thing can be said, and a list of different ways to say a similar thing, from which we trim down to the best candidates. Trimming down is easy if we already have 1 & 2 solved.\n\n\nAn anonymizer is considerably easier -- it just has to fool current models of authorship detection, which aren't very robust to begin with. It could simply follow generic style guide.",
"date": "2013-05-06"
},
{
"vote": 3,
"title": "A Trio of Information Density Papers",
"text": "Information Density is a really interesting concept. (Average entropy per word sequence). I thought that the community might enjoy these.\n\n\n1) Word lengths are optimized for\nefï¬cient communication\n\n\n2) Redundancy and reduction: Speakers manage syntactic information density\n\n\n[3) Why are some word orders more common than\nothers? A uniform information density account] (\nhttp://books.nips.cc/papers/files/nips23/NIPS2010_0369.pdf\n)",
"date": "2013-04-24"
},
{
"vote": 0,
"title": "Anyone using Provalis's wordstat and qda miner ?",
"text": "I'm MA student and currently working corpus linguistics for second language acquisition researches. At first I tried wordsmith and now trying to learn wordstat. In fact provalis is quite good for frequency based researches. But I wonder what else I can do with that program. I need some ideas I can work on so that I can learn software's other features.",
"date": "2013-04-21"
},
{
"vote": 0,
"title": "Matching \"help wanted\" against \"for hire\". Needs your help to improve.",
"text": null,
"date": "2013-04-03"
},
{
"vote": 2,
"title": "Introduction To Chatbot",
"text": null,
"date": "2013-04-01"
},
{
"vote": 1,
"title": "When Language Switching matters",
"text": null,
"date": "2013-03-26"
},
{
"vote": 0,
"title": "What is the language of computer?",
"text": null,
"date": "2013-03-16"
},
{
"vote": 3,
"title": "Question[xpost from linguistics and compiling]: does anyone know if there is a set of parts-of-speech rules that I can use (in software or code form] to apply to a text or set of texts? More info in text.",
"text": "First, sorry to double x-post, I just discovered this subreddit, and you're the most-relevant one yet. Is there an online system or software or combination of programs that I can use to automatically assign rules to parts-of-speech, for example to be able to 'programmatically' assign words like \"run\" as a noun or verb based on the words before/after, or for example a word preceded by a personal pronoun is always a verb preceded by a possessive pronoun is always a noun.  The intent is to be able to decide word parts of speech so that \ncontext-sensitive\n definitions can be added to them.\n\n\nIf this doesn't exist does anyone know what would be the best way to go about doing this? I'm not a programmer myself but have access to some, so feel free to get as technical as possible. Thanks!\n\n\n-From the other thread, I have been given NTLK, OpenNLP, and Ellogon as possible solutions, and want to ask if there are other solutions or kinds of solutions.",
"date": "2013-02-22"
},
{
"vote": 9,
"title": "Really the most common English idioms? Let's check using corpus linguistics",
"text": null,
"date": "2013-02-11"
},
{
"vote": 0,
"title": "SDL to Supply Multiple European Union Institutions with Computer-Aided Translation Solutions",
"text": "[deleted]",
"date": "2013-01-29"
},
{
"vote": 3,
"title": "Is it possible to find words from one language that have sounds present in words in another language?",
"text": "For example cross referencing words from two languages using IPA.",
"date": "2013-01-26"
},
{
"vote": 2,
"title": "Modality As A Semantic Category",
"text": null,
"date": "2013-01-20"
},
{
"vote": 3,
"title": "Researchers reveal carders, hackers on underground forums.",
"text": null,
"date": "2013-01-09"
},
{
"vote": 0,
"title": "What is language?",
"text": null,
"date": "2013-01-03"
},
{
"vote": 0,
"title": "First Language Acquisition",
"text": null,
"date": "2012-12-13"
},
{
"vote": 9,
"title": "New to NLP!",
"text": "Hi guys, I'm currently studying Translation but I'm also really interested in Linguistics (took Generative Grammar and Linguistics courses so far and Discourse Analysis is next). I would like to know where could I find useful resources for a noob like me :3 I should warn you that I still have to learn in depth programming, and brush up my rusty calculus knowledge.\nAnyway, do you know of any good e-books or .pdfs or any books I can read from? Now I got an iPad and I'd like to give it good use.\nThanks in advance!",
"date": "2012-12-12"
},
{
"vote": 0,
"title": "Lexical Functional Grammar",
"text": null,
"date": "2012-12-09"
},
{
"vote": 0,
"title": "Introduction To Tamil Language",
"text": null,
"date": "2012-12-01"
},
{
"vote": 2,
"title": "Clustering Jeopardy! questions together with Python",
"text": "[deleted]",
"date": "2012-11-12"
},
{
"vote": 1,
"title": "Your TRUE Translation Partner",
"text": null,
"date": "2012-10-05"
},
{
"vote": 3,
"title": "Lining up A known text with audio",
"text": null,
"date": "2012-10-03"
},
{
"vote": 4,
"title": "Dabbling with building my own grammar using the Python nltk.  It's getting rather complex, and I'm finding that the nltk's parsers are starting to run slowly.  Can I optimize this somehow?",
"text": "Essentially, I'm wondering what sort of things tend to slow down nltk's parsers when they use a particular grammar so that I can try to rework said grammar and make it run more quickly.  Are there any Python nltk whizzes here?",
"date": "2012-09-25"
},
{
"vote": 1,
"title": "Makna - Jasa Penerjemah Handal",
"text": null,
"date": "2012-09-23"
},
{
"vote": 1,
"title": "Where can I find a setup for typing with a SATTS keyboard.",
"text": "I need a way to type Arabic with an English keyboard (i.e. SATTS: Standard Arabic Technical Transliteration System) and I figured this was a good place to ask. On \nsome\n computers in the language settings I find a keyboard entitled Iraqi (Custom) that is basically perfect, but not on all computers. Mostly I just find variants of Arabic keyboards that I can't use (well) without a labeled keyboard. Can anyone tell me where to look or why, if at all, I should just use an Arabic one?",
"date": "2012-08-28"
},
{
"vote": 1,
"title": "What are some good resources for doing clause extraction?",
"text": "Books, favorite websites/blogs, etc.  I'm going to be starting work on writing something to separate open-ended text into clauses, and I'm still formulating a gameplan.  Any advice is most appreciated!",
"date": "2012-08-06"
},
{
"vote": 4,
"title": "SemGraph library for reading and visualising dependency graphs with Java",
"text": null,
"date": "2012-06-16"
},
{
"vote": 1,
"title": "SemGraph - library for reading and visualising dependency graphs",
"text": "[removed]",
"date": "2012-06-15"
},
{
"vote": 3,
"title": "How to answer a question: a simple system",
"text": null,
"date": "2012-06-14"
},
{
"vote": 6,
"title": "Is it possible to algorithmically rank words based on \"spellability\"?",
"text": "A word is highly \"spellable\" if a high percentage of people would spell it correctly if asked.\n\n\nFor example, \"cup\" is more spellable than \"buoy\".\n\n\nI'm interested to know if this is possible and if so how it could be accomplished? References to work on this problem are appreciated.",
"date": "2012-05-01"
},
{
"vote": 1,
"title": "If you own or know Jurafsky and Martin's book on Speech and Language Processing",
"text": "[removed]",
"date": "2012-04-21"
},
{
"vote": 2,
"title": "NLP Challenge: Find semantically related terms over a large vocabulary (&gt;1M)",
"text": null,
"date": "2012-03-11"
},
{
"vote": 1,
"title": "Looking for a comparison of probabilistic language models",
"text": "[removed]",
"date": "2012-02-14"
},
{
"vote": 6,
"title": "Word Association Study - Crowdsourcing Word Associations",
"text": null,
"date": "2012-02-11"
},
{
"vote": 3,
"title": "Semantic Link: automatically find related words",
"text": null,
"date": "2012-01-16"
},
{
"vote": 3,
"title": "What is the most accurate method in paraphrase generation?",
"text": "I've gone through most of the literature and it seems \nQuirk, Brocket, and Dolan (2004)\n are still on top in terms of human-judged correctness (although there is not so much variety in paraphrase content).\n\n\nIs anyone familiar with a method that trumps this one in terms of reliability?",
"date": "2011-12-24"
},
{
"vote": 1,
"title": "NLP/CL Proposal on StackExchange",
"text": null,
"date": "2011-12-07"
},
{
"vote": 1,
"title": "Looking for good paraphrase corpus (not Microsoft's)",
"text": "[removed]",
"date": "2011-12-06"
},
{
"vote": 5,
"title": "AutoCorpus - natural language corpora from large public datasets",
"text": null,
"date": "2011-11-28"
},
{
"vote": 1,
"title": "Handling metaphors",
"text": "I am working on a project in which i need to detect metaphors in text documents and replace them with their meanings. It would be great if someone can help me out with this.",
"date": "2011-11-20"
},
{
"vote": 2,
"title": "Tagging personal names - what are the available methods?",
"text": "I am new to natural language processing, so this is in a sense a question driven by my own ignorance (aren't they all). What are the available methods for extracting/tagging personal names in a text? I have found a few papers, but perhaps this is a solved problem and there are clearly superior methods to do this?",
"date": "2011-11-02"
},
{
"vote": 1,
"title": "Universal Grammar",
"text": null,
"date": "2010-10-19"
},
{
"vote": 1,
"title": "Interlingual Machine Translation",
"text": null,
"date": "2010-10-07"
},
{
"vote": 1,
"title": "Parts-Of-Speech Tagging",
"text": null,
"date": "2010-03-13"
},
{
"vote": 1,
"title": "Maximum Entropy",
"text": null,
"date": "2010-03-13"
},
{
"vote": 1,
"title": "Tokenization",
"text": null,
"date": "2010-03-10"
},
{
"vote": 1,
"title": "Example based machine translation",
"text": null,
"date": "2010-03-10"
}
]