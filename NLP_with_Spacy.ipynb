{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Spacy\n",
    "\n",
    "- installation : https://spacy.io/usage\n",
    "\n",
    "~~~\n",
    "$pip install -U spacy\n",
    "$python -m spacy download en_core_web_sm\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## load English model and instantiate\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization and lemmatization\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I got no car and it's breaking my heart. But I've found a driver and that's a start\"\n",
    "\n",
    "## process text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "got\n",
      "no\n",
      "car\n",
      "and\n",
      "it\n",
      "'s\n",
      "breaking\n",
      "my\n",
      "heart\n",
      ".\n",
      "But\n",
      "I\n",
      "'ve\n",
      "found\n",
      "a\n",
      "driver\n",
      "and\n",
      "that\n",
      "'s\n",
      "a\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "## doc is iterable. it gives token\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tI\n",
      "got\tget\n",
      "no\tno\n",
      "car\tcar\n",
      "and\tand\n",
      "it\tit\n",
      "'s\tbe\n",
      "breaking\tbreak\n",
      "my\tmy\n",
      "heart\theart\n",
      ".\t.\n",
      "But\tbut\n",
      "I\tI\n",
      "'ve\t've\n",
      "found\tfind\n",
      "a\ta\n",
      "driver\tdriver\n",
      "and\tand\n",
      "that\tthat\n",
      "'s\tbe\n",
      "a\ta\n",
      "start\tstart\n"
     ]
    }
   ],
   "source": [
    "## token.lemma_ gives lemmatized form\n",
    "for token in doc:\n",
    "    print(token, token.lemma_, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'get',\n",
       " 'no',\n",
       " 'car',\n",
       " 'and',\n",
       " 'it',\n",
       " 'be',\n",
       " 'break',\n",
       " 'my',\n",
       " 'heart',\n",
       " '.',\n",
       " 'but',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'find',\n",
       " 'a',\n",
       " 'driver',\n",
       " 'and',\n",
       " 'that',\n",
       " 'be',\n",
       " 'a',\n",
       " 'start']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make token list\n",
    "tokens = [token.lemma_ for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tPRON\n",
      "got\tVERB\n",
      "no\tDET\n",
      "car\tNOUN\n",
      "and\tCCONJ\n",
      "it\tPRON\n",
      "'s\tAUX\n",
      "breaking\tVERB\n",
      "my\tPRON\n",
      "heart\tNOUN\n",
      ".\tPUNCT\n",
      "But\tCCONJ\n",
      "I\tPRON\n",
      "'ve\tAUX\n",
      "found\tVERB\n",
      "a\tDET\n",
      "driver\tNOUN\n",
      "and\tCCONJ\n",
      "that\tPRON\n",
      "'s\tAUX\n",
      "a\tDET\n",
      "start\tNOUN\n"
     ]
    }
   ],
   "source": [
    "## token.pos_ gives lemmatized form\n",
    "for token in doc:\n",
    "    print(token, token.pos_, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tTrue\n",
      "got\tFalse\n",
      "no\tTrue\n",
      "car\tFalse\n",
      "and\tTrue\n",
      "it\tTrue\n",
      "'s\tTrue\n",
      "breaking\tFalse\n",
      "my\tTrue\n",
      "heart\tFalse\n",
      ".\tFalse\n",
      "But\tTrue\n",
      "I\tTrue\n",
      "'ve\tTrue\n",
      "found\tFalse\n",
      "a\tTrue\n",
      "driver\tFalse\n",
      "and\tTrue\n",
      "that\tTrue\n",
      "'s\tTrue\n",
      "a\tTrue\n",
      "start\tFalse\n"
     ]
    }
   ],
   "source": [
    "## token.is_stop is boolean \n",
    "for token in doc:\n",
    "    print(token, token.is_stop, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'car', 'break', 'heart', '.', 'find', 'driver', 'start']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make lemmatized token list without stopwords\n",
    "tokens = [token.lemma_ for token in doc if token.is_stop == False]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
